[{"categories":["golang","八股文","设计模式"],"contents":" 桥接模式 一个类存在多个独立变化维度，我们通过组合的方式让多个维度可以独立进行扩展。桥接模式的目的是将抽象部分与实现部分解耦，使它们可以独立地变化。\n举例：实现一个告警系统：告警系统含有多个告警类别和多种告警方式，告警类别和告警方式之间可以任意对应使用，方便灵活调整。也就是说，告警方式和告警类别可以独立变化，因为这两个没有依赖关系。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 // @author cold bin // @date 2023/9/2 package bridge import \u0026#34;fmt\u0026#34; // AlertMethod 告警方式的接口 type AlertMethod interface { SendAlert(message string) } // 具体的告警方式 type EmailAlert struct{} func (e *EmailAlert) SendAlert(message string) { fmt.Println(\u0026#34;通过邮件发送告警：\u0026#34;, message) } type SMSAlert struct{} func (s *SMSAlert) SendAlert(message string) { fmt.Println(\u0026#34;通过短信发送告警：\u0026#34;, message) } // AlertLevel 告警级别的接口 type AlertLevel interface { SetAlertMethod(method AlertMethod) Alert(message string) } // 具体的告警级别 type WarningAlert struct { method AlertMethod } func (w *WarningAlert) SetAlertMethod(method AlertMethod) { w.method = method } func (w *WarningAlert) Alert(message string) { w.method.SendAlert(\u0026#34;[Warning] \u0026#34; + message) } type ErrorAlert struct { method AlertMethod } func (e *ErrorAlert) SetAlertMethod(method AlertMethod) { e.method = method } func (e *ErrorAlert) Alert(message string) { e.method.SendAlert(\u0026#34;[Error] \u0026#34; + message) } ","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go设计模式之桥接模式"},{"categories":["golang","八股文","设计模式"],"contents":" 代理模式 单例模式、工厂模式、建造者模式、原型模式、函数选项模式都是属于创建型模式，指导如何创建对象。\n而结构型模式主要指导如何将对象或类组合在一起，有代理模式、桥接模式、装饰器模式、门面模式、组合模式、享元模式。\n什么是代理模式呢？顾名思义，在不改变原始类（被代理类）代码的情况下，通过引入代理类给原始类附加功能。\n应用场景 RPC\n缓存\n监控\n鉴权\n限流\n事务\n\u0026hellip;\n这里给出一个文件上传代理模式框架实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 // @author cold bin // @date 2023/9/1 package proxy import ( \u0026#34;context\u0026#34; ) // Uploader 上传文件的抽象 type Uploader interface { UploadSingle(ctx context.Context, key string, value []byte) UploadMultiple(ctx context.Context, keys []string, values [][]byte) } // UploaderProxy 上传的代理者 // //\t将所有实现都进行代理，我们可以在Up执行前或后做一些事情，例如实现一些hook type UploaderProxy struct { Up Uploader } func (u *UploaderProxy) UploadSingle(ctx context.Context, key string, value []byte) { // 这里可以做一些事情： // 1、校验文件名是否正确 // 2、记录图床开始上传时间 // 3、统计上传频率，做限流等 u.Up.UploadSingle(ctx, key, value) // 这里也可以做一些事情： // 1、hook结果 // 2、记录图床结束上传时间 // 3、记录图床上传时间 } func (u *UploaderProxy) UploadMultiple(ctx context.Context, keys []string, values [][]byte) { u.Up.UploadMultiple(ctx, keys, values) } type QiniuOss struct { // 注入七牛的依赖 } func (q *QiniuOss) UploadSingle(ctx context.Context, key string, value []byte) { } func (q *QiniuOss) UploadMultiple(ctx context.Context, keys []string, values [][]byte) { } type AliOss struct { // 注入阿里云oss依赖 } func (a *AliOss) UploadSingle(ctx context.Context, key string, value []byte) { } func (a *AliOss) UploadMultiple(ctx context.Context, keys []string, values [][]byte) { } 总结 看的出来，代理模式的实现是在“基于接口而非实现”的抽象之上的\n可以说，代理模式也实现了基于接口而非实现的设计原则，并且在此基础上，通过实现代理类，来给原有业务实现添加一些功能。添加的功能作用于局限于原业务执行前后，但是不能作用于原业务中。\n其实，作用于原业务中就不使用代理模式了，只能重构原业务代码实现。\n代理模式指的是：通过引入原始类的方式来给原始类增加功能。我们也可以引入原始类实现的接口，这样可以对所有的实现进行统一代理\n","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go设计模式之代理模式"},{"categories":["golang","八股文","设计模式"],"contents":" 原型模式 如果对象创建成本比较大（有些字段赋值可能需要rpc、网络、磁盘读取等），而且同一个类的对象差异不大（大部分字段都相同）。在这种情况下，可以从已有对象拷贝出新的对象使用，可以减少对象创建时的成本。\n实现方法 浅拷贝\n针对引用或指针类型的浅拷贝，如果对象发生变化，所有指向对象的指针所得值都会跟着变化。\n深拷贝\n拷贝得到的对象完全独立，不会受源对象的影响。\n深度递归实现（注意环路数据结构） 序列后与反序列化 例子 需求: 假设现在数据库中有大量数据，包含了关键词，关键词被搜索的次数等信息，模块 A 为了业务需要\n会在启动时加载这部分数据到内存中 并且需要定时更新里面的数据 同时展示给用户的数据每次必须要是相同版本的数据，不能一部分数据来自版本 1 一部分来自版本 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // @author cold bin // @date 2023/8/31 package prototype import ( \u0026#34;encoding/json\u0026#34; \u0026#34;time\u0026#34; ) // Keyword 搜索关键字 type Keyword struct { Word string Visit int UpdatedAt *time.Time } // Clone 这里使用序列化与反序列化的方式深拷贝 func (k *Keyword) Clone() *Keyword { var newKeyword Keyword b, _ := json.Marshal(k) json.Unmarshal(b, \u0026amp;newKeyword) return \u0026amp;newKeyword } // Keywords 关键字 map // 数据库缓存 type Keywords map[string]*Keyword // Clone 复制一个新的 keywords // updatedWords: 需要更新的关键词列表，由于从数据库中获取数据常常是数组的方式 func (words Keywords) Clone(updatedWords []*Keyword) Keywords { newKeywords := Keywords{} for k, v := range words { // 这里是浅拷贝，直接拷贝了地址 newKeywords[k] = v } // 替换掉需要更新的字段，这里用的是深拷贝 for _, word := range updatedWords { newKeywords[word.Word] = word.Clone() } return newKeywords } ","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go设计模式之原型模式"},{"categories":["golang","八股文","设计模式"],"contents":" 来源于topgoer\n函数选项模式 默认值 有时候一个函数会有很多参数，为了方便函数的使用，我们会给希望给一些参数设定默认值，调用时只需要传与默认值不同的参数即可，类似于 python 里面的默认参数和字典参数，虽然 golang 里面既没有默认参数也没有字典参数，但是我们有选项模式。\n应用 从这里可以看到，为了实现选项的功能，我们增加了很多的代码，实现成本相对还是较高的，所以实践中需要根据自己的业务场景去权衡是否需要使用。个人总结满足下面条件可以考虑使用选项模式\n参数确实比较复杂，影响调用方使用 参数确实有比较清晰明确的默认值 为参数的后续拓展考虑 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // @author cold bin // @date 2023/8/30 package option const ( DefaultMaxTotal = 10 DefaultMaxIdle = 9 DefaultMinIdle = 1 ) type Pool struct { name string maxTotal int maxIdle int minIdle int } func NewPool(name string, opts ...Option) *Pool { // 填必须参数 p := \u0026amp;Pool{name: name, maxTotal: DefaultMaxTotal, maxIdle: DefaultMaxIdle, minIdle: DefaultMinIdle} // 填入选项 for _, opt := range opts { opt(p) } return p } type Option func(*Pool) func WithMaxTotal(maxTotal int) Option { return func(pool *Pool) { pool.maxTotal = maxTotal } } func WithMaxIdle(maxIdle int) Option { return func(pool *Pool) { pool.maxIdle = maxIdle } } func WithMinIdle(minIdle int) Option { return func(pool *Pool) { pool.minIdle = minIdle } } ","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go设计模式之函数选项模式"},{"categories":["golang","八股文","设计模式"],"contents":" 建造者模式 与工厂模式不同，建造者模式只创建一种类型的复杂对象，可以通过设置可选参数，定制化地创建不同对象。\n简而言之，创建参数复杂的对象\n应用场景 类属性较多 类属性之间含有依赖关系 存在必选和非必选参数 希望创建不可变对象 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // @author cold bin // @date 2023/8/30 package builder import \u0026#34;errors\u0026#34; const ( DefaultMaxTotal = 10 DefaultMaxIdle = 9 DefaultMinIdle = 1 ) type Build struct { name string maxTotal int maxIdle int minIdle int } // Build 建造器 // //\t这里负责参数校验，通过后真正new对象 func (b *Build) Build() (*Pool, error) { if b.maxIdle \u0026lt;= 0 { b.maxIdle = DefaultMaxIdle } if b.minIdle \u0026lt;= 0 { b.minIdle = DefaultMinIdle } if b.maxTotal \u0026lt;= 0 { b.maxTotal = DefaultMaxTotal } if b.minIdle \u0026gt; b.maxIdle { return nil, errors.New(\u0026#34;minIdle is more than maxIdle\u0026#34;) } if b.maxIdle \u0026gt; b.maxTotal { return nil, errors.New(\u0026#34;maxIdle is more than maxTotal\u0026#34;) } return \u0026amp;Pool{ name: b.name, maxTotal: b.maxTotal, maxIdle: b.maxIdle, minIdle: b.minIdle, }, nil } func (b *Build) SetName(name string) { b.name = name } func (b *Build) SetMaxTotal(maxTotal int) { b.maxTotal = maxTotal } func (b *Build) SetMaxIdle(maxIdle int) { b.maxIdle = maxIdle } func (b *Build) SetMinIdle(minIdle int) { b.minIdle = minIdle } type Pool struct { name string maxTotal int maxIdle int minIdle int } ","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go设计模式之建造者模式"},{"categories":["golang","八股文","设计模式"],"contents":" 工厂模式 与单例模式不同，工厂模式根据传入参数不同，会创建出不同的但是相关联的对象，由给定参数来决定是哪一种对象。像一个工厂一样，传入什么，生产什么，不止一种。\n简单工厂 传出不同对象需要，使用接口多态特性\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // @author cold bin // @date 2023/8/28 package factory type Config interface { Parse(data []byte) error UnParse(src []byte, dst []byte) error } type Json struct { } func (j Json) Parse(data []byte) error { panic(\u0026#34;implement me\u0026#34;) } func (j Json) UnParse(src []byte, dst []byte) error { panic(\u0026#34;implement me\u0026#34;) } type Yaml struct { } func (y Yaml) Parse(data []byte) error { panic(\u0026#34;implement me\u0026#34;) } func (y Yaml) UnParse(src []byte, dst []byte) error { panic(\u0026#34;implement me\u0026#34;) } // NewConfig 这里直接使用简单工厂方法创建最终对象 // //\t如果创建对象不复杂，不涉及对象之间的组合，可以使用 func NewConfig(name, typ string) Config { switch typ { case \u0026#34;json\u0026#34;: return \u0026amp;Json{} case \u0026#34;yaml\u0026#34;: return \u0026amp;Yaml{} } return nil } 工厂方法 简单工厂适合直接创建一些较简单的对象，如果涉及多个对象之间的组合以及初始化，可以考虑使用工厂方法。工厂方法并不是直接拿的直接对象，而是拿的工厂，拿到之后，可以在有需要的时候根据工厂来创建对象并组合。\n而且，工厂可以复用，避免重复创建工厂\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // @author cold bin // @date 2023/8/29 package factory // ConfigFactory Config 的工厂方法抽象 type ConfigFactory interface { Create() Config } type JsonFactory struct{} func (j JsonFactory) Create() Config { return Json{} } type YamlFactory struct{} func (y YamlFactory) Create() Config { return Json{} } // NewConfigFactory 这里使用简单工厂的方式创建工厂 // //\t与简单工厂不同的是，这里拿的还是工厂，而不是直接的对象 func NewConfigFactory(typ string) ConfigFactory { switch typ { case \u0026#34;json\u0026#34;: return JsonFactory_ case \u0026#34;yaml\u0026#34;: return YamlFactory_ } return nil } var ( JsonFactory_ = JsonFactory{} YamlFactory_ = YamlFactory{} ) 抽象工厂 略\n应用——DI DI是依赖注入的意思。DI的底层设计其实就是工厂模式的应用。\n总结 工厂模式是用以创建不同但是相关联的对象，根据传入参数来决定创建那种对象。\n不用工厂模式创建多个有关联对象：if-else逻辑过多、创建逻辑、业务代码耦合在一起\n简单工厂可以将多个对象的创建逻辑放到一个工厂类里\n工厂方法可以将不同创建逻辑拆分到不同工厂类里，然后可以通过特定工厂创建对象\n适合简单工厂创建对象过于复杂的情形\n","permalink":"https://cold-bin.github.io/post/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go语言设计模式之工厂模式"},{"categories":["golang","八股文","设计模式"],"contents":"单例模式 简而言之：一个类只允许创建一个对象或示例。\n饿汉式 项目初始化的时候加载并初始化对象。创建过程线程安全，而且使得问题尽早暴露。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // @author cold bin // @date 2023/8/28 package singleton /* 单例模式的饿汉式:项目编译运行的初期就已经初始化了对象 */ type Logger interface { print(...any) error } type log struct{} func (l *log) print(...any) error { return nil } var log_ *log // 只要导入包，编译时便会初始化这个对象 func init() { log_ = \u0026amp;log{} } func GetInstance() Logger { return log_ } 懒汉式（双重检测） 支持延迟加载，在第一次使用对象的时候创建对象，也就是有需要使用的时候再创建对象。相比懒汉式，支持对象创建的并发安全。代码实现思路：一是通过sync.Mutex直接加锁；二是使用sync.Once保证对象创建只调用一次。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // @author cold bin // @date 2023/8/28 package singleton import \u0026#34;sync\u0026#34; /* 单例模式的懒汉式：支持延迟加载，实现范式会导致频繁加锁和释放锁，而且并发度也低。双重检测版本 */ var ( log__ *log once sync.Once ) func GetLazyInstance() Logger { if log__ == nil { once.Do(func() { log__ = \u0026amp;log{} }) } return log__ } ","permalink":"https://cold-bin.github.io/post/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","tags":[],"title":"Go语言设计模式之单例模式"},{"categories":["八股文","设计模式"],"contents":" 设计原则 SOLID原则\nSRP、OCP、LSP、ISP、DIP\nKISS原则\nYAGNI原则\nDRY原则\nLOD原则\nSRP 单一职责原则（Single Responsibility Principle）：一个模块或类只负责完成一个功能。不要设计大而全的类。\n是否越单一越好？\n越单一的模块，会使得代码内聚性变低。我们要做到高内聚，低耦合。\n如何判断模块设计的是否满足SRP原则？\n以业务而定。\nOCP 开闭原则（Open / Closed Principle）：通过在已有代码的基础上扩展代码（添加新的模块、类、方法、属性等），而非修改代码（修改已有的模块、类、方法、属性等）。\n如何理解开闭原则？\n扩展和修改的定义是相对于作用对象而言的。\n添加一个方法或属性在类的层面就是修改类，但是在方法或属性层面就是扩展了属性或方法。换言之，同样的代码改动，在不同对象粒度上，会有不同看法。 OCP并非完全排斥修改，而是将修改对代码可维护性的影响降至最低 怎么做到开闭原则？\n提前了解项目需求，预测短期内可能出现的业务变更，组件变更等，在这些地方预留扩展点。怎么留扩展点？运用一些设计模式技巧、基于接口而非实现编程、依赖注入、多态等\nLSP 里氏替换原则（Liskov Subsititution Principle）：子类对象能够替换程序中对父类对象出现的任何地方，并且保证原来程序的逻辑行为不变及正确不被破坏。\n哪些子类设计违反LSP\n子类违背父类声明要实现的功能 子类违背父类对输入、输出和异常的要求 子类违背父类注释中所罗列的任何特殊声明 简而言之，子类对父类方法重构的实际输入输出、功能需求不能发生变化。\nLSP和多态有什么区别？\nLSP是设计原则，多态是编程方法。LSP指导子类如何设计，以让子类可以替换父类出现的任何位置。\nISP 接口隔离原则（Interface Segregation Principle）：客户端不应该强迫依赖它不需要的接口。\n三种理解：\n一组API集合\n某组API只被某个客户端使用，那就没有必要将这组API暴露给其他客户端，将它隔离出来\n一个API或函数\n类似于单一职责原则，函数或API功能单一，不应该包含其他功能或业务。\nOOP里的接口\n语法上的接口，应该尽可能使得接口里面的方法少，只包含必要的方法\nDIP 依赖反转原则（Dependency Inversion Principle）：高层模块不要依赖低层模块，高层模块和低层模块应该通过抽象来相互依赖。抽象不要依赖具体实现细节，具体实现以来抽象。\nIOC 控制反转（Inversion Of Control）：通过框架约束程序执行流程。\nDI 依赖注入（Dependency Injection）：是一种实现IOC的且用于解决依赖性问题的设计模式，我们不在内部创建对象并使用它，而是将依赖对象直接注入来使用，这样我们就只依赖依赖注入的对象。\n下面是摘自飞书技术。\n依赖注入前\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // dal/user.go func (u *UserDal) Create(ctx context.Context, data *UserCreateParams) error { db := mysql.GetDB().Model(\u0026amp;entity.User{}) user := entity.User{ Username: data.Username, Password: data.Password, } return db.Create(\u0026amp;user).Error } // service/user.go func (u *UserService) Register(ctx context.Context, data *schema.RegisterReq) (*schema.RegisterRes, error) { params := dal.UserCreateParams{ Username: data.Username, Password: data.Password, } err := dal.GetUserDal().Create(ctx, params) if err != nil { return nil, err } registerRes := schema.RegisterRes{ Msg: \u0026#34;register success\u0026#34;, } return \u0026amp;registerRes, nil } 在这段代码里，层级依赖关系为 service -\u0026gt; dal -\u0026gt; db，上游层级通过 Getxxx实例化依赖。但在实际生产中，我们的依赖链比较少是垂直依赖关系，更多的是横向依赖。即我们一个方法中，可能要多次调用Getxxx的方法，这样使得我们代码极不简洁。\n不仅如此，我们的依赖都是写死的，即依赖者的代码中写死了被依赖者的生成关系。当被依赖者的生成方式改变，我们也需要改变依赖者的函数，这极大的增加了修改代码量以及出错风险。\n接下来我们用依赖注入的方式对代码进行改造：\n依赖注入后\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // dal/user.go type UserDal struct{ DB *gorm.DB } func NewUserDal(db *gorm.DB) *UserDal{ return \u0026amp;UserDal{ DB: db } } func (u *UserDal) Create(ctx context.Context, data *UserCreateParams) error { db := u.DB.Model(\u0026amp;entity.User{}) user := entity.User{ Username: data.Username, Password: data.Password, } return db.Create(\u0026amp;user).Error } // service/user.go type UserService struct{ UserDal *dal.UserDal } func NewUserService(userDal dal.UserDal) *UserService{ return \u0026amp;UserService{ UserDal: userDal } } func (u *UserService) Register(ctx context.Context, data *schema.RegisterReq) (*schema.RegisterRes, error) { params := dal.UserCreateParams{ Username: data.Username, Password: data.Password, } err := u.UserDal.Create(ctx, params) if err != nil { return nil, err } registerRes := schema.RegisterRes{ Msg: \u0026#34;register success\u0026#34;, } return \u0026amp;registerRes, nil } // main.go db := mysql.GetDB() userDal := dal.NewUserDal(db) userService := dal.NewUserService(userDal) 如上编码情况中，我们通过将 db 实例对象注入到 dal 中，再将 dal 实例对象注入到 service 中，实现了层级间的依赖注入。解耦了部分依赖关系。\n在系统简单、代码量少的情况下上面的实现方式确实没什么问题。但是项目庞大到一定程度，结构之间的关系变得非常复杂时，手动创建每个依赖，然后层层组装起来的方式就会变得异常繁琐，并且容易出错。这个时候勇士 wire 出现了！\nKISS 尽量保持简单（Keep It Simple and Stupid）\n如何写出满足KISS原则的代码？\n注重代码可读性 不要重复造轮子（生产开发 不要过度优化 YAGNI 你不会需要它（You Aren\u0026rsquo;t Gonna Need It）：核心就是不要过度设计，把一些目前没必要使用的模块也引入\nDRY 不要重复自己（Don\u0026rsquo;t repeat yourself）：\n实现逻辑重复，但功能逻辑不重复，并不违反DRY原则 实现逻辑不重复，但功能逻辑重复，违反DRY原则 代码执行重复也算违背DRY原则 如何提高代码复用性？\n减少代码耦合 满足单一职责原则 模块化 业务与非业务逻辑分离 通用代码下沉 抽象、多态和封装 使用合适的设计模式 LOD 迪米特法则（Law Of Demeter）：每个模块只应该了解哪些与它关系密切模块的优先知识。\n如何理解高内聚、松耦合？\n高内聚就是指把功能相近或为一体的放到同一个模块实现，松耦合就是减少模块之间不必要的依赖。\n","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","tags":[],"title":"Go设计模式之设计原则"},{"categories":["八股文","设计模式"],"contents":"OOP OOP指的就是面向对象编程。有三大特性：封装、继承和多态\n封装：通过对象访问控制权限实现，只对外暴露必要的方法修改对象，不能直接将对象全部暴露，也不能暴露可能存在安全隐患的字段。 继承：继承父类对象的属性和方法，达到代码复用的目的 多态：go语言里可以这么认为，实现之前先定义一个好的抽象的接口，然后再让对象去实现这个接口，如果后面要迁移或者换一个对象实现，只需要重新用新对象实现这个接口即可，可以保证不修改调用者代码。 ","permalink":"https://cold-bin.github.io/post/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8Boop/","tags":[],"title":"Go设计模式之OOP"},{"categories":["数据结构与算法"],"contents":"滑动窗口算法 1. 简介 滑动窗口算法（Sliding Window）是一种常用的双指针算法，被广泛应用于字符串和数组等数据结构中的子串或子数组问题，例如字符串匹配、最长子串、最小覆盖子串等问题。滑动窗口算法可以优化暴力枚举的时间复杂度，使得算法的执行效率更高。\n本文将详细介绍滑动窗口算法的基本思想、应用场景、实现方法、时间复杂度和常见问题等相关内容。\n2. 基本思想 滑动窗口算法的基本思想是维护一个窗口，通过移动窗口的两个边界来处理问题。\n具体来说，我们可以使用两个指针 $left$ 和 $right$ 分别表示滑动窗口的左右边界，然后通过不断移动右指针 $right$ 来扩大窗口，同时根据问题的要求调整左指针 $left$ 来缩小窗口。当右指针 $right$ 扫描到字符串或数组的末尾时，算法的执行就完成了。\n在扩大或缩小窗口的过程中，可以记录下一些中间结果，例如最大值、最小值、子串长度等等，从而求解问题的最终答案。\n3. 应用场景 滑动窗口算法可以用于解决一些字符串和数组问题，例如：\n字符串匹配问题，例如 Leetcode 第 28 题和第 76 题； 最长子串或子数组问题，例如 Leetcode 第 3 题、第 209 题和第 424 题； 最小覆盖子串问题，例如 Leetcode 第 76 题； 字符串排列问题，例如 Leetcode 第 567 题； 求解字符串或数组中的一些性质，例如 Leetcode 第 438 题、第 567 题和第 1004 题等。 4. 实现方法 滑动窗口算法的实现方法相对简单，主要分为以下几个步骤：\n初始化左右指针 $left$ 和 $right$，并根据问题的要求进行一些初始化操作。 不断移动右指针 $right$，直到出现不符合条件的情况，或者扫描到字符串或数组的末尾。 对于每个右指针位置 $i$，更新一些中间结果。 移动左指针 $left$，直到出现符合条件的情况，或者左右指针重合。 重复第 2 步至第 4 步，直到右指针扫描到字符串或数组的末尾。 ","permalink":"https://cold-bin.github.io/post/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B9%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","tags":["算法","滑动窗口"],"title":"常见算法之滑动窗口"},{"categories":["八股文","操作系统"],"contents":" 以下文章摘自小林coding\n文件系统 不多 BB，直接上「硬菜」。\n#文件系统的基本组成 文件系统是操作系统中负责管理持久数据的子系统，说简单点，就是负责把用户的文件存到磁盘硬件中，因为即使计算机断电了，磁盘里的数据并不会丢失，所以可以持久化的保存文件。\n文件系统的基本数据单位是文件，它的目的是对磁盘上的文件进行组织管理，那组织的方式不同，就会形成不同的文件系统。\nLinux 最经典的一句话是：「一切皆文件」，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的。\nLinux 文件系统会为每个文件分配两个数据结构：索引节点（*index node*）和目录项（*directory entry*），它们主要用来记录文件的元信息和目录层次结构。\n索引节点，也就是 inode，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间。 目录项，也就是 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。 由于索引节点唯一标识一个文件，而目录项记录着文件的名字，所以目录项和索引节点的关系是多对一，也就是说，一个文件可以有多个别名。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。\n注意，目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。\n目录项和目录是一个东西吗？\n虽然名字很相近，但是它们不是一个东西，目录是个文件，持久化存储在磁盘，而目录项是内核一个数据结构，缓存在内存。\n如果查询目录频繁从磁盘读，效率会很低，所以内核会把已经读过的目录用目录项这个数据结构缓存在内存，下次再次读到相同的目录时，只需从内存读就可以，大大提高了文件系统的效率。\n注意，目录项这个数据结构不只是表示目录，也是可以表示文件的。\n那文件数据是如何存储在磁盘的呢？\n磁盘读写的最小单位是扇区，扇区的大小只有 512B 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。\n所以，文件系统把多个扇区组成了一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 4KB，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。\n以上就是索引节点、目录项以及文件数据的关系，下面这个图就很好的展示了它们之间的关系：\n索引节点是存储在硬盘上的数据，那么为了加速文件的访问，通常会把索引节点加载到内存中。\n另外，磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。\n超级块，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。 索引节点区，用来存储索引节点； 数据块区，用来存储文件或目录数据； 我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：\n超级块：当文件系统挂载时进入内存； 索引节点区：当文件被访问时进入内存； #虚拟文件系统 文件系统的种类众多，而操作系统希望对用户提供一个统一的接口，于是在用户层与文件系统层引入了中间层，这个中间层就称为虚拟文件系统（*Virtual File System，VFS*）。\nVFS 定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。\n在 Linux 文件系统中，用户空间、系统调用、虚拟文件系统、缓存、文件系统以及存储之间的关系如下图：\nLinux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类：\n磁盘的文件系统，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。 内存的文件系统，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 /proc 和 /sys 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。 网络的文件系统，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。 文件系统首先要先挂载到某个目录才可以正常使用，比如 Linux 系统在启动时，会把文件系统挂载到根目录。\n#文件的使用 我们从用户角度来看文件的话，就是我们要怎么使用文件？首先，我们得通过系统调用来打开一个文件。\n1 2 3 4 5 fd = open(name, flag); # 打开文件 ... write(fd,...); # 写数据 ... close(fd); # 关闭文件 上面简单的代码是读取一个文件的过程：\n首先用 open 系统调用打开文件，open 的参数中包含文件的路径名和文件名。 使用 write 写数据，其中 write 使用 open 所返回的文件描述符，并不使用文件名作为参数。 使用完文件后，要用 close 系统调用关闭文件，避免资源的泄露。 我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪呢，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「文件描述符」，所以说文件描述符是打开文件的标识。\n操作系统在打开文件表中维护着打开文件的状态和信息：\n文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的； 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目； 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取； 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求； 在用户视角里，文件就是一个持久化的数据结构，但操作系统并不会关心你想存在磁盘上的任何的数据结构，操作系统的视角是如何把文件数据和磁盘块对应起来。\n所以，用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统则是以数据块来读写文件，那屏蔽掉这种差异的工作就是文件系统了。\n我们来分别看一下，读文件和写文件的过程：\n当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。 当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。 所以说，文件系统的基本操作单位是数据块。\n#文件的存储 文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：\n连续空间存放方式 非连续空间存放方式 其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。\n不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。\n#连续空间存放方式 连续空间存放方式顾名思义，文件存放在磁盘「连续的」物理空间中。这种模式下，文件的数据都是紧密相连，读写效率很高，因为一次磁盘寻道就可以读出整个文件。\n使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。\n所以，文件头里需要指定「起始块的位置」和「长度」，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。\n注意，此处说的文件头，就类似于 Linux 的 inode。\n连续空间存放的方式虽然读写效率高，但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。\n如下图，如果文件 B 被删除，磁盘上就留下一块空缺，这时，如果新来的文件小于其中的一个空缺，我们就可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但却小于空缺大小之和，则虽然磁盘上有足够的空缺，但该文件还是不能存放。当然了，我们可以通过将现有文件进行挪动来腾出空间以容纳新的文件，但是这个在磁盘挪动文件是非常耗时，所以这种方式不太现实。\n另外一个缺陷是文件长度扩展不方便，例如上图中的文件 A 要想扩大一下，需要更多的磁盘空间，唯一的办法就只能是挪动的方式，前面也说了，这种方式效率是非常低的。\n那么有没有更好的方式来解决上面的问题呢？答案当然有，既然连续空间存放的方式不太行，那么我们就改变存放的方式，使用非连续空间存放方式来解决这些缺陷。\n#非连续空间存放方式 非连续空间存放方式分为「链表方式」和「索引方式」。\n我们先来看看链表的方式。\n链表的方式存放是离散的，不用连续的，于是就可以消除磁盘碎片，可大大提高磁盘空间的利用率，同时文件的长度可以动态扩展。根据实现的方式的不同，链表可分为「隐式链表」和「显式链接」两种形式。\n文件要以「隐式链表」的方式存放的话，实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置，这样一个数据块连着一个数据块，从链头开始就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。\n隐式链表的存放方式的缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间。隐式链接分配的稳定性较差，系统在运行过程中由于软件或者硬件错误导致链表中的指针丢失或损坏，会导致文件数据的丢失。\n如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「显式链接」，它指把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中，该表在整个磁盘仅设置一张，每个表项中存放链接指针，指向下一个数据块号。\n对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为文件分配表（*File Allocation Table，FAT*）。\n由于查找记录的过程是在内存中进行的，因而不仅显著地提高了检索速度，而且大大减少了访问磁盘的次数。但也正是整个表都存放在内存中的关系，它的主要的缺点是不适用于大磁盘。\n比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。\n接下来，我们来看看索引的方式。\n链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。\n索引的实现是为每个文件创建一个「索引数据块」，里面存放的是指向文件数据块的指针列表，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。\n另外，文件头需要包含指向「索引数据块」的指针，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。\n创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。\n索引的方式优点在于：\n文件的创建、增大、缩小很方便； 不会有碎片的问题； 支持顺序读写和随机读写； 由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。\n如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。\n先来看看链表 + 索引的组合，这种组合称为「链式索引块」，它的实现方式是在索引数据块留出一个存放下一个索引数据块的指针，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。\n还有另外一种组合方式是索引 + 索引的方式，这种组合称为「多级索引块」，实现方式是通过一个索引块来存放多个索引数据块，一层套一层索引，像极了俄罗斯套娃是吧。\n#Unix 文件的实现方式 我们先把前面提到的文件实现方式，做个比较：\n那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图：\n它是根据文件的大小，存放的方式会有所变化：\n如果存放文件所需的数据块小于 10 块，则采用直接查找的方式； 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式； 如果前面两种方式都不够存放大文件，则采用二级间接索引方式； 如果二级间接索引也不够存放大文件，这采用三级间接索引方式； 那么，文件头（Inode）就需要包含 13 个指针：\n10 个指向数据块的指针； 第 11 个指向索引块的指针； 第 12 个指向二级索引块的指针； 第 13 个指向三级索引块的指针； 所以，这种方式能很灵活地支持小文件和大文件的存放：\n对于小文件使用直接查找的方式可减少索引数据块的开销； 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询； 这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。\n为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。\n#空闲空间管理 前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？\n那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：\n空闲表法 空闲链表法 位图法 #空闲表法 空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：\n当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。\n这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。\n#空闲链表法 我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：\n当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。\n这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。\n空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。\n#位图法 位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。\n当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：\n1 1111110011111110001110110111111100111 ... 在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。\n#文件系统的结构 前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。\n数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 4 * 1024 * 8 = 2^15 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 2^15 * 4 * 1024 = 2^27 个 byte，也就是 128M。\n也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。\n在 Linux 文件系统，把这个结构称为一个块组，那么有 N 多的块组，就能够表示 N 大的文件。\n下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：\n最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：\n超级块，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。 块组描述符，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。 数据位图和 inode 位图， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。 inode 列表，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。 数据块，包含文件的有用数据。 你可以会发现每个块组里有很多重复的信息，比如超级块和块组描述符表，这两个都是全局信息，而且非常的重要，这么做是有两个原因：\n如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。 不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。\n#目录的存储 在前面，我们知道了一个普通文件是如何存储的，但还有一个特殊的文件，经常用到的目录，它是如何保存的呢？\n基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 vim 打开它，它也有 inode，inode 里面也是指向一些块。\n和普通文件不同的是，普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。\n在目录文件的块中，最简单的保存格式就是列表，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。\n列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。\n通常，第一项是「.」，表示当前目录，第二项是「..」，表示上一级目录，接下来就是一项一项的文件名和 inode。\n如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。\n于是，保存目录的格式改成哈希表，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。\nLinux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。\n目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。\n#软链接和硬链接 有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过硬链接（*Hard Link*） 和软链接（*Symbolic Link*） 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。\n硬链接是多个目录项中的「索引节点」指向一个文件，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以硬链接是不可用于跨文件系统的。由于多个目录项都是指向一个 inode，那么只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。\n软链接相当于重新创建一个文件，这个文件有独立的 inode，但是这个文件的内容是另外一个文件的路径，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以软链接是可以跨文件系统的，甚至目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。\n#文件 I/O 文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有\n缓冲与非缓冲 I/O 直接与非直接 I/O 阻塞与非阻塞 I/O VS 同步与异步 I/O 接下来，分别对这些分类讨论讨论。\n#缓冲与非缓冲 I/O 文件操作的标准库是可以实现数据的缓存，那么根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O：\n缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。 这里所说的「缓冲」特指标准库内部实现的缓冲。\n比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。\n#直接与非直接 I/O 我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。\n那么，根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O：\n直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。 如果你在使用文件操作类的系统调用函数时，指定了 O_DIRECT 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。\n如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？\n以下几种场景会触发内核缓存的数据写入磁盘：\n在调用 write 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上； 用户主动调用 sync，内核缓存会刷到磁盘上； 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上； 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上； #阻塞与非阻塞 I/O VS 同步与异步 I/O 为什么把阻塞 / 非阻塞与同步与异步放一起说的呢？因为它们确实非常相似，也非常容易混淆，不过它们之间的关系还是有点微妙的。\n先来看看阻塞 I/O，当用户程序执行 read ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，read 才会返回。\n注意，阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程。过程如下图：\n知道了阻塞 I/O ，来看看非阻塞 I/O，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，read 调用才可以获取到结果。过程如下图：\n注意，这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。\n举个例子，访问管道或 socket 时，如果设置了 O_NONBLOCK 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。\n应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。\n为了解决这种傻乎乎轮询方式，于是 I/O 多路复用技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。\n这个做法大大改善了 CPU 的利用率，因为当调用了 I/O 多路复用接口，如果没有事件发生，那么当前线程就会发生阻塞，这时 CPU 会切换其他线程执行任务，等内核发现有事件到来的时候，会唤醒阻塞在 I/O 多路复用接口的线程，然后用户可以进行后续的事件处理。\n整个流程要比阻塞 IO 要复杂，似乎也更浪费性能。但 I/O 多路复用接口最大的优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求（参见：I/O 多路复用：select/poll/epoll (opens new window)）。用户可以注册多个 socket，然后不断地调用 I/O 多路复用接口读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。\n下图是使用 select I/O 多路复用过程。注意，read 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个同步的过程，需要等待：\n实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。\n而真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。\n当我们发起 aio_read 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图：\n下面这张图，总结了以上几种 I/O 模型：\n在前面我们知道了，I/O 是分为两个过程的：\n数据准备的过程 数据从内核空间拷贝到用户进程缓冲区的过程 阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。\n异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。\n用故事去理解这几种 I/O 模型\n举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。\n阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。\n非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。\n基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（select 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。\n异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","tags":["操作系统文件系统"],"title":"八股系列|操作系统|文件系统"},{"categories":["八股文","操作系统"],"contents":"调度算法 进程调度 FIFO\n先来先服务。每次从就绪队列里调度最先进入就绪队列的进程执行，执行完毕再调度下一个次先进入就绪队列的进程执行。\n**缺点：**如果长进程先进入就绪队列，会造成短进程周转时间变长，引发饥饿。\n**优点：**对长进程有利，适合于CPU繁忙型的系统\nSJF\n最短作业优先调度算法。每次从就绪队列调度执行时间最短的进程执行，执行完毕再调度下一个次短的进程执行\n**缺点：**会对长进程造成饥饿问题\n**优点：**利于短进程，提高系统吞吐量（单位时间完成的进程数）\nHRRN\n高响应比优先调度算法。综合前两种算法优劣，每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，「响应比优先级」的计算公式：优先权=（等待时间+要求服务时间）/要求服务时间\n**优点：**等待时间越长，优先级越高（利于长进程）；要求服务时间越短，优先级越高（利于短进程）\n**缺点：**很难估计等待时间，算法实现难度大，几乎没有现成的实现\nRR\n时间片轮转调度算法。最公平的调度算法，就绪队列里的每个进程没有优先级，依次获得相同CPU时间片使用权，时间片用完后或提前结束运行，就会调度下一个线程执行。\n**优点：**不会出现进程的饥饿问题\n**缺点：**没有优先级，无法即时响应紧急任务\nHPF\n最高优先级调度算法。从就绪队列中选择最高优先级的进程调度并执行。\n进程的优先级可以分为，静态优先级和动态优先级：\n静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化； 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是随着时间的推移增加等待进程的优先级。 该算法也有两种处理优先级高的方法，非抢占式和抢占式：\n非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。 **缺点：**可能会导致低优先级的进程饥饿，甚至无法被调度执行\n**优点：**能及时响应紧急任务\nMFQ\n多级反馈队列调度算法。该调度算法是综合最高优先级和时间片轮转调度算法的优劣。\n**多级：**多个就绪队列，每个队列优先级从高到低，同时优先级越高时间片越短\n**反馈：**新的高优先级进入就绪队列，CPU停止执行当前进程，并将其移到当前就绪队列尾部，并调度新进程执行。\n过程：\n多个就绪队列，从高到低优先级降低，并且优先级越高，时间片越短 新的进程会被放到第一级队列末尾，按照先来先服务原则执行，时间片用完后还没执行完毕就将其放到下一级就绪队列 当较高优先级就绪队列执行完毕，则下一级就绪队列开始执行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行 页面置换 最佳页面置换\n置换在未来最长时间不访问的页面\n**缺点：**无法实现预测未来，仅仅只能作为最优参考\n先进先出置换\n选择在内存驻留时间最长的页面置换。\n**缺点：**可能会将频繁访问的内存驻留时间最长的页面置换出去，反而会增加后续页面置换的概率\n最近最久未使用置换（LRU）\n选择最长时间没有被访问的页面进行置换。需要为所有页面保存一个访问链表，只要访问了某页面，就将页面放到链表尾部，需要置换的时候，就置换链表首部页面，即淘汰最近最久未使用页面。\n**缺点：**开销较高\n**优点：**实现效果接近最佳页面置换\n最不常用（LFU）\n选择访问频率最低的页面置换。\n**缺点：**没有考虑到时间维度，可能会把以前访问频率高但是近期或以后不会被访问的页面驻留在内存里；而且硬件需要为每个页面都保留一个计数器，成本太高。\n时钟页面置换\n磁盘调度 假设有下面一个请求序列，每个数字代表磁道的位置：\n98，183，37，122，14，124，65，67\n初始磁头当前的位置是在第 53 磁道。\n先来先服务\n按照请求序列的顺序，移动磁头到指定磁道上。可以看出总共移动了(98-53)+(183-98)+(183-37)+(122-37)+(122-14)+(124-14)+(124-65)+(67-65) = 640。\n简单粗暴，如果大量进程的请求序列分散，会导致磁头移动很距离，导致寻道时间过长，拖垮磁盘IO性能\n最短寻道时间优先\n优先选择从当前磁头位置所需寻道时间最短的请求。\n所以，针对请求序列排序：65，67，37，14，98，122，124，183。磁头移动的总距离是 236 磁道，相比先来先服务性能提高了不少。\n**缺点：**磁头有可能在一小块区域来回移动，会让某些请求饥饿。\n**优点：**解决了先来先服务性能问题\n扫描算法\n最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。\n为了防止这个问题，可以规定：磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（*Scan*）算法。\n还是以这个序列为例子，磁头的初始位置是 53：\n98，183，37，122，14，124，65，67\n那么，假设扫描调度算先朝磁道号减少的方向移动，具体请求则会是下列从左到右的顺序：\n37，14，0，65，67，98，122，124，183\n磁头先响应左边的请求，直到到达最左端（ 0 磁道）后，才开始反向移动，响应右边的请求（同理到最右端）。\n缺点：由于扫描算法如果没有完成所有请求就会扫描至端点处，所以会出现中间部分被来回扫描的情况，也就是多扫了一遍\n循环扫描法\n为了解决扫描算法的缺点，循环扫描（Circular Scan, CSCAN ）规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请求，该算法的特点，就是磁道只响应一个方向上的请求。\n还是以这个序列为例子，磁头的初始位置是 53：\n98，183，37，122，14，124，65，67\n那么，假设循环扫描调度算先朝磁道增加的方向移动，具体请求会是下列从左到右的顺序：\n65，67，98，122，124，183，199，0，14，37\n循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。\nLOOK 与 C-LOOK算法\n我们前面说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。\n那这其实是可以优化的，优化的思路就是磁头在移动到「最远的请求」位置，然后立即反向移动。\n那针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，反向移动的途中会响应请求。\n而针 C-SCAN 算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，反向移动的途中不会响应请求。\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/","tags":["操作系统调度算法"],"title":"八股系列|操作系统|调度算法"},{"categories":["八股文","操作系统"],"contents":"进程管理 并行和并发的区别？ 并行：多个进程或线程同一时刻运行在多个CPU上 并发：多个进程或线程同一时间段交替运行在单个CPU上 什么是进程？ 运行中的程序就是进程，也是程序向操作系统申请资源的基本单位。\n进程的状态有哪些？ 进程有五种基本状态：创建、就绪、运行、阻塞和终止。还有其他两种特殊状态：激活和挂起.\n创建：进程正在被创建的过程\n申请空白的PCB； 然后在PCB中填入进程标识符等信息；初始化虚拟内存等资源； 将PCB插入就绪队列，等待被调度运行 就绪：进程已经具备运行条件，但缺乏CPU执行权\n将创建完成的PCB放入就绪队列，等待被CPU调度；\n运行：正在被CPU调度运行\n从就绪队列里调度进程运行，分配CPU时间片；\n阻塞：进程由于IO等事件阻塞操作导致CPU资源空闲，此时让出CPU资源，并进入阻塞态等待事件完成，唤醒阻塞\n阻塞：\n找到需要阻塞的进程标识符对应的PCB； 如果当前状态为运行态，则保存当前待阻塞进程上下文，将其状态变更为阻塞态； 将PCB移入阻塞队列； 唤醒：\n在该事件的阻塞队列中找到相应进程的 PCB； 将其从阻塞队列中移出，并置其状态为就绪状态； 把该 PCB 插入到就绪队列中，等待调度程序调度； 终止：进程执行完毕或异常，系统回收进程资源\n先找到要终止进程的PCB 如果当前进程正在运行，则终止运行，让出CPU使用权 如果进程还有子进程，则将子进程交给1号进程托管 回收进程所有资源 删除PCB 什么是CPU上下文切换？ 当前任务执行结束或被中断，需要保存当前任务的CPU现场，包括当前寄存器、程序计数器和程序状态字等信息，然后将当前程序计数器指向下一个任务并执行。\n什么是进程上下文切换？ 进程是由内核管理的，所以只能在内核切换。进程的上下文切换不仅包括虚拟内存等用户空间资源，还有内核堆栈、寄存器等资源。切换时，将这些资源暂存于PCB中，等到CPU调度执行时，再恢复到CPU中。\n什么是线程上下文切换？ 线程切换分为进程内线程切换和进程间线程切换。\n进程内线程切换\n进程内部共享虚拟地址空间，全局变量等资源，仅仅独享一部分堆栈和寄存器。切换的时候，只需要暂存进程内线程的私有数据即可。\n进程间线程切换\n同进程切换\n发生进程上下文切换有哪些场景？ CPU时间片用完或者被CPU调度执行 系统资源不足，运行进程被挂起，让出CPU执行权 进程主动调用睡眠函数让自己挂起 更高优先级进程打断当前进程执行 中断执行，当前进程让出CPU执行权 什么是线程？ 线程就是进程中的一条执行流程，是CPU执行的基本单位。\n为什么会有线程？ 进程之间通过虚拟内存隔离，无法直接共享内部数据 进程上下文切换、资源回收与创建等管理成本较高 单个进程一个时间点只能做一件事，无法做到并发 线程的实现方式 用户级线程：在用户空间实现的线程，不是由内核管理的，而是由用户态线程库函数管理的，操作系统无法感知TCB的存在。 内核级线程：在内核空间实现的线程，是由内核管理的。 1. 2. 多核CPU，内核能同时调度同一进程的多个线程映射到多个CPU核上并行运行，做到并行 轻量级进程（LWP）：在内核中支持用户线程，与内核线程一一对应。所以，LWP的前提是拥有内核线程。 用户级线程是什么？有什么优缺点？ 用户级线程是在用户空间由线程库函数管理的，内核不参与管理，也并不知道TCB。用户级线程就相当于多个用户线程对应一个LWP，一个LWP对应内核线程。\n缺点\n而且同一进程中只能同时有一个用户级线程在运行，如果有一个用户级线程使用了系统调用而阻塞，那么整个进程都会被挂起，可以节约更多的系统资源。所以，用户级线程模型不具有并发性能。 用户级线程执行后，OS由于不能感知TCB的存在，无法打断执行，而且进程内的其他线程也无法打断，只能等待用户级线程自己交出CPU执行权 用户级线程是用户空间进程内的多个线程之一，得到的CPU时间片较短，执行慢 优点\n用户级线程管理无需内核参与，减少内核管理线程上下文切换的开销；\n内核级线程是什么？有什么优缺点？ 内核级现车给是在内核空间实现的。内核线程对应一个LWP，一个LWP对应一个用户线程。\n缺点 应用程序的运行在用户态，而线程的切换创建等管理工作交由内核完成，会有线程上下文切换的开销； 优点 内核中的线程阻塞，还可以调度其他就绪线程执行，不会发生阻塞 分配给线程，多线程的进程获得更多的 CPU 运行时间； 进程调度算法有哪些？ FIFO\n先来先服务。每次从就绪队列里调度最先进入就绪队列的进程执行，执行完毕再调度下一个次先进入就绪队列的进程执行。\n**缺点：**如果长进程先进入就绪队列，会造成短进程周转时间变长，引发饥饿。\n**优点：**对长进程有利，适合于CPU繁忙型的系统\nSJF\n最短作业优先调度算法。每次从就绪队列调度执行时间最短的进程执行，执行完毕再调度下一个次短的进程执行\n**缺点：**会对长进程造成饥饿问题\n**优点：**利于短进程，提高系统吞吐量（单位时间完成的进程数）\nHRRN\n高响应比优先调度算法。综合前两种算法优劣，每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，「响应比优先级」的计算公式：优先权=（等待时间+要求服务时间）/要求服务时间\n**优点：**等待时间越长，优先级越高（利于长进程）；要求服务时间越短，优先级越高（利于短进程）\n**缺点：**很难估计等待时间，算法实现难度大，几乎没有现成的实现\nRR\n时间片轮转调度算法。最公平的调度算法，就绪队列里的每个进程没有优先级，依次获得相同CPU时间片使用权，时间片用完后或提前结束运行，就会调度下一个线程执行。\n**优点：**不会出现进程的饥饿问题\n**缺点：**没有优先级，无法即时响应紧急任务\nHPF\n最高优先级调度算法。从就绪队列中选择最高优先级的进程调度并执行。\n进程的优先级可以分为，静态优先级和动态优先级：\n静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化； 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是随着时间的推移增加等待进程的优先级。 该算法也有两种处理优先级高的方法，非抢占式和抢占式：\n非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。 **缺点：**可能会导致低优先级的进程饥饿，甚至无法被调度执行\n**优点：**能及时响应紧急任务\nMFQ\n多级反馈队列调度算法。该调度算法是综合最高优先级和时间片轮转调度算法的优劣。\n**多级：**多个就绪队列，每个队列优先级从高到低，同时优先级越高时间片越短\n**反馈：**新的高优先级进入就绪队列，CPU停止执行当前进程，并将其移到当前就绪队列尾部，并调度新进程执行。\n过程：\n多个就绪队列，从高到低优先级降低，并且优先级越高，时间片越短 新的进程会被放到第一级队列末尾，按照先来先服务原则执行，时间片用完后还没执行完毕就将其放到下一级就绪队列 当较高优先级就绪队列执行完毕，则下一级就绪队列开始执行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行 进程有哪些通信方式？ 管道|\n管道分匿名管道和命名管道，本质上就是内核的一段缓存，只能单向写入或读出如果需要进程间相互通信，需要创建两个管道A和B，管道A写数据到管道B，管道B从管道A读数据。而且写入管道数据后，会阻塞到读取数据完毕\n优点：操作简单\n缺点：因为管道本质上是内核的一段缓存，所以需要进程的上下文切换成本和内核态与用户态数据拷贝的开销，再加上管道读写是同步的，导致通信效率低，不适合进程间频繁的数据交换。\n消息队列\n消息队列指内核里的消息链表。消息链表的节点单元是消息体，不能无限制存储数据，大小限制。如果进程读取了消息体，内核会把这个消息体删除。消息队列与管道通信不同，进程间的通信是异步的：进程A将数据放到消息体，进程B找个时间来读取，读取之后从消息链表里删除。\n缺点：通信不及时；不能传输太大的数据；和管道一样，都是内核数据，会存在进程上下文切换和内核态与用户态数据拷贝。\n共享内存\n共享内存指的是共享物理内存。将进程中的虚拟地址映射为同一块物理地址，那么就可以消除管道和消息队列通信的内核态与用户态的数据拷贝过程。\n优点：消除内核态与用户态之间的数据拷贝\n缺点：多进程并发读写共享内存会导致冲突\n信号量\n信号量其实是一个整形计数器，大小就是资源数目。\n信号量分两种：\n互斥信号量：信号量值为1，实现对资源的互斥访问。 同步信号量：信号量值为0，控制进程之间的执行顺序。 信号量还有两种原子操作：\nP操作：将当前信号量-1，如果当前信号量\u0026lt;0，说明资源已被占用，则会阻塞当前进程；如果当前信号量\u0026gt;=0，说明资源可用，不会阻塞 V操作：将当前信号量+1，如果当前信号量\u0026lt;=0，则说明有进程阻塞，那就去唤醒阻塞的进程；如果当前信号量\u0026gt;0，说明没有进程阻塞，继续执行 信号\n内核里的管道和消息队列、共享内存以及信号量都是系统正常情况下的进程之间常用的通信方式，那么异常情况下，则使用信号来通知进程。例如linux中，kill -9 pid指的就是杀死进程id为pid的进程，-9指的是信号SIGKILL。\n用户进程可以对信号有几种处理方式：\n执行默认操作。进程收到信号过后都会有默认的操作 捕捉信号。可以为信号设置信号处理函数，等到信号发生时就触发处理函数 忽略信号。收到信号什么都不做，但是有些信号不能忽略，例如SIGKILL和SIGSTOP socket\n套接字，与前面的所有通信方式不同，socket不仅可以实现主机内部进程的通信，也可以实现不同主机进程之间的通信。\nsocket有几种实现\n字节流（TCP）：TCP/IP协议栈，面向连接的，需要bind\\listen\\connect\\accept 数据报（UDP）：UDP协议，面向无连接的，只需要bind端口。 原始套接字：相比于字节流和数据报更加底层，原始套接字可以直接收发MAC帧数据，没有IP头和TCP或UDP头。 线程有哪些通信方式？ 同进程内的线程是共享资源的。\n全局变量\n锁\n加锁操作可以解决并发线程/进程的互斥问题。拿到锁才能访问临界区资源，否则只能阻塞。\n信号量\n信号量可以实现进程/线程的同步与互斥。\n同步：先执行的线程/进程使用V操作，后执行的线程/进程使用P操作\n互斥：线程/进程进入临界区前先执行P操作，锁住临界资源，离开临界区前再执行V操作，释放临界资源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // @author cold bin // @date 2023/8/8 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/sync/semaphore\u0026#34; \u0026#34;sync\u0026#34; ) var ( resource = 0 sig = semaphore.NewWeighted(1) // 初始化互斥信号量 wg sync.WaitGroup ) func main() { for i := 0; i \u0026lt; 1000; i++ { go inrResource(1) wg.Add(1) } wg.Wait() fmt.Println(resource) } func inrResource(offset int) { err := sig.Acquire(context.Background(), 1) // p操作 if err != nil { return } resource += offset sig.Release(1) // v操作 wg.Done() } 多线程冲突了怎么办？ 多线程访问共享资源发生冲突，我们可以采取：\n进程内的锁 信号量 原子操作 什么是死锁？ 在两个及两个以上的线程在执行过程中，由于竞争资源或彼此通信而造成的阻塞现象，若无外力作用，它们无法继续执行下去。此时称死锁。\n死锁如何预防或解除？ 死锁的产生有四个必要不充分条件，也就是说，满足下面四个条件才可能发生死锁，不满足一定不会死锁：\n互斥条件：多个线程不能同时使用一个资源 持有并等待条件：某个线程已经持有了资源，再去请求另一个资源时阻塞，不会释放自己持有的资源 不可剥夺条件：当线程持有了资源，在自己使用完毕前不能被其他线程获取 环路等待条件：多个线程之间资源等待图构成环路 预防死锁 预防指的是，所有线程或进程运行之前。\n我们只需要破坏上面任意一个条件，即可预防死锁的产生：\n破坏互斥条件：将资源互斥访问改成资源共享访问\n缺点：不是所有资源都可以共享。某些资源无法通过这种方式来预防死锁\n破坏持有并等待条件：①一次性把所有资源申请到位；②每使用完一个资源就及时释放它\n缺点：第一种方式会造成资源浪费，其他线程会饥饿；第二种方式可以很好解决饥饿的问题\n破坏不可剥夺条件：线程持有资源后，在没释放之前可以被其他线程剥夺使用。\n破坏循环等待条件：避免资源等待图出现环路。\n避免死锁 避免指的是，所有线程或进程已经运行但还没有发生死锁。\n银行家算法 解除死锁 解除是指死锁已经发生。我们可以补救：\n进程终止法：出现死锁，kill掉当前进程 资源剥夺法：出现死锁也就意味着循环资源等待图存在环路，我们找出构成环路的成本最小的路径，释放掉对应的请求资源即可 进程回退法：保存进程历史快照，回滚到足以避免死锁的快照版本 有哪些常用的锁？ 互斥锁：线程抢锁失败时会释放CPU，然后由内核将其调度到阻塞队列里，直到锁释放，才会在一定的时机去唤醒锁。这个过程会涉及到线程的上下文切换。而且，只要抢锁失败的线程就会多经历两次线程上下文切换。所以，互斥锁针对高并发场景会有频繁的上下文切换。 自旋锁：线程抢锁失败时不会释放CPU，会一直在CPU上空转等待锁释放并拿到锁。自旋锁相较于互斥锁而言，没有线程上下文切换的开销，但是会占用CPU资源，适用于自旋等待时间（临界区代码执行时间）要少于线程上下文切换的时间时，可以考虑使用自旋锁来代替互斥锁，否则，使用互斥锁更佳。但是瞬间太多线程自旋也会造成CPU压力暴增。 读锁：当写锁没有被线程持有时，多个线程能够并发地抢到读锁，但不能抢写锁 写锁：当写锁被某个线程持有时，多个线程不能并发抢读锁和写锁 悲观锁：总是认为数据修改经常发生。线程访问临界区之前，会事先加锁，防止冲突发生。自旋锁、互斥锁和读写锁都是悲观锁 乐观锁：总是认为数据修改不经常发生。线程修改数据之前，会携带版本号，如果与临界区资源版本号一致则可以修改并更新版本号；不一致则更新失败，放弃操作等待重试。 互斥锁和自旋锁是底层实现。读锁、写锁都是基于这两个实现的。\n锁的开销在哪里？ 互斥锁：每一个线程抢锁失败就会多两次线程上下文切换。\n并发线程数为1000的场景，使用互斥锁，最好的情况下也有2000次线程上下文切换。每次上下文切换几us，所以总计差不多2ms的成本。\n并发线程数为10000的场景，使用互斥锁，最好的情况下也有20000次线程上下文切换。每次上下文切换几us，所以总计差不多20ms的成本。\n并发线程数为100000的场景，使用互斥锁，最好的情况下也有200000次线程上下文切换。每次上下文切换几us，所以总计差不多200ms的成本。\n并发线程数为1000000的场景，使用互斥锁，最好的情况下也有2000000次线程上下文切换。每次上下文切换几us，所以总计差不多2000ms的成本。\n自旋锁：线程抢锁失败会一直在CPU上空转，增加CPU负担。\n读写锁：适合读多写少的场景\n乐观锁：不适合写多读少的场景，因为会造成大量线程执行失败重试的情况。\n如何做到进程的优雅关机? 捕获外部信号kill，不能捕获kill -9\n线程崩溃，进程也会崩溃吗？ 线程崩溃，可能是因为非法内存访问等错误。由于进程内的线程共享资源和地址空间，所以就可能会导致其他线程持有的资源被非法访问。所以，进程会收到SIGSEGV信号表示非法内存，然后这个信号内核有默认的handler就是kill掉出现非法访问内存的bug。所以，看具体实现。\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/","tags":["操作系统进程管理"],"title":"八股系列|操作系统|进程管理其一"},{"categories":["八股文","操作系统","linux"],"contents":" 来源于小林coding\nlinux虚拟内存管理 4.6 深入理解 Linux 虚拟内存管理 这一篇会比较硬核，是比较全面的一篇 Linux 虚拟内存管理的文章，文章多达 3.5 万字 + 60 张图，耐心读下去，肯定对 Linux 虚拟内存管理有很深刻的理解！\n开车！\n内存管理子系统可谓是 Linux 内核众多子系统中最为复杂最为庞大的一个，其中包含了众多繁杂的概念和原理，通过内存管理这条主线我们把可以把操作系统的众多核心系统给拎出来，比如：进程管理子系统，网络子系统，文件子系统等。\n由于内存管理子系统过于复杂庞大，其中涉及到的众多繁杂的概念又是一环套一环，层层递进。如何把这些繁杂的概念具有层次感地，并且清晰地，给大家梳理呈现出来真是一件比较有难度的事情，因此关于这个问题，我在动笔写这个内存管理源码解析系列之前也是思考了很久。\n万事开头难，那么到底什么内容适合作为这个系列的开篇呢 ？我还是觉得从大家日常开发工作中接触最多最为熟悉的部分开始比较好，比如：在我们日常开发中创建的类，调用的函数，在函数中定义的局部变量以及 new 出来的数据容器（Map，List，Set \u0026hellip;..等）都需要存储在物理内存中的某个角落。\n而我们在程序中编写业务逻辑代码的时候，往往需要引用这些创建出来的数据结构，并通过这些引用对相关数据结构进行业务处理。\n当程序运行起来之后就变成了进程，而这些业务数据结构的引用在进程的视角里全都都是虚拟内存地址，因为进程无论是在用户态还是在内核态能够看到的都是虚拟内存空间，物理内存空间被操作系统所屏蔽进程是看不到的。\n进程通过虚拟内存地址访问这些数据结构的时候，虚拟内存地址会在内存管理子系统中被转换成物理内存地址，通过物理内存地址就可以访问到真正存储这些数据结构的物理内存了。随后就可以对这块物理内存进行各种业务操作，从而完成业务逻辑。\n那么到底什么是虚拟内存地址 ？ Linux 内核为啥要引入虚拟内存而不直接使用物理内存 ？ 虚拟内存空间到底长啥样？ 内核如何管理虚拟内存？ 什么又是物理内存地址 ？如何访问物理内存？ 本文我就来为大家详细一一解答上述几个问题，让我们马上开始吧~~~~\n#1. 到底什么是虚拟内存地址 首先人们提出地址这个概念的目的就是用来方便定位现实世界中某一个具体事物的真实地理位置，它是一种用于定位的概念模型。\n举一个生活中的例子，比如大家在日常生活中给亲朋好友邮寄一些本地特产时，都会填写收件人地址以及寄件人地址。以及在日常网上购物时，都会在相应电商 APP 中填写自己的收货地址。\n随后快递小哥就会根据我们填写的收货地址找到我们的真实住所，将我们网购的商品送达到我们的手里。\n收货地址是用来定位我们在现实世界中真实住所地理位置的，而现实世界中我们所在的城市，街道，小区，房屋都是一砖一瓦，一草一木真实存在的。但收货地址这个概念模型在现实世界中并不真实存在，它只是人们提出的一个虚拟概念，通过收货地址这个虚拟概念将它和现实世界真实存在的城市，小区，街道的地理位置一一映射起来，这样我们就可以通过这个虚拟概念来找到现实世界中的具体地理位置。\n综上所述，收货地址是一个虚拟地址，它是人为定义的，而我们的城市，小区，街道是真实存在的，他们的地理位置就是物理地址。\n比如现在的广东省深圳市在过去叫宝安县，河北省的石家庄过去叫常山，安徽省的合肥过去叫泸州。不管是常山也好，石家庄也好，又或是合肥也好，泸州也罢，这些都是人为定义的名字而已，但是地方还是那个地方，它所在的地理位置是不变的。也就说虚拟地址可以人为的变来变去，但是物理地址永远是不变的。\n现在让我们把视角在切换到计算机的世界，在计算机的世界里内存地址用来定义数据在内存中的存储位置的，内存地址也分为虚拟地址和物理地址。而虚拟地址也是人为设计的一个概念，类比我们现实世界中的收货地址，而物理地址则是数据在物理内存中的真实存储位置，类比现实世界中的城市，街道，小区的真实地理位置。\n说了这么多，那么到底虚拟内存地址长什么样子呢？\n我们还是以日常生活中的收货地址为例做出类比，我们都很熟悉收货地址的格式：xx省xx市xx区xx街道xx小区xx室，它是按照地区层次递进的。同样，在计算机世界中的虚拟内存地址也有这样的递进关系。\n这里我们以 Intel Core i7 处理器为例，64 位虚拟地址的格式为：全局页目录项（9位）+ 上层页目录项（9位）+ 中间页目录项（9位）+ 页表项（9位）+ 页内偏移（12位）。共 48 位组成的虚拟内存地址。\n虚拟内存地址中的全局页目录项就类比我们日常生活中收获地址里的省，上层页目录项就类比市，中间层页目录项类比区县，页表项类比街道小区，页内偏移类比我们所在的楼栋和几层几号。\n这里大家只需要大体明白虚拟内存地址到底长什么样子，它的格式是什么，能够和日常生活中的收货地址对比理解起来就可以了，至于页目录项，页表项以及页内偏移这些计算机世界中的概念，大家暂时先不用管，后续文章中我会慢慢给大家解释清楚。\n32 位虚拟地址的格式为：页目录项（10位）+ 页表项（10位） + 页内偏移（12位）。共 32 位组成的虚拟内存地址。\n进程虚拟内存空间中的每一个字节都有与其对应的虚拟内存地址，一个虚拟内存地址表示进程虚拟内存空间中的一个特定的字节。\n#2. 为什么要使用虚拟地址访问内存 经过第一小节的介绍，我们现在明白了计算机世界中的虚拟内存地址的含义及其展现形式。那么大家可能会问了，既然物理内存地址可以直接定位到数据在内存中的存储位置，那为什么我们不直接使用物理内存地址去访问内存而是选择用虚拟内存地址去访问内存呢？\n在回答大家的这个疑问之前，让我们先来看下，如果在程序中直接使用物理内存地址会发生什么情况？\n假设现在没有虚拟内存地址，我们在程序中对内存的操作全都都是使用物理内存地址，在这种情况下，程序员就需要精确的知道每一个变量在内存中的具体位置，我们需要手动对物理内存进行布局，明确哪些数据存储在内存的哪些位置，除此之外我们还需要考虑为每个进程究竟要分配多少内存？内存紧张的时候该怎么办？如何避免进程与进程之间的地址冲突？等等一系列复杂且琐碎的细节。\n如果我们在单进程系统中比如嵌入式设备上开发应用程序，系统中只有一个进程，这单个进程独享所有的物理资源包括内存资源。在这种情况下，上述提到的这些直接使用物理内存的问题可能还好处理一些，但是仍然具有很高的开发门槛。\n然而在现代操作系统中往往支持多个进程，需要处理多进程之间的协同问题，在多进程系统中直接使用物理内存地址操作内存所带来的上述问题就变得非常复杂了。\n这里我为大家举一个简单的例子来说明在多进程系统中直接使用物理内存地址的复杂性。\n比如我们现在有这样一个简单的 Java 程序。\n1 2 3 4 5 public static void main(String[] args) throws Exception { string i = args[0]; .......... } 在程序代码相同的情况下，我们用这份代码同时启动三个 JVM 进程，我们暂时将进程依次命名为 a , b , c 。\n这三个进程用到的代码是一样的，都是我们提前写好的，可以被多次运行。由于我们是直接操作物理内存地址，假设变量 i 保存在 0x354 这个物理地址上。这三个进程运行起来之后，同时操作这个 0x354 物理地址，这样这个变量 i 的值不就混乱了吗？ 三个进程就会出现变量的地址冲突。\n所以在直接操作物理内存的情况下，我们需要知道每一个变量的位置都被安排在了哪里，而且还要注意和多个进程同时运行的时候，不能共用同一个地址，否则就会造成地址冲突。\n现实中一个程序会有很多的变量和函数，这样一来我们给它们都需要计算一个合理的位置，还不能与其他进程冲突，这就很复杂了。\n那么我们该如何解决这个问题呢？程序的局部性原理再一次救了我们~~\n程序局部性原理表现为：时间局部性和空间局部性。时间局部性是指如果程序中的某条指令一旦执行，则不久之后该指令可能再次被执行；如果某块数据被访问，则不久之后该数据可能再次被访问。空间局部性是指一旦程序访问了某个存储单元，则不久之后，其附近的存储单元也将被访问。\n从程序局部性原理的描述中我们可以得出这样一个结论：进程在运行之后，对于内存的访问不会一下子就要访问全部的内存，相反进程对于内存的访问会表现出明显的倾向性，更加倾向于访问最近访问过的数据以及热点数据附近的数据。\n根据这个结论我们就清楚了，无论一个进程实际可以占用的内存资源有多大，根据程序局部性原理，在某一段时间内，进程真正需要的物理内存其实是很少的一部分，我们只需要为每个进程分配很少的物理内存就可以保证进程的正常执行运转。\n而虚拟内存的引入正是要解决上述的问题，虚拟内存引入之后，进程的视角就会变得非常开阔，每个进程都拥有自己独立的虚拟地址空间，进程与进程之间的虚拟内存地址空间是相互隔离，互不干扰的。每个进程都认为自己独占所有内存空间，自己想干什么就干什么。\n系统上还运行了哪些进程和我没有任何关系。这样一来我们就可以将多进程之间协同的相关复杂细节统统交给内核中的内存管理模块来处理，极大地解放了程序员的心智负担。这一切都是因为虚拟内存能够提供内存地址空间的隔离，极大地扩展了可用空间。\n这样进程就以为自己独占了整个内存空间资源，给进程产生了所有内存资源都属于它自己的幻觉，这其实是 CPU 和操作系统使用的一个障眼法罢了，任何一个虚拟内存里所存储的数据，本质上还是保存在真实的物理内存里的。只不过内核帮我们做了虚拟内存到物理内存的这一层映射，将不同进程的虚拟地址和不同内存的物理地址映射起来。\n当 CPU 访问进程的虚拟地址时，经过地址翻译硬件将虚拟地址转换成不同的物理地址，这样不同的进程运行的时候，虽然操作的是同一虚拟地址，但其实背后写入的是不同的物理地址，这样就不会冲突了。\n#3. 进程虚拟内存空间 上小节中，我们介绍了为了防止多进程运行时造成的内存地址冲突，内核引入了虚拟内存地址，为每个进程提供了一个独立的虚拟内存空间，使得进程以为自己独占全部内存资源。\n那么这个进程独占的虚拟内存空间到底是什么样子呢？在本小节中，我就为大家揭开这层神秘的面纱~~~\n在本小节内容开始之前，我们先想象一下，如果我们是内核的设计人员，我们该从哪些方面来规划进程的虚拟内存空间呢？\n本小节我们只讨论进程用户态虚拟内存空间的布局，我们先把内核态的虚拟内存空间当做一个黑盒来看待，在后面的小节中我再来详细介绍内核态相关内容。\n首先我们会想到的是一个进程运行起来是为了执行我们交代给进程的工作，执行这些工作的步骤我们通过程序代码事先编写好，然后编译成二进制文件存放在磁盘中，CPU 会执行二进制文件中的机器码来驱动进程的运行。所以在进程运行之前，这些存放在二进制文件中的机器码需要被加载进内存中，而用于存放这些机器码的虚拟内存空间叫做代码段。\n在程序运行起来之后，总要操作变量吧，在程序代码中我们通常会定义大量的全局变量和静态变量，这些全局变量在程序编译之后也会存储在二进制文件中，在程序运行之前，这些全局变量也需要被加载进内存中供程序访问。所以在虚拟内存空间中也需要一段区域来存储这些全局变量。\n那些在代码中被我们指定了初始值的全局变量和静态变量在虚拟内存空间中的存储区域我们叫做数据段。 那些没有指定初始值的全局变量和静态变量在虚拟内存空间中的存储区域我们叫做 BSS 段。这些未初始化的全局变量被加载进内存之后会被初始化为 0 值。 上面介绍的这些全局变量和静态变量都是在编译期间就确定的，但是我们程序在运行期间往往需要动态的申请内存，所以在虚拟内存空间中也需要一块区域来存放这些动态申请的内存，这块区域就叫做堆。注意这里的堆指的是 OS 堆并不是 JVM 中的堆。\n除此之外，我们的程序在运行过程中还需要依赖动态链接库，这些动态链接库以 .so 文件的形式存放在磁盘中，比如 C 程序中的 glibc，里边对系统调用进行了封装。glibc 库里提供的用于动态申请堆内存的 malloc 函数就是对系统调用 sbrk 和 mmap 的封装。这些动态链接库也有自己的对应的代码段，数据段，BSS 段，也需要一起被加载进内存中。\n还有用于内存文件映射的系统调用 mmap，会将文件与内存进行映射，那么映射的这块内存（虚拟内存）也需要在虚拟地址空间中有一块区域存储。\n这些动态链接库中的代码段，数据段，BSS 段，以及通过 mmap 系统调用映射的共享内存区，在虚拟内存空间的存储区域叫做文件映射与匿名映射区。\n最后我们在程序运行的时候总该要调用各种函数吧，那么调用函数过程中使用到的局部变量和函数参数也需要一块内存区域来保存。这一块区域在虚拟内存空间中叫做栈。\n现在进程的虚拟内存空间所包含的主要区域，我就为大家介绍完了，我们看到内核根据进程运行的过程中所需要不同种类的数据而为其开辟了对应的地址空间。分别为：\n用于存放进程程序二进制文件中的机器指令的代码段 用于存放程序二进制文件中定义的全局变量和静态变量的数据段和 BSS 段。 用于在程序运行过程中动态申请内存的堆。 用于存放动态链接库以及内存映射区域的文件映射与匿名映射区。 用于存放函数调用过程中的局部变量和函数参数的栈。 以上就是我们通过一个程序在运行过程中所需要的数据所规划出的虚拟内存空间的分布，这些只是一个大概的规划，那么在真实的 Linux 系统中，进程的虚拟内存空间的具体规划又是如何的呢？我们接着往下看~~\n#4. Linux 进程虚拟内存空间 在上小节中我们介绍了进程虚拟内存空间中各个内存区域的一个大概分布，在此基础之上，本小节我就带大家分别从 32 位 和 64 位机器上看下在 Linux 系统中进程虚拟内存空间的真实分布情况。\n#4.1 32 位机器上进程虚拟内存空间分布 在 32 位机器上，指针的寻址范围为 2^32，所能表达的虚拟内存空间为 4 GB。所以在 32 位机器上进程的虚拟内存地址范围为：0x0000 0000 - 0xFFFF FFFF。\n其中用户态虚拟内存空间为 3 GB，虚拟内存地址范围为：0x0000 0000 - 0xC000 000 。\n内核态虚拟内存空间为 1 GB，虚拟内存地址范围为：0xC000 000 - 0xFFFF FFFF。\n但是用户态虚拟内存空间中的代码段并不是从 0x0000 0000 地址开始的，而是从 0x0804 8000 地址开始。\n0x0000 0000 到 0x0804 8000 这段虚拟内存地址是一段不可访问的保留区，因为在大多数操作系统中，数值比较小的地址通常被认为不是一个合法的地址，这块小地址是不允许访问的。比如在 C 语言中我们通常会将一些无效的指针设置为 NULL，指向这块不允许访问的地址。\n保留区的上边就是代码段和数据段，它们是从程序的二进制文件中直接加载进内存中的，BSS 段中的数据也存在于二进制文件中，因为内核知道这些数据是没有初值的，所以在二进制文件中只会记录 BSS 段的大小，在加载进内存时会生成一段 0 填充的内存空间。\n紧挨着 BSS 段的上边就是我们经常使用到的堆空间，从图中的红色箭头我们可以知道在堆空间中地址的增长方向是从低地址到高地址增长。\n内核中使用 start_brk 标识堆的起始位置，brk 标识堆当前的结束位置。当堆申请新的内存空间时，只需要将 brk 指针增加对应的大小，回收地址时减少对应的大小即可。比如当我们通过 malloc 向内核申请很小的一块内存时（128K 之内），就是通过改变 brk 位置实现的。\n堆空间的上边是一段待分配区域，用于扩展堆空间的使用。接下来就来到了文件映射与匿名映射区域。进程运行时所依赖的动态链接库中的代码段，数据段，BSS 段就加载在这里。还有我们调用 mmap 映射出来的一段虚拟内存空间也保存在这个区域。注意：在文件映射与匿名映射区的地址增长方向是从高地址向低地址增长。\n接下来用户态虚拟内存空间的最后一块区域就是栈空间了，在这里会保存函数运行过程所需要的局部变量以及函数参数等函数调用信息。栈空间中的地址增长方向是从高地址向低地址增长。每次进程申请新的栈地址时，其地址值是在减少的。\n在内核中使用 start_stack 标识栈的起始位置，RSP 寄存器中保存栈顶指针 stack pointer，RBP 寄存器中保存的是栈基地址。\n在栈空间的下边也有一段待分配区域用于扩展栈空间，在栈空间的上边就是内核空间了，进程虽然可以看到这段内核空间地址，但是就是不能访问。这就好比我们在饭店里虽然可以看到厨房在哪里，但是厨房门上写着 “厨房重地，闲人免进” ，我们就是进不去。\n#4.2 64 位机器上进程虚拟内存空间分布 上小节中介绍的 32 位虚拟内存空间布局和本小节即将要介绍的 64 位虚拟内存空间布局都可以通过 cat /proc/pid/maps 或者 pmap pid 来查看某个进程的实际虚拟内存布局。\n我们知道在 32 位机器上，指针的寻址范围为 2^32，所能表达的虚拟内存空间为 4 GB。\n那么我们理所应当的会认为在 64 位机器上，指针的寻址范围为 2^64，所能表达的虚拟内存空间为 16 EB 。虚拟内存地址范围为：0x0000 0000 0000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。\n好家伙 !!! 16 EB 的内存空间，我都没见过这么大的磁盘，在现实情况中根本不会用到这么大范围的内存空间，\n事实上在目前的 64 位系统下只使用了 48 位来描述虚拟内存空间，寻址范围为 2^48 ，所能表达的虚拟内存空间为 256TB。\n其中低 128 T 表示用户态虚拟内存空间，虚拟内存地址范围为：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000 。\n高 128 T 表示内核态虚拟内存空间，虚拟内存地址范围为：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。\n这样一来就在用户态虚拟内存空间与内核态虚拟内存空间之间形成了一段 0x0000 7FFF FFFF F000 - 0xFFFF 8000 0000 0000 的地址空洞，我们把这个空洞叫做 canonical address 空洞。\n那么这个 canonical address 空洞是如何形成的呢？\n我们都知道在 64 位机器上的指针寻址范围为 2^64，但是在实际使用中我们只使用了其中的低 48 位来表示虚拟内存地址，那么这多出的高 16 位就形成了这个地址空洞。\n大家注意到在低 128T 的用户态地址空间：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000 范围中，所以虚拟内存地址的高 16 位全部为 0 。\n如果一个虚拟内存地址的高 16 位全部为 0 ，那么我们就可以直接判断出这是一个用户空间的虚拟内存地址。\n同样的道理，在高 128T 的内核态虚拟内存空间：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 范围中，所以虚拟内存地址的高 16 位全部为 1 。\n也就是说内核态的虚拟内存地址的高 16 位全部为 1 ，如果一个试图访问内核的虚拟地址的高 16 位不全为 1 ，则可以快速判断这个访问是非法的。\n这个高 16 位的空闲地址被称为 canonical 。如果虚拟内存地址中的高 16 位全部为 0 （表示用户空间虚拟内存地址）或者全部为 1 （表示内核空间虚拟内存地址），这种地址的形式我们叫做 canonical form，对应的地址我们称作 canonical address 。\n那么处于 canonical address 空洞 ：0x0000 7FFF FFFF F000 - 0xFFFF 8000 0000 0000 范围内的地址的高 16 位 不全为 0 也不全为 1 。如果某个虚拟地址落在这段 canonical address 空洞区域中，那就是既不在用户空间，也不在内核空间，肯定是非法访问了。\n未来我们也可以利用这块 canonical address 空洞，来扩展虚拟内存地址的范围，比如扩展到 56 位。\n在我们理解了 canonical address 这个概念之后，我们再来看下 64 位 Linux 系统下的真实虚拟内存空间布局情况：\n从上图中我们可以看出 64 位系统中的虚拟内存布局和 32 位系统中的虚拟内存布局大体上是差不多的。主要不同的地方有三点：\n就是前边提到的由高 16 位空闲地址造成的 canonical address 空洞。在这段范围内的虚拟内存地址是不合法的，因为它的高 16 位既不全为 0 也不全为 1，不是一个 canonical address，所以称之为 canonical address 空洞。 在代码段跟数据段的中间还有一段不可以读写的保护段，它的作用是防止程序在读写数据段的时候越界访问到代码段，这个保护段可以让越界访问行为直接崩溃，防止它继续往下运行。 用户态虚拟内存空间与内核态虚拟内存空间分别占用 128T，其中低128T 分配给用户态虚拟内存空间，高 128T 分配给内核态虚拟内存空间。 #5. 进程虚拟内存空间的管理 在上一小节中，我为大家介绍了 Linux 操作系统在 32 位机器上和 64 位机器上进程虚拟内存空间的布局分布，我们发现无论是在 32 位机器上还是在 64 位机器上，进程虚拟内存空间的核心区域分布的相对位置是不变的，它们都包含下图所示的这几个核心内存区域。\n唯一不同的是这些核心内存区域在 32 位机器和 64 位机器上的绝对位置分布会有所不同。\n那么在此基础之上，内核如何为进程管理这些虚拟内存区域呢？这将是本小节重点为大家介绍的内容~~\n既然我们要介绍进程的虚拟内存空间管理，那就离不开进程在内核中的描述符 task_struct 结构。\n1 2 3 4 5 6 7 8 9 10 11 12 struct task_struct { // 进程id pid_t\tpid; // 用于标识线程所属的进程 pid pid_t\ttgid; // 进程打开的文件信息 struct files_struct\t*files; // 内存描述符表示进程虚拟地址空间 struct mm_struct\t*mm; .......... 省略 ....... } 在进程描述符 task_struct 结构中，有一个专门描述进程虚拟地址空间的内存描述符 mm_struct 结构，这个结构体中包含了前边几个小节中介绍的进程虚拟内存空间的全部信息。\n每个进程都有唯一的 mm_struct 结构体，也就是前边提到的每个进程的虚拟地址空间都是独立，互不干扰的。\n当我们调用 fork() 函数创建进程的时候，表示进程地址空间的 mm_struct 结构会随着进程描述符 task_struct 的创建而创建。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 long _do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, unsigned long tls) { ......... 省略 .......... struct pid *pid; struct task_struct *p; ......... 省略 .......... // 为进程创建 task_struct 结构，用父进程的资源填充 task_struct 信息 p = copy_process(clone_flags, stack_start, stack_size, child_tidptr, NULL, trace, tls, NUMA_NO_NODE); ......... 省略 .......... } 随后会在 copy_process 函数中创建 task_struct 结构，并拷贝父进程的相关资源到新进程的 task_struct 结构里，其中就包括拷贝父进程的虚拟内存空间 mm_struct 结构。这里可以看出子进程在新创建出来之后它的虚拟内存空间是和父进程的虚拟内存空间一模一样的，直接拷贝过来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 static __latent_entropy struct task_struct *copy_process( unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *child_tidptr, struct pid *pid, int trace, unsigned long tls, int node) { struct task_struct *p; // 创建 task_struct 结构 p = dup_task_struct(current, node); ....... 初始化子进程 ........... ....... 开始继承拷贝父进程资源 ....... // 继承父进程打开的文件描述符 retval = copy_files(clone_flags, p); // 继承父进程所属的文件系统 retval = copy_fs(clone_flags, p); // 继承父进程注册的信号以及信号处理函数 retval = copy_sighand(clone_flags, p); retval = copy_signal(clone_flags, p); // 继承父进程的虚拟内存空间 retval = copy_mm(clone_flags, p); // 继承父进程的 namespaces retval = copy_namespaces(clone_flags, p); // 继承父进程的 IO 信息 retval = copy_io(clone_flags, p); ...........省略......... // 分配 CPU retval = sched_fork(clone_flags, p); // 分配 pid pid = alloc_pid(p-\u0026gt;nsproxy-\u0026gt;pid_ns_for_children); . ..........省略......... } 这里我们重点关注 copy_mm 函数，正是在这里完成了子进程虚拟内存空间 mm_struct 结构的的创建以及初始化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 static int copy_mm(unsigned long clone_flags, struct task_struct *tsk) { // 子进程虚拟内存空间，父进程虚拟内存空间 struct mm_struct *mm, *oldmm; int retval; ...... 省略 ...... tsk-\u0026gt;mm = NULL; tsk-\u0026gt;active_mm = NULL; // 获取父进程虚拟内存空间 oldmm = current-\u0026gt;mm; if (!oldmm) return 0; ...... 省略 ...... // 通过 vfork 或者 clone 系统调用创建出的子进程（线程）和父进程共享虚拟内存空间 if (clone_flags \u0026amp; CLONE_VM) { // 增加父进程虚拟地址空间的引用计数 mmget(oldmm); // 直接将父进程的虚拟内存空间赋值给子进程（线程） // 线程共享其所属进程的虚拟内存空间 mm = oldmm; goto good_mm; } retval = -ENOMEM; // 如果是 fork 系统调用创建出的子进程，则将父进程的虚拟内存空间以及相关页表拷贝到子进程中的 mm_struct 结构中。 mm = dup_mm(tsk); if (!mm) goto fail_nomem; good_mm: // 将拷贝出来的父进程虚拟内存空间 mm_struct 赋值给子进程 tsk-\u0026gt;mm = mm; tsk-\u0026gt;active_mm = mm; return 0; ...... 省略 ...... 由于本小节中我们举的示例是通过 fork() 函数创建子进程的情形，所以这里大家先占时忽略 if (clone_flags \u0026amp; CLONE_VM) 这个条件判断逻辑，我们先跳过往后看~~\ncopy_mm 函数首先会将父进程的虚拟内存空间 current-\u0026gt;mm 赋值给指针 oldmm。然后通过 dup_mm 函数将父进程的虚拟内存空间以及相关页表拷贝到子进程的 mm_struct 结构中。最后将拷贝出来的 mm_struct 赋值给子进程的 task_struct 结构。\n通过 fork() 函数创建出的子进程，它的虚拟内存空间以及相关页表相当于父进程虚拟内存空间的一份拷贝，直接从父进程中拷贝到子进程中。\n而当我们通过 vfork 或者 clone 系统调用创建出的子进程，首先会设置 CLONE_VM 标识，这样来到 copy_mm 函数中就会进入 if (clone_flags \u0026amp; CLONE_VM) 条件中，在这个分支中会将父进程的虚拟内存空间以及相关页表直接赋值给子进程。这样一来父进程和子进程的虚拟内存空间就变成共享的了。也就是说父子进程之间使用的虚拟内存空间是一样的，并不是一份拷贝。\n子进程共享了父进程的虚拟内存空间，这样子进程就变成了我们熟悉的线程，是否共享地址空间几乎是进程和线程之间的本质区别。Linux 内核并不区别对待它们，线程对于内核来说仅仅是一个共享特定资源的进程而已。\n内核线程和用户态线程的区别就是内核线程没有相关的内存描述符 mm_struct ，内核线程对应的 task_struct 结构中的 mm 域指向 Null，所以内核线程之间调度是不涉及地址空间切换的。\n当一个内核线程被调度时，它会发现自己的虚拟地址空间为 Null，虽然它不会访问用户态的内存，但是它会访问内核内存，聪明的内核会将调度之前的上一个用户态进程的虚拟内存空间 mm_struct 直接赋值给内核线程，因为内核线程不会访问用户空间的内存，它仅仅只会访问内核空间的内存，所以直接复用上一个用户态进程的虚拟地址空间就可以避免为内核线程分配 mm_struct 和相关页表的开销，以及避免内核线程之间调度时地址空间的切换开销。\n父进程与子进程的区别，进程与线程的区别，以及内核线程与用户态线程的区别其实都是围绕着这个 mm_struct 展开的。\n现在我们知道了表示进程虚拟内存空间的 mm_struct 结构是如何被创建出来的相关背景，那么接下来我就带大家深入 mm_struct 结构内部，来看一下内核如何通过这么一个 mm_struct 结构体来管理进程的虚拟内存空间的。\n#5.1 内核如何划分用户态和内核态虚拟内存空间 通过 《3. 进程虚拟内存空间》小节的介绍我们知道，进程的虚拟内存空间分为两个部分：一部分是用户态虚拟内存空间，另一部分是内核态虚拟内存空间。\n那么用户态的地址空间和内核态的地址空间在内核中是如何被划分的呢？\n这就用到了进程的内存描述符 mm_struct 结构体中的 task_size 变量，task_size 定义了用户态地址空间与内核态地址空间之间的分界线。\n1 2 3 struct mm_struct { unsigned long task_size;\t/* size of task vm space */ } 通过前边小节的内容介绍，我们知道在 32 位系统中用户态虚拟内存空间为 3 GB，虚拟内存地址范围为：0x0000 0000 - 0xC000 000 。\n内核态虚拟内存空间为 1 GB，虚拟内存地址范围为：0xC000 000 - 0xFFFF FFFF。\n32 位系统中用户地址空间和内核地址空间的分界线在 0xC000 000 地址处，那么自然进程的 mm_struct 结构中的 task_size 为 0xC000 000。\n我们来看下内核在 /arch/x86/include/asm/page_32_types.h 文件中关于 TASK_SIZE 的定义。\n1 2 3 4 /* * User space process size: 3GB (default). */ #define TASK_SIZE\t__PAGE_OFFSET 如下图所示：__PAGE_OFFSET 的值在 32 位系统下为 0xC000 000。\n而在 64 位系统中，只使用了其中的低 48 位来表示虚拟内存地址。其中用户态虚拟内存空间为低 128 T，虚拟内存地址范围为：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000 。\n内核态虚拟内存空间为高 128 T，虚拟内存地址范围为：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。\n64 位系统中用户地址空间和内核地址空间的分界线在 0x0000 7FFF FFFF F000 地址处，那么自然进程的 mm_struct 结构中的 task_size 为 0x0000 7FFF FFFF F000 。\n我们来看下内核在 /arch/x86/include/asm/page_64_types.h 文件中关于 TASK_SIZE 的定义。\n1 2 3 4 5 6 7 8 #define TASK_SIZE\t(test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX) #define TASK_SIZE_MAX\ttask_size_max() #define task_size_max()\t((_AC(1,UL) \u0026lt;\u0026lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE) #define __VIRTUAL_MASK_SHIFT\t47 我们来看下在 64 位系统中内核如何来计算 TASK_SIZE，在 task_size_max() 的计算逻辑中 1 左移 47 位得到的地址是 0x0000800000000000，然后减去一个 PAGE_SIZE （默认为 4K），就是 0x00007FFFFFFFF000，共 128T。所以在 64 位系统中的 TASK_SIZE 为 0x00007FFFFFFFF000 。\n这里我们可以看出，64 位虚拟内存空间的布局是和物理内存页 page 的大小有关的，物理内存页 page 默认大小 PAGE_SIZE 为 4K。\nPAGE_SIZE 定义在 /arch/x86/include/asm/page_types.h文件中：\n1 2 3 /* PAGE_SHIFT determines the page size */ #define PAGE_SHIFT\t12 #define PAGE_SIZE\t(_AC(1,UL) \u0026lt;\u0026lt; PAGE_SHIFT) 而内核空间的起始地址是 0xFFFF 8000 0000 0000 。在 0x00007FFFFFFFF000 - 0xFFFF 8000 0000 0000 之间的内存区域就是我们在 《4.2 64 位机器上进程虚拟内存空间分布》小节中介绍的 canonical address 空洞。\n#5.2 内核如何布局进程虚拟内存空间 在我们理解了内核是如何划分进程虚拟内存空间和内核虚拟内存空间之后，那么在 《3. 进程虚拟内存空间》小节中介绍的那些虚拟内存区域在内核中又是如何划分的呢？\n接下来我就为大家介绍下内核是如何划分进程虚拟内存空间中的这些内存区域的，本小节的示例图中，我只保留了进程虚拟内存空间中的核心区域，方便大家理解。\n前边我们提到，内核中采用了一个叫做内存描述符的 mm_struct 结构体来表示进程虚拟内存空间的全部信息。在本小节中我就带大家到 mm_struct 结构体内部去寻找下相关的线索。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 struct mm_struct { unsigned long task_size; /* size of task vm space */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; unsigned long mmap_base; /* base of mmap area */ unsigned long total_vm; /* Total pages mapped */ unsigned long locked_vm; /* Pages that have PG_mlocked set */ unsigned long pinned_vm; /* Refcount permanently increased */ unsigned long data_vm; /* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm; /* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm; /* VM_STACK */ ...... 省略 ........ } 内核中用 mm_struct 结构体中的上述属性来定义上图中虚拟内存空间里的不同内存区域。\nstart_code 和 end_code 定义代码段的起始和结束位置，程序编译后的二进制文件中的机器码被加载进内存之后就存放在这里。\nstart_data 和 end_data 定义数据段的起始和结束位置，二进制文件中存放的全局变量和静态变量被加载进内存中就存放在这里。\n后面紧挨着的是 BSS 段，用于存放未被初始化的全局变量和静态变量，这些变量在加载进内存时会生成一段 0 填充的内存区域 （BSS 段）， BSS 段的大小是固定的，\n下面就是 OS 堆了，在堆中内存地址的增长方向是由低地址向高地址增长， start_brk 定义堆的起始位置，brk 定义堆当前的结束位置。\n我们使用 malloc 申请小块内存时（低于 128K），就是通过改变 brk 位置调整堆大小实现的。\n接下来就是内存映射区，在内存映射区内存地址的增长方向是由高地址向低地址增长，mmap_base 定义内存映射区的起始地址。进程运行时所依赖的动态链接库中的代码段，数据段，BSS 段以及我们调用 mmap 映射出来的一段虚拟内存空间就保存在这个区域。\nstart_stack 是栈的起始位置在 RBP 寄存器中存储，栈的结束位置也就是栈顶指针 stack pointer 在 RSP 寄存器中存储。在栈中内存地址的增长方向也是由高地址向低地址增长。\narg_start 和 arg_end 是参数列表的位置， env_start 和 env_end 是环境变量的位置。它们都位于栈中的最高地址处。\n在 mm_struct 结构体中除了上述用于划分虚拟内存区域的变量之外，还定义了一些虚拟内存与物理内存映射内容相关的统计变量，操作系统会把物理内存划分成一页一页的区域来进行管理，所以物理内存到虚拟内存之间的映射也是按照页为单位进行的。这部分内容我会在后续的文章中详细介绍，大家这里只需要有个概念就行。\nmm_struct 结构体中的 total_vm 表示在进程虚拟内存空间中总共与物理内存映射的页的总数。\n注意映射这个概念，它表示只是将虚拟内存与物理内存建立关联关系，并不代表真正的分配物理内存。\n当内存吃紧的时候，有些页可以换出到硬盘上，而有些页因为比较重要，不能换出。locked_vm 就是被锁定不能换出的内存页总数，pinned_vm 表示既不能换出，也不能移动的内存页总数。\ndata_vm 表示数据段中映射的内存页数目，exec_vm 是代码段中存放可执行文件的内存页数目，stack_vm 是栈中所映射的内存页数目，这些变量均是表示进程虚拟内存空间中的虚拟内存使用情况。\n现在关于内核如何对进程虚拟内存空间进行布局的内容我们已经清楚了，那么布局之后划分出的这些虚拟内存区域在内核中又是如何被管理的呢？我们接着往下看~~~\n#5.3 内核如何管理虚拟内存区域 在上小节的介绍中，我们知道内核是通过一个 mm_struct 结构的内存描述符来表示进程的虚拟内存空间的，并通过 task_size 域来划分用户态虚拟内存空间和内核态虚拟内存空间。\n而在划分出的这些虚拟内存空间中如上图所示，里边又包含了许多特定的虚拟内存区域，比如：代码段，数据段，堆，内存映射区，栈。那么这些虚拟内存区域在内核中又是如何表示的呢？\n本小节中，我将为大家介绍一个新的结构体 vm_area_struct，正是这个结构体描述了这些虚拟内存区域 VMA（virtual memory area）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 struct vm_area_struct { unsigned long vm_start;\t/* Our start address within vm_mm. */ unsigned long vm_end;\t/* The first byte after our end address within vm_mm. */ /* * Access permissions of this VMA. */ pgprot_t vm_page_prot; unsigned long vm_flags;\tstruct anon_vma *anon_vma;\t/* Serialized by page_table_lock */ struct file * vm_file;\t/* File we map to (can be NULL). */ unsigned long vm_pgoff;\t/* Offset (within vm_file) in PAGE_SIZE units */\tvoid * vm_private_data;\t/* was vm_pte (shared mem) */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; } 每个 vm_area_struct 结构对应于虚拟内存空间中的唯一虚拟内存区域 VMA，vm_start 指向了这块虚拟内存区域的起始地址（最低地址），vm_start 本身包含在这块虚拟内存区域内。vm_end 指向了这块虚拟内存区域的结束地址（最高地址），而 vm_end 本身包含在这块虚拟内存区域之外，所以 vm_area_struct 结构描述的是 [vm_start，vm_end) 这样一段左闭右开的虚拟内存区域。\n#5.4 定义虚拟内存区域的访问权限和行为规范 vm_page_prot 和 vm_flags 都是用来标记 vm_area_struct 结构表示的这块虚拟内存区域的访问权限和行为规范。\n上边小节中我们也提到，内核会将整块物理内存划分为一页一页大小的区域，以页为单位来管理这些物理内存，每页大小默认 4K 。而虚拟内存最终也是要和物理内存一一映射起来的，所以在虚拟内存空间中也有虚拟页的概念与之对应，虚拟内存中的虚拟页映射到物理内存中的物理页。无论是在虚拟内存空间中还是在物理内存中，内核管理内存的最小单位都是页。\nvm_page_prot 偏向于定义底层内存管理架构中页这一级别的访问控制权限，它可以直接应用在底层页表中，它是一个具体的概念。\n页表用于管理虚拟内存到物理内存之间的映射关系，这部分内容我后续会详细讲解，这里大家有个初步的概念就行。\n虚拟内存区域 VMA 由许多的虚拟页 (page) 组成，每个虚拟页需要经过页表的转换才能找到对应的物理页面。页表中关于内存页的访问权限就是由 vm_page_prot 决定的。\nvm_flags 则偏向于定于整个虚拟内存区域的访问权限以及行为规范。描述的是虚拟内存区域中的整体信息，而不是虚拟内存区域中具体的某个独立页面。它是一个抽象的概念。可以通过 vma-\u0026gt;vm_page_prot = vm_get_page_prot(vma-\u0026gt;vm_flags) 实现到具体页面访问权限 vm_page_prot 的转换。\n下面我列举一些常用到的 vm_flags 方便大家有一个直观的感受：\nvm_flags 访问权限 VM_READ 可读 VM_WRITE 可写 VM_EXEC 可执行 VM_SHARD 可多进程之间共享 VM_IO 可映射至设备 IO 空间 VM_RESERVED 内存区域不可被换出 VM_SEQ_READ 内存区域可能被顺序访问 VM_RAND_READ 内存区域可能被随机访问 VM_READ，VM_WRITE，VM_EXEC 定义了虚拟内存区域是否可以被读取，写入，执行等权限。\n比如代码段这块内存区域的权限是可读，可执行，但是不可写。数据段具有可读可写的权限但是不可执行。堆则具有可读可写，可执行的权限（Java 中的字节码存储在堆中，所以需要可执行权限），栈一般是可读可写的权限，一般很少有可执行权限。而文件映射与匿名映射区存放了共享链接库，所以也需要可执行的权限。\nVM_SHARD 用于指定这块虚拟内存区域映射的物理内存是否可以在多进程之间共享，以便完成进程间通讯。\n设置这个值即为 mmap 的共享映射，不设置的话则为私有映射。这个等后面我们讲到 mmap 的相关实现时还会再次提起。\nVM_IO 的设置表示这块虚拟内存区域可以映射至设备 IO 空间中。通常在设备驱动程序执行 mmap 进行 IO 空间映射时才会被设置。\nVM_RESERVED 的设置表示在内存紧张的时候，这块虚拟内存区域非常重要，不能被换出到磁盘中。\nVM_SEQ_READ 的设置用来暗示内核，应用程序对这块虚拟内存区域的读取是会采用顺序读的方式进行，内核会根据实际情况决定预读后续的内存页数，以便加快下次顺序访问速度。\nVM_RAND_READ 的设置会暗示内核，应用程序会对这块虚拟内存区域进行随机读取，内核则会根据实际情况减少预读的内存页数甚至停止预读。\n我们可以通过 posix_fadvise，madvise 系统调用来暗示内核是否对相关内存区域进行顺序读取或者随机读取。相关的详细内容，大家可以看下我上篇文章 《从 Linux 内核角度探秘 JDK NIO 文件读写本质》 (opens new window)中的第 9 小节文件页预读部分。\n通过这一系列的介绍，我们可以看到 vm_flags 就是定义整个虚拟内存区域的访问权限以及行为规范，而内存区域中内存的最小单位为页（4K），虚拟内存区域中包含了很多这样的虚拟页，对于虚拟内存区域 VMA 设置的访问权限也会全部复制到区域中包含的内存页中。\n#5.5 关联内存映射中的映射关系 接下来的三个属性 anon_vma，vm_file，vm_pgoff 分别和虚拟内存映射相关，虚拟内存区域可以映射到物理内存上，也可以映射到文件中，映射到物理内存上我们称之为匿名映射，映射到文件中我们称之为文件映射。\n那么这个映射关系在内核中该如何表示呢？这就用到了 vm_area_struct 结构体中的上述三个属性。\n当我们调用 malloc 申请内存时，如果申请的是小块内存（低于 128K）则会使用 do_brk() 系统调用通过调整堆中的 brk 指针大小来增加或者回收堆内存。\n如果申请的是比较大块的内存（超过 128K）时，则会调用 mmap 在上图虚拟内存空间中的文件映射与匿名映射区创建出一块 VMA 内存区域（这里是匿名映射）。这块匿名映射区域就用 struct anon_vma 结构表示。\n当调用 mmap 进行文件映射时，vm_file 属性就用来关联被映射的文件。这样一来虚拟内存区域就与映射文件关联了起来。vm_pgoff 则表示映射进虚拟内存中的文件内容，在文件中的偏移。\n当然在匿名映射中，vm_area_struct 结构中的 vm_file 就为 null，vm_pgoff 也就没有了意义。\nvm_private_data 则用于存储 VMA 中的私有数据。具体的存储内容和内存映射的类型有关，我们暂不展开论述。\n#5.6 针对虚拟内存区域的相关操作 struct vm_area_struct 结构中还有一个 vm_ops 用来指向针对虚拟内存区域 VMA 的相关操作的函数指针。\n1 2 3 4 5 6 7 8 struct vm_operations_struct { void (*open)(struct vm_area_struct * area); void (*close)(struct vm_area_struct * area); vm_fault_t (*fault)(struct vm_fault *vmf); vm_fault_t (*page_mkwrite)(struct vm_fault *vmf); ..... 省略 ....... } 当指定的虚拟内存区域被加入到进程虚拟内存空间中时，open 函数会被调用 当虚拟内存区域 VMA 从进程虚拟内存空间中被删除时，close 函数会被调用 当进程访问虚拟内存时，访问的页面不在物理内存中，可能是未分配物理内存也可能是被置换到磁盘中，这时就会产生缺页异常，fault 函数就会被调用。 当一个只读的页面将要变为可写时，page_mkwrite 函数会被调用。 struct vm_operations_struct 结构中定义的都是对虚拟内存区域 VMA 的相关操作函数指针。\n内核中这种类似的用法其实有很多，在内核中每个特定领域的描述符都会定义相关的操作。比如在前边的文章 《从 Linux 内核角度探秘 JDK NIO 文件读写本质》 (opens new window)中我们介绍到内核中的文件描述符 struct file 中定义的 struct file_operations *f_op。里面定义了内核针对文件操作的函数指针，具体的实现根据不同的文件类型有所不同。\n针对 Socket 文件类型，这里的 file_operations 指向的是 socket_file_ops。\n在 ext4 文件系统中管理的文件对应的 file_operations 指向 ext4_file_operations，专门用于操作 ext4 文件系统中的文件。还有针对 page cache 页高速缓存相关操作定义的 address_space_operations 。\n还有我们在 《从 Linux 内核角度看 IO 模型的演变》 (opens new window)一文中介绍到，socket 相关的操作接口定义在 inet_stream_ops 函数集合中，负责对上给用户提供接口。而 socket 与内核协议栈之间的操作接口定义在 struct sock 中的 sk_prot 指针上，这里指向 tcp_prot 协议操作函数集合。\n对 socket 发起的系统 IO 调用时，在内核中首先会调用 socket 的文件结构 struct file 中的 file_operations 文件操作集合，然后调用 struct socket 中的 ops 指向的 inet_stream_opssocket 操作函数，最终调用到 struct sock 中 sk_prot 指针指向的 tcp_prot 内核协议栈操作函数接口集合。\n#5.7 虚拟内存区域在内核中是如何被组织的 在上一小节中，我们介绍了内核中用来表示虚拟内存区域 VMA 的结构体 struct vm_area_struct ，并详细为大家剖析了 struct vm_area_struct 中的一些重要的关键属性。\n现在我们已经熟悉了这些虚拟内存区域，那么接下来的问题就是在内核中这些虚拟内存区域是如何被组织的呢？\n我们继续来到 struct vm_area_struct 结构中，来看一下与组织结构相关的一些属性：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 struct vm_area_struct { struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; struct list_head anon_vma_chain; struct mm_struct *vm_mm;\t/* The address space we belong to. */ unsigned long vm_start; /* Our start address within vm_mm. */ unsigned long vm_end; /* The first byte after our end address within vm_mm. */ /* * Access permissions of this VMA. */ pgprot_t vm_page_prot; unsigned long vm_flags; struct anon_vma *anon_vma; /* Serialized by page_table_lock */ struct file * vm_file; /* File we map to (can be NULL). */ unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */ void * vm_private_data; /* was vm_pte (shared mem) */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; } 在内核中其实是通过一个 struct vm_area_struct 结构的双向链表将虚拟内存空间中的这些虚拟内存区域 VMA 串联起来的。\nvm_area_struct 结构中的 vm_next ，vm_prev 指针分别指向 VMA 节点所在双向链表中的后继节点和前驱节点，内核中的这个 VMA 双向链表是有顺序的，所有 VMA 节点按照低地址到高地址的增长方向排序。\n双向链表中的最后一个 VMA 节点的 vm_next 指针指向 NULL，双向链表的头指针存储在内存描述符 struct mm_struct 结构中的 mmap 中，正是这个 mmap 串联起了整个虚拟内存空间中的虚拟内存区域。\n1 2 3 struct mm_struct { struct vm_area_struct *mmap;\t/* list of VMAs */ } 在每个虚拟内存区域 VMA 中又通过 struct vm_area_struct 中的 vm_mm 指针指向了所属的虚拟内存空间 mm_struct。\n我们可以通过 cat /proc/pid/maps 或者 pmap pid 查看进程的虚拟内存空间布局以及其中包含的所有内存区域。这两个命令背后的实现原理就是通过遍历内核中的这个 vm_area_struct 双向链表获取的。\n内核中关于这些虚拟内存区域的操作除了遍历之外还有许多需要根据特定虚拟内存地址在虚拟内存空间中查找特定的虚拟内存区域。\n尤其在进程虚拟内存空间中包含的内存区域 VMA 比较多的情况下，使用红黑树查找特定虚拟内存区域的时间复杂度是 O( logN ) ，可以显著减少查找所需的时间。\n所以在内核中，同样的内存区域 vm_area_struct 会有两种组织形式，一种是双向链表用于高效的遍历，另一种就是红黑树用于高效的查找。\n每个 VMA 区域都是红黑树中的一个节点，通过 struct vm_area_struct 结构中的 vm_rb 将自己连接到红黑树中。\n而红黑树中的根节点存储在内存描述符 struct mm_struct 中的 mm_rb 中：\n1 2 3 struct mm_struct { struct rb_root mm_rb; } #6. 程序编译后的二进制文件如何映射到虚拟内存空间中 经过前边这么多小节的内容介绍，现在我们已经熟悉了进程虚拟内存空间的布局，以及内核如何管理这些虚拟内存区域，并对进程的虚拟内存空间有了一个完整全面的认识。\n现在我们再来回到最初的起点，进程的虚拟内存空间 mm_struct 以及这些虚拟内存区域 vm_area_struct 是如何被创建并初始化的呢？\n在 《3. 进程虚拟内存空间》小节中，我们介绍进程的虚拟内存空间时提到，我们写的程序代码编译之后会生成一个 ELF 格式的二进制文件，这个二进制文件中包含了程序运行时所需要的元信息，比如程序的机器码，程序中的全局变量以及静态变量等。\n这个 ELF 格式的二进制文件中的布局和我们前边讲的虚拟内存空间中的布局类似，也是一段一段的，每一段包含了不同的元数据。\n磁盘文件中的段我们叫做 Section，内存中的段我们叫做 Segment，也就是内存区域。\n磁盘文件中的这些 Section 会在进程运行之前加载到内存中并映射到内存中的 Segment。通常是多个 Section 映射到一个 Segment。\n比如磁盘文件中的 .text，.rodata 等一些只读的 Section，会被映射到内存的一个只读可执行的 Segment 里（代码段）。而 .data，.bss 等一些可读写的 Section，则会被映射到内存的一个具有读写权限的 Segment 里（数据段，BSS 段）。\n那么这些 ELF 格式的二进制文件中的 Section 是如何加载并映射进虚拟内存空间的呢？\n内核中完成这个映射过程的函数是 load_elf_binary ，这个函数的作用很大，加载内核的是它，启动第一个用户态进程 init 的是它，fork 完了以后，调用 exec 运行一个二进制程序的也是它。当 exec 运行一个二进制程序的时候，除了解析 ELF 的格式之外，另外一个重要的事情就是建立上述提到的内存映射。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 static int load_elf_binary(struct linux_binprm *bprm) { ...... 省略 ........ // 设置虚拟内存空间中的内存映射区域起始地址 mmap_base setup_new_exec(bprm); ...... 省略 ........ // 创建并初始化栈对应的 vm_area_struct 结构。 // 设置 mm-\u0026gt;start_stack 就是栈的起始地址也就是栈底，并将 mm-\u0026gt;arg_start 是指向栈底的。 retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack); ...... 省略 ........ // 将二进制文件中的代码部分映射到虚拟内存空间中 error = elf_map(bprm-\u0026gt;file, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags, total_size); ...... 省略 ........ // 创建并初始化堆对应的的 vm_area_struct 结构 // 设置 current-\u0026gt;mm-\u0026gt;start_brk = current-\u0026gt;mm-\u0026gt;brk，设置堆的起始地址 start_brk，结束地址 brk。 起初两者相等表示堆是空的 retval = set_brk(elf_bss, elf_brk, bss_prot); ...... 省略 ........ // 将进程依赖的动态链接库 .so 文件映射到虚拟内存空间中的内存映射区域 elf_entry = load_elf_interp(\u0026amp;loc-\u0026gt;interp_elf_ex, interpreter, \u0026amp;interp_map_addr, load_bias, interp_elf_phdata); ...... 省略 ........ // 初始化内存描述符 mm_struct current-\u0026gt;mm-\u0026gt;end_code = end_code; current-\u0026gt;mm-\u0026gt;start_code = start_code; current-\u0026gt;mm-\u0026gt;start_data = start_data; current-\u0026gt;mm-\u0026gt;end_data = end_data; current-\u0026gt;mm-\u0026gt;start_stack = bprm-\u0026gt;p; ...... 省略 ........ } setup_new_exec 设置虚拟内存空间中的内存映射区域起始地址 mmap_base setup_arg_pages 创建并初始化栈对应的 vm_area_struct 结构。置 mm-\u0026gt;start_stack 就是栈的起始地址也就是栈底，并将 mm-\u0026gt;arg_start 是指向栈底的。 elf_map 将 ELF 格式的二进制文件中.text ，.data，.bss 部分映射到虚拟内存空间中的代码段，数据段，BSS 段中。 set_brk 创建并初始化堆对应的的 vm_area_struct 结构，设置 current-\u0026gt;mm-\u0026gt;start_brk = current-\u0026gt;mm-\u0026gt;brk，设置堆的起始地址 start_brk，结束地址 brk。 起初两者相等表示堆是空的。 load_elf_interp 将进程依赖的动态链接库 .so 文件映射到虚拟内存空间中的内存映射区域 初始化内存描述符 mm_struct #7. 内核虚拟内存空间 现在我们已经知道了进程虚拟内存空间在内核中的布局以及管理，那么内核态的虚拟内存空间又是什么样子的呢？本小节我就带大家来一层一层地拆开这个黑盒子。\n之前在介绍进程虚拟内存空间的时候，我提到不同进程之间的虚拟内存空间是相互隔离的，彼此之间相互独立，相互感知不到其他进程的存在。使得进程以为自己拥有所有的内存资源。\n而内核态虚拟内存空间是所有进程共享的，不同进程进入内核态之后看到的虚拟内存空间全部是一样的。\n什么意思呢？比如上图中的进程 a，进程 b，进程 c 分别在各自的用户态虚拟内存空间中访问虚拟地址 x 。由于进程之间的用户态虚拟内存空间是相互隔离相互独立的，虽然在进程a，进程b，进程c 访问的都是虚拟地址 x 但是看到的内容却是不一样的（背后可能映射到不同的物理内存中）。\n但是当进程 a，进程 b，进程 c 进入到内核态之后情况就不一样了，由于内核虚拟内存空间是各个进程共享的，所以它们在内核空间中看到的内容全部是一样的，比如进程 a，进程 b，进程 c 在内核态都去访问虚拟地址 y。这时它们看到的内容就是一样的了。\n这里我和大家澄清一个经常被误解的概念：由于内核会涉及到物理内存的管理，所以很多人会想当然地认为只要进入了内核态就开始使用物理地址了，这就大错特错了，千万不要这样理解，进程进入内核态之后使用的仍然是虚拟内存地址，只不过在内核中使用的虚拟内存地址被限制在了内核态虚拟内存空间范围中，这也是本小节我要为大家介绍的主题。\n在清楚了这个基本概念之后，下面我分别从 32 位体系 和 64 位体系下为大家介绍内核态虚拟内存空间的布局。\n#7.1 32 位体系内核虚拟内存空间布局 在前边《5.1 内核如何划分用户态和内核态虚拟内存空间》小节中我们提到，内核在 /arch/x86/include/asm/page_32_types.h 文件中通过 TASK_SIZE 将进程虚拟内存空间和内核虚拟内存空间分割开来。\n1 2 3 4 /* * User space process size: 3GB (default). */ #define TASK_SIZE __PAGE_OFFSET __PAGE_OFFSET 的值在 32 位系统下为 0xC000 000\n在 32 位体系结构下进程用户态虚拟内存空间为 3 GB，虚拟内存地址范围为：0x0000 0000 - 0xC000 000 。内核态虚拟内存空间为 1 GB，虚拟内存地址范围为：0xC000 000 - 0xFFFF FFFF。\n本小节我们主要关注 0xC000 000 - 0xFFFF FFFF 这段虚拟内存地址区域也就是内核虚拟内存空间的布局情况。\n#7.1.1 直接映射区 在总共大小 1G 的内核虚拟内存空间中，位于最前边有一块 896M 大小的区域，我们称之为直接映射区或者线性映射区，地址范围为 3G \u0026ndash; 3G + 896m 。\n之所以这块 896M 大小的区域称为直接映射区或者线性映射区，是因为这块连续的虚拟内存地址会映射到 0 - 896M 这块连续的物理内存上。\n也就是说 3G \u0026ndash; 3G + 896m 这块 896M 大小的虚拟内存会直接映射到 0 - 896M 这块 896M 大小的物理内存上，这块区域中的虚拟内存地址直接减去 0xC000 0000 (3G) 就得到了物理内存地址。所以我们称这块区域为直接映射区。\n为了方便为大家解释，我们假设现在机器上的物理内存为 4G 大小\n虽然这块区域中的虚拟地址是直接映射到物理地址上，但是内核在访问这段区域的时候还是走的虚拟内存地址，内核也会为这块空间建立映射页表。关于页表的概念我后续会为大家详细讲解，这里大家只需要简单理解为页表保存了虚拟地址到物理地址的映射关系即可。\n大家这里只需要记得内核态虚拟内存空间的前 896M 区域是直接映射到物理内存中的前 896M 区域中的，直接映射区中的映射关系是一比一映射。映射关系是固定的不会改变。\n明白了这个关系之后，我们接下来就看一下这块直接映射区域在物理内存中究竟存的是什么内容~~~\n在这段 896M 大小的物理内存中，前 1M 已经在系统启动的时候被系统占用，1M 之后的物理内存存放的是内核代码段，数据段，BSS 段（这些信息起初存放在 ELF格式的二进制文件中，在系统启动的时候被加载进内存）。\n我们可以通过 cat /proc/iomem 命令查看具体物理内存布局情况。\n当我们使用 fork 系统调用创建进程的时候，内核会创建一系列进程相关的描述符，比如之前提到的进程的核心数据结构 task_struct，进程的内存空间描述符 mm_struct，以及虚拟内存区域描述符 vm_area_struct 等。\n这些进程相关的数据结构也会存放在物理内存前 896M 的这段区域中，当然也会被直接映射至内核态虚拟内存空间中的 3G \u0026ndash; 3G + 896m 这段直接映射区域中。\n当进程被创建完毕之后，在内核运行的过程中，会涉及内核栈的分配，内核会为每个进程分配一个固定大小的内核栈（一般是两个页大小，依赖具体的体系结构），每个进程的整个调用链必须放在自己的内核栈中，内核栈也是分配在直接映射区。\n与进程用户空间中的栈不同的是，内核栈容量小而且是固定的，用户空间中的栈容量大而且可以动态扩展。内核栈的溢出危害非常巨大，它会直接悄无声息的覆盖相邻内存区域中的数据，破坏数据。\n通过以上内容的介绍我们了解到内核虚拟内存空间最前边的这段 896M 大小的直接映射区如何与物理内存进行映射关联，并且清楚了直接映射区主要用来存放哪些内容。\n写到这里，我觉得还是有必要再次从功能划分的角度为大家介绍下这块直接映射区域。\n我们都知道内核对物理内存的管理都是以页为最小单位来管理的，每页默认 4K 大小，理想状况下任何种类的数据页都可以存放在任何页框中，没有什么限制。比如：存放内核数据，用户数据，缓冲磁盘数据等。\n但是实际的计算机体系结构受到硬件方面的限制制约，间接导致限制了页框的使用方式。\n比如在 X86 体系结构下，ISA 总线的 DMA （直接内存存取）控制器，只能对内存的前16M 进行寻址，这就导致了 ISA 设备不能在整个 32 位地址空间中执行 DMA，只能使用物理内存的前 16M 进行 DMA 操作。\n因此直接映射区的前 16M 专门让内核用来为 DMA 分配内存，这块 16M 大小的内存区域我们称之为 ZONE_DMA。\n用于 DMA 的内存必须从 ZONE_DMA 区域中分配。\n而直接映射区中剩下的部分也就是从 16M 到 896M（不包含 896M）这段区域，我们称之为 ZONE_NORMAL。从字面意义上我们可以了解到，这块区域包含的就是正常的页框（使用没有任何限制）。\nZONE_NORMAL 由于也是属于直接映射区的一部分，对应的物理内存 16M 到 896M 这段区域也是被直接映射至内核态虚拟内存空间中的 3G + 16M 到 3G + 896M 这段虚拟内存上。\n注意这里的 ZONE_DMA 和 ZONE_NORMAL 是内核针对物理内存区域的划分。\n现在物理内存中的前 896M 的区域也就是前边介绍的 ZONE_DMA 和 ZONE_NORMAL 区域到内核虚拟内存空间的映射我就为大家介绍完了，它们都是采用直接映射的方式，一比一就行映射。\n#7.1.2 ZONE_HIGHMEM 高端内存 而物理内存 896M 以上的区域被内核划分为 ZONE_HIGHMEM 区域，我们称之为高端内存。\n本例中我们的物理内存假设为 4G，高端内存区域为 4G - 896M = 3200M，那么这块 3200M 大小的 ZONE_HIGHMEM 区域该如何映射到内核虚拟内存空间中呢？\n由于内核虚拟内存空间中的前 896M 虚拟内存已经被直接映射区所占用，而在 32 体系结构下内核虚拟内存空间总共也就 1G 的大小，这样一来内核剩余可用的虚拟内存空间就变为了 1G - 896M = 128M。\n显然物理内存中 3200M 大小的 ZONE_HIGHMEM 区域无法继续通过直接映射的方式映射到这 128M 大小的虚拟内存空间中。\n这样一来物理内存中的 ZONE_HIGHMEM 区域就只能采用动态映射的方式映射到 128M 大小的内核虚拟内存空间中，也就是说只能动态的一部分一部分的分批映射，先映射正在使用的这部分，使用完毕解除映射，接着映射其他部分。\n知道了 ZONE_HIGHMEM 区域的映射原理，我们接着往下看这 128M 大小的内核虚拟内存空间究竟是如何布局的？\n内核虚拟内存空间中的 3G + 896M 这块地址在内核中定义为 high_memory，high_memory 往上有一段 8M 大小的内存空洞。空洞范围为：high_memory 到 VMALLOC_START 。\nVMALLOC_START 定义在内核源码 /arch/x86/include/asm/pgtable_32_areas.h 文件中：\n1 2 3 #define VMALLOC_OFFSET\t(8 * 1024 * 1024) #define VMALLOC_START\t((unsigned long)high_memory + VMALLOC_OFFSET) #7.1.3 vmalloc 动态映射区 接下来 VMALLOC_START 到 VMALLOC_END 之间的这块区域成为动态映射区。采用动态映射的方式映射物理内存中的高端内存。\n1 2 3 4 5 #ifdef CONFIG_HIGHMEM # define VMALLOC_END\t(PKMAP_BASE - 2 * PAGE_SIZE) #else # define VMALLOC_END\t(LDT_BASE_ADDR - 2 * PAGE_SIZE) #endif 和用户态进程使用 malloc 申请内存一样，在这块动态映射区内核是使用 vmalloc 进行内存分配。由于之前介绍的动态映射的原因，vmalloc 分配的内存在虚拟内存上是连续的，但是物理内存是不连续的。通过页表来建立物理内存与虚拟内存之间的映射关系，从而可以将不连续的物理内存映射到连续的虚拟内存上。\n由于 vmalloc 获得的物理内存页是不连续的，因此它只能将这些物理内存页一个一个地进行映射，在性能开销上会比直接映射大得多。\n关于 vmalloc 分配内存的相关实现原理，我会在后面的文章中为大家讲解，这里大家只需要明白它在哪块虚拟内存区域中活动即可。\n#7.1.4 永久映射区 而在 PKMAP_BASE 到 FIXADDR_START 之间的这段空间称为永久映射区。在内核的这段虚拟地址空间中允许建立与物理高端内存的长期映射关系。比如内核通过 alloc_pages() 函数在物理内存的高端内存中申请获取到的物理内存页，这些物理内存页可以通过调用 kmap 映射到永久映射区中。\nLAST_PKMAP 表示永久映射区可以映射的页数限制。\n1 2 3 4 #define PKMAP_BASE\t\\ ((LDT_BASE_ADDR - PAGE_SIZE) \u0026amp; PMD_MASK) #define LAST_PKMAP 1024 #8.1.5 固定映射区 内核虚拟内存空间中的下一个区域为固定映射区，区域范围为：FIXADDR_START 到 FIXADDR_TOP。\nFIXADDR_START 和 FIXADDR_TOP 定义在内核源码 /arch/x86/include/asm/fixmap.h 文件中：\n1 2 3 4 #define FIXADDR_START\t(FIXADDR_TOP - FIXADDR_SIZE) extern unsigned long __FIXADDR_TOP; // 0xFFFF F000 #define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP) 在内核虚拟内存空间的直接映射区中，直接映射区中的虚拟内存地址与物理内存前 896M 的空间的映射关系都是预设好的，一比一映射。\n在固定映射区中的虚拟内存地址可以自由映射到物理内存的高端地址上，但是与动态映射区以及永久映射区不同的是，在固定映射区中虚拟地址是固定的，而被映射的物理地址是可以改变的。也就是说，有些虚拟地址在编译的时候就固定下来了，是在内核启动过程中被确定的，而这些虚拟地址对应的物理地址不是固定的。采用固定虚拟地址的好处是它相当于一个指针常量（常量的值在编译时确定），指向物理地址，如果虚拟地址不固定，则相当于一个指针变量。\n那为什么会有固定映射这个概念呢 ? 比如：在内核的启动过程中，有些模块需要使用虚拟内存并映射到指定的物理地址上，而且这些模块也没有办法等待完整的内存管理模块初始化之后再进行地址映射。因此，内核固定分配了一些虚拟地址，这些地址有固定的用途，使用该地址的模块在初始化的时候，将这些固定分配的虚拟地址映射到指定的物理地址上去。\n#7.1.6 临时映射区 在内核虚拟内存空间中的最后一块区域为临时映射区，那么这块临时映射区是用来干什么的呢？\n我在之前文章 《从 Linux 内核角度探秘 JDK NIO 文件读写本质》 (opens new window)的 “ 12.3 iov_iter_copy_from_user_atomic ” 小节中介绍在 Buffered IO 模式下进行文件写入的时候，在下图中的第四步，内核会调用 iov_iter_copy_from_user_atomic 函数将用户空间缓冲区 DirectByteBuffer 中的待写入数据拷贝到 page cache 中。\n但是内核又不能直接进行拷贝，因为此时从 page cache 中取出的缓存页 page 是物理地址，而在内核中是不能够直接操作物理地址的，只能操作虚拟地址。\n那怎么办呢？所以就需要使用 kmap_atomic 将缓存页临时映射到内核空间的一段虚拟地址上，这段虚拟地址就位于内核虚拟内存空间中的临时映射区上，然后将用户空间缓存区 DirectByteBuffer 中的待写入数据通过这段映射的虚拟地址拷贝到 page cache 中的相应缓存页中。这时文件的写入操作就已经完成了。\n由于是临时映射，所以在拷贝完成之后，调用 kunmap_atomic 将这段映射再解除掉。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 size_t iov_iter_copy_from_user_atomic(struct page *page, struct iov_iter *i, unsigned long offset, size_t bytes) { // 将缓存页临时映射到内核虚拟地址空间的临时映射区中 char *kaddr = kmap_atomic(page), *p = kaddr + offset; // 将用户缓存区 DirectByteBuffer 中的待写入数据拷贝到文件缓存页中 iterate_all_kinds(i, bytes, v, copyin((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len), memcpy_from_page((p += v.bv_len) - v.bv_len, v.bv_page, v.bv_offset, v.bv_len), memcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len) ) // 解除内核虚拟地址空间与缓存页之间的临时映射，这里映射只是为了临时拷贝数据用 kunmap_atomic(kaddr); return bytes; } #7.1.7 32位体系结构下 Linux 虚拟内存空间整体布局 到现在为止，整个内核虚拟内存空间在 32 位体系下的布局，我就为大家详细介绍完毕了，我们再次结合前边《4.1 32 位机器上进程虚拟内存空间分布》小节中介绍的进程虚拟内存空间和本小节介绍的内核虚拟内存空间来整体回顾下 32 位体系结构 Linux 的整个虚拟内存空间的布局：\n#7.2 64 位体系内核虚拟内存空间布局 内核虚拟内存空间在 32 位体系下只有 1G 大小，实在太小了，因此需要精细化的管理，于是按照功能分类划分除了很多内核虚拟内存区域，这样就显得非常复杂。\n到了 64 位体系下，内核虚拟内存空间的布局和管理就变得容易多了，因为进程虚拟内存空间和内核虚拟内存空间各自占用 128T 的虚拟内存，实在是太大了，我们可以在这里边随意翱翔，随意挥霍。\n因此在 64 位体系下的内核虚拟内存空间与物理内存的映射就变得非常简单，由于虚拟内存空间足够的大，即便是内核要访问全部的物理内存，直接映射就可以了，不在需要用到《7.1.2 ZONE_HIGHMEM 高端内存》小节中介绍的高端内存那种动态映射方式。\n在前边《5.1 内核如何划分用户态和内核态虚拟内存空间》小节中我们提到，内核在 /arch/x86/include/asm/page_64_types.h 文件中通过 TASK_SIZE 将进程虚拟内存空间和内核虚拟内存空间分割开来。\n1 2 3 4 5 6 7 8 #define TASK_SIZE\t(test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX) #define TASK_SIZE_MAX\ttask_size_max() #define task_size_max()\t((_AC(1,UL) \u0026lt;\u0026lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE) #define __VIRTUAL_MASK_SHIFT\t47 64 位系统中的 TASK_SIZE 为 0x00007FFFFFFFF000\n在 64 位系统中，只使用了其中的低 48 位来表示虚拟内存地址。其中用户态虚拟内存空间为低 128 T，虚拟内存地址范围为：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000 。\n内核态虚拟内存空间为高 128 T，虚拟内存地址范围为：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。\n本小节我们主要关注 0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 这段内核虚拟内存空间的布局情况。\n64 位内核虚拟内存空间从 0xFFFF 8000 0000 0000 开始到 0xFFFF 8800 0000 0000 这段地址空间是一个 8T 大小的内存空洞区域。\n紧着着 8T 大小的内存空洞下一个区域就是 64T 大小的直接映射区。这个区域中的虚拟内存地址减去 PAGE_OFFSET 就直接得到了物理内存地址。\nPAGE_OFFSET 变量定义在 /arch/x86/include/asm/page_64_types.h 文件中：\n1 2 #define __PAGE_OFFSET_BASE _AC(0xffff880000000000, UL) #define __PAGE_OFFSET __PAGE_OFFSET_BASE 从图中 VMALLOC_START 到 VMALLOC_END 的这段区域是 32T 大小的 vmalloc 映射区，这里类似用户空间中的堆，内核在这里使用 vmalloc 系统调用申请内存。\nVMALLOC_START 和 VMALLOC_END 变量定义在 /arch/x86/include/asm/pgtable_64_types.h 文件中：\n1 2 3 4 5 #define __VMALLOC_BASE_L4\t0xffffc90000000000UL #define VMEMMAP_START\t__VMEMMAP_BASE_L4 #define VMALLOC_END\t(VMALLOC_START + (VMALLOC_SIZE_TB \u0026lt;\u0026lt; 40) - 1) 从 VMEMMAP_START 开始是 1T 大小的虚拟内存映射区，用于存放物理页面的描述符 struct page 结构用来表示物理内存页。\nVMEMMAP_START 变量定义在 /arch/x86/include/asm/pgtable_64_types.h 文件中：\n1 2 3 #define __VMEMMAP_BASE_L4\t0xffffea0000000000UL # define VMEMMAP_START\t__VMEMMAP_BASE_L4 从 __START_KERNEL_map 开始是大小为 512M 的区域用于存放内核代码段、全局变量、BSS 等。这里对应到物理内存开始的位置，减去 __START_KERNEL_map 就能得到物理内存的地址。这里和直接映射区有点像，但是不矛盾，因为直接映射区之前有 8T 的空洞区域，早就过了内核代码在物理内存中加载的位置。\n__START_KERNEL_map 变量定义在 /arch/x86/include/asm/page_64_types.h 文件中：\n1 #define __START_KERNEL_map _AC(0xffffffff80000000, UL) #7.2.1 64位体系结构下 Linux 虚拟内存空间整体布局 到现在为止，整个内核虚拟内存空间在 64 位体系下的布局我就为大家详细介绍完毕了，我们再次结合前边《4.2 64 位机器上进程虚拟内存空间分布》小节介绍的进程虚拟内存空间和本小节介绍的内核虚拟内存空间来整体回顾下 64 位体系结构 Linux 的整个虚拟内存空间的布局：\n#8. 到底什么是物理内存地址 聊完了虚拟内存，我们接着聊一下物理内存，我们平时所称的内存也叫随机访问存储器（ random-access memory ）也叫 RAM 。而 RAM 分为两类：\n一类是静态 RAM（ SRAM ），这类 SRAM 用于 CPU 高速缓存 L1Cache，L2Cache，L3Cache。其特点是访问速度快，访问速度为 1 - 30 个时钟周期，但是容量小，造价高。 另一类则是动态 RAM ( DRAM )，这类 DRAM 用于我们常说的主存上，其特点的是访问速度慢（相对高速缓存），访问速度为 50 - 200 个时钟周期，但是容量大，造价便宜些（相对高速缓存）。 内存由一个一个的存储器模块（memory module）组成，它们插在主板的扩展槽上。常见的存储器模块通常以 64 位为单位（ 8 个字节）传输数据到存储控制器上或者从存储控制器传出数据。\n如图所示内存条上黑色的元器件就是存储器模块（memory module）。多个存储器模块连接到存储控制器上，就聚合成了主存。\n而 DRAM 芯片就包装在存储器模块中，每个存储器模块中包含 8 个 DRAM 芯片，依次编号为 0 - 7 。\n而每一个 DRAM 芯片的存储结构是一个二维矩阵，二维矩阵中存储的元素我们称为超单元（supercell），每个 supercell 大小为一个字节（8 bit）。每个 supercell 都由一个坐标地址（i，j）。\ni 表示二维矩阵中的行地址，在计算机中行地址称为 RAS (row access strobe，行访问选通脉冲)。 j 表示二维矩阵中的列地址，在计算机中列地址称为 CAS (column access strobe,列访问选通脉冲)。\n下图中的 supercell 的 RAS = 2，CAS = 2。\nDRAM 芯片中的信息通过引脚流入流出 DRAM 芯片。每个引脚携带 1 bit的信号。\n图中 DRAM 芯片包含了两个地址引脚( addr )，因为我们要通过 RAS，CAS 来定位要获取的 supercell 。还有 8 个数据引脚（data），因为 DRAM 芯片的 IO 单位为一个字节（8 bit），所以需要 8 个 data 引脚从 DRAM 芯片传入传出数据。\n注意这里只是为了解释地址引脚和数据引脚的概念，实际硬件中的引脚数量是不一定的。\n#8.1 DRAM 芯片的访问 我们现在就以读取上图中坐标地址为（2，2）的 supercell 为例，来说明访问 DRAM 芯片的过程。\n首先存储控制器将行地址 RAS = 2 通过地址引脚发送给 DRAM 芯片。 DRAM 芯片根据 RAS = 2 将二维矩阵中的第二行的全部内容拷贝到内部行缓冲区中。 接下来存储控制器会通过地址引脚发送 CAS = 2 到 DRAM 芯片中。 DRAM芯片从内部行缓冲区中根据 CAS = 2 拷贝出第二列的 supercell 并通过数据引脚发送给存储控制器。 DRAM 芯片的 IO 单位为一个 supercell ，也就是一个字节(8 bit)。\n#8.2 CPU 如何读写主存 前边我们介绍了内存的物理结构，以及如何访问内存中的 DRAM 芯片获取 supercell 中存储的数据（一个字节）。本小节我们来介绍下 CPU 是如何访问内存的：\nCPU 与内存之间的数据交互是通过总线（bus）完成的，而数据在总线上的传送是通过一系列的步骤完成的，这些步骤称为总线事务（bus transaction）。\n其中数据从内存传送到 CPU 称之为读事务（read transaction），数据从 CPU 传送到内存称之为写事务（write transaction）。\n总线上传输的信号包括：地址信号，数据信号，控制信号。其中控制总线上传输的控制信号可以同步事务，并能够标识出当前正在被执行的事务信息：\n当前这个事务是到内存的？还是到磁盘的？或者是到其他 IO 设备的？ 这个事务是读还是写？ 总线上传输的地址信号（物理内存地址），还是数据信号（数据）？。 这里大家需要注意总线上传输的地址均为物理内存地址。比如：在 MESI 缓存一致性协议中当 CPU core0 修改字段 a 的值时，其他 CPU 核心会在总线上嗅探字段 a 的物理内存地址，如果嗅探到总线上出现字段 a 的物理内存地址，说明有人在修改字段 a，这样其他 CPU 核心就会失效字段 a 所在的 cache line 。\n如上图所示，其中系统总线是连接 CPU 与 IO bridge 的，存储总线是来连接 IO bridge 和主存的。\nIO bridge 负责将系统总线上的电子信号转换成存储总线上的电子信号。IO bridge 也会将系统总线和存储总线连接到IO总线（磁盘等IO设备）上。这里我们看到 IO bridge 其实起的作用就是转换不同总线上的电子信号。\n#8.3 CPU 从内存读取数据过程 假设 CPU 现在需要将物理内存地址为 A 的内容加载到寄存器中进行运算。\n大家需要注意的是 CPU 只会访问虚拟内存，在操作总线之前，需要把虚拟内存地址转换为物理内存地址，总线上传输的都是物理内存地址，这里省略了虚拟内存地址到物理内存地址的转换过程，这部分内容我会在后续文章的相关章节详细为大家讲解，这里我们聚焦如何通过物理内存地址读取内存数据。\n首先 CPU 芯片中的总线接口会在总线上发起读事务（read transaction）。 该读事务分为以下步骤进行：\nCPU 将物理内存地址 A 放到系统总线上。随后 IO bridge 将信号传递到存储总线上。 主存感受到存储总线上的地址信号并通过存储控制器将存储总线上的物理内存地址 A 读取出来。 存储控制器通过物理内存地址 A 定位到具体的存储器模块，从 DRAM 芯片中取出物理内存地址 A 对应的数据 X。 存储控制器将读取到的数据 X 放到存储总线上，随后 IO bridge 将存储总线上的数据信号转换为系统总线上的数据信号，然后继续沿着系统总线传递。 CPU 芯片感受到系统总线上的数据信号，将数据从系统总线上读取出来并拷贝到寄存器中。 以上就是 CPU 读取内存数据到寄存器中的完整过程。\n但是其中还涉及到一个重要的过程，这里我们还是需要摊开来介绍一下，那就是存储控制器如何通过物理内存地址 A 从主存中读取出对应的数据 X 的？\n接下来我们结合前边介绍的内存结构以及从 DRAM 芯片读取数据的过程，来总体介绍下如何从主存中读取数据。\n#8.4 如何根据物理内存地址从主存中读取数据 前边介绍到，当主存中的存储控制器感受到了存储总线上的地址信号时，会将内存地址从存储总线上读取出来。\n随后会通过内存地址定位到具体的存储器模块。还记得内存结构中的存储器模块吗 ？\n而每个存储器模块中包含了 8 个 DRAM 芯片，编号从 0 - 7 。\n存储控制器会将物理内存地址转换为 DRAM 芯片中 supercell 在二维矩阵中的坐标地址(RAS，CAS)。并将这个坐标地址发送给对应的存储器模块。随后存储器模块会将 RAS 和 CAS 广播到存储器模块中的所有 DRAM 芯片。依次通过 (RAS，CAS) 从 DRAM0 到 DRAM7 读取到相应的 supercell 。\n我们知道一个 supercell 存储了一个字节（ 8 bit ） 数据，这里我们从 DRAM0 到 DRAM7 依次读取到了 8 个 supercell 也就是 8 个字节，然后将这 8 个字节返回给存储控制器，由存储控制器将数据放到存储总线上。\nCPU 总是以 word size 为单位从内存中读取数据，在 64 位处理器中的 word size 为 8 个字节。64 位的内存每次只能吞吐 8 个字节。\nCPU 每次会向内存读写一个 cache line 大小的数据（ 64 个字节），但是内存一次只能吞吐 8 个字节。\n所以在物理内存地址对应的存储器模块中，DRAM0 芯片存储第一个低位字节（ supercell ），DRAM1 芯片存储第二个字节，\u0026hellip;\u0026hellip;依次类推 DRAM7 芯片存储最后一个高位字节。\n由于存储器模块中这种由 8 个 DRAM 芯片组成的物理存储结构的限制，内存读取数据只能是按照物理内存地址，8 个字节 8 个字节地顺序读取数据。所以说内存一次读取和写入的单位是 8 个字节。\n而且在程序员眼里连续的物理内存地址实际上在物理上是不连续的。因为这连续的 8 个字节其实是存储于不同的 DRAM 芯片上的。每个 DRAM 芯片存储一个字节（supercell）\n#8.5 CPU 向内存写入数据过程 我们现在假设 CPU 要将寄存器中的数据 X 写到物理内存地址 A 中。同样的道理，CPU 芯片中的总线接口会向总线发起写事务（write transaction）。写事务步骤如下：\nCPU 将要写入的物理内存地址 A 放入系统总线上。 通过 IO bridge 的信号转换，将物理内存地址 A 传递到存储总线上。 存储控制器感受到存储总线上的地址信号，将物理内存地址 A 从存储总线上读取出来，并等待数据的到达。 CPU 将寄存器中的数据拷贝到系统总线上，通过 IO bridge 的信号转换，将数据传递到存储总线上。 存储控制器感受到存储总线上的数据信号，将数据从存储总线上读取出来。 存储控制器通过内存地址 A 定位到具体的存储器模块，最后将数据写入存储器模块中的 8 个 DRAM 芯片中。 #总结 本文我们从虚拟内存地址开始聊起，一直到物理内存地址结束，包含的信息量还是比较大的。首先我通过一个进程的运行实例为大家引出了内核引入虚拟内存空间的目的及其需要解决的问题。\n在我们有了虚拟内存空间的概念之后，我又近一步为大家介绍了内核如何划分用户态虚拟内存空间和内核态虚拟内存空间，并在次基础之上分别从 32 位体系结构和 64 位体系结构的角度详细阐述了 Linux 虚拟内存空间的整体布局分布。\n我们可以通过 cat /proc/pid/maps 或者 pmap pid 命令来查看进程用户态虚拟内存空间的实际分布。 还可以通过 cat /proc/iomem 命令来查看进程内核态虚拟内存空间的的实际分布。 在我们清楚了 Linux 虚拟内存空间的整体布局分布之后，我又介绍了 Linux 内核如何对分布在虚拟内存空间中的各个虚拟内存区域进行管理，以及每个虚拟内存区域的作用。在这个过程中还介绍了相关的内核数据结构，近一步从内核源码实现角度加深大家对虚拟内存空间的理解。\n最后我介绍了物理内存的结构，以及 CPU 如何通过物理内存地址来读写内存中的数据。这里我需要特地再次强调的是 CPU 只会访问虚拟内存地址，只不过在操作总线之前，通过一个地址转换硬件将虚拟内存地址转换为物理内存地址，然后将物理内存地址作为地址信号放在总线上传输，由于地址转换的内容和本文主旨无关，考虑到文章的篇幅以及复杂性，我就没有过多的介绍。\n好了，本文的内容到这里就全部结束了，感谢大家的耐心观看。\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%85%B6%E4%BA%8C/","tags":["操作系统内存管理","linux虚拟内存管理"],"title":"八股系列|操作系统|内存管理其二"},{"categories":["八股文","操作系统"],"contents":"内存管理 为什么要有虚拟内存？ 设想以下：如果我们在指定物理地址的内存上装载两个及以上的应用程序。这两个程序事先并没有协商，那么很容易出现一个程序占用了另一个程序的内存空间或者此时放入一个新的值到一个程序的内存空间，这都是非常危险的。\n所以，我们需要让进程使用的物理内存相互隔离起来，即让操作系统为每个进程独立分配一套虚拟地址，使得彼此互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。而且，每个进程的虚拟地址范围都是一样的，但是映射于不同的物理地址。\n操作系统会提供一种机制，把不同进程的虚拟地址与不同内存的物理地址映射起来\n如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。\n虚拟内存地址是怎么转化为物理内存地址的？ 进程持有的虚拟地址会借助CPU芯片里的内存管理单元（MMU）转化为物理地址，然后根据物理地址访问物理内存。\n分段机制下，虚拟地址和物理地址是如何映射的？ 分段方式是早期提出的非连续内存分配方式。分段的意思就是将进程所需的内存一次性分配出去，要多少，就分配多少。\n段选择因子和段内偏移量：\n段选择子就保存在段寄存器里面。段选择子里面最重要的是段号，用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。 虚拟地址中的段内偏移量应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。 在上面，知道了虚拟地址是通过段表与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，\n分段为什么会产生内存碎片的问题？ 这种分配方式很明显，不会有内存的内部碎片，但是会有外部碎片。因为分段方式是一次性分配连续空间，假如：进程A、B装载到内存里，两者中间刚好只有4M的空间，此时又装载20M的进程C，显然，A和B之间放不下，出现间隙，也就是外部碎片。\n分段为什么会导致内存交换效率低的问题？ 除了外部碎片的问题，分段方式由于是一次性连续分配进程所需内存空间，可能分配的比较大，那么后续内存占满了再有进程装载内存时，会发生内存交换，与磁盘交换一个容量较大的进程，显然，也会变慢，也就是内存交换效率变低\n但是分段也有好处，分段会产生连续的内存空间。\n分页机制下，虚拟地址和物理地址如何映射？ 分页方式的出现解决了分段方式带来的内存外部碎片和内存交换效率慢的问题。其实归根结底，分段方式为什么会带来内存外部碎片？主要原因还是按照分段方式分配的内存空间，视进程而定，有些需要较大内存的进程会申请较大的连续的内存空间。这就会导致外部碎片和内存交换效率低下。\n在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。\n总结一下，对于一个内存地址转换，其实就是这样三个步骤：\n把虚拟内存地址，切分成页号和偏移量； 根据页号，从页表里面，查询对应的物理页号； 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。 分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？ 我们减小分段分配的内存大小，预先把内存划分为小的连续排布的块（页）。进程要多少内存，我们分配N个固定大小的页，也就是分页。我们分配的是紧凑排布的页，不会出现外部碎片的问题，但是有可能最后一个分配的页没有使用完，会导致内部碎片。而发生内存交换时，也是以页为单位进行交换，页相对段来说更小，所以内存交换效率也更高。\n而且，分页方式的使得我们装载程序，不用一次性将整个程序都装载到内存里。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。\n简单的分页有什么缺陷？如何解决？ 因为分页的单位页通常来说很小，需要的页表也会变大。\n在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表。而操作系统会运行数百以上的进程，光是存储页表就需要4MB*数百。内存开销大.\n在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 4KB 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。\n采用多级分页的方式：\n我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 1024 个页表（二级页表），每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。如下图所示：\n页表项大小：4bytes\n从上面的图看出：页表有两个（一级页表和二级页表）。一级页表索引二级页表，二级页表索引物理页号。其中一级页表（只有一个）大小4KB=1024*4B，二级页表总大小4mB=1024*1024*4B，那么总大小是4KB+4MB\u0026gt;4MB.\n好像二级分页变得更大了？ 其实不然，如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表。\n多级分页带来了繁杂的地址转换，TLB来优化 程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。\n我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（Translation Lookaside Buffer） ，通常称为页表缓存、转址旁路缓存、快表等。\n虚拟内存有什么作用？ 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。 内存分配过程是怎样的？ 如果内存充足情况下，不需要触发内存回收，足够给新的进程分配新的内存空间；\n如果此时内存有一定压力，但是还是可以继续分配出足够空间给进程（页低阈值page_low\u0026lt;剩余内存\u0026lt;页高阈值page_high）；\n如果内存此时压力太大，剩余内存不多，不足以继续分配出足够地内存（页高阈值page_high\u0026lt;剩余内存\u0026lt;页最小阈值(page_min)），那么会触发后台内存回收的内核线程去异步非阻塞地回收内存（匿名页和文件页）;\n如果此时剩余内存几乎耗尽（剩余内存\u0026lt;页最小阈值(page_min)），那么会触发直接内存回收，同步阻塞地回收内存。此时，系统无法为新的进程分配内存空间，从外面看上去系统很卡顿。\n如果直接内存回收无法满足内存需求，那么就会触发OOM（out of memory）机制：\n内存溢出(Out Of Memory，简称OOM)是指应用系统中存在无法回收的内存或使用的内存过多，最终使得程序运行要用到的内存大于能提供的最大内存。此时程序就运行不了，系统会提示内存溢出。\nOOM发生后，会根据算法选择一个占用物理内存较高地进程，将其杀死并回收其内存资源，如果还不够，再循环这个过程。OOM是一个隐藏的性能杀手，我们应该尽量避免发生OOM后，系统杀死一些关键进程，如我们的关键业务进程，系统基础服务进程等。\n哪些内存可以被回收？ 不是所有的内存都可以回收，有些内存磁盘里没有直接回收会造成数据丢失的问题。我们可以采取LRU算法策略回收文件页和匿名页。\n文件页\n文件页是操作系统用来缓存文件数据的内存页，位于page cache里，使得读写磁盘的时候，不直接与磁盘交互，从而提升读写性能。所以，这部分数据在物理磁盘上使用存储的，我们直接将它回收，如果要再次使用再加载进来即可。当然，回收干净页可以直接回收，但是脏页需要先写入磁盘（fsync调用）才能被回收。\n匿名页\n这部分数据和文件页不一样，没有预先持久化，丢掉就没有了。所以，在被回收之前，需要先持久化到磁盘里（也就是换出），然后再被回收，需要的时候再加载到内存（换入）。\nLRU指的是回收近期最不常访问的内存\n内存回收会带来哪些性能影响？以及如何改善？ 内存回收的方式有两种，后台内存回收对性能影响甚微，但是可以缓解我们的内存剩余紧张问题，所以，我们应该尽早进行后台内存回收，上调后台内存回收的阈值，尽早触发后台内存回收；\n还有就是直接内存回收，会同步阻塞地执行，会阻塞后续进程的内存申请，CPU利用率会升高\n回收干净的文件页不会触发磁盘IO，但是回收脏页可能会触发两次磁盘IO，回收匿名页也可能触发两次IO。\n其中脏页触发第二次磁盘IO的概率小点，而匿名页很可能会再次被使用，触发第二次可能性更高。所以综合来看，回收文件页要比回收匿名页划算一些。\n除了OOM，内存满了还会发生什么？ OOM发生，会去kill内存占用较大的进程，以此释放内存空间。除了内存满了，会触发kill以外，还会发生：\n某些进程崩溃或无法创建新的进程：因为此时内存满了，无法再给进程或新的进程分配所需内存空间，导致进程崩溃或无法建立 系统变得异常缓慢或无响应：系统无法给新的进程或线程分配所需的内存资源，所以无法正常响应用户的请求。 如何保护一个进程不被OOM？ 调整OOM机制算法的参数oom_score_adj，使得进程被OOM kill掉的几率变低。\n在4GB物理内存的机器上，申请8GB内存会怎么样？ 如果是32位系统，进程理论上能申请最大3GB的虚拟内存（总共4GB，内核空间占了1GB），显然8GB\u0026gt;3GB，无法申请成功 而64位系统，进程理论上能申请最大128TB的虚拟内存，即时只有4GB物理内存。因为申请了这么多虚拟内存，不一定都使用对应的物理内存。 如果没有开启swap，且物理内存不够，就会触发OOM，杀死进程，不能分配8GB内存 如果开起swap，就算物理内存不够，还可以通过swap来不断换入换出，让4GB物理内存可以运行8GB内存需求的应用。 如何改进LRU以避免或减轻缓存污染和预读失效的问题？ 我们知道传统LRU就是：LRU算法使用数据结构链表来实现，表头是最近被访问的数据，表尾是最久之前被访问的数据。每当数据被访问，就将数据移到表头；当内存容量满了，就从表尾把数据换出到磁盘或者删除。\n缓存污染\n由于数据被访问就会移到表头，可能会造成这样一种情况：近期对大量非热点数据进行访问，导致近期热点数据移至链表尾部甚至被淘汰，然而这些非热点数据后续访问热度低下，从而对缓存造成了污染\n预读失效\n一个操作系统page cache的预读机制：系统读写文件数据，没有在page cache里找到数据，根据空间局部性原理，除了会从磁盘里加载数据所在的页到page cache里，还会加载相邻的几个页也到page cache里。然而如果这些预读的页没有被访问，那么就是预读失效，而且还会把热点数据挤出去。\n如何解决缓存污染问题？ lru会出现缓存污染问题的根本原因在于“只要数据被访问一次，就将数据移到链表首部”。仅仅访问一次，并不能代表这就是热点数据，有可能只是偶尔突发性地访问了大量数据，但是后续没有访问，这样会把一些热点数据挤出去。所以，参考lru-k算法，我们提交真正移到lru链表首部的条件，例如访问两次过后，我们认为这确实是一个热点数据，将这个热点数据放到活跃lru链表里。\nLinux 操作系统：在内存页被访问第二次的时候，才将页从 inactive list 升级到 active list 里。\n如何解决预读失效问题？ 预读失效的原因在于空间局部性原理，但是空间局部性原理在大部分情况下依然是有效的。\n我们让真正被访问的页才移动到lru链表头部，这样就能保证真正被访问的数据可以在内存里驻留更长的时间，防止被预读页挤出去。预读页有可能会被访问到，没访问之前，我们放在另一个不活跃lru链表；被访问之后，我们在将其移到活跃lru链表首部。这正是linux系统page cache缓存淘汰策略lru的改进。这个有点类似于2Q算法的思想：冷热分离。\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%85%B6%E4%B8%80/","tags":["操作系统内存管理"],"title":"八股系列|操作系统|内存管理篇一"},{"categories":["八股文","操作系统"],"contents":"系统结构 什么是内核与操作系统？ 内核属于操作系统，是操作系统最核心的部分，是用户和系统硬件的桥梁 操作系统是管理计算机系统资源的软件 内核具备什么功能？ 管理进程、线程，具备进程调度能力 管理内存，决定内存分配与回收 管理硬件设备 提供系统调用。应用程序运行内核指令时，都是需要中断并陷入内核态，内核去指向 内核是怎么工作的？ 计算机系统内存被划分为用户空间和内核空间，分别代表用户态和内核态。\n用户态：应用程序可以执行一些非特权指令和访问局部内存空间 内核态：如果需要执行特权指令，需要由内核来完成。并且可以访问所有内存空间。 所以，一般来说，程序在用户态执行，就是指程序在用户空间执行；而程序在内核空间时，就是指内核态执行\n当用户需要执行内核指令的时候，通过执行系统调用trap，然后cpu中断当前应用程序，由内核执行系统调用，完成后再次触发中断，把执行权交给用户态。\n对于内核的架构一般有这三种类型：\n宏内核，包含多个模块，整个内核像一个完整的程序； 微内核，有一个最小版本的内核，一些模块和服务则由用户态管理； 混合内核，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序； Linux 的内核设计是采用了宏内核，Window 的内核设计则是采用了混合内核。\n这两个操作系统的可执行文件格式也不一样， Linux 可执行文件格式叫作 ELF，Windows 可执行文件格式叫作 PE。\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/","tags":["操作系统结构"],"title":"八股系列|操作系统|结构"},{"categories":["杂"],"contents":"记一次picgo+github图床上传失败的过程 问题 这次遇到一个非常非常奇怪的问题。我和往常一样使用picgo在github上上传图片。但是突然没有预兆的给我报了err: connected etimedout的错误（指连接超时）。\n我ping了一下api.github.com，发现链路不通，数据包送不过去，但是浏览器还可以请求api.github.com。\n上网冲浪后得知：应该是服务端设置了相关策略对网络层icmp回显请求报文进行了限制；而访问网页用的是http协议，因此会出现此现象。\n所以这个现象聊胜于无。\n后来看到picgo的issue中，有不少人都提了这个问题。\n解决 其实这个就是系统代理本身的问题了：并不是所有软件或工具的网络请求都会走系统代理，有些应用的网络请求可能绕过代理，直接与网络通信。\n所以，我们需要给picgo手动设置代理，让picgo的所有请求一定要经过代理。如下图：\n我们在picgo中设置了vpn的服务端口。这样所有请求就可以转发到这里，包括一些外网的请求。\n因此，解决了代理问题。\n意外惊喜 我最近一直遇到这样问题：我明明已经开了梯子，但为什么我在bash中git pull或git push时总是报这个两个错：\n1 fatal: unable to access \u0026#39;https://github.com/cold-bin/cold-bin.github.io.git/\u0026#39;: OpenSSL SSL_read: Connection was reset, errno 10054 or\n1 fatal: unable to access \u0026#39;https://github.com/cold-bin/cold-bin.github.io.git/\u0026#39;: Failed to connect to github.com port 443 after 21109 ms: Timed out 其实和上面的问题一样，有些软件不走你的系统代理，直接走网络通信。解决方案也和上面一样：给git设置vpn代理的端口。\n1 2 3 git config --global http.proxy http://127.0.0.1:7890 git config --global https.proxy http://127.0.0.1:7890 # 我的clash在7890上系统代理 ","permalink":"https://cold-bin.github.io/post/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/","tags":[],"title":"记一次github图床上传异常的bug"},{"categories":["数据结构与算法"],"contents":" 以下文章摘自labuladong的算法小抄\n并查集（Union-Find）算法 读完本文，你不仅学会了算法套路，还可以顺便解决如下题目：\nLeetCode 力扣 难度 130. Surrounded Regionsopen in new window 130. 被围绕的区域open in new window 🟠 323. Number of Connected Components in an Undirected Graphopen in new window🔒 323. 无向图中连通分量的数目open in new window🔒 🟠 990. Satisfiability of Equality Equationsopen in new window 990. 等式方程的可满足性open in new window 🟠 并查集（Union-Find）算法是一个专门针对「动态连通性」的算法，我之前写过两次，因为这个算法的考察频率高，而且它也是最小生成树算法的前置知识，所以我整合了本文，争取一篇文章把这个算法讲明白。\n首先，从什么是图的动态连通性开始讲。\n#一、动态连通性 简单说，动态连通性其实可以抽象成给一幅图连线。比如下面这幅图，总共有 10 个节点，他们互不相连，分别用 0~9 标记：\n现在我们的 Union-Find 算法主要需要实现这两个 API：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 class UF { /* 将 p 和 q 连接 */ public void union(int p, int q); /* 判断 p 和 q 是否连通 */ public boolean connected(int p, int q); /* 返回图中有多少个连通分量 */ public int count(); } 这里所说的「连通」是一种等价关系，也就是说具有如下三个性质：\n1、自反性：节点 p 和 p 是连通的。\n2、对称性：如果节点 p 和 q 连通，那么 q 和 p 也连通。\n3、传递性：如果节点 p 和 q 连通，q 和 r 连通，那么 p 和 r 也连通。\n比如说之前那幅图，0～9 任意两个不同的点都不连通，调用 connected 都会返回 false，连通分量为 10 个。\n如果现在调用 union(0, 1)，那么 0 和 1 被连通，连通分量降为 9 个。\n再调用 union(1, 2)，这时 0,1,2 都被连通，调用 connected(0, 2) 也会返回 true，连通分量变为 8 个。\n判断这种「等价关系」非常实用，比如说编译器判断同一个变量的不同引用，比如社交网络中的朋友圈计算等等。\n这样，你应该大概明白什么是动态连通性了，Union-Find 算法的关键就在于 union 和 connected 函数的效率。那么用什么模型来表示这幅图的连通状态呢？用什么数据结构来实现代码呢？\n#二、基本思路 注意我刚才把「模型」和具体的「数据结构」分开说，这么做是有原因的。因为我们使用森林（若干棵树）来表示图的动态连通性，用数组来具体实现这个森林。\n怎么用森林来表示连通性呢？我们设定树的每个节点有一个指针指向其父节点，如果是根节点的话，这个指针指向自己。比如说刚才那幅 10 个节点的图，一开始的时候没有相互连通，就是这样：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class UF { // 记录连通分量 private int count; // 节点 x 的父节点是 parent[x] private int[] parent; /* 构造函数，n 为图的节点总数 */ public UF(int n) { // 一开始互不连通 this.count = n; // 父节点指针初始指向自己 parent = new int[n]; for (int i = 0; i \u0026lt; n; i++) parent[i] = i; } /* 其他函数 */ } 如果某两个节点被连通，则让其中的（任意）一个节点的根节点接到另一个节点的根节点上：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class UF { // 为了节约篇幅，省略上文给出的代码部分... public void union(int p, int q) { int rootP = find(p); int rootQ = find(q); if (rootP == rootQ) return; // 将两棵树合并为一棵 parent[rootP] = rootQ; // parent[rootQ] = rootP 也一样 count--; // 两个分量合二为一 } /* 返回某个节点 x 的根节点 */ private int find(int x) { // 根节点的 parent[x] == x while (parent[x] != x) x = parent[x]; return x; } /* 返回当前的连通分量个数 */ public int count() { return count; } } 这样，如果节点 p 和 q 连通的话，它们一定拥有相同的根节点：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 class UF { // 为了节约篇幅，省略上文给出的代码部分... public boolean connected(int p, int q) { int rootP = find(p); int rootQ = find(q); return rootP == rootQ; } } 至此，Union-Find 算法就基本完成了。是不是很神奇？竟然可以这样使用数组来模拟出一个森林，如此巧妙的解决这个比较复杂的问题！\n那么这个算法的复杂度是多少呢？我们发现，主要 API connected 和 union 中的复杂度都是 find 函数造成的，所以说它们的复杂度和 find 一样。\nfind 主要功能就是从某个节点向上遍历到树根，其时间复杂度就是树的高度。我们可能习惯性地认为树的高度就是 logN，但这并不一定。logN 的高度只存在于平衡二叉树，对于一般的树可能出现极端不平衡的情况，使得「树」几乎退化成「链表」，树的高度最坏情况下可能变成 N。\n所以说上面这种解法，find , union , connected 的时间复杂度都是 O(N)。这个复杂度很不理想的，你想图论解决的都是诸如社交网络这样数据规模巨大的问题，对于 union 和 connected 的调用非常频繁，每次调用需要线性时间完全不可忍受。\n问题的关键在于，如何想办法避免树的不平衡呢？只需要略施小计即可。\n#三、平衡性优化 我们要知道哪种情况下可能出现不平衡现象，关键在于 union 过程：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class UF { // 为了节约篇幅，省略上文给出的代码部分... public void union(int p, int q) { int rootP = find(p); int rootQ = find(q); if (rootP == rootQ) return; // 将两棵树合并为一棵 parent[rootP] = rootQ; // parent[rootQ] = rootP 也可以 count--; } } 我们一开始就是简单粗暴的把 p 所在的树接到 q 所在的树的根节点下面，那么这里就可能出现「头重脚轻」的不平衡状况，比如下面这种局面：\n长此以往，树可能生长得很不平衡。我们其实是希望，小一些的树接到大一些的树下面，这样就能避免头重脚轻，更平衡一些。解决方法是额外使用一个 size 数组，记录每棵树包含的节点数，我们不妨称为「重量」：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class UF { private int count; private int[] parent; // 新增一个数组记录树的“重量” private int[] size; public UF(int n) { this.count = n; parent = new int[n]; // 最初每棵树只有一个节点 // 重量应该初始化 1 size = new int[n]; for (int i = 0; i \u0026lt; n; i++) { parent[i] = i; size[i] = 1; } } /* 其他函数 */ } 比如说 size[3] = 5 表示，以节点 3 为根的那棵树，总共有 5 个节点。这样我们可以修改一下 union 方法：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class UF { // 为了节约篇幅，省略上文给出的代码部分... public void union(int p, int q) { int rootP = find(p); int rootQ = find(q); if (rootP == rootQ) return; // 小树接到大树下面，较平衡 if (size[rootP] \u0026gt; size[rootQ]) { parent[rootQ] = rootP; size[rootP] += size[rootQ]; } else { parent[rootP] = rootQ; size[rootQ] += size[rootP]; } count--; } } 这样，通过比较树的重量，就可以保证树的生长相对平衡，树的高度大致在 logN 这个数量级，极大提升执行效率。\n此时，find , union , connected 的时间复杂度都下降为 O(logN)，即便数据规模上亿，所需时间也非常少。\n#四、路径压缩 这步优化虽然代码很简单，但原理非常巧妙。\n其实我们并不在乎每棵树的结构长什么样，只在乎根节点。\n因为无论树长啥样，树上的每个节点的根节点都是相同的，所以能不能进一步压缩每棵树的高度，使树高始终保持为常数？\n这样每个节点的父节点就是整棵树的根节点，find 就能以 O(1) 的时间找到某一节点的根节点，相应的，connected 和 union 复杂度都下降为 O(1)。\n要做到这一点主要是修改 find 函数逻辑，非常简单，但你可能会看到两种不同的写法。\n第一种是在 find 中加一行代码：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 class UF { // 为了节约篇幅，省略上文给出的代码部分... private int find(int x) { while (parent[x] != x) { // 这行代码进行路径压缩 parent[x] = parent[parent[x]]; x = parent[x]; } return x; } } 这个操作有点匪夷所思，看个 GIF 就明白它的作用了（为清晰起见，这棵树比较极端）：\n用语言描述就是，每次 while 循环都会让部分子节点向上移动，这样每次调用 find 函数向树根遍历的同时，顺手就将树高缩短了。\n路径压缩的第二种写法是这样：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 class UF { // 为了节约篇幅，省略上文给出的代码部分... // 第二种路径压缩的 find 方法 public int find(int x) { if (parent[x] != x) { parent[x] = find(parent[x]); } return parent[x]; } } 我一度认为这种递归写法和第一种迭代写法做的事情一样，但实际上是我大意了，有读者指出这种写法进行路径压缩的效率是高于上一种解法的。\n这个递归过程有点不好理解，你可以自己手画一下递归过程。我把这个函数做的事情翻译成迭代形式，方便你理解它进行路径压缩的原理：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 这段迭代代码方便你理解递归代码所做的事情 public int find(int x) { // 先找到根节点 int root = x; while (parent[root] != root) { root = parent[root]; } // 然后把 x 到根节点之间的所有节点直接接到根节点下面 int old_parent = parent[x]; while (x != root) { parent[x] = root; x = old_parent; old_parent = parent[old_parent]; } return root; } 这种路径压缩的效果如下：\n比起第一种路径压缩，显然这种方法压缩得更彻底，直接把一整条树枝压平，一点意外都没有。就算一些极端情况下产生了一棵比较高的树，只要一次路径压缩就能大幅降低树高，从 摊还分析 的角度来看，所有操作的平均时间复杂度依然是 O(1)，所以从效率的角度来说，推荐你使用这种路径压缩算法。\n另外，如果使用路径压缩技巧，那么 size 数组的平衡优化就不是特别必要了。所以你一般看到的 Union Find 算法应该是如下实现：\njava 🟢cpp 🤖python 🤖go 🤖javascript 🤖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class UF { // 连通分量个数 private int count; // 存储每个节点的父节点 private int[] parent; // n 为图中节点的个数 public UF(int n) { this.count = n; parent = new int[n]; for (int i = 0; i \u0026lt; n; i++) { parent[i] = i; } } // 将节点 p 和节点 q 连通 public void union(int p, int q) { int rootP = find(p); int rootQ = find(q); if (rootP == rootQ) return; parent[rootQ] = rootP; // 两个连通分量合并成一个连通分量 count--; } // 判断节点 p 和节点 q 是否连通 public boolean connected(int p, int q) { int rootP = find(p); int rootQ = find(q); return rootP == rootQ; } public int find(int x) { if (parent[x] != x) { parent[x] = find(parent[x]); } return parent[x]; } // 返回图中的连通分量个数 public int count() { return count; } } Union-Find 算法的复杂度可以这样分析：构造函数初始化数据结构需要 O(N) 的时间和空间复杂度；连通两个节点 union、判断两个节点的连通性 connected、计算连通分量 count 所需的时间复杂度均为 O(1)。\n到这里，相信你已经掌握了 Union-Find 算法的核心逻辑，总结一下我们优化算法的过程：\n1、用 parent 数组记录每个节点的父节点，相当于指向父节点的指针，所以 parent 数组内实际存储着一个森林（若干棵多叉树）。\n2、用 size 数组记录着每棵树的重量，目的是让 union 后树依然拥有平衡性，保证各个 API 时间复杂度为 O(logN)，而不会退化成链表影响操作效率。\n3、在 find 函数中进行路径压缩，保证任意树的高度保持在常数，使得各个 API 时间复杂度为 O(1)。使用了路径压缩之后，可以不使用 size 数组的平衡优化。\n","permalink":"https://cold-bin.github.io/post/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B9%8B%E5%B9%B6%E6%9F%A5%E9%9B%86/","tags":["算法","并查集"],"title":"常见算法之并查集"},{"categories":["八股文","操作系统"],"contents":" 这里记录一些不是很体系化，但是偶尔乍现的问题与其解\n操作系统之硬件结构 CPU是如何执行程序的？ CPU是计算机的核心部件，包含控制器、运算器和寄存器等部件。CPU是通过“取指执行”过程来执行程序的。我们编写的高级语言程序，最终会被解释成CPU看得懂的指令。然后CPU执行以下过程来执行指令：\n取出指令：CPU根据程序计数器（PC）中存储的地址，将指令从主存（内存）读到指令寄存器（IR）中，然后PC指向下一个指令； 指令译码：按照预定的指令格式对指令进行译码，识别出不同指令的格式以及获取操作数的方式； 执行指令：CPU按照指令译码得到的控制信号进行操作，期间可能会发生主存（内存）访问； 访存取数：CPU根据指令的需要，有可能会访问内存，读取操作数。不同指令可能会采用不同的内存访问方式（立即寻址、直接寻找，间接寻址，寄存器寻址等）得到主存地址，然后根据地址取出操作数； 结果写回：CPU执行完当前指令后，会把结果写入寄存器、主存或其他位置。 然后又可以回到步骤1，循环执行； 当然，指令执行并非一番风顺，可能会发生无法执行的指令，CPU会中断现场，从而让中断程序来处理。\n例如：a = 2÷0这段程序，CPU无法执行，会发生中断报除0错误。\n磁盘比内存快几万倍？ 磁盘是机械结构，读写数据需要物理移动磁头，而内存读写是刷新DRAM里的电信号，两者相比内存要比磁盘块好几个数量级。\n下面是计算机系统存储系统组成图：\n从上到下的存储器构成了一个类似于多级缓存体系的结构，CPU先读速度与CPU最匹配的寄存器，没有就去读速度稍慢的L1级cache；还没有再去读速度更慢的L2、L3级cache；再没有去读速度更更慢内存，还没有就读速度最慢的磁盘\n寄存器：属于CPU部件，寄存器的访问速度非常快，一般要求在半个 CPU 时钟周期内完成读写 cache：使用SRAM芯片构成，只要有电就有数据，断电会丢失数据。每个 CPU 核心都有一块属于自己的 L1 高速缓存，指令和数据在 L1 是分开存放的，所以 L1 高速缓存通常分成指令缓存和数据缓存(哈弗结构)。 主存（内存）：使用DRAM芯片构成。相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。DRAM是通过电容带电来保存数据，所以需要不断的充放电，也就是刷新，来保证DRAM数据不丢失。和SRAM一样，断电会丢失数据，一直充电才能保存数据 SSD/HHD：数据保存在物理上，断电后依然存在，可以作为持久化存储的介质 如何写出让CPU跑得快的代码？ 如何才能让我们的代码跑得更快呢？应该从四方面考虑：\n如何提升数据的cache命中率？\n举个例子：我们先遍历第一行，再遍历第二行和先遍历第一列，再遍历第二列哪个更快？\n因为二维数组是按照行来存储的。所以，我们先遍历第一行，再遍历第二行的方式其实就是顺序读取，一次读取cache会把相邻的数据也加载到cache，而相邻cache的数据有极大概率是数组的下一个数据；而先遍历第一列，再遍历第二列并不是顺序读取，假设读取数组第一行第一列的数据，cache加载第一行的数据进来，刚好cache满了，此时再读第二行第一列的数据，cache里没有，只能再去内存里加载进来第二行，如此循环往复，cache的命中率显然低于前者。（CPU的局部性原理）\n总结：提升数据的缓存命中率的方式，是按照内存布局顺序访问\n如何提升指令的cache命中率？\n简单来说，就是在if判断之前，就预测接下来执行哪个分支的指令，从而把这些提前缓存下来，提高cache的命中率。\n如何提升多核CPU的cache命中率？\n每个CPU内核都有自己的L1、L2级cache和共同的L3级cache。如果一个线程在不同核心之间切换，可能会降低cache的命中率，因此我们可以把线程绑定到某个特定的核心上执行，从而提高cache命中率。\n如何使用到使用多核CPU的优势？\n单核CPU并不能做到并行，多核CPU可以做到并发+并行。我们可以将多个线程绑定到多个内核执行。也就是使用并发编程。\n怎样保证cache和主存的缓存一致性的？ CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相比内存高出很多。对于 Cache 里没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache 里面，最后 CPU 再从 Cache 读取数据。\n而对于数据的写入，CPU 都会先写入到 Cache 里面，然后再在找个合适的时机写入到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性：\n写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度； 写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好； 如何保证多级cache的缓存一致性？ 当今 CPU 都是多核的，每个核心都有各自独立的 L1/L2 Cache，只有 L3 Cache 是多个核心之间共享的。所以，我们要确保多核缓存是一致性的，否则会出现错误的结果。\n要想实现缓存一致性，关键是要满足 2 点：\n第一点是写传播，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心； 第二点是事物的串行化，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的； 基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。\nMESI 协议，是已修改、独占、共享、已失效这四个状态的英文缩写的组合。整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心。\n为什么 CPU 里会有 L1~L3 Cache ？ cache的访问速度比内存快的多，CPU先访问cache，cache没命中再从内存里调入块到cache中，这样可以避免直接访问速度更慢的内存，加快了访问速度。\n说一下CPU的局部性原理 空间局部性原理：相邻存储的数据或指令，很有可能会被再次使用 时间局部性原理：近期使用的数据或指令，很有可能会被再次使用 我们利用局部性原理，我们加载数据的时候，无论是从磁盘到内存，还是内存到磁盘，都不仅仅只是加载一个字，而是一个块（64字节）。\n说一下cache的伪共享问题 cache伪共享\ncache的伪共享指的就是如果并发地更改同一个cache块的两个变量，cache本身就没有起作用，还需要从主存再次加载cache进来。\n现代CPU架构中多核且含多级cache，其中L1 cache和L2 cache每个核独有，L3 cache共享\n但是，CPU修改数据并不是直接修改主存，一定是修改L1 cache中的数据，如果没有会去后面的cache去调，还没有再从主存里调块过来。因此，为了避免伪共享问题发生，我们空间换时间：\n可能发生并发读写同一个结构体里多个变量（地址虽然不一定连续，但是可能都在一个cache块）这种情况，我们就在每个可能并发修改的变量中间加个padding（cache block size），让这并发修改的两个变量位于两个不同的cache块。这样就避免了伪共享问题发生\n线程为什么叫轻量级进程呢？ 在 Linux 内核中，进程和线程都是用 task_struct 结构体表示的，区别在于线程的 task_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 Linux 中的线程也被称为轻量级进程，因为线程的task_struct相比进程的 task_struct 承载的 资源比较少，因此以「轻」得名。\n中断是什么？ 中断就是CPU接收来自硬件或软件的中断信号后，会中断当前程序的执行，转而去执行中断服务程序。中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。\n什么是软中断和硬中断？什么是软件中断？ 硬中断\n由与系统相连的硬件(比如网卡、硬盘)自动产生的。主要是用来通知操作系统系统外设状态的变化。比如当网卡收到数据包的时候，就会发出一个中断。\n软中断\n为了满足实时系统的要求，中断处理应该是越快越好。linux为了实现这个特点，当中断发生的时候，硬中断处理那些短时间就可以完成的工作，而将那些处理事件比较长的工作，放到后面来完成，也就是软中断(softirq)来完成。\n软中断由内核触发中断，异步处理上半部分剩下的工作\n所以，linux中断服务程序分为上半部（硬中断）和下半部（软中断）\n软中断指令\nint是软中断指令。\n中断向量表是中断号和中断处理函数地址的对应表。\nint n \u0026ndash; 触发软中断n。相应的中断处理函数的地址为：中断向量表地址 + 4 * n。\n软中断和硬中断区别\n软中断是执行中断指令产生的，而硬中断是由外设引发的。 硬中断的中断号是由中断控制器提供的，软中断的中断号由指令直接指出，无需使用中断控制器。 硬中断是可屏蔽的，软中断不可屏蔽。 硬中断处理程序要确保它能快速地完成任务，这样程序执行时才不会等待较长时间，称为上半部。 软中断处理硬中断未完成的工作，是一种推后执行的机制，属于下半部。 为什么负数要用补码表示？而不用原码或反码？ 原码就是数据二进制和最高位的符号位表示。最高位：0表示正数，1表示负数；\n原码就是原始数据。\n反码就是原码基础上所有数据位取反，符号位不变。\n反码就相当于数学里的负数，使得2-1=2+(-1)，把减法转化为加法，统一了加减法，让硬件电路设计更加简单。\n补码就是在反码基础上+1\n反码中0的表示有-0和+0两个不同的编码，但是代表同一个值。对于计算机来说很不好区分。所以采用补码的方式将0的表示统一\n因此，计算机中负数采用补码表示，可以降低硬件电路设计复杂度和消除0的编码不唯一性。\n计算机如何存储小数？ 现在绝大多数计算机使用的浮点数，一般采用的是 IEEE 制定的国际标准，这种标准形式如下图：\n这三个重要部分的意义如下：\n符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数； 指数位：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大； 尾数位：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度； 对于我们在从 float 的二进制浮点数转换成十进制时，要考虑到这个隐含的 1，转换公式如下：\n0.1 + 0.2 == 0.3 吗？ 答案是不等。为什么呢？那就要看浮点数是如何存储在计算机中的了：0.1是十进制表示，转化为二进制时，是一串无限循环二进制数，那么在IEEE 754规格化过程中势必要损失尾部多余的二进制位。同样0.2也无法完整表示。\n所以，0.1+0.2 ≈0.3\n","permalink":"https://cold-bin.github.io/post/%E5%85%AB%E8%82%A1%E7%B3%BB%E5%88%97%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84/","tags":["操作系统硬件结构"],"title":"八股系列|操作系统|硬件结构"},{"categories":["分布式系统"],"contents":"lab1——实现简易版的mapreduce框架 论文回顾 mapreduce架构 严格来讲，MapReduce是一种分布式计算模型，用于解决大于1TB数据量的大数据计算处理。著名的开源项目Hadoop和Spark在计算方面都实现的是MapReduce模型。从论文中可以看到花了不少篇幅在讲解这个模型的原理和运行过程，但同时也花了一点篇幅来讲解处理分布式系统实现中可能遇到的问题。\nMapReduce的模型设计很容易进行水平横向扩展以加强系统的能力，基本分为两种任务：map和reduce，通过map任务完成程序逻辑的并发，通过reduce任务完成并发结果的归约和收集，使用这个框架的开发者的任务就是把自己的业务逻辑先分为这两种任务，然后丢给MapReduce模型去运行。设计上，执行这两种任务的worker可以运行在普通的PC机器上，不需要使用太多资源。当系统整体能力不足时，通过增加worker即可解决。\n性能瓶颈 那么什么更容易导致系统性能扩展的瓶颈？CPU？内存？磁盘？还是网络？在2004年这篇文章问世的时候回答还是”网络带宽“最受限，论文想方设法的减少数据在系统内的搬运与传输，而到如今数据中心的内网速度要比当时快多了，因此如今更可能的答案恐怕就是磁盘了，新的架构会减少数据持久化到磁盘的次数，更多的利用内存甚至网络（这正是Spark的设计理念）\n如何处理较慢的网络？参考论文3.4节减少网络带宽资源的浪费，都尽量让输入数据保存在构成集群机器的本地硬盘上，并通过使用分布式文件系统GFS进行本地磁盘的管理。尝试分配map任务到尽量靠近这个任务的输入数据库的机器上执行，这样从GFS读时大部分还是在本地磁盘读出来。中间数据传输（map到reduce）经过网络一次，但是分多个key并行执行\n负载均衡 某个task运行时间比较其他N-1个都长，大家都必须等其结束那就尴尬了，因此参考论文3.5节、3.6节系统设计保证task比worker数量要多，做的快的worker可以继续先执行其他task，减少等待。（框架的任务调度后来发现更值得研究）\n容错 参考论文3.3节重新执行那些失败的MR任务即可，因此需要保证MR任务本身是幂等且无状态的。\n更特别一些，worker失效如何处理？将失败任务调配到其他worker重新执行，保证最后输出到GFS上的中间结果过程是原子性操作即可。（减少写错数据的可能）\nMaster失效如何处理？因为master是单点，只能人工干预，系统干脆直接终止，让用户重启重新执行这个计算\n其他 其实还有部分工程问题，这篇文章中并没有讨论，可能因为这些更偏重工程实践，比如：task任务的状态如何监控、数据如何移动、worker故障后如何恢复等。\n实现思路 lec1中讲到，mapreduce隐藏的细节，我们需要实现下面的这些：\n将应用程序代码发送到服务器 跟踪哪些任务已完成 将中间数据从 Maps“移到” Reduces 平衡服务器负载 从失败中恢复 coordinator 在lab1中，coordinator类似于论文中提到的master，是集群的管理者，负责分配job的task给worker进程。\ntask分两种：\n一种是map任务，负责将给定分区文件的数据处理成中间结果，然后将中间结果输出到本地磁盘。输出时，就进行分区。（分区映射函数hash(key) mod R）\n另一种是reduce任务，负责将中间结果收集合并，输出到文件里。\n一般来说，lab1中的一个reduce任务就对应一个输出文件，在map任务输出时，就已经在map worker磁盘本地分好区了。后面reduce任务就会从所有map worker里去取自己分区的中间结果集。\n1 2 3 4 5 6 7 8 // TaskType 任务类型，worker会承担map任务或者reduce任务 type TaskType uint8 const ( Map TaskType = iota + 1 Reduce Done ) coordinator管理过程 首先将map任务分配给worker，直到所有map任务完成为止 所有map任务完成后，coordinator才开始分发reduce任务 coordinator数据结构 在上面谈到，lab里并没有要求我们实现一个工业级别的mapreduce，仅仅要求我们实现简易版的demo\n所有的map任务完成时，才能分配reduce任务\n这也就意味着，如果只是部分map任务执行完毕时，我们需要等待其他map任务都执行完毕，才能执行reduce任务。怎么实现呢？在lab的hints里，给出了提示:使用sync.Cond条件变量实现\n1 2 3 当所有的task没有完成时，我们需要调用`sync.Cond`的`Wait`方法等待`Boradcast`唤醒 \u0026gt; 这样做的好处是只需要满足所有map worker都执行完毕才唤醒当前线程，使其执行下一步，也就是分配reduce任务，这条可以避免轮询cpu带来的性能消耗 上面的点提到”当所有task没有完成“这句话，显然，我们还需要记录map任务和reduce任务分别的完成情况。\n以前google内部的mapreduce（2004）实现在输入前，还会分区。但是lab里只需要将不同文件直接当作不同分区，不再细分为64m的block了，而且一个分区对应一个文件，一个文件对应一个map任务。所以，需要给出分区列表（也就是文件名列表）。\nlab里还要求解决”落伍者“问题。”落伍者“的大概意思就是，如果worker执行太久（lab里规定为10s）而没有finished，那么就认为这个worker寄掉了，此时，我们需要将task重新分配给其他的worker。\nlab中提到，我们需要探测集群中的所有worker是否存活。这里我们是否有必要给worker额外起一个协程来Ping/Pong吗？\n其实我们可以减少这个网络成本。\nworker首先会从coordinator拿task去执行，执行完毕后又会返回task的完成信息。我们认为只要在10s内，worker没有返回了完成信息，那么这个worker就寄掉了。\n如果10s过后，完成信息返回到了coordinator，又该怎么办呢？\n我们保证确认完成信息是幂等性的就可以了。例如我要在coodinator中更新map任务的完成情况mapTasksFinished的核心代码就是\n1 mapTasksFinished[task_id] = true 我们可以看到，这段代码是幂等的。\n所以，我们需要记录worker的完成时间点和拿取任务时间点的差值是否超过了10s，就代表worker是否寄掉。\n还有其他数据也需要附带，例如：锁、当前map和reduce任务的数目，job状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 type Coordinator struct { // Your definitions here. mu sync.Mutex cond *sync.Cond mapFiles []string // map任务的输入文件 nMapTasks int // map任务数，lab里规定的一个map任务对应一个worker。len(mapFiles) = nMapTasks nReduceTasks int // 记录任务的完成情况和何时分配，用以跟踪worker的处理进度 mapTasksFinished []bool // 记录map任务完成情况 mapTasksWhenAssigned []time.Time // 记录map任务何时分配 reduceTasksFinished []bool // 记录reduce任务完成情况 reduceTasksWhenAssigned []time.Time // 记录reduce任务何时分配 done bool // 所有reduce任务是否已经完成，也就是整个job是否完成 } coordinator提供的rpc 由上面得知，我们不需要提供pingrpc。我们需要提供申请task和完成task这两个rpc。\n申请task rpc\ntask有两种（map和reduce），刚开始我们只能分配map任务，map任务执行完毕过后，才能分配reduce任务。通过sync.Cond来实现”等待“。别忘了落伍者的处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 func (c *Coordinator) GetTask(args *GetTaskArgs, reply *GetTaskReply) error { c.mu.Lock() defer c.mu.Unlock() reply.NMapTasks = c.nMapTasks reply.NReduceTasks = c.nReduceTasks // 分配worker map任务，直到所有map任务执行完毕 for { mapDone := true for i, done := range c.mapTasksFinished { if !done /*task没有完成*/ { if c.mapTasksWhenAssigned[i].IsZero() || /*任务是否被分配*/ time.Since(c.mapTasksWhenAssigned[i]).Seconds() \u0026gt; 10 /*分配出去10s还没完成，认为worker寄掉了*/ { reply.TaskType = Map reply.TaskId = i reply.MapFileName = c.mapFiles[i] c.mapTasksWhenAssigned[i] = time.Now() return nil } else { mapDone = false } } } if !mapDone { /*没有完成，我们需要阻塞*/ c.cond.Wait() } else { /*全部map任务已经完成了*/ break } } // 此时，所有的map任务已经做完了，可以开始reduce任务了 for { rDone := true for i, done := range c.reduceTasksFinished { if !done /*task没有完成*/ { if c.reduceTasksWhenAssigned[i].IsZero() || /*任务是否被分配*/ time.Since(c.reduceTasksWhenAssigned[i]).Seconds() \u0026gt; 10 /*分配出去10s还没完成，认为worker寄掉了*/ { reply.TaskType = Reduce reply.TaskId = i c.reduceTasksWhenAssigned[i] = time.Now() return nil } else { rDone = false } } } if !rDone { /*没有完成，我们需要等待*/ c.cond.Wait() } else { /*全部map任务已经完成了*/ break } } // if the job is done reply.TaskType = Done c.done = true return nil } 完成task rpc\n当任务完成时，需要回传完成信息。并且，需要唤醒阻塞，进入下一步（可能是等待所有map任务的阻塞，或者是最终等待所有reduce任务的阻塞）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (c *Coordinator) FinishedTask(args *FinishedTaskArgs, reply *FinishedTaskReply) error { c.mu.Lock() defer c.mu.Unlock() switch args.TaskType { case Map: c.mapTasksFinished[args.TaskId] = true case Reduce: c.reduceTasksFinished[args.TaskId] = true default: return errors.New(\u0026#34;coordinator: not support this task type\u0026#34;) } c.cond.Broadcast() return nil } 其他 定时轮询\n我们其实并不知道何时所有map任务完成或所有reduce任务完成，所以，我们需要轮询去唤醒阻塞，然后检测是否满足条件，不满就继续阻塞等待唤醒\n1 2 3 4 5 6 7 8 9 // 唤醒 go func() { for { c.mu.Lock() c.cond.Broadcast() c.mu.Unlock() time.Sleep(time.Second) } }() 原子重命名文件与重命名\n若在mapf或之后要使用的reducef中，进程由于某些原因退出了，这样产生的文件就是不完整的。但reduce任务分不清哪些是成功的map任务输出，哪些不是。所以，我们一开始不使用mr-m-n形式的文件名，而是创建一个命名不确定的临时文件。当所有任务成功完成之后，再把这个文件重命名为正确的mr-m-n。\nreduce的输出临时文件与map任务差不多。\nworker worker的职责就是负责执行map或reduce任务。lab里要求仅仅只是实现一个单机多进程的mapreduce。\n首先，worker要去coordinator那里调用拿任务的rpc，刚开始map没执行完毕拿的是map任务；后面执行完毕则拿的是reduce任务； 然后，拿到任务就执行，执行完毕时，需要调用完成任务的rpc，返回coodinator一些信息 最后，不断循环这个过程，以复用worker的资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { // Your worker implementation here. for { args := GetTaskArgs{} reply := GetTaskReply{} call(\u0026#34;Coordinator.GetTask\u0026#34;, \u0026amp;args, \u0026amp;reply) switch reply.TaskType { case Map: performMapTask(reply.MapFileName, reply.TaskId, reply.NReduceTasks, mapf) case Reduce: performReduce(reply.TaskId, reply.NMapTasks, reducef) case Done: return default: log.Fatal(\u0026#34;worker: not support this task type\u0026#34;) } call(\u0026#34;Coordinator.FinishedTask\u0026#34;, \u0026amp;FinishedTaskArgs{TaskId: reply.TaskId, TaskType: reply.TaskType}, \u0026amp;FinishedTaskReply{}) } } 如何执行map任务 首先，打开输入文件，读取所有文件数据（这里lab简单规定一个文件一个分区，对应一个map任务，忽略论文中的分区 然后，调用map函数得到中间结果 再将中间结果序列化写入R个临时文件（先通过哈希函数找到key的分区）（一是可以避免文件名重复，二是为了map和reduce的原子性，例如可以避免中途断电时，使得最后只有一部分结果这种情况。 最后，我们map执行已经完毕了，这时我们再将临时文件重命名为map的中间文件结果，以供reduce任务读取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 func performMapTask(fn string, taskN int, nReduceTasks int, mapf func(string, string) []KeyValue) { f, err := os.Open(fn) if err != nil { log.Fatalf(\u0026#34;open error:%v\u0026#34;, err) } content, err := io.ReadAll(f) if err != nil { log.Fatalf(\u0026#34;read error:%v\u0026#34;, err) } f.Close() kvs := mapf(fn, quickB2S(content)) // 处理中间文件 tmpFs := make([]*os.File, 0, nReduceTasks) tmpFNs := make([]string, 0, nReduceTasks) encoders := make([]*json.Encoder, 0, nReduceTasks) for r := 0; r \u0026lt; nReduceTasks; r++ { tmpF, err := os.CreateTemp(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;) if err != nil { log.Fatalf(\u0026#34;create temporary file error: %v\u0026#34;, err) } tmpFs = append(tmpFs, tmpF) tmpFNs = append(tmpFNs, tmpF.Name()) encoders = append(encoders, json.NewEncoder(tmpF)) } // 写入临时的中间文件 // 先json编码 for _, kv := range kvs { err := encoders[ihash(kv.Key)%nReduceTasks].Encode(\u0026amp;kv) if err != nil { log.Fatalf(\u0026#34;json encode error: %v\u0026#34;, err) } } // 关闭临时文件 for _, f := range tmpFs { f.Close() } // 原子重命名，其实是为了防止重名和原子性破坏 for r := 0; r \u0026lt; nReduceTasks; r++ { err := renameIntermediateFIle(tmpFNs[r], taskN, r) if err != nil { log.Fatalf(\u0026#34;rename error: %v\u0026#34;, err) } } } 如何执行reduce任务 首先打开map输出的中间文件，并反序列化文件内容，拿到(key,list(values))； 然后对输入列表排序，让中间 key/value pair 数据的处理顺序是按照 key 值增量顺序处理。方便后面实现随机存取； 再次，我们还是通过创建临时文件来保存我们的结果，然后开始聚合相同key的数据集； 最后，我们的reduce任务已经完毕了，这时我们再将临时文件重命名为reduce的输出文件。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 func performReduce(taskN, nMapTasks int, reducef func(string, []string) string) { kvs := make([]KeyValue, 0) for i := 0; i \u0026lt; nMapTasks; i++ { intermediatef := getIntermediateFIle(i, taskN) f, err := os.Open(intermediatef) if err != nil { log.Fatalf(\u0026#34;open error: %v\u0026#34;, err) } dec := json.NewDecoder(f) for { var kv KeyValue if err := dec.Decode(\u0026amp;kv); err != nil { break } kvs = append(kvs, kv) } f.Close() } // 排序 sort.Sort(ByKey(kvs)) // 再创建临时文件来写reduce的结果，后面通过原子重命名只有一个最终结果文件 tmpf, err := os.CreateTemp(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;) if err != nil { log.Fatalf(\u0026#34;create temporary file error: %v\u0026#34;, err) } // 把相同的key的所有value聚合在一起 i := 0 j := 0 for i \u0026lt; len(kvs) { j = i + 1 for j \u0026lt; len(kvs) \u0026amp;\u0026amp; kvs[j].Key == kvs[i].Key { j++ } vs := make([]string, 0, len(kvs)) for k := i; k \u0026lt; j; k++ { vs = append(vs, kvs[k].Value) } output := reducef(kvs[i].Key, vs) _, err := fmt.Fprintf(tmpf, \u0026#34;%v %v\\n\u0026#34;, kvs[i].Key, output) if err != nil { log.Fatalf(\u0026#34;open or write error: %v\u0026#34;, err) } i = j } err = renameReduceOutFile(tmpf.Name(), taskN) if err != nil { log.Fatalf(\u0026#34;rename error: %v\u0026#34;, err) } } 感言 论文其实大部分还是看得懂，最初的时候实现起来毫无头绪，论文给的是一个参考的、极其模糊的实现，而且还是分布式下的。但是lab里没有要求我们需要通过网络来把map worker的中间结果集输送到reduce worker机子里，仅仅只是实现一个单机多进程版本的mapreduce。\n这里的实现也是参考了一下lab1 Q\u0026amp;A 才知道我们仅仅只是做一个单机多进程版本的mapreduce，不需要考虑在网络传输map worker的中间结果集。\n实现上有很多有意思的地方：\n中间任务的输出结果，我们写入临时文件里，处理完毕过后，我们再重命名为最终结果文件；如果中途失败，那么我们得不到这个中间文件，因为该文件是临时文件。这样可以保证我们的map任务或reduce任务要么失败没有输出，要么成功有输出，从而保证了原子性 lab并不像论文中描述的：一旦map任务有一个执行完毕了，那么reduce任务就可以开始启动了。而是采取更简单的实现策略：所有map任务都完成了，才可以启动reduce任务。所以，这里需要有一个等待机制，满足“所有map任务执行完毕”的条件时，我们才能分配reduce任务。所以，这里使用sync.Cond来实现。 如何实现“落伍者”的检测呢？我们可以记录分配任务时和完成任务时的时间差，太长就认为worker超时了。 代码传输门\n","permalink":"https://cold-bin.github.io/post/mit6.824%E4%B9%8Blab1/","tags":["mit6.824 labs"],"title":"Mit6.824之lab1"},{"categories":["golang"],"contents":"插件 以下文章摘自go语言设计与实践里的插件系统\n熟悉 Go 语言的开发者一般都非常了解 Goroutine 和 Channel 的原理，包括如何设计基于 CSP 模型的应用程序，但是 Go 语言的插件系统是很少有人了解的模块，通过插件系统，我们可以在运行时加载动态库实现一些比较有趣的功能。\n8.1.1 设计原理 # Go 语言的插件系统基于 C 语言动态库实现的，所以它也继承了 C 语言动态库的优点和缺点，我们在本节中会对比 Linux 中的静态库和动态库，分析它们各自的特点和优势。\n静态库或者静态链接库是由编译期决定的程序、外部函数和变量构成的，编译器或者链接器会将程序和变量等内容拷贝到目标的应用并生成一个独立的可执行对象文件1； 动态库或者共享对象可以在多个可执行文件之间共享，程序使用的模块会在运行时从共享对象中加载，而不是在编译程序时打包成独立的可执行文件2； 由于特性不同，静态库和动态库的优缺点也比较明显；只依赖静态库并且通过静态链接生成的二进制文件因为包含了全部的依赖，所以能够独立执行，但是编译的结果也比较大；而动态库可以在多个可执行文件之间共享，可以减少内存的占用，其链接的过程往往也都是在装载或者运行期间触发的，所以可以包含一些可以热插拔的模块并降低内存的占用。\n图 8-1 静态库与动态库\n使用静态链接编译二进制文件在部署上有非常明显的优势，最终的编译产物也可以直接运行在大多数的机器上，静态链接带来的部署优势远比更低的内存占用显得重要，所以很多编程语言包括 Go 都将静态链接作为默认的链接方式。\n插件系统 # 在今天，动态链接带来的低内存占用优势虽然已经没有太多作用，但是动态链接的机制却可以为我们提供更多的灵活性，主程序可以在编译后动态加载共享库实现热插拔的插件系统。\n图 8-2 插件系统\n通过在主程序和共享库直接定义一系列的约定或者接口，我们可以通过以下的代码动态加载其他人编译的 Go 语言共享对象，这样做的好处是主程序和共享库的开发者不需要共享代码，只要双方的约定不变，修改共享库后也不需要重新编译主程序。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Driver interface { Name() string } func main() { p, err := plugin.Open(\u0026#34;driver.so\u0026#34;) if err != nil { panic(err) } newDriverSymbol, err := p.Lookup(\u0026#34;NewDriver\u0026#34;) if err != nil { panic(err) } newDriverFunc := newDriverSymbol.(func() Driver) newDriver := newDriverFunc() fmt.Println(newDriver.Name()) } Go\n上述代码定义了 Driver 接口并认为共享库中一定包含 func NewDriver() Driver 函数，当我们通过 plugin.Open 读取包含 Go 语言插件的共享库后，获取文件中的 NewDriver 符号并转换成正确的函数类型，可以通过该函数初始化新的 Driver 并获取它的名字了。\n操作系统 # 不同的操作系统会实现不同的动态链接机制和共享库格式，Linux 中的共享对象会使用 ELF 格式3并提供了一组操作动态链接器的接口，在本节的实现中我们会看到以下的几个接口4：\n1 2 3 4 void *dlopen(const char *filename, int flag); char *dlerror(void); void *dlsym(void *handle, const char *symbol); int dlclose(void *handle); C\ndlopen 会根据传入的文件名加载对应的动态库并返回一个句柄（Handle）；我们可以直接使用 dlsym 函数在该句柄中搜索特定的符号，也就是函数或者变量，它会返回该符号被加载到内存中的地址。因为待查找的符号可能不存在于目标动态库中，所以在每次查找后我们都应该调用 dlerror 查看当前查找的结果。\n8.1.2 动态库 # Go 语言插件系统的全部实现都包含在 plugin 中，这个包实现了符号系统的加载和决议。插件是一个带有公开函数和变量的包，我们需要使用下面的命令编译插件：\n1 go build -buildmode=plugin ... Bash\n该命令会生成一个共享对象 .so 文件，当该文件被加载到 Go 语言程序时会使用下面的结构体 plugin.Plugin 表示，该结构体中包含文件的路径以及包含的符号等信息：\n1 2 3 4 5 type Plugin struct { pluginpath string syms map[string]interface{} ... } Go\n与插件系统相关的两个核心方法分别是用于加载共享文件的 plugin.Open 和在插件中查找符号的 plugin.Plugin.Lookup，本节将详细介绍它们的实现原理。\nCGO # 在具体分析 plugin 包中几个公有方法之前，我们需要先了解一下包中使用的两个 C 语言函数 plugin.pluginOpen 和 plugin.pluginLookup；plugin.pluginOpen 只是简单包装了一下标准库中的 dlopen 和 dlerror 函数并在加载成功后返回指向动态库的句柄：\n1 2 3 4 5 6 7 static uintptr_t pluginOpen(const char* path, char** err) { void* h = dlopen(path, RTLD_NOW|RTLD_GLOBAL); if (h == NULL) { *err = (char*)dlerror(); } return (uintptr_t)h; } C\nplugin.pluginLookup 使用了标准库中的 dlsym 和 dlerror 获取动态库句柄中的特定符号：\n1 2 3 4 5 6 7 static void* pluginLookup(uintptr_t h, const char* name, char** err) { void* r = dlsym((void*)h, name); if (r == NULL) { *err = (char*)dlerror(); } return r; } C\n这两个函数的实现原理都比较简单，它们的作用也只是简单封装标准库中的 C 语言函数，让它们的签名看起来更像是 Go 语言中的函数签名，方便在 Go 语言中调用。\n加载过程 # 用于加载共享对象的函数 plugin.Open 会将共享对象文件的路径作为参数并返回 plugin.Plugin 结构：\n1 2 3 func Open(path string) (*Plugin, error) { return open(path) } Go\n上述函数会调用私有的函数 plugin.open 加载插件，它是插件加载过程的核心函数，我们可以将该函数拆分成以下几个步骤：\n准备 C 语言函数 plugin.pluginOpen 的参数； 通过 cgo 调用 plugin.pluginOpen 并初始化加载的模块； 查找加载模块中的 init 函数并调用该函数； 通过插件的文件名和符号列表构建 plugin.Plugin 结构； 首先是使用 cgo 提供的一些结构准备调用 plugin.pluginOpen 所需要的参数，下面的代码会将文件名转换成 *C.char 类型的变量，该类型的变量可以作为参数传入 C 函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func open(name string) (*Plugin, error) { cPath := make([]byte, C.PATH_MAX+1) cRelName := make([]byte, len(name)+1) copy(cRelName, name) if C.realpath( (*C.char)(unsafe.Pointer(\u0026amp;cRelName[0])), (*C.char)(unsafe.Pointer(\u0026amp;cPath[0]))) == nil { return nil, errors.New(`plugin.Open(\u0026#34;` + name + `\u0026#34;): realpath failed`) } filepath := C.GoString((*C.char)(unsafe.Pointer(\u0026amp;cPath[0]))) ... var cErr *C.char h := C.pluginOpen((*C.char)(unsafe.Pointer(\u0026amp;cPath[0])), \u0026amp;cErr) if h == 0 { return nil, errors.New(`plugin.Open(\u0026#34;` + name + `\u0026#34;): ` + C.GoString(cErr)) } ... } Go\n当我们拿到了指向动态库的句柄之后会调用 plugin.lastmoduleinit，链接器会将它会链接到运行时的 runtime.plugin_lastmoduleinit 函数上，它会解析文件中的符号并返回共享文件的目录和其中包含的全部符号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func open(name string) (*Plugin, error) { ... pluginpath, syms, errstr := lastmoduleinit() if errstr != \u0026#34;\u0026#34; { plugins[filepath] = \u0026amp;Plugin{ pluginpath: pluginpath, err: errstr, } pluginsMu.Unlock() return nil, errors.New(`plugin.Open(\u0026#34;` + name + `\u0026#34;): ` + errstr) } ... } Go\n在该函数的最后，我们会构建一个新的 plugin.Plugin 结构体并遍历 plugin.lastmoduleinit 返回的全部符号，为每一个符号调用 plugin.pluginLookup：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func open(name string) (*Plugin, error) { ... p := \u0026amp;Plugin{ pluginpath: pluginpath, } plugins[filepath] = p ... updatedSyms := map[string]interface{}{} for symName, sym := range syms { isFunc := symName[0] == \u0026#39;.\u0026#39; if isFunc { delete(syms, symName) symName = symName[1:] } fullName := pluginpath + \u0026#34;.\u0026#34; + symName cname := make([]byte, len(fullName)+1) copy(cname, fullName) p := C.pluginLookup(h, (*C.char)(unsafe.Pointer(\u0026amp;cname[0])), \u0026amp;cErr) valp := (*[2]unsafe.Pointer)(unsafe.Pointer(\u0026amp;sym)) if isFunc { (*valp)[1] = unsafe.Pointer(\u0026amp;p) } else { (*valp)[1] = p } updatedSyms[symName] = sym } p.syms = updatedSyms return p, nil } Go\n上述函数在最后会返回一个包含符号名到函数或者变量映射的 plugin.Plugin 结构体，调用方可以将该结构体作为句柄查找其中的符号，需要注意的是，我们在这段代码中省略了查找 init 并初始化插件的过程。\n符号查找 # plugin.Plugin.Lookup 可以在 plugin.Open 返回的结构体中查找符号 plugin.Symbol，该符号是 interface{} 类型的一个别名，我们可以将它转换成变量或者函数真实的类型：\n1 2 3 4 5 6 7 8 9 10 func (p *Plugin) Lookup(symName string) (Symbol, error) { return lookup(p, symName) } func lookup(p *Plugin, symName string) (Symbol, error) { if s := p.syms[symName]; s != nil { return s, nil } return nil, errors.New(\u0026#34;plugin: symbol \u0026#34; + symName + \u0026#34; not found in plugin \u0026#34; + p.pluginpath) } Go\n上述方法调用的私有函数 plugin.lookup 实现比较简单，它直接利用了结构体中的符号表，如果没有找到对应的符号会直接返回错误。\n8.1.3 小结 # Go 语言的插件系统利用了操作系统的动态库实现模块化的设计，它提供功能虽然比较有趣，但是在实际使用中会遇到比较多的限制，目前的插件系统也仅支持 Linux、Darwin 和 FreeBSD，在 Windows 上是没有办法使用的。因为插件系统的实现基于一些黑魔法，所以跨平台的编译也会遇到一些比较奇葩的问题，作者在使用插件系统时也踩过很多坑，如果对 Go 语言不是特别了解，还是不建议使用该模块的。\n","permalink":"https://cold-bin.github.io/post/go%E6%8F%92%E4%BB%B6/","tags":["go插件开发"],"title":"Go插件"},{"categories":["计算机网络"],"contents":"lab0 reference link lab0实验手册 implementation Set up GNU/Linux on your computer 简单地安装CS144 VirtualBox和c++环境，以方便后续地测试。\nNetworking by hand 接下来就是使用简单地telnet来构造HTTP请求和SMTP请求，并得到对应的响应。 （亲手写出一部分报文\nListening and connecting 通过netcat建立一个全双工通信的服务端，感受netcat的使用。 （类似于即时通讯服务\nWriting a network program by using an OS stream socket\npart 1 lab0的最后一项任务就是使用原生的tcp socket来编写http报文结构，从而在socket的基础上发起一次http请求 详见lab0/minow/apps/webget.cc代码实现。\npart2 part2实现可靠字节流，使用一个普通string来存储字节流\nlab1 reference link lab1实验手册 博客 implementation 在tcp/ip协议栈中，数据传输的可靠性不是网络来承担的，而是交由端系统来承担的。网络中的有序字节流从一端发送到另一端，跨越了诸多网络的路由器 ，期间难免出现字节流传输的乱序、重复、重叠等。而lab1就是需要在这样不可靠字节流中去建立可靠的字节流。\n具体实现 这里参考了上面的博客 lab2 reference link lab2实验手册 implementation wrap与unwrap wrap是将absolute seqno转化为seqno 由于absolute seqno是非循环序号，seqno是循环序号，所以需要取模转化。（当然也可以直接截断。\nunwrap是将seqno转化为absolute seqno\ncheckpoint其实就是first_unassembled_index\n这里的处理比较麻烦。我最开始的想法是，循环找出最小的checkpoint - (seqno+x * 2^32)，也就是离得最近的x。 但是复杂度比较高，看了大佬的博客，利用位运算，可以将O(x)的时间复杂度降低为O(1). 官方让我们找到离checkpoint最近的absolute seqno，因为给出seqno，会有多个absolute seqno与之对应，\nTCP Receiver 参考上面seqno、absolute seqno与stream index的对应关系图\nreceive时，直接在reassembler中插入absolute seqno，显然不是期待的stream_index； send时，需要考虑到available_capacity不能超过UINT_MAX以及close时，发送的fin报文也要占据一个序号 lab3 reference link lab3实验手册 博客 implementation 这里参考了博客实现\n实现ARQ重传机制 lab4 reference link lab4实验手册 博客 implementation Lab4 要求实现网络接口部分， 打通网络数据报 （Internet datagrams） 和链路层的以太网帧（link-layer Ethernet frames） 之间的桥梁。 也就是，实现ip数据报转化为mac帧。ip数据报在转化为mac帧时，最重要的就是要知道目的mac地址，而mac地址我们可以通过arp协议来学习到。 所以，这里就牵扯到了arp协议的实现：\n目的ip地址与mac地址的缓存映射。最多保存30s； 端系统可以组装ip报文和arp请求与响应报文，也能解析之； 不论是arp请求还是arp响应，端系统拿到过后都可以学习到对等端的ip的其mac地址的映射关系 而且，我们并不能任性发送arp请求，我们只能等待相同的arp请求发出去5秒后没有收到arp响应才再次发送，这是为了防止频繁地arp广播导致链路阻塞 端系统在接收报文时，收到ip数据报自然不用说，该怎么处理就怎么处理，但是收到arp报文时，需要进一步处理： 如果是arp请求报文 那么我们在校验合法性通过后，还需要学习arp请求的来源ip地址和来源mac地址的映射关系，并且构造arp响应，返回自己的ip与mac地址的映射关系，以供arp请求方学习 如果是arp响应报文 那么我们在校验合法性通过后，再从arp响应报文中学习到目的ip地址和目的mac地址的映射关系。除了mac地址学习外，我们还需要将arp请求等待列表清空 总之，我们通过arp协议拿到了目的mac地址过后，剩下的事情就非常简单了（将ip数据报组装成mac帧，以方便发送到数据链路上 实验手册给的实现已经很详细了，翻译成代码即可。\nlab5 reference link lab5实验手册 博客 implementation lab5的要求是在lab4实现的网络接口上，实现ip router.路由器有多个网络接口，可以在其中任何一个接口上接收 Internet 数据报。 路由器的工作是根据路由表转发它获得的数据报：路由表是一个规则列表，告诉路由器对于任何给定的数据报:\n在哪个network interface发出去 确定next hop 实验手册的Q \u0026amp; A中对route table的数据结构要求很低，允许实现O(N)时间复杂度。所以，这里直接使用std:list\u0026lt;type\u0026gt;。\nlab5的要求其实很简单了。我们只需要实现路由最长前缀匹配即可，并不需要实现动态路由的一些协议（RIP、OSPF、BGP 或 SDN 控制器） 当然除了这些，我们还需要注意：\n每次路由转发ip数据报时，ttl需要减一，直至ttl等于0时，路由器会自动丢弃它 如果路由表里没有找到next hop，也会丢弃ip数据报 如果路由的数据报缓存已满，也会丢弃掉（lab5里并没有说 lab6 no code\n使用以前的所有实现的lab来创建一个真实的网络，其中包括网络堆栈（主机和路由器），与另一台主机上实现的网络堆栈进行通信。\nhow to do lab6实验手册 ","permalink":"https://cold-bin.github.io/post/cs144-lab/","tags":["cs144-labs"],"title":"Cs144 Lab"},{"categories":["计算机网络"],"contents":"[toc]\n一、网络攻击的类型 网络攻击分为两大类：被动攻击和主动攻击。\n截获：从网络上窃听他人的通信内容，但不干扰原报文在源端和目的端的传送。\n篡改：捕获报文并篡改报文后再发送给目的站。\n恶意程序：计算机病毒。\n拒绝服务 DoS：指攻击者向互联网上的某个服务器不停地发送大量分组，该服务器的网络资源耗尽而无法向其他正常客户端主机提供服务。若从互联网上的成百上千的网站集中攻击一个网站，则称为分布式拒绝服务 DDoS 。 有时也把这种攻击称为网络带宽攻击。\n对付被动攻击：采用加密技术（让截获者读不懂报文内容）\n对付主动攻击：采用加密技术 + 鉴别技术（指鉴别对方身份）\n二、密钥密码体制 安全的计算机网络应达到以下目标\n保密性：只有信息的发送方和接收方才能懂得所发送信息的内容。\n端点鉴别（身份鉴别）：鉴别信息的发送方和接收方的真实身份。\n信息的完整性：确保信息的内容未被篡改过。\n可以通过密钥加密来做到以上3点。下面是数据加密模型：\n通信双方：A 和 B。\nA 和 B 分别持有密钥 K(E) 和 K(D)，这可以是一对对称密钥或者一对非对称密钥。\nA 和 B 分别持有一对加密算法 E 和 D。\nA将明文X通过密钥K(e) 和 算法E进行加密得到密文Y，可以把E看做是一个函数，密钥 K(e)和明文X作为这个函数的参数，返回值是Y。\n传输给B后，用D(K(d), Y)解密得到X。\n需要注意：\n0、密钥 K本质是一串字符串。\n1、D 和 E 都既可以是加密算法，也可以是解密算法，其本质都是将传入的字符串数据变为另一个字符串，所以我更倾向于说成是 D运算和E运算，而不是说 加密算法 和 解密算法。\n2、D 和 E算法 是一对算法，即不论密钥K(e)和K(d)是对称密钥（即 K(e) = K(d)）还是非对称密钥（即 K(e) ≠ K(d)）都有： D运算是E运算的逆运算，E运算也是D运算的逆运算。\nE(K(e), X) = Y\nD(K(d), Y) = X\n所以：\nD(K(d), E(K(e), X)) = X\n3、使用密钥和加密算法来加密或解密一串数据时，所花费的时间和数据本身的长度成正比。\n4、加密算法 E 和解密算法 D 是公开的。而密钥 K(e) 和 K(d)如果是对称密钥，则这两个密钥是保密的，如果密钥 K(e) 和 K(d)如果是非对称密钥则公钥公开，私钥保密。\n下面介绍两种密码体制：对称密钥密码体制 和 非对称密钥密码体制\n1、对称密钥密码体制 该体制是 加密密钥与解密密钥都使用相同密钥 的密码体制。对称密钥又称为共享密钥。\n加密过程如下：\nE(K, X) = Y\nD(K, Y) = X\n所以：\nD(K, E(K, X)) = X\n2、非对称密钥密码体制 通信双方持有一对密钥分别是对外公开的公钥PK 和 只有自己知道的私钥SK。例如通信双方 A 和 B，A有一对密钥 PK(a) 和 SK(a)，B也有一对密钥 PK(b) 和 SK(b)。\n当A作为发送方向B发送数据时，A要用B的公钥 PK(b) 对明文X加密得到Y，B用私钥 SK(b) 对Y解密得到X。当B作为发送方向A发送数据时，则和上述过程一样要用A的一对密钥加解密。\n目前最流行的公钥密码体制是RSA体制。\n公钥算法的特点：\n(1) 密钥对产生器产生出接收者 B 的一对密钥：\n加密密钥 PKB 和 解密密钥 SKB 。\n加密密钥 PKB 就是接收者 B 的公钥，向公众公开。\n解密密钥 SKB 就是接收者 B 的私钥，对其他任何人都保密。\n(2) 发送者 A 用 B 的公钥 PKB 对明文 X 加密（E 运算），然后发送给 B。 接收者 B 用自己的私钥 SKB 解密（D 运算），即可恢复出明文：D(SK(b), E(PK(b), X)) = X\n(3) 从已知的 PKB 实际上不可能推导出 SKB。\n(4) 加密密钥是公开的，但不能用来解密：D(PK(b), E(PK(b), X)) ≠ X\n3、对称密钥和公开密钥的区别 使用对称密钥： 在通信信道上可以进行一对一的双向保密通信。即拥有公钥的只有一个客户端和一个服务端。\n使用公开密钥： 在通信信道上可以是多对一的单向保密通信。 即拥有公钥的客户端可以有多个，这些客户端都可以和有私钥的服务端通信，单向是指 一对密钥只能用在一个方向的通信（A-\u0026gt;B用B的一对密钥），另一个方向的通信要用另一对密钥才行。\n公钥加密算法的开销（运算量）较大（是对称密钥算法的3倍）。因此现实中都是使用对称密钥进行加密。\n使用密钥进行加密满足了保密性的要求，但还没有满足实体鉴别和完整性鉴别的要求。不过密钥不仅可以用来加密，还可以用来签名，密钥用来签名可以做到实体鉴别。\n三、报文鉴别 报文鉴别需要做到两点：验证通信的对方是否是自己要通信的对象而不是篡改者 和 验证报文内容是否被篡改。\n报文鉴别通过数字签名实现。\n1、数字签名原理 例如 A 发送报文X给 B，B要鉴别发送者是A：\n1、首先，A 用其私钥 SKA 对报文 X 进行 D 运算得到的密文，这个过程就是签名。\n2、B 为了核实签名，用 A 的公钥 PKA 对密文Y进行 E 运算，还原出明文 X\n为什么这样可以做到实体鉴别？\n因为SKA只有A才有，B如果能用PKA解密成功（得到的明文是可读的），就说明对方是A。如果B解密出来的明文是不可读的，这段不可读报文也不会对B产生危害。\n签名的本质是什么？\n签名的本质是在报文中加一些只有发送方才有的特殊信息，接收方看到报文中的这个特殊信息就知道一定是A发过来的，例如上面对报文用A的私钥进行D运算，A的私钥就是A独有的东西。\n所以具体来说，签名就是用自己的私钥给 待签名的对象 进行签名运算。\n核实签名的本质是对签名运算的逆运算（即图中的E运算）。\n这样做达到了实体鉴别的目的，但没有做到保密性，因为任何一个接收方收到报文都可以用A的公钥（公钥是公开的）解密得到明文。假如有中间者截获，依旧可以直到报文的内容。\n为了同时做到实体鉴别和保密，应该使用A的私钥进行数字签名，用B的公钥进行内容加密，这么一来只有B才能用B的私钥解密。这里A的私钥和B的公钥的作用是不同的。\n这里我们无需深究使用的是D运算还是E运算，只要知道签名和核实签名使用的是互逆的运算，加密和解密用的也是互逆的运算即可。\n综上总结：签名用发送者的私钥，加密用接收者的公钥。\n这种方式的很少用，因为需要对报文进行两次 D 运算和两次 E 运算，运算量太大，花费非常多的 CPU 时间。\n现实应用中，普遍使用开销小得多的对称密钥实现报文加密，使用非对称密钥实现数字签名，而且一定要设法减小公钥密码算法的开销。\n2、报文鉴别码 现在我们遇到的问题是：签名需要用发送者的私钥对整个明文进行签名运算（为了避免签名和加密混淆，所以这里说成是签名运算而不是加密运算），因而会消耗很多时间。\n为了减小签名的开销，可以使用散列函数H()代替签名算法（签名函数）对明文X进行散列处理（如MD5、SHA-1、SHA-2x）。\n散列函数的特点如下（和D、E这样的签名/加密运算对比）：\n返回值的长度较短和固定 (签名运算返回值的长度和输入值的长度成正比，输入值越长，返回值越长)。\n不同的输入产生相同的输出的可能性极低 (签名运算不会有重复，因为其返回值是什么样的和它输入值是什么样的直接相关)。\n单向运算，不能逆向变换，因此严格来说，散列函数不能算一个加密函数 (签名运算可逆运算，可以根据密文还原成原文)。\n仅改动输入的一个比特，输出也会相差极大。\n为什么散列函数可能出现重复值？\n原因很简单，散列函数的输入值可以是无限多的任意取值，而返回值是固定位数，例如MD5的返回值是 128位，产生的值的范围在 0~2^128-1，以无限多的输入值得到有限个数的返回值，那么肯定会发生2个不同的输入值对应同一个返回值的重复现象。\n下面我们可以简单看看MD5加密的原理：\n1、附加：把任意长的报文按模 2^64 计算其余数（余数的长度64位），追加在报文的后面。\n2、 填充：在报文和余数之间填充 1～512 位，使得填充后的总长度是 512 的整数倍。填充的首位是 1，后面都是 0。\n3、分组：把追加和填充后的报文分割为多个 512 位的数据块，每个 512 位的报文数据再分成 4 个 128 位的数据块。\n4、计算：将 4 个 128 位的数据块依次送到不同的散列函数进行 4 轮计算。每一轮都按 32 位的小数据块进行复杂的位运算，而且一轮运算的结果会作为下一轮运算的参数。一直到最后计算出 MD5 结果（128 位）。\n散列函数的开销相比于签名函数的开销可以忽略不计。\n回到之前的问题，如何借助散列函数进行报文鉴别？\n1、A对X使用散列函数处理得到一个散列值 H(X)。\n2、A对 H(x)用A的私钥经过D签名运算得到报文鉴别码MAC，这是一个经过签名的鉴别码。\n3、将 报文X + MAC 拼接起来，经过B的公钥对整个扩展报文加密（图中没有画出来）并发送。\n4、B用B的私钥对扩展报文解密，将MAC和X分开，用A的公钥对MAC逆运算得到 H(X0)。\n5、B用 H()散列运算对报文X进行计算得到 H(X1) ,如果H(X1) == H(X2)，就既能证明报文没有被篡改，又能证明对方是A。\n需要注意：第二步没有对报文进行加密（D运算），而是对很短的散列 H(X) 进行 D 运算，因为 H(X)很短，所以D(H(X))的开销和耗时很小，这样就解决了之前开销大的问题。\n上图是非对称密钥的签名过程，下图是对称密钥的签名过程：\n对称密钥的签名方式就不是使用 D 运算，而是直接 把对称密钥K 拼在报文 X 之后，直接进行散列运算。图中也是省略了加密这一步，其实也应该要加密。\nTLS会话阶段就是用的对称密钥对报文签名的。\n四、实体鉴别 实体鉴别与报文鉴别不同。\n报文鉴别：对每一个收到的报文都要鉴别报文的发送者和完整性。\n实体鉴别：在通信开始前，对和自己通信的对方实体只需验证一次。\n最简单的实体鉴别就是使用共享的对称密钥 K(AB)，因为这个密钥只有A和B才有，因此B如果能用K(AB)这个密钥解密成功，就知道对方肯定只能是A。\n该方法存在明显漏洞：不能抵抗重放攻击。\n重放攻击 ：原理是把以前窃听到的数据原封不动地重新发送给接收方。入侵者 C 捕获到报文后不需要破译报文，而是在之后攻击者想攻击的时候直接把由 A 加密的报文发送给 B，使 B 误认为 C 就是 A。B 就会向伪装成 A 的 C 发送许多本来应当发给 A 的报文。\n回到上图，C不用拥有K(AB)这个密钥，C只需截获这个报文，之后想攻击的时候，就发送这个报文给B就让B以为C拥有K(AB)。从而让B以为C是A。\n为了防御重放攻击，可以使用不重数（即不重复使用的大随机数）。\n如下图所示：\n1、A发送明文随机数Ra，并且Ra保存在A的内存中。\n2、B用私钥SKb对Ra进行签名（是签名不是加密，这样A就知道对方是B）并加上自己的随机数Rb（Rb也会存在B的内存）。A会验证B发过来的签名了的随机数是不是A上次所发的Ra。\n3、A发送用SKa签名的Rb，B就知道对方是A。B再验证Rb是否就是上次B发出去的Rb，如果验证成功就在内存中删除存储的Rb。\n4、A和B开始通信。\n如果C要使用重放攻击，C就要捕获第三次报文，这是因为C没有A的私钥，B就知道C不是A，而第三次报文是用 A 的私钥签名过的，因此C必须捕获第三次报文。下次重放这个报文B用A的公钥核实签名成功，就以为C是A。但即使如此，B核实完签名又发现里面的随机数Rb是自己不认识的数（因为Rb已经过期和失效），因此知道有内鬼，终止交易。\n此外，还有一种更难防御的“中间人攻击”。即使使用不重数也无法防御：\n1、C 冒充是 A，发送报文给 B，说：“我是 A”。\n2、 B 选择一个不重数 RB，发送给 A，但被 C 截获了。 C 用自己的私钥 SKC 冒充是 A 的私钥，对 RB 签名，并发送给 B。此时B还无法核实签名，因为B没有C的公钥。\n3、 B 向 A 发送报文，要求对方把解密（不应该是解密，应该是核实签名）用的公钥发送过来，但这报文也被 C 截获了。 C 把自己的公钥 PKC 冒充是 A 的公钥发送给 B。 B 用收到的公钥 PKC 对收到的加密的 RB 进行解密，其结果当然正确。\n4、于是 B 相信通信的对方是 A，接着就向 A 发送许多敏感数据，但都被 C 截获了。\n五、公钥的分配 我们知道 发送者A 发送消息 给接收者B的时候，要用B的公钥对报文加密（在TLS传输中，对数据传输阶段的数据加密不使用不对称密钥，而是使用对称密钥，因此通信前需要传输对称密钥。为了安全传输对称密钥，需要用B的公钥对要传输的对称密钥进行加密），那么A怎么得到B的公钥呢？\n有一个权威第三方机构：认证中心 CA。它负责为拥有公钥的实体（人或者机器）提供数字证书，数字证书 = 实体的公钥 + 实体的信息（如主机名，实体的IP地址，实体人姓名等），数字证书（又称公钥证书）实际上是对公钥和对应实体的绑定。\n这是一个数字证书的样子：\n数字整数是公开的（因为里面放的是公钥，公钥本来就是公开的），无需加密，但是数字整数需要签名，CA用自己的私钥K(ca)对数字证书签名，目的是让大家知道这个证书是CA出品的，是正规的证书。\n数字证书的制作过程如下：\n1、CA对B的未签名证书散列运算得到 H(X)\n2、CA用自己的私钥K(ca)对H(X)进行签名（D运算)。\n3、把签名追加到明文证书的后面，得到签名后的证书。\n核实：A 拿到 B 的数字证书后，使用数字证书上给出的 CA 的公钥，对数字证书中 CA 的数字签名进行 E 运算，得出一个数值。再对 B 的数字证书 (CA 数字签名除外的部分) 进行散列运算，又得出一个H(X)。比较这两个H(X)。若一致，则数字证书是真的。\n六、TLS 运输层安全协议 TLS协议是SSL协议（安全套接字层协议）的改进，现在我们所说的SSL是 SSL/TLS的统称，实际上用的是TLS协议，真正的SSL协议在多年前已经被废弃。\nTLS 安全运输层位于应用层和运输层之间，作用是为应用层提供保密性、数据完整性和鉴别功能安全传输服务（安全会话）。TLS层具体要做的是：在发送方，TLS 接收应用层的数据，对数据进行加密，然后把加密后的数据送往 TCP 套接字。\n不要搞混可靠传输服务和安全服务，前者是指数据传输的有序、不丢失和无差错，后者指保密性、数据完整性和鉴别通信双方。\n应用层使用 TLS 最多的就是 HTTP（即HTTPS）。TLS 可用于任何应用层协议。TCP 的 HTTPS 端口号是 443，而不是平时使用的端口号 80。\nTLS 具有双向鉴别的功能，但实际应用中只会用到单向鉴别：客户端（浏览器）需要鉴别服务器（即安全服务的第三点），具体来说客户端要对方证明他就是 http://www.baidu.com这个站点的服务。这就需要服务端提供CA证书，CA 证书是运输层安全协议 TLS 的基石。\nTLS如何建立安全会话？\n建立安全会话两个阶段：\n握手阶段：使用握手协议，这个阶段需要做三件事：协商加密算法、鉴别接收方的CA证书和生成用于加密的对称密钥。\n会话阶段：使用记录协议。\n1、握手阶段 1、协商加密算法\n浏览器 A 向服务器 B 发送浏览器的 TLS 版本号和一些可选的加密算法（是在TCP第三次握手的ACK报文段中顺带传输这些信息的），以及A的不重数Ra。\nB 从中选定自己所支持的加密算法（如 RSA）和生成主密钥的算法，并告知 A，同时把自己的 CA 数字证书、B的不重数Rb 和 用B私钥加密（应该说是签名）的SKB(Ra) 发送给 A。\n2、服务器鉴别。客户 A 用数字证书中 CA 的公钥对数字证书进行验证鉴别（校验过程请参考上文）。鉴别成功后，A就从证书中得到了B的公钥PKB。\n3、客户 A生成了预主密钥PMS，并且根据预主密钥 PMS 、Ra 和 Rb生成主密钥MS。A再用 B 的公钥 PKB 对PMS 加密，得出加密的预主密钥 PKB(PMS)，发送给服务器 B。\n4、服务器 B 用自己的私钥SKB把预主密钥PMS解密出来 。这样，客户 A 和服务器 B 都有了PMS，将预主密钥 PMS 、Ra 和 Rb通过双方已商定的密钥生成算法，生成为后面数据传输用的对称主密钥 MS。\n5、这一步时图中没有画出来的，服务端B最后发送一个加密的\u0026quot;Finished\u0026quot;消息，表示握手阶段结束。A和B使用对称密钥MS进行加密传输，而不是用非对称密钥加密，这是因为用对称密钥加密的开销和时间远小于非对称加密。\n6、为了使双方的通信更加安全，客户 A 和服务器 B 最好使用不同的密钥。主密钥被分割成 4 个不同的密钥。\n每一方都拥有这样 4 个密钥（注意：这些都是对称密钥）：\n客户 A 发送数据时使用的会话密钥 KA （用于A对数据加密）\n客户 A 发送数据时使用的 MAC 密钥MA（用于A对数据签名）\n服务器 B 发送数据时使用的会话密钥 KB （用于B对数据加密）\n服务器 B 发送数据时使用的 MAC 密钥 MB （用于B对数据签名）\n也就是说数据收发阶段，每个报文都要加密和签名这两步。\n最后总结：使用了TLS安全传输的会比传统的TCP三次握手多出三次握手（对于客户端多出2个RTT时延， 对于服务端多出1个RTT）才能开始数据发送阶段。如下图绿色部分：\nTLS 的浏览器端和服务端都会保存之前的TLS会话参数（包括协商好的加密算法和主密钥等等），以及这些参数的会话ID（会话ID - 会话参数的映射）。下次浏览器再访问同一个服务器时，直接发送会话ID就能够通知服务端使用上一次的密钥和加密算法进行通信，节省了一个RTT时间和生成主密钥的时间：\n2、会话阶段 即数据报文传输阶段。以A发送数据给B这一个传输方向为例：\n1、把长的数据划分为较小的数据块（TCP把一个应用层消息分隔为多个报文段），叫做记录。\n对每一个记录进行鉴别运算(用密钥MA)和加密运算（用密钥KA）。\n2、记录协议对每一个记录按发送顺序赋予序号，第一个记录作为 0。发送下一个记录时序号就加 1，序号最大值不得超过 264 – 1，且不允许序号绕回。\n序号未写在记录之中，而是在进行散列运算时，把序号包含进去。客户 A 向服务器 B 发送一个记录前，对 密钥 MA 、记录的当前序号和明文记录进行散列运算得到数字签名MAC，MAC在拼到明文记录后面，附有签名的数据记录就生成好了。\n使用会话密钥 KA 对附有签名的数据记录进行加解密。\n上面过程，在握手阶段使用了实体鉴别，在会话阶段使用了报文鉴别。\n关闭 TLS 连接：\n关闭 TLS 连接之前，A 或 B 应当先发送关闭 TLS 的记录，以防止截断攻击 。\n截断攻击：在 A 和 B 正在进行会话时，入侵者突然发送 TCP 的 FIN 报文段来关闭 TCP 连接。\n如果 A 或 B 没有发送一个要关闭 TLS 的记录的情况下收到了 TCP 的 FIN 报文段时，就知道这是入侵者的截断攻击了。因为入侵者无法伪造关闭 TLS 的记录。\n","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/","tags":["tls","证书","https"],"title":"计算机网络之网络安全"},{"categories":["计算机网络"],"contents":"[toc]\n应用层 应用层协议原理 应用程序架构 应用层程序的体系结构一般包括：P2P、CS这两种架构。\nP2P：P2P（Peer-to-Peer）是一种分布式计算和通信模型，它允许计算机之间直接连接和通信，而不需要通过中央服务器进行中转。在P2P网络中，所有的计算机节点都可以充当客户端和服务器的角色，可以共享资源、提供服务和请求服务。\n典型例子：p2p的下载等\nCS：一直打开的一端被称为服务端（S），它会处理来自客户端请求；主动请求的一方就是客户端（C）。\n典型CS架构的应用：web服务\n进程通信与套接字 进程通信\n应用层解决的是两端应用程序（进程）通信的问题。两端进程是通过套接字通信，为用户进程提供套接字也是应用层的主要功能。网络通信的用户进程是运行在应用层上的。\n套接字\n套接字是同一台主机内应用层与运输层之间的接口，也是提供给应用程序的可编程接口，应用程序可以通过套接字控制和使用应用层的一切功能，但无法通过套接字控制运输层。\n进程寻址 通过IP地址和端口号寻找接收方进程（具体来说是寻找接收方进程对应的套接字）。\n动态主机配置协议：DHCP DHCP 的全称是 Dynamic Host Configuration Protocol 动态主机配置协议。使用 DHCP 就能实现自动设置 IP 地址、统一管理 IP 地址分配。也就是不管你是在开会还是在工位干活，都省去了手动配置 IP 地址这一步繁琐的操作，同时 DHCP 也大大减少了可能由于你手动分配 IP 地址导致错误的几率。\nDHCP 与 IP 密切相关，它是 IP 网络上所使用的协议。如果你想要使用 DHCP 提供服务的话，那么在整条通信链路上就需要 DHCP 服务器的存在，连接到网络的设备使用 DHCP 协议从 DHCP 服务器请求 IP 地址。DHCP 服务器会为设备分配一个唯一的 IP 地址。\n除了 IP 地址外，DHCP 服务器还会把子网掩码，默认路由，DNS 服务器告诉你。\nDHCP 服务器 现在，你不需要手动配置 IP 地址，也不再需要管理 IP 地址了，管理权已经移交给了 DHCP 服务器，DHCP 服务器会维护 IP 地址池，在网络上启动时会将地址租借给启用 DHCP 的客户端。\n由于 IP 地址是动态的(临时分配)**而不是**静态的(永久分配)，因此不再使用的 IP 地址会自动返回 IP 地址池中进行重新分配。\n那么 DHCP 服务器由谁维护呢？\n网络管理员负责建立 DHCP 服务器，并以租约的形式向启用 DHCP 的客户端提供地址配置，啊，既然不需要我管理，那就很舒服了～\n好了，现在你能舒舒服服的开发了，你用 postman 配了一条 192.168.1.4/x/x 的接口进行请求，请求能够顺利进行，但是过了一段时间后，你发现 192.168.1.4/x/x 这个接口请求不通了，这是为啥呢？然后你用 ipconfig 查询了一下自己的 IP 地址，发现 IP 地址变成了 192.168.1.7，怎么我用着用着 IP 地址还改了？DHCP 是个垃圾，破玩意！！@#¥%¥%……¥%\n其实，这也是一个 DHCP 服务器的一个功能，DHCP 服务器通常为每个客户端分配一个唯一的动态 IP 地址，当该 IP 地址的客户端租约到期时，该地址就会更改。\n唯一意思说的就是，如果你手动设置了一个静态 IP，同时 DHCP 服务器分配了一个动态 IP，这个动态 IP 和静态 IP 一样，那么必然会有一个客户端无法上网。\n我就遇到过这种情况，我使用虚拟机配置的静态 IP 是192.168.1.8，手机使用 DHCP 也同样配置了 192.168.1.8 的 IP 地址，此时我的虚拟机还没有接入网络，当我接入网络时，我怎样也连不上虚拟机了，一查才发现 IP 地址冲突了 …\n虽然 DHCP 服务器能提供 IP 地址，但是他怎么知道哪些 IP 地址空闲，哪些 IP 地址正在使用呢？\n实际上，这些信息都配置在了数据库中，下面我们就来一起看一下 DHCP 服务器维护了哪些信息。\n网络上所有有效的 TCP/IP 配置参数 这些参数主要包括主机名（Host name）、DHCP 客户端（DHCP client）、域名（Domain name）、IP 地址IP address）、网关（Netmask）、广播地址（Broadcast address）、默认路由（default rooter）。\n有效的 IP 地址和排除的 IP 地址，保存在 IP 地址池中等待分配给客户端 为某些特定的 DHCP 客户端保留的地址，这些地址是静态 IP，这样可以将单个 IP 地址一致地分配给单个DHCP 客户端 好了，现在你知道 DHCP 服务器都需要保存哪些信息了，并且看过上面的内容，你应该知道一个 DHCP 的组件有哪些了，下面我们就来聊一聊 DHCP 中都有哪些组件，这些组件缺一不可。\nDHCP 的组件 使用 DHCP 时，了解所有的组件很重要，下面我为你列出了一些 DHCP 的组件和它们的作用都是什么。\nDHCP Server，DHCP 服务器，这个大家肯定都知道，因为我们上面就一直在探讨 DHCP 服务器的内容，使用 DHCP ，是一定要有 DHCP 服务器的，要不然谁给你提供服务呢？ DHCP Client，DHCP 客户端，这个大家应该也知道，毕竟只有一个服务端不行啊，没有客户端你为谁服务啊？DHCP 的客户端可以是计算机、移动设备或者其他需要连接到网络的任何设备，默认情况下，大多数配置为接收 DHCP 信息。 Ip address pool: 你得有 IP 地址池啊，虽然说你 DHCP 提供服务，但是你也得有工具啊，没有工具玩儿啥？IP 地址池是 DHCP 客户端可用的地址范围，这个地址范围通常由最低 -\u0026gt; 最高顺序发送。 Subnet：这个组件是子网，IP 网络可以划分一段一段的子网，子网更有助于网络管理。 Lease：租期，这个表示的就是 IP 地址续约的期限，同时也代表了客户端保留 IP 地址信息的时间长度，一般租约到期时，客户端必须续约。 DHCP relay：DHCP 中继器，这个一般比较难想到，DHCP 中继器一般是路由器或者主机。DHCP 中继器通常应对 DHCP 服务器和 DHCP 客户端不再同一个网断的情况，如果 DHCP 服务器和 DHCP 客户端在同一个网段下，那么客户端可以正确的获得动态分配的 IP 地址；如果不在的话，就需要使用 DHCP 中继器进行中继代理。 现在 DHCP 的组件你了解后，下面我就要和你聊聊 DHCP 的工作机制了。\nDHCP 工作机制 在聊 DHCP 工作机制前，先来看一下 DHCP 的报文消息\nDHCP 报文 DHCP 报文共有一下几种：\nDHCP DISCOVER ：客户端开始 DHCP 过程发送的包，是 DHCP 协议的开始 DHCP OFFER ：服务器接收到 DHCPDISCOVER 之后做出的响应，它包括了给予客户端的 IP 租约过期时间、服务器的识别符以及其他信息 DHCP REQUEST ：客户端对于服务器发出的 DHCPOFFER 所做出的响应。在续约租期的时候同样会使用。 DHCP ACK ：服务器在接收到客户端发来的 DHCPREQUEST 之后发出的成功确认的报文。在建立连接的时候，客户端在接收到这个报文之后才会确认分配给它的 IP 和其他信息可以被允许使用。 DHCP NAK ：DHCPACK 的相反的报文，表示服务器拒绝了客户端的请求。 DHCP RELEASE ：一般出现在客户端关机、下线等状况。这个报文将会使 DHCP 服务器释放发出此报文的客户端的 IP 地址 DHCP INFORM ：客户端发出的向服务器请求一些信息的报文 DHCP DECLINE :当客户端发现服务器分配的 IP 地址无法使用（如 IP 地址冲突时），将发出此报文，通知服务器禁止使用该 IP 地址。 DHCP 的工作机制比较简单，无非就是客户端向服务器租借 IP ，服务器提供 IP 给客户端的这个过程呗。嗯，你很聪明，大致是这样的，不过有一些细节需要注意下，下面我通过两张图来和你聊一下。\n关于从 DHCP 中获取 IP 地址的流程，主要分为两个阶段。\n第一个阶段是 DHCP 查找包的阶段\n查找包的阶段主要分为两步：第一步是 DHCP 发现包，第二步是 DHCP 提供包。\nDHCP 客户端在通信链路上发起广播，看看链路上有没有能提供 DHCP 包的服务器，然后通信链路上的各个节点会检查自己是否能够提供 DHCP 包，这时 DHCP 服务器说它能够提供 DHCP 包，然后 DHCP 就发出一个 DHCP 包沿着通信链路返回给 DHCP 客户端。\n第二个阶段是 DHCP 的请求阶段。\nDHCP 的请求包也分为两步：第一步是 DHCP 请求包，第二步是 DHCP 确认包。\nDHCP 客户端在通信链路上发起 DHCP 请求包，请求包主要是告诉 DHCP 服务器，它想要用上一步提供的网络设置，然后 DHCP 服务器向 DHCP 客户端发送确认包，表示允许 DHCP 客户端使用第二步发送的网络设置。\n至此，DHCP 的网络设置就结束了，然后通信链路上的主机之间就可以进行 TCP/IP 通信了。\n当不需要 IP 地址时，可以发送 DHCP 解除包(DHCP RELEASE)进行解除。另外，DHCP 的设置中通常会有一个租期时间的设定，DHCP 客户端在这个时限内可以发送 DHCP 请求包通知想要延长这个期限。\n超文本传输协议：HTTP HTTP概况 http概况\nHTTP是一个在计算机的世界里专门在两点之间传输超文本数据的约定和规范。 非持续连接与持续连接 RTT RTT(Round-Trip Time) 往返时间指的是一个段分组从客户到服务器然后再返回客户所花费的时间。我们尝试用RTT来表示客户点击超链接后页面出现的时间，如下图所示：\n粗略地讲，总的响应时间就是两个RTT 再加上服务器传输HTML文件的时间\n非持续连接 每个请求/响应都是一个单独的TCP连接，每次请求完毕，都会释放连接。\n非持续连接的缺点：\n必须为每个请求的对象都创建对象，建立和维护一个全新的连接。这对web服务端来说，会引起内存开销大，耗损性能的问题 每个请求的对象经受两倍的RTT交付时延，累积起来后，成本依旧不菲. 持续连接 每个请求/响应都是经过相同的TCP连接发送的，复用了这个TCP连接。\nHTTP性能 HTTP事务延时 一个HTTP事务是指，一个HTTP请求从发出到响应到达发送端的过程，如下所示：\n上图显示一个HTTP事务的时延主要包括4部分：\n1、DNS查询：如果从本地DNS缓存就能得到目标IP地址，则耗时0RTT；如果从本地DNS服务器就能得到目标IP地址，则耗时1个RTT；如果涉及到DNS的递归和迭代请求，则耗时多个RTT。\n2、TCP连接的三次握手，占 1 个RTT，如果使用TLS安全连接则占3个RTT。\n3、请求到达服务器和响应返回的时间，占1个RTT。\n4、服务器应用程序处理请求的时间，假设请求的是一个静态文件，则处理时间可以忽略不记。\nTCP相关的时延 HTTP协议是基于TCP协议的，因此一个HTTP事务的性能很大程度取决于TCP通道的性能，下面是一些TCP连接相关的可能会拖慢HTTP的地方：\n1、TCP连接的3次握手时延\n花费 1个 RTT。越小的HTTP事务在TCP连接下的效率越低。\n2、捎带机制\n接收方在接收到一个序号的分组后不会马上返回ACK报文，而是允许接收方在有数据发往发送方的分组中“捎带”这个ACK消息（ACK标志位和ACK号），这是考虑到ACK确认报文很小，单独发一个报文会比较浪费（TCP头部就占了40~60字节）。\n这也会给HTTP响应带来时延。\n3、TCP慢启动\nTCP慢启动使 刚打开的连接 传输分组的速度很慢。\n4、Nagle算法 和 TCP_NODELAY\nNagle算法是为了防止一次分组发送的实际数据量太少，造成网络效率降低，所以要求缓冲区需要到达MSS及以上大小的数据时才发送分组（或者发送计时器到时）。如果缓冲区里的数据不足一个MSS大小，那么要等到在前一个分组被确认之后或者发送计时器超时才能发下一个分组。\nNaggle会引发几种HTTP性能问题：小的HTTP报文无法填满一个MSS，所以只能等到计时器到达才能发送；延迟确认会阻止Naggle发送下一个分组（延迟确认和Naggle共同作用）。\nHTTP应用程序会设置参数TCP_NODELAY禁用Naggle算法。当然，TCP应用Nagle算法时，要保证应用程序每次向缓冲区输送的数据块得是较大的数据块（如512B）而不能时逐字节发送。\n5、TIME_WAIT 累积和端口耗尽\n主动关闭连接的一方（不论是服务器还是客户端）进入TIME_WAIT后，主动关闭方会在内存中记录最近所关闭的对端IP地址和端口，记录有效期维持2MSL，2MSL内连接并不会真正关闭。在这段期间内，该端不会与对端建立相同IP和端口的连接。\n假如A和B以很快的速率创建和关闭连接（也就是在2MSL时间内，端口还是被占用了），B就没有端口可以和A的80服务建立连接了，这就是端口耗尽（是客户端B的端口耗尽），当然这种情况很少出现，只会出现在性能测试的时候。\n对于服务端，服务端如果有大量的TIME_WAIT未关闭连接也会使操作系统的速度严重减慢（如果使用多线程则一个连接可能就是一个线程）和消耗大量内存（毕竟一个连接是一个套接字）。\nTCP连接过多意味着什么？\n1、大量的套接字会占用客户端、服务器以及代理的内存（输入输出缓冲区要预先分配的）和CPU；\n2、并行 TCP 连接接收和发送数据时竞争共享的带宽；\n3、应用的并行能力也受限制（管理一个套接字可能就要用一个线程）。\nHTTP优化建议 从大方向上，HTTP优化只会围绕2点进行：减少 网络延迟 和 减少要传输的字节。具体可以通过以下几点进行优化：\n1、减少DNS查询次数\n尽量命中本地DNS缓存和加长DNS缓存记录的过期时间。\n2、减少HTTP请求\n任何请求都不如没有请求更快，因此要去掉页面上没有必要的资源，多次请求尽可能合并为1次请求。\n3、使用CDN\n从地理上把数据放到接近客户端的地方，可以显著减少每次请求的传播时延。\n4、添加Expires首部并配置ETag标签\n相关资源应该缓存，以避免重复请求每个页面中相同的资源。Expires 首部可用 于指定缓存时间，在这个时间内可以直接从缓存取得资源，完全避免 HTTP 请求。ETag 及 Last-Modified 首部提供了一个与缓存相关的机制，相当于最后一次 更新的指纹或时间戳。\n5、Gzip资源\n所有文本资源都应该使用 Gzip 压缩，再传输。\n6、避免HTTP重定向\nHTTP 重定向极其耗时，特别是把客户端定向到一个完全不同的域名的情况下， 还会导致额外的 DNS 查询、TCP 连接延迟，等等。\nweb缓存 web缓存器也叫代理服务器，主要用于缓存本地对于远端服务器响应的内容，减少对远端服务器的请求。\nHTTP协议演进 HTTP1.0 特点\n无状态：服务器不跟踪不记录请求过的状态 无连接：浏览器每次请求都需要建立TCP连接 HTTP/1.0规定浏览器和服务器保持短连接。浏览器的每次请求都需要与服务器建立一个TCP连接，服务器处理完成后立即断开TCP连接（无连接），服务器不跟踪每个客户端也不记录过去的请求（无状态）\n无状态导致的问题可以借助cookie/session机制来做身份认证和状态记录解决。\n然而，无连接特性将会导致以下性能缺陷：\n无法复用连接\n每次发送请求的时候，都需要进行一次TCP连接，而TCP的连接释放过程又是比较费事的。这种无连接的特性会导致网络的利用率非常低。\n队头堵塞(head of line blocking)\n由于HTTP/1.0规定下一个请求必须在前一个请求响应到达之前才能发送。假设一个请求响应一直不到达，那么下一个请求就不发送，就到导致阻塞后面的请求。\n为了解决这些问题，HTTP/1.1出现了。\nHTTP1.1 长连接\nHTTP/1.1增加了一个Connection字段，通过设置Keep-alive（默认已设置）可以保持连接不断开，避免了每次客户端与服务器请求都要重复建立释放TCP连接，提高了网络的利用率。如果客户端想关闭HTTP连接，可以在请求头中携带Connection:false来告知服务器关闭请求\n支持断点续传\n通过使用请求头中的 Range 来实现。\n支持请求管道化（pipelining）\n基于HTTP/1.1的长连接，使得请求管线化成为可能。多个请求可以同时发送，但是服务器还是按照顺序，先回应 A 请求，完成后再回应 B 请求。要是 前面的回应特别慢，后面就会有许多请求排队等着。这称为「队头堵塞」。\nHTTP2 http2.0是一种安全高效的下一代http传输协议。安全是因为http2.0建立在https协议的基础上，高效是因为它是通过二进制分帧来进行数据传输。正因为这些特性，http2.0协议也在被越来越多的网站支持。\n特点 对1.x协议语意的完全兼容\n2.0协议是在1.x基础上的升级而不是重写，1.x协议的方法，状态及api在2.0协议里是一样的。\n建立在HTTPS上\n性能的大幅提升\n2.0协议重点是对终端用户的感知延迟、网络及服务器资源的使用等性能的优化。\n二进制分帧 http2.0之所以能够突破http1.X标准的性能限制，改进传输性能，实现低延迟和高吞吐量，就是因为其新增了二进制分帧层。\n帧(frame)包含部分：类型Type, 长度Length, 标记Flags, 流标识Stream和frame payload有效载荷。\n消息(message)：一个完整的请求或者响应，比如请求、响应等，由一个或多个 Frame 组成。\n流是连接中的一个虚拟信道，可以承载双向消息传输。每个流有唯一整数标识符。为了防止两端流ID冲突，客户端发起的流具有奇数ID，服务器端发起的流具有偶数ID。\n流标识是描述二进制frame的格式，使得每个frame能够基于http2发送，与流标识联系的是一个流，每个流是一个逻辑联系，一个独立的双向的frame存在于客户端和服务器端之间的http2连接中。一个http2连接上可包含多个并发打开的流，这个并发流的数量能够由客户端设置。\n在二进制分帧层上，http2.0会将所有传输信息分割为更小的消息和帧，并对它们采用二进制格式的编码将其封装，新增的二进制分帧层同时也能够保证http的各种动词，方法，首部都不受影响，兼容上一代http标准。其中，http1.X中的首部信息header封装到Headers帧中，而request body将被封装到Data帧中。\n多路复用/连接共享 而http2.0中的多路复用优化了这一性能。多路复用允许同时通过单一的http/2 连接发起多重的请求-响应消息。有了新的分帧机制后，http/2 不再依赖多个TCP连接去实现多流并行了。每个数据流都拆分成很多互不依赖的帧，而这些帧可以交错（乱序发送），还可以分优先级，最后再在另一端把它们重新组合起来。\nhttp 2.0 连接都是持久化的，而且客户端与服务器之间也只需要一个连接（每个域名一个连接）即可。http2连接可以承载数十或数百个流的复用，多路复用意味着来自很多流的数据包能够混合在一起通过同样连接传输。当到达终点时，再根据不同帧首部的流标识符重新连接将不同的数据流进行组装。\n上图展示了一个连接上的多个传输数据流：客户端向服务端传输数据帧stream5，同时服务端向客户端乱序发送stream1和stream3。这次连接上有三个响应请求乱序并行交换。\n上图就是http1.X和http2.0在传输数据时的区别。以货物运输为例再现http1.1与http2.0的场景：\nhttp1.1过程：货轮1从A地到B地去取货物，取到货物后，从B地返回，然后货轮2在A返回并卸下货物后才开始再从A地出发取货返回，如此有序往返。\nhttp2.0过程：货轮1、2、3、4、5从A地无序全部出发，取货后返回，然后根据货轮号牌卸载对应货物。\n显然，第二种方式运输货物多，河道的利用率高。\n头部压缩 缓存与压缩头部\nhttp1.x的头带有大量信息，而且每次都要重复发送。http/2使用encoder来减少需要传输的header大小，通讯双方各自缓存一份头部字段表，既避免了重复header的传输，又减小了需要传输的大小。\n当然，HTTP1.x的GIZP压缩是对body的压缩，与http2的头部压缩不冲突。\n请求优先级 http2采用二进制分帧，所以可以允许帧乱序传输，会在客户端重新组装成正确的序列。因此，我们可以在建立好的HTTP2连接里，可以为优先级高的请求优先发送数据帧。\n例如：\n●优先级最高：主要的html\n●优先级高：CSS文件\n●优先级中：js文件\n●优先级低：图片\n服务器端推送 服务器可以对一个客户端请求发送多个响应，服务器向客户端推送资源无需客户端明确地请求。并且，服务端推送能把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤。\nHTTP2的性能瓶颈 启用http2.0后会给性能带来很大的提升，但同时也会带来新的性能瓶颈。因为现在所有的压力集中在底层一个TCP连接之上，TCP很可能就是下一个性能瓶颈，比如TCP分组的队首阻塞问题，单个TCP packet丢失导致整个连接阻塞，无法逃避，此时所有消息都会受到影响。\nHTTP/1.x keep-alive 与 HTTP/2 多路复用区别 HTTP/1.x 是基于文本的，只能整体去传；HTTP/2 是基于二进制流的，可以分解为独立的帧，交错发送 HTTP/1.x keep-alive 必须按照请求发送的顺序返回响应；HTTP/2 多路复用不按序响应 HTTP/1.x keep-alive 为了解决队头阻塞，将同一个页面的资源分散到不同域名下，开启了多个 TCP 连接；HTTP/2 同域名下所有通信都在单个连接上完成 HTTP/1.x keep-alive 单个 TCP 连接在同一时刻只能处理一个请求（两个请求的生命周期不能重叠）；HTTP/2 单个 TCP 同一时刻可以发送多个请求和响应 HTTP3 基于Google的QUIC，HTTP3 背后的主要思想是放弃 TCP，转而使用基于 UDP 的 QUIC 协议。\n与 HTTP2 在技术上允许未加密的通信不同，QUIC 严格要求加密后才能建立连接。此外，加密不仅适用于 HTTP 负载，还适用于流经连接的所有数据，从而避免了一大堆安全问题。建立持久连接、协商加密协议，甚至发送第一批数据都被合并到 QUIC 中的单个请求/响应周期中，从而大大减少了连接等待时间。如果客户端具有本地缓存的密码参数，则可以通过简化的握手（0-RTT）重新建立与已知主机的连接。\n为了解决传输级别的线头阻塞问题，通过 QUIC 连接传输的数据被分为一些流。流是持久性 QUIC 连接中短暂、独立的“子连接”。每个流都处理自己的错误纠正和传递保证，但使用连接全局压缩和加密属性。每个客户端发起的 HTTP 请求都在单独的流上运行，因此丢失数据包不会影响其他流/请求的数据传输。\n特点\n基于google的QUIC协议，而quic协议是使用udp实现的； 减少了tcp三次握手时间，以及tls握手时间； 解决了http 2.0中前一个stream丢包导致后一个stream被阻塞的问题； 优化了重传策略，重传包和原包的编号不同，降低后续重传计算的消耗； 连接迁移，不再用tcp四元组确定一个连接，而是用一个64位随机数来确定这个连接； 更合适的流量控制。 简单邮件传输协议：SMTP 下图给出了电子邮件系统的总体情况，我们可以看到它有3个主要的组成部分：用户代理、邮件服务器、SMTP\n用户代理，比如说：Foxmail，Outlook，Apple Mail 这类的软件\n邮件服务器：qq邮箱，163邮箱，Gmail之类的邮件服务\nSMTP则是简单邮件传输协议。\nSMTP是因特网电子邮件的核心。它使用TCP可靠数据传输服务。每台邮件服务器上既运行着SMTP的客户端也运行着SMTP的服务器端。当一个邮件服务器向其他邮件服务器发送邮件时，他就是SMTP的客户；当邮件服务器从其他邮件服务器上接收邮件时，他就表现为一个SMTP的服务器。\n假设Alice 想给 Bob发送一份简单的报文。\nAlice调用它的邮件代理程序并提供Bob的邮件地址，撰写报文，然后指示用户代理发送该报文 Alice的用户代理把报文发给她的邮件服务器，在那里，该报文被放在报文队列中 运行在Alice的邮件服务器上的SMTP客户端发现了报文队列中的这个报文，他就创建一个到运行在Bob的邮件服务器上的SMTP服务器的TCP连接 在经过一些初始的SMTP握手后，SMTP客户通过该TCP连接发送Alice的报文。 在Bob的邮件服务器上，SMTP的服务器端接收该报文。Bob的邮件服务器然后将该报文放入Bob的邮箱中。 在Bob有时间的时候，它调用用户代理阅读该报文。 注意：SMTP 一般不适用中间邮件服务器发送邮件，如果Bob的邮件服务器没有开机，该报文会保留在ALice的邮件服务器上并等待新的尝试。\n现在你知道了两台邮件服务器邮件发送的大体过程，那么，SMTP 是如何将邮件从 Alice 邮件服务器发送到 Bob 的邮件服务器的呢？\n主要分为下面三个阶段：\n建立连接：在这一阶段，SMTP 客户请求与服务器的25端口建立一个 TCP 连接。一旦连接建立，SMTP 服务器和客户就开始相互通告自己的域名，同时确认对方的域名。 邮件传送：一旦连接建立后，就开始邮件传输。SMTP 依靠 TCP 能够将邮件准确无误地传输到接收方的邮件服务器中。SMTP 客户将邮件的源地址、目的地址和邮件的具体内容传递给 SMTP 服务器，SMTP 服务器进行相应的响应并接收邮件。 连接释放：SMTP 客户发出退出命令，服务器在处理命令后进行响应，随后关闭 TCP 连接。 HTTP vs SMTP 相同点 HTTP 是从Web服务器向Web客户传送文件；SMTP事一个邮件服务器向另一个邮件服务器传送文件 当进行文件传送时，持续的HTTP和SMTP都采用持续连接 不同点 HTTP主要是 拉协议(pull protocol)，即用户使用HTTP从该服务器拉取信息;而SMTP基本上是一个推协议(push protocol)，及发送邮件服务器把文件推向接收邮件服务器 SMTP要求每个报文采用 7 比特的ASCII 码格式。HTTP数据则不受这些限制 HTTP把每个对象分装到它自己的HTTP响应报文中去，而SMTP则把所有报文对象放在一个报文之中。 因特网邮件访问协议 Bob的用户代理不能使用SMTP得到报文，因为获取报文是一个拉操作，而SMTP协议是一个推协议。这就需要引入一个特殊的邮件访问协议来解决，该协议将Bob邮件服务器上的报文传送给他本地的PC。目前有一些流行的访问协议：POP3、IMAP以及HTTP\nPOP3 POP3 是一个极为简单的邮件访问协议。POP3按照三个阶段进行工作：\nAuthorization(特许)，用户代理发送用户名和口令以鉴别用户 事务处理阶段，用户代理取回报文。同时还能进行如下操作：对报文做删除标记，取消报文删除标记，获取邮件统计信息 更新阶段，出现在客户发出了quit命令之后，目的是结束该POP3会话。这时，该邮件服务器会删除那些被标记为删除的报文 在POP3的事务处理过程中，用户代理的回答可能有OK(正常)和-ERR(出现差错)\n在事务处理阶段，POP3的用户代理通常被配置为”下载并删除”或者“下载并保留”方式。\n下载并删除 这种方式存在的问题是，邮件接收方Bob可能是移动的，可能希望从多个不同的机器访问他的邮件报文，如从办公室的PC和笔记本来访问邮件。那么，如果Bob先在办公室的PC上收取了一条邮件，那么晚上当他在家里时，他便不能再通过笔记本收取该邮件\n下载并保留 用户代理下载某邮件之后该邮件仍然保留在邮件服务器上。这样Bob就能通过不同的及其重新读取这些邮件\nIMAP IMAP 是另一个邮件访问协议，它比 POP3具有更多的特色，不过也比POP3复杂得多。 POP3 会对移动用户带来问题。IMAP更喜欢使用一个在远程服务器上的层次文件夹，这样他可以从任何一台机器上对所有报文进行访问，但是POP3协议并没有给用户提供任何创建远程文件夹并为报文指派文件夹的方法。\nIMAP服务器把每个报文与一个文件夹联系起来，当报文第一次到达服务器时，它与收件人的INBOX文件夹相关联。收件人作为能够把邮件移到一个新的用户创建的文件夹中来阅读邮件、删除邮件等。 IMAP协议为用户提供了创建文件夹以及将邮件从一个文件夹移动到另一个文件夹的命令 IMAP 还为用户提供了在远程文件夹中查询邮件的命令 IMAP 服务器维护了IMAP会话的用户状态信息 IMAP 具有允许用户代理获取报文某些部分的命令。例如，一个用户代理可以只读取一个报文的首部。 基于Web的电子邮件 比如网页端的QQ邮箱，网易邮箱。使用这种服务，用户代理就是普通的浏览器，用户和他远程邮箱之间的通信通过HTTP进行：\n收件人从邮箱中访问一个报文时，该电子邮件报文从Bob的邮件服务器发送到他的浏览器，使用的是HTTP而不是POP3或者IMAP协议 当发件人要发送一封电子邮件报文时，该电子邮件豹纹从浏览器发送到邮件服务器使用的是HTTP报文而不是SMTP 但是，邮件服务器之间发送和接收邮件时，仍然使用SMTP 文件传输协议：FTP FTP 概述 文件传送协议 FTP (File Transfer Protocol) 是因特网上使用得最广泛的文件传送协议。 FTP 提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限。 FTP 屏蔽了各计算机系统的细节，因而适合于在异构网络中任意计算机之间传送文件。 FTP 的基本工作原理 FTP 使用 TCP 进行连接，它需要两个连接来传送一个文件： 控制连接：服务器打开端口号 21 等待客户端的连接，客户端主动建立连接后，使用这个连接将客户端的命令传送给服务器，并传回服务器的应答。 数据连接：用来传送一个文件数据。 根据数据连接是否是服务器端主动建立，FTP 有主动和被动两种模式： 主动模式：服务器端主动建立数据连接，其中服务器端的端口号为 20，客户端的端口号随机，但是必须大于 1024，因为 0~1023 是熟知端口号。 被动模式：客户端主动建立数据连接，其中客户端的端口号由客户端自己指定，服务器端的端口号随机。 主动模式要求客户端开放端口号给服务器端，需要去配置客户端的防火墙。被动模式只需要服务器端开放端口号即可，无需客户端配置防火墙。但是被动模式会导致服务器端的安全性减弱，因为开放了过多的端口号。 当NAT(Network Address Translation)设备以主动模式访问FTP服务器时，由于NAT设备不会聪明的变更FTP包中的IP地址，从而导致无法访问服务器。 简单文件传送协议 TFTP TFTP 是一个很小且易于实现的文件传送协议。\nTFTP 使用客户服务器方式和使用 UDP 数据报，因此 TFTP 需要有自己的差错改正措施。\nTFTP 只支持文件传输而不支持交互。\nTFTP 没有一个庞大的命令集，没有列目录的功能，也不能对用户进行身份鉴别。\nTFTP 的主要特点是\n(1) 每次传送的数据 PDU 中有 512 字节的数据，但最后一次可不足 512 字节。\n(2) 数据 PDU 也称为文件块(block)，每个块按序编号，从 1 开始。\n(3) 支持 ASCII 码或二进制传送。\n(4) 可对文件进行读或写。\n(5) 使用很简单的首部。\n远程终端协议 TELNET 简述 TELNET 是一个简单的远程终端协议，也是因特网的正式标准。 用户用 TELNET 就可在其所在地通过 TCP 连接注册（即登录）到远地的另一个主机上（使用主机名或 IP 地址）。 TELNET 能将用户的击键传到远地主机，同时也能将远地主机的输出通过 TCP 连接返回到用户屏幕。这种服务是透明的，因为用户感觉到好像键盘和显示器是直接连在远地主机上。 域名系统：DNS 首先我们要搞清楚 主机名和IP地址的关系。\n主机名如：facebook.com、google.com 等 但是，主机名几乎没有提供关于主机在因特网中位置的信息，这让路由器难以处理\nIP 地址： 由4个字节组成，并有着严格的层次结构。例如 121.7.106.83 这样一个 IP 地址，其中的每个字节都可以用 . 进行分割，表示了 0 - 255 的十进制数字。\n我们需要一种进行主机名到IP地址转换的目录服务，这就是DNS的主要任务。DNS是：\n一个由分层的DNS服务器实现的分布式数据库 一个使得主机能够查询分布式数据库的应用层协议 如下图所示：\n总的来说，在单一DNS服务器上运行集中式数据库完全没有可扩展能力。因此，DNS采用了分布式设计方案。\nDNS 是一个复杂的系统，我们在这里只是就其运行的主要方面进行学习，下面给出一个 DNS 工作过程的总体概述\n假设运行在用户主机上的某些应用程序（如 Web 浏览器或邮件阅读器） 需要将主机名转换为 IP 地址。这些应用程序将调用 DNS 的客户端，并指明需要被转换的主机名。用户主机上的 DNS 收到后，会使用 UDP 通过 53 端口向网络上发送一个 DNS 查询报文，经过一段时间后，用户主机上的 DNS 会收到一个主机名对应的 DNS 回答报文。因此，从用户主机的角度来看，DNS 就像是一个黑盒子，其内部的操作你无法看到。但是实际上，实现 DNS 这个服务的黑盒子非常复杂，它由分布于全球的大量 DNS 服务器以及定义了 DNS 服务器与查询主机通信方式的应用层协议组成。\nDNS 最早的一种简单设计只是在因特网上使用一个 DNS 服务器。该服务器会包含所有的映射。这是一种集中式的设计，这种设计并不适用于当今的互联网，因为互联网有着数量巨大并且持续增长的主机，这种集中式的设计会存在以下几个问题\n单点故障(a single point of failure)，如果 DNS 服务器崩溃，那么整个网络随之瘫痪。 通信容量(traaffic volume)，单个 DNS 服务器不得不处理所有的 DNS 查询，这种查询级别可能是上百万上千万级 远距离集中式数据库(distant centralized database)，单个 DNS 服务器不可能 邻近 所有的用户，假设在美国的 DNS 服务器不可能临近让澳大利亚的查询使用，其中查询请求势必会经过低速和拥堵的链路，造成严重的时延。 维护(maintenance)，维护成本巨大，而且还需要频繁更新。 所以 DNS 不可能集中式设计，它完全没有可扩展能力，因此采用分布式设计，所以这种设计的特点如下\n分布式、层次数据库 没有一台DNS服务器拥有因特网上所有主机的映射。大致来说，有3种类型的DNS服务器：根DNS服务器、顶级域DNS服务器和权威DNS服务器。\n根DNS服务器 有400多个根服务器遍及全世界。根名字服务器提供TLD服务器的IP地址\n顶级域DNS服务器。 对于每个顶级域(如com、org、edu) 和所有国家的顶级域(uk,cn)等，都有TLD(Top-Level Domain)服务器. TLD服务器提供了权威DNS服务器的IP地址\n权威DNS服务器 一个组织机构的权威DNS收藏了DNS记录。另一种方法是，该组织能够支付费用，让这些记录存储在某个服务提供商的一个权威DNS服务器中。多数大学和大公司实现和维护他们自己基本和备份的权威DNS服务器。\n查询方式 第一个步骤是本机向本地域名服务器发出一个DNS请求报文，报文里携带需要查询的域名；第二个步骤是本地域名服务器向本机回应一个DNS响应报文，里面包含域名对应的IP地址或者别名等。由两种查询方法：\n**递归查询：**本机向本地域名服务器发出一次查询请求，就静待最终的结果。如果本地域名服务器无法解析，自己会以DNS客户机的身份向其它域名服务器查询，直到得到最终的IP地址告诉本机\n**迭代查询：**本地域名服务器向根域名服务器查询，根域名服务器告诉它下一步到哪里去查询，然后它再去查，每次它都是以客户机的身份去各个服务器查询\n注意：理论上，任何DNS查询机可以是迭代的也可以是递归的。 在实践中，第一个步骤从主机到本地域名服务器是递归查询；第二大步骤中采用的是迭代查询，其实是包含了很多小步骤的，如下图所示。\n","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/","tags":["dhcp协议","http协议","smtp协议","ftp协议","dns协议"],"title":"计算机网络之应用层"},{"categories":["c++"],"contents":" 本文主要说明C++的指针和引用在函数调用中的异同点\n指针 向函数传递参数的指针调用方法，把参数的地址复制给形式参数。在函数内，该地址用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。\n按指针传递值，参数指针被传递给函数，就像传递其他值给函数一样。因此相应地，在下面的函数 swap() 中，您需要声明函数参数为指针类型，该函数用于交换参数所指向的两个整数变量的值。\n1 2 3 4 5 6 7 8 9 10 // 函数定义 void swap(int *x, int *y) { int temp; temp = *x; /* 保存地址 x 的值 */ *x = *y; /* 把 y 赋值给 x */ *y = temp; /* 把 x 赋值给 y */ return; } 引用 向函数传递参数的引用调用方法，把引用的地址复制给形式参数。在函数内，该引用用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。\n按引用传递值，参数引用被传递给函数，就像传递其他值给函数一样。因此相应地，在下面的函数 swap() 中，您需要声明函数参数为引用类型，该函数用于交换参数所指向的两个整数变量的值。\n1 2 3 4 5 6 7 8 9 10 // 函数定义 void swap(int \u0026amp;x, int \u0026amp;y) { int temp; temp = x; /* 保存地址 x 的值 */ x = y; /* 把 y 赋值给 x */ y = temp; /* 把 x 赋值给 y */ return; } 区别 引用是 C++ 中的概念，初学者容易把引用和指针混淆一起。\n一下程序中，n 是m的一个引用（reference），m 是被引用物（referent）。\n1 2 int m; int \u0026amp;n = m; n 相当于 m 的别名（绰号），对 n 的任何操作就是对m的操作。\n所以 n 既不是m的拷贝，也不是指向 m 的指针，其实n就是 m 它自己。\n引用的规则：\n（1）引用被创建的同时必须被初始化（指针则可以在任何时候被初始化）。 （2）不能有 NULL 引用，引用必须与合法的存储单元关联（指针则可以是 NULL）。 （3）一旦引用被初始化，就不能改变引用的关系（指针则可以随时改变所指的对象）。 以下示例程序中，k 被初始化为i的引用。\n语句 k = j 并不能将 k 修改成为j的引用，只是把k的值改变成为 6。\n由于 k 是 i 的引用，所以i的值也变成了 6。\n1 2 3 4 int i = 5; int j = 6; int \u0026amp;k = i; k = j; // k 和 i 的值都变成了 6; 引用的主要功能是传递函数的参数和返回值。\nC++ 语言中，函数的参数和返回值的传递方式有三种：值传递、指针传递和引用传递。\n以下是\u0026quot;值传递\u0026quot;的示例程序。\n由于 Func1 函数体内的 x 是外部变量 n 的一份拷贝，改变 x 的值不会影响 n, 所以 n 的值仍然是 0。\n1 2 3 4 5 6 7 8 void Func1(int x) { x = x + 10; } ... int n = 0; Func1(n); cout \u0026lt;\u0026lt; \u0026#34;n = \u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; endl; // n = 0 以下是\u0026quot;指针传递\u0026quot;的示例程序。\n由于 Func2 函数体内的 x 是指向外部变量 n 的指针，改变该指针的内容将导致 n 的值改变，所以 n 的值成为 10。\n1 2 3 4 5 6 7 8 void Func2(int *x) { (* x) = (* x) + 10; } ... int n = 0; Func2(\u0026amp;n); cout \u0026lt;\u0026lt; \u0026#34;n = \u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; endl; // n = 10 以下是\u0026quot;引用传递\u0026quot;的示例程序。\n由于 Func3 函数体内的 x 是外部变量 n 的引用，x 和 n 是同一个东西，改变 x 等于改变 n，所以 n 的值成为 10。\n1 2 3 4 5 6 7 8 void Func3(int \u0026amp;x) { x = x + 10; } ... int n = 0; Func3(n); cout \u0026lt;\u0026lt; \u0026#34;n = \u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; endl; // n = 10 对比上述三个示例程序，会发现\u0026quot;引用传递\u0026quot;的性质象\u0026quot;指针传递\u0026quot;，而书写方式象\u0026quot;值传递\u0026quot;。\n实际上\u0026quot;引用\u0026quot;可以做的任何事情\u0026quot;指针\u0026quot;也都能够做，为什么还要\u0026quot;引用\u0026quot;这东西？\n答案是\u0026quot;用适当的工具做恰如其分的工作\u0026quot;。\n指针能够毫无约束地操作内存中的任何东西，尽管指针功能强大，但是非常危险。\n如果的确只需要借用一下某个对象的\u0026quot;别名\u0026quot;，那么就用\u0026quot;引用\u0026quot;，而不要用\u0026quot;指针\u0026quot;，以免发生意外。\n","permalink":"https://cold-bin.github.io/post/c++%E4%B9%8B%E6%8C%87%E9%92%88%E4%B8%8E%E5%BC%95%E7%94%A8/","tags":["c++指针","c++引用"],"title":"C++之指针与引用"},{"categories":["计算机网络"],"contents":"[toc]\n运输层概述 运输层的基本功能 运输层协议为运行在不同主机上的应用进程之间提供了逻辑通信功能，使得运行在不同主机上的进程像直连一样。 复用和分用 差错检验 进程到进程的数据交付（多路复用和多路分解）和差错检查是两个最低限度的运输层服务，也是UDP能提供的仅有的两种服务。\n多路复用和多路分解 多路复用\n在数据的发送端，传输层收集各个套接字中需要发送的数据，将它们封装上首部信息后（之后用于分解），交给网络层；\n多路分解\n在数据的接收端，传输层接收到网络层的报文后，将它交付到正确的套接字上；\n复用强调的是多个用户进程能够复用相同的运输层协议，例如不同进程的UDP套接字的发送缓冲区数据能被收集后统一交给运输层的UDP协议处理和封装。\n运输层和网络层的关系 运输层为不同主机的应用程序提供了逻辑通信和数据交付服务 网络层为主机和主机提供了逻辑通信和数据交付服务。 运输层是基于网络层的服务的，只有实现了主机间逻辑通信才能再此基础上实现端到端进程间通信；运输层是网络层的功能扩展，网络层不能保证数据的可靠传输，而运输层则扩展了这个功能。\n总结就是：网络层是运输层的基础和服务者，运输层是网络层的扩展，其实协议栈的每个上下层都是这个关系。\n端口号 在TCP/IP体系的运输层中，不同操作系统采用的进程标识符结构不一样，所以，为了保证主机与主机中各个进程之间的通信，所以我们采用16位端口号来映射主机内的某个进程。\n端口号分为3类\n熟知端口（0~1023号端口）：作为指定用途的端口（如80是Web服务器端口），不会随机对口进行分配 登记端口（1024~49151）：可以作为服务器进程被随机分配的端口 短暂端口（49152~65535）：客户端在与对端连接通信时动态选择，连接关闭后端口就关闭，被系统收回，因此又叫短暂端口。 UDP协议 UDP是一种无连接的、面向报文的、尽最大努力交付（不保证可靠交付）的运输层协议。\n特点 不建立连接：减少了3次握手的时延。 不保证可靠交付：报文可能丢失、乱序（但不会比特差错），而且不负责重发。丢失时可以通知应用层，让应用层组织重发。因为不保证可靠交付，所以也没有确认应答机制。 面向报文：一次交付一个完整报文。UDP协议不会对应用层交付给运输层的报文切分（即使这个应用层报文很大），而是直接对应用层报文加上首部就交给网络层（但网络层会对其分片的），保留了报文的边界。需要应用层决定一次发送多少数据以避免发生IP分片。 没有拥塞控制：网络出现拥塞时也不限制发送端的发送速率（对发送端自己有利，但对其所在的网络不利）。这意味着发送速率稳定，但发生拥塞时会因为路由器缓存溢出而丢失分组，而且不限速的发送也会导致拥塞和加重拥塞。 支持多种交互通信：支持一对一，一对多，多对多，多对一的交互通信，而TCP只能做到一对一。这里的一对一和一对多是指socket，具体是指一个服务端的socket可以为多个客户端的socket通信和服务。而TCP服务端的一个client套接字只能为一个客户端socket收发消息。 首部开销小：8个字节，比TCP的20字节首部短。 UDP的使用场景 实时应用：语音电话或视频会议。利用了UDP开销小、效率高、没有拥塞的特点。 一次性传小量数据的应用：如果一次性大量数据则不利于UDP不切分数据的特点。 多媒体应用：如播放视频。因为视频对传输可靠性没那么高，即使小部分数据丢失也不影响视频播放。 UDP协议报文格式 每个UDP报文分为UDP报头和UDP数据区两部分。报头由4个16位长(2字节) 字段组成，分别说明该报文的源端口、目的端口、报文长度、校验值\nTCP协议 TCP协议是一种面向连接、面向字节流、提供可靠传输服务的一对一通信传输层协议。\n特点 面向连接：数据收发前需要建立连接，数据收发要通过这个逻辑的虚拟信道。 全双工通信：通信两端都可收可发，而且双向的收发可以同时发生。 一对一通信：一个client socket只能与一个server socket进行数据收发。如果要实现多个client数据接收，只能开启相应数量的server 面向字节流：TCP只把数据看成一连串有序而无结构的字节流。 可靠传输服务：数据不丢失、不重复、无差错、不乱序。 TCP功能在UDP功能（数据交付和差错检验）的基础上加上了连接管理、超时重传、流量控制和拥塞控制。\nTCP报文段格式 TCP首部的前20个字节是固定的（TCP首部最小长度为20字节，最大长度60），后面4n字节是按需增加的选项。\n序号（seq）：TCP数据的每个字节是按序编号的，TCP首部的“序号”字段就是本报文携带数据的第一个字节的编号，范围在0~2^32-1， 超过则下一个序号会回到0重新增长。\n初始序号（ISN）在建立连接时设置，ISN是一个随时间动态增长的非0序号，这也是为了防止被攻击者伪造初始序号和TCP报文。\n序号是TCP实现可靠传输的基础，其作用有：防止接收方接收重复分组；确认应答机制基于序号，给字节标记序号便于超时重传触发时发送方知道自己应该重发那哪分组。\n确认号（ACK）：是一端期望收到对端下一个报文段数据的第一个字节的序号，也是本端上一次发送的报文段的最后一个字节序号+1。\n数据偏移：表示TCP首部的长度，占4个比特，每个比特的单位是4字节。因此TCP首部最大长度为60字节。\n6个标志位\n紧急位 URG：URG = 1时报头的紧急指针字段生效，表示报文段有紧急数据要尽快传送。\nsocket有发送缓冲区，运输层会择机将发送缓冲区的数据发送出去（3个时机），但如果URG = 1的报文段则无需等待这3个时机，可以直接发送。\n确认位 ACK：ACK = 1时，报头的确认号(ACK)字段才有效，表示本报文是一个确认报文（tcp的应答确认机制）。TCP规定在连接建立后所有传送的报文段都必须把ACK置为1。\n推送位 PSH：PSH=1的报文段，接收方会尽快上交给应用进程（不要在接收缓冲区中缓存），该标志位是针对接收方的。\n重置连接位 RST：RST =1 的报文段标明TCP连接出现差错，必须释放连接，然后重新建立连接。\n同部位 SYN：SYN = 1表示这是一个连接请求报文。\n终止位 FIN：FIN = 1表示这是一个请求释放连接的报文\n窗口字段：接收方告诉发送方，下一次容许发送方能够发送的最大数据长度（取决于接收方的接收缓冲区大小）。\n检验和：同UDP校验和。\n紧急指针：指出本报文段中紧急数据有多少字节，紧急数据放在报文段数据的最前面，所以紧急指针等于紧急数据在本报文的最后一个字节的位置。窗口为0时也可以发送紧急数据。\n选项字段（最大40字节）：该字段长度可变。有如下可选项：\n所有可选选项都包含该选项的类型长度还有实际内容（例如下面的时间戳选项，kind=8表示这是一个时间戳选项，length表示时间戳选项长度为10字节).所有可选选项都在建立连接时的SYN报文指定开启。\n窗口扩大选项 使用窗口扩大项后，可以是窗口大小从原本最大 2^16 - 1 扩大到最大 2^30 - 1个字节。\n这是考虑到链路可能有长又肥（即带宽很大，且链路很长(即时延很长)），如果一次发的数据太少就无法充分利用带宽，吞吐率也低。发送端的发送窗口（TCP头部的窗口大小）由接收方的接收缓存大小 **（流量窗口）**以及 **链路带宽和拥塞情况（拥塞窗口）**共同决定的。\n选择确认选项：接收方告诉发送方自己收到的连续字节块。用于数据段失序到达时，发送端重复发送数据段。\n时间戳选项：占10字节，包含最主要的是时间戳值字段（4字节）和时间戳回送回答字段（4字节）。\n有两个作用：\nA. 计算报文在两端传输层的往返时延（接近于RTT）\nA发送报文时会将发送时间戳放入 timestamp（时间戳值字段）， B接收到报文后将timestamp复制到timestamp echo（时间戳回显重试字段），并在返回ACK报文时将当前时间戳放入timestamp。\n回复报文到达A后，A可以用当前时间戳 - timestamp echo得到往返时间，而且该往返时间可认为就是RTT。\nB. 防止序号回绕带来的问题\n需要注意，填充的时间戳不是真实的时间戳，而是一个自增的整型，而且发送方填入的timestamp和接收方填入的timestamp可以是独立的，例如发送方填入timestamp = 5012， 接收方填入timestamp = 197720862，也就是说两端的时间戳可以不用同步。\nMSS 最大报文段大小\nMSS 最大报文段数据大小，用于告诉对端，我所在的局域网链路能容纳的最大报文段的数据长度。MSS和窗口无关，和网络带宽有关。在建立TCP连接时，通信双方都要在SYN报文指明自己允许的MSS大小，MSS是双向的。\nMSS可以控制TCP的传输效率，MSS太大可能导致报文段在网络层分片，太小可能导致传输效率降低（假设MSS设为1个字节，那么一个报文段的数据包含只有1个字节但头部有20个字节，你说效率低不低）。应该尽量设置MSS接近网络层一个分片的大小，使得该报文段刚好不用分片，这取决于从源主机到目的主机链路的最小MTU（MTU是网络层的包的最大长度）。\nMSS + TCP头部 = TCP报文段长度。\nMSS + TCP头部 + IP头部 \u0026lt;= MTU\nMSS默认是536字节（这也是合理的最小MSS），因为任何主机都应该至少处理576字节的IPv4数据报（含IP头部），如果按最小的IPv4和TCP头部计算，最小IPv4数据报下的最大MSS = 576 - 20 - 20 = 536。\n在以太网中IPv4协议下，MSS应该设置的比较合适的值是略小于1460，因为以太网MTU=1500, 而TCP和IP头部分别为20字节，1460 + 20 +20 = 1500，刚好达到网络层不用对IP包分片的最大包大小，也刚好到达以太网链路的最大报文数据的传输大小。\n在以太网中IPv6协议下，MSS应该设置的比较合适的值是略小于1440，因为IPv6的头部为40字节。\nTCP可靠传输原理 停止等待协议 简单来说，就是发一个数据包，就等待一个ACK，然后再发一个数据包；如果等不到ACK就需要重发一个数据包。\n停止等待协议，是一个保证可靠传输的协议，主要有以下机制构成：\n确认应答机制 接收方接收到一个正确分组时，需要回复一个带确认号和ACK位为1的确认报文给发送方。此时发送方就知道分组已到达；\n分组由于比特差错被接收方检测到会被直接丢弃，不发送ACK报文（在TCP协议中即使一个分组发生比特错误，对端也会发送ACK，但确认的是出错报文之前的报文）；\n分组由于丢失无法到达接收方则不发送ACK报文；\n停等机制 指发送方每发送完一个分组就停止发送，等待对方确认，收到确认后再发下一个分组。\n自动超时重传机制（ARQ） 发送方为每个分组设置一个超时计时器（TCP中是为每个窗口设置一个超时计时器），超过指定重传时间（RTO）未收到接收方的ACK报文，发送方就会重发报文。\nARQ的意思是不用接收方请求重传，而是发送方通过计时器计时来自动重传。\n这里需要注意3点：\n发送方必须暂时保留已发送的分组的副本以便重传时使用，收到相应分组的确认才可以清除副本。 分组必须编号，这才能明确哪个分组得到确认（而且编号也有助于接收方丢弃重复接收过的分组）。 重传时间RTO 应该略大于RTT 一个问题：超时重传时间应该定为多少？\n答：超时重传时间太大会使网络链路空闲，降低传输效率；太小会使报文不必要的重传，加大网络负荷引起拥塞。TCP采用了一种自适应算法计算超时时间RTO。RTO应该略大于报文往返时间RTT。\n1 2 3 4 5 新的RTTS = (1-α) * (旧的RTTS) + α * (新的RTT样本) RTTD = （1-β）* 旧的RTTD + β * | RTTS - 新的RTT样本 | RTO = RTTS + 4*RTTD TCP需要计算一个加权平均往返时间RTTS，反映多次报文传输的整体RTT，每次确认报文到达发送端，发送端都会更新一次RTTS。\n“新的RTT样本”可以使用发送方接收到ACK分组的时间戳 - 分组头部时间戳选项计算得到。\nα由系统和协议栈开发者决定，α接近0表示新的RTT影响不大，更新较慢；建议标准的RFC规范推荐α=1/8。\nRTTD是RTT的加权平均标准差，反应了多次RTT的抖动程度。β推荐为 1/4。\n另一个问题：假设发生了重传，并且收到了确认报文，如何确定该ACK报文是对先发送的报文的确认还是重传报文的确认？\n这个问题对RTTS的计算很重要（假设时间戳选项没开启）。如果该ACK是对重传报文的确认，却被误认为是对原来报文的确认，则更新后的RTTS和RTO会偏大。该问题不解决，RTO会因为重传越变越大。\n答：Karn算法提出，如果报文段重传收到ACK，无需判断ACK是之前报文的确认还是重传报文的确认，直接不更新本次的RTTS和RTO即可。\n这会带来新的问题：如果一段时间内TCP会重传很多报文，采用上述做法会导致RTO失去多次更新，变得不准确。\n修正的Karn算法提出，报文段每重传一次就把RTO增大一点：\n1 重传时新RTO = γ * 旧的RTO 举例 停止等待协议下的通信会出现如下情况\n无差错的情况 有差错的情况（分组丢失或差错） 在接收⽅ B 会出现两种情况：\nB 接收 M1 时检测出了差错，就丢弃 M1，其他什么也不做； M1 在传输过程中丢失了，这时 B 当然什么都不知道，也什么都不做。 解决⽅法：超时重传（ARQ）\nA 为每⼀个已发送的分组都设置超时计时器，A由于重传时间RTO内（略大于RTT）没收到M1的确认因此会重发M1；A 在超时计时器到期之前收到了相应的确认，撤销该超时计时器，继续发送下⼀个分组 M2 。 确认报文丢失或迟到 假设B收到了报文（发送端的报文没丢失）而且报文没有错误，但是B的确认报文丢失或过了很久才到达A。\n子情况1：确认报文丢失。\n由于A收不到 M1 的确认报文，A 在超时计时器到期后重传 M1。 B ⼜收到了重传的分组 M1，所以丢弃这个重复的分组 M1（通过序号和时间戳选项判断分组是否重复），不向上层交付，并向 A 发送确认。\n子情况2：确认报文迟到\n由于A收不到 M1 的确认报文，A 在超时计时器到期后重传 M1。 B ⼜收到了重传的分组 M1，所以丢弃这个重复的分组 M1，并向 A 发送确认。A由于会收到2个ACK号相同的确认报文，A会丢弃其中晚到达的一个。\n结论：停等协议可以保证可靠传输的实现，但通信效率不高\n连续ARQ协议 在一定限度k内，发送方连续发出k个包后停下，如果ACK报文返回则可以继续发送；如果一定时间内没有返回ack则重传；\n连续ARQ协议在停止等待协议的基础之上提升了信道利用率，它除了要遵循确认应答机制和自动超时重传机制外，还包括以下机制：\n流水线传输机制 指发送方可以连续发送多个分组不必每发一个分组就等待对方的确认，与停止等待协议对立，可以提高信道利用率和链路的吞吐量。当然连续发送的分组数量是有限的，这取决于滑动窗口的大小。\n累积确认机制 指接收方不必为每个到达的分组都发送确认，而是收到若干个分组后对按序到达的最后一个分组发送确认。当然TCP协议既可能出现累积确认，也可能出现对单个分组确认。\n确认的时机（这里也是TCP确认的时机）：\n收到 2*MSS 长度的数据就做确认应答（有些系统是不管数据长度，而是收到2个报文就确认）； 最大延迟0.5秒发送确认应答，即使即使只收到一个分组也要确认（多数系统是0.2秒），该延迟时间由延迟应答计时器来计时； 当接收方接收到失序的报文段时就立刻发出确认（对最后一个有序分组的确认），以便快速重传（当发送方收到连续3个ack号相同的ack报文时就会重传）。 若干分组到达后，累计确认 累积确认是为了提高信道的利用率，提升系统性能，能少发报文就少发报文。当然如果延迟确认的时间长了可能引发发送端重传，也会降低传输效率。\n捎带确认机制 如果接收方发送确认时刚好也要发送自己的数据报文，那么这个ACK确认可能会捎带到这个数据报文中，此时就减少了一个报文头部的开销，这叫做捎带确认。\n回退N机制 指当报文乱序到达接收端，接收端收到的分组不是连续的，而是缺了某些分组，此时接收端只确认第一个空缺分组之前的分组，空缺分组以及其之后的n个分组都要发送方重传，这就是回退N。\nB收到1 2 4 5号分组但由于3号分组未收到就到达确认时机，只能选择确认1和2分组，发送端A需要重发 3~5 号分组。\n为了避免回退N机制重复发送已经发过的报文，可以使用TCP选项中的“选择确认SACK功能”。其机制如下：\n上图空缺的序号是 1000~1500 和 3001~3501。\nSACK的原理是，把乱序到达的分组先暂存在接收缓冲区，并且把空缺分组相邻分组的左右边界序号（必须是成对边界，在本例子中有3个边界0~999 / 1501~3000 / 3501~4500，共 6 * 4字节=24字节）放到头部的SACK选项告诉发送方，发送方就会只重传空缺的数据给接收方，而无需回退N步。\n⾸部选项的⻓度最⼤有 40 字节，指明⼀个对序号⽤掉 8 字节，因此在选项中最多只能指明 4 对序号的边界信息，也就是指明最多3个空缺范围（4对序号用掉 32字节，还需要2个字节指明选项类型和长度）。\nSACK文档并未有指明发送方应该怎样响应SACK，所以大多数的实现还是会回退N，重传所有未确认的数据块1000~4500。\n对比 TCP可靠传输的实现 TCP的可靠传输以上述连续ARQ协议为基础做出了一些变动，并研究更多的细节如滑动窗口的实现、超时计时器如何设置超时时间、选择确认、流量控制和拥塞控制。\nTCP可靠传输基于4点：窗口、序号、确认和重传。其中后3点实现了可靠传输，第1点提高TCP传输效率。\nTCP的发送时机 为了保证TCP传输效率，TCP不会在发送缓冲区一有数据就立刻发送，而是会遵循一些发送的时机，当满足以下3个条件中的一个才会发送数据：\nTCP维持一个MSS变量，当缓存中的数据到达MSS字节时，就组装成一个报文段发送； 当发送方进程指明要求推送报文段（PSH=1）或者是发送紧急数据（URG=1） 发送方设置一个TCP发送报文计时器，如果到时了，即便发送缓存中的数据量不够MSS也要发送出去。 当然情况1和3要在发送方的可用窗口大于0的情况下才能发送出去。\nNagle算法 Nagle算法用于发送方比较空闲，没有什么数据要发送的情况下，为提高传输效率的一种算法。假设发送方的应用层是逐字节发送数据给协议栈的缓冲区，则Nagle会这样处理：\n若进程要把发送的数据逐个字节的发送到TCP的发送缓存，则发送⽅先发送第⼀个数据字节，缓存后⾯到达的数据字节； 发送⽅收到对第⼀个数据字符的确认后，把发送缓存中的所有数据组装成⼀个报⽂段（不超过MSS）发送出去，继续对随后到达的数据进⾏缓存； 只有在收到对前⼀个报⽂段的确认后继续发送下⼀个报⽂段（相当于退化成停等协议）； 当到达缓冲区的数据已达到发送窗⼝⼤⼩的⼀半或已达到报⽂段的最⼤⻓度时，（即使上一个报文的确认没到达）也⽴即发送⼀个报⽂段。 接收方此时应该适当延迟回发确认报文，并尽量使用捎带确认。 该算法总结一下就是，在应用进程的数据到达发送缓冲区的速度比较慢的时候就退化成停等协议，比较快且快到满足上述第4点的时候就立刻发送报文。\n重传机制 TCP 实现可靠传输的方式之一，是通过序列号与确认应答。\n在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。\n但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？\n所以 TCP 针对数据包丢失的情况，会用重传机制解决。\n接下来说说常见的重传机制：\n超时重传 快速重传 SACK D-SACK 超时重传 重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据，也就是我们常说的超时重传。\nTCP 会在以下两种情况发生超时重传：\n数据包丢失 确认应答丢失 超时时间应该设置为多少呢？\n我们先来了解一下什么是 RTT（Round-Trip Time 往返时延），从下图我们就可以知道：\nRTT 指的是数据发送时刻到接收到确认的时刻的差值，也就是包的往返时间。\n超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。\n假设在重传的情况下，超时时间 RTO 「较长或较短」时，会发生什么事情呢？\n上图中有两种超时时间不同的情况：\n当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 精确的测量超时时间 RTO 的值是非常重要的，这可让我们的重传机制更高效。\n根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。\n至此，可能大家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。\n好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记一个 t1，于是 RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。\n实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个动态变化的值。\n我们来看看 Linux 是如何计算 RTO 的呢？\n估计往返时间，通常需要采样以下两个：\n需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个平滑 RTT 的值，而且这个值还是要不断变化的，因为网络状况不断地变化。 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况。 RFC6289 建议使用以下的公式计算 RTO：\n其中 SRTT 是计算平滑的RTT ，DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。\n在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4。别问怎么来的，问就是大量实验中调出来的。\n如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。\n也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。\n超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？\n于是就可以用「快速重传」机制来解决超时重发的时间等待。\n快速重传 TCP 还有另外一种快速重传（Fast Retransmit）机制，它不以时间为驱动，而是以数据驱动重传。\n快速重传机制，是如何工作的呢？其实很简单，一图胜千言。\n在上图，发送方发出了 1，2，3，4，5 份数据：\n第一份 Seq1 先送到了，于是就 Ack 回 2； 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；（因为数据包乱序到达接收端，所以不能给乱序的到达的数据包发送ACK，只能给最后一个有序到达的数据包发ACK，因此也就重发ACK2） 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到； 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。（至于为什么会是三次，而不是两次、一次。如果次数较少，可能会频繁触发重传） 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。\n快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传一个，还是重传所有的问题。\n举个例子，假设发送方发了 6 个数据，编号的顺序是 Seq1 ~ Seq6 ，但是 Seq2、Seq3 都丢失了，那么接收方在收到 Seq4、Seq5、Seq6 时，都是回复 ACK2 给发送方，但是发送方并不清楚这连续的 ACK2 是接收方收到哪个报文而回复的， 那是选择重传 Seq2 一个报文，还是重传 Seq2 之后已发送的所有报文呢（Seq2、Seq3、 Seq4、Seq5、 Seq6） 呢？\n如果只选择重传 Seq2 一个报文，那么重传的效率很低。因为对于丢失的 Seq3 报文，还得在后续收到三个重复的 ACK3 才能触发重传。 如果选择重传 Seq2 之后已发送的所有报文，虽然能同时重传已丢失的 Seq2 和 Seq3 报文，但是 Seq4、Seq5、Seq6 的报文是已经被接收过了，对于重传 Seq4 ～Seq6 折部分数据相当于做了一次无用功，浪费资源。 可以看到，不管是重传一个报文，还是重传已发送的报文，都存在问题。\n为了解决不知道该重传哪些 TCP 报文，于是就有 SACK 方法。\nSACK 方法 还有一种实现重传机制的方式叫：SACK（ Selective Acknowledgment）， 选择性确认。\n这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将已收到的数据的信息发送给「发送方」，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。\n如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。\n如果要支持 SACK，必须双方都要支持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux 2.4 后默认打开）。\nDuplicate SACK Duplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。\n下面举例两个栗子，来说明 D-SACK 的作用。\n栗子一号：ACK 丢包\n「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499） 于是「接收方」发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 D-SACK。 这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。 栗子二号：网络延时\n数据包（1000~1499） 被网络延迟了，导致「发送方」没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」； 所以「接收方」回了一个 SACK=1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。 可见，D-SACK 有这么几个好处：\n可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以知道是不是「发送方」的数据包被网络延迟了; 可以知道网络中是不是把「发送方」的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能（Linux 2.4 后默认打开）。\n滑动窗口 通信过程中一个传输方向上所有的字节的序号（seq）可以看做一个序列（这些序列在socket的缓冲区中），而窗口则是序列中的一个子集。该窗口用于做流量控制以及拥塞控制。\n滑动窗口的单位是字节而不是分组。一个连接的两端都各有一对窗口分别是发送窗口和接收窗口，因此一个连接有4个窗口，且是动态变化的。\n发送窗口包含 已发送但未确认的数据 和 准备发送的数据。\n已发送但未确认的数据是为了方便超时重传的实现。\n接收窗口包含 按序到达但未被应⽤程序接收的数据 和 不按序到达的数据。\n接收窗口缓存一下未被接收的数据；缓存一下不按序到达的数据，以避免大量数据的重传。\n窗口指针保存在套接字中。在不考虑拥塞的情况，发送端A的发送窗口和接收方B的接收窗口大小从整个传输过程来看是一致的（但不是强一致，并不总是一样大）。\n发送窗口越大，发送方在收到确认前能连续发送的数据越多，在不考虑网络拥塞因素下传输效率越高。\n滑动窗口的工作过程 只观察发送方的发送缓冲区和接收方的接收缓冲区:\n1、某一时刻，A 收到了 B 的确认报⽂段：报文携带的窗口值 20 字节，确认号为 31。A 可以把落⼊发送窗⼝中的序号字节⼀次连续性全部发送出去：边发送边接收确认。\n2、下一刻A发送了31~41号字节，在确认前会保留在窗口中以便超时重传时使用。\nB的接收窗口显示，B没有收到31，32~33是未按需到达的数据，要临时存放在接收窗口，不能上交给应用进程。B的接收窗口也不能移动。\n3、下一刻B收到了31，B将31~33字节交付应用层，并从接收缓冲区中删除，且B接收窗口右移3个字节，发送ack=33的确认报文（假设确认报文的窗口大小字段仍是20）。A收到确认后窗口右移3字节。\n4、当P2=P3时，可用窗口为0，会停止发送。\n缓冲区与窗口的关联 发送方的发送缓存与发送窗口 发送窗⼝通常只是 发送缓存的⼀部分，具体来说 发送缓存 = 发送应用程序最后写入发送缓冲区的最后一个字节 - 发送窗口P1字节。缓冲区中，p1指针之前的数据由于已经发送和收到确认，因此被释放出缓冲区。\n接受方的接收缓存与接收窗口 流量控制 流量控制是指动态控制滑动窗口的大小使得发送方发送数据的速率略小于或等于接收方接收的速率（接收速率其实又由应用程序取走接收缓冲区的速率决定），防止接收方的接收缓存溢出造成分组丢失。\n流量控制的实现是通过在接收方的ACK报文携带窗口大小（假设大小X，X=接收方的接收缓冲区的空闲空间大小）同步给发送方，使发送方调整自己的可用窗口（P3-P2部分）为X。\n需要注意的是发送窗口的p2-p1取决于ack号，P3-P2取决于ACK报文中的窗口大小。\n下面是一个流量控制的过程（不考虑拥塞的情况下）：\n图中rwnd(receiver window)表示容许的接收方窗口。\n持续计时器 考虑一种情况，如果B向A发送了一个零窗口报文后，A停止向B发数据，后来B又向A发送了一个rwnd=400的ACK报文M。但M丢失了，A一直在等B的非零窗口通知，B也在等A发过来的数据，陷入死锁局面。\n解决方法：\nTCP为每个连接设有一个持续计时器，只要一端A收到零窗口通知就启动该计时器，时间到期，A就发送一个仅携带1字节的“零窗口探测报文”。对端B就会确认这个报文的时候携带新的rwnd值。如果rwnd仍为0则重新启动持续计时器，否则A开始发送数据。\n拥塞控制 什么情况下叫做拥塞？\n网络中，链路容量（带宽）、交换机和路由器中的缓存和处理机都是网络的资源，在某段时间，若对网络中某一资源的需求超过了该资源能提供的部分，导致分组在链路中丢失，这种情况就叫拥塞。\n有了TCP的窗口控制后，使计算机网络中两个主机之间不再是以单个数据段的形式发送了，而是能够连续发送大量的数据包。然而，大量数据包同时夜伴随着其他问题，比如说网络负载、网络拥堵等问题。TCP因此使用了拥塞控制机制，使得在面临网络拥塞时遏制发送方的数据发送。\n拥塞控制主要有两种方法\n端到端的拥塞控制： 因为网络层没有为运输层拥塞控制提供显示支持。所以即使网络中存在拥塞情况，端系统也要通过对网络行为的观察来推断。TCP 就是使用了端到端的拥塞控制方式。IP 层不会向端系统提供有关网络拥塞的反馈信息。那么 TCP 如何推断网络拥塞呢？如果超时或者三次冗余确认就被认为是网络拥塞，TCP 会减小窗口的大小，或者增加往返时延来避免。 网络辅助的拥塞控制: 在网络辅助的拥塞控制中，路由器会向发送方提供关于网络中拥塞状态的反馈。这种反馈信息就是一个比特信息，它指示链路中的拥塞情况。 拥塞的特点：\n1、网络拥塞是由网络资源中的短板资源所决定的，只有所有类型的网络资源同时提高供给才会真正改善网络性能（例如你提高了带宽，但是路由器的缓存较小，瓶颈就转移到了缓存那里）。\n2、拥塞趋于恶化，例如某个路由器没有足够的缓存，缓存溢出导致丢包和端系统重传，一旦重传又会加重网络拥塞。\n3、拥塞的直接表现就是丢包和重传，当端系统的重传次数明显增加，就表明网络很可能发生了拥塞。举个例子：如果是带宽出现瓶颈，则RTT会增加，导致超时重传；如果是路由器缓存瓶颈，分组到达路由器后因缓存溢出而丢包，又会导致超时重传。因此重传就是拥塞的表现。\n拥塞的其他指标（了解即可）：\n• 由于缺少缓存空间⽽被丢弃的分组的百分数；\n• 平均队列⻓度；\n• 超时重传的分组数；\n• 平均分组时延；\n• 分组时延的标准差，等等。\n简单的记就是：丢包率、重传率和时延。拥塞控制是防⽌过多的数据注⼊到⽹络中，使⽹络中的路由器或 链路不致过载。\n拥塞控制和流量控制的区别\n流量控制是解决端与端的发送与接收速率不匹配的问题，需要发送方同步接收方的接收速度； 拥塞控制是解决端系统的通信量与网络链路资源不匹配引起的路由器和链路过载问题，需要控制端系统注入到网络的数据量和速度。 拥塞控制的方法 拥塞控制需要解决的三个问题\nTCP 发送方如何限制它向其他连接发送报文段的速率呢？\nTCP是由接收缓存、发送缓存等组成。发送方的TCP拥塞控制机制会跟踪一个变量，即拥塞窗口的变量，拥塞窗口表示为cwnd, 用于限制TCP在接收到ACK之前可以发送到网络的数据量，而接收窗口是用来告诉接收方能够接受的数据量\n一般来说，发送方未确认的数据量不得超过 cwnd 和 rwnd 的最小值，也就是LastByteSent−LastByteAcked\u0026lt;=min(cwnd,rwnd)\n由于每个数据包的往返时间是RTT，我们假设接收端有足够的缓存空间用于接收数据，我们就不用考虑rwnd了，只用专注于cwnd，那么，该发送方的 发送速率 = cwnd/RTT 字节/秒 . 通过调节cwnd，发送方因此能调整它向连接发送数据的速率。\n一个 TCP 发送方是如何感知到网络拥塞的呢？\nTCP 根据超时或者 3 个冗余 ACK(丢包了) 来感知的。这就是TCP的快速重传机制。\n当发送方感知到端到端的拥塞时，采用何种算法来改变其发送速率呢？\n慢开始、拥塞避免、快重传和快恢复。这4种方法的基本思路是，只要网络没有拥塞，拥塞窗口就可以增大些，出现拥塞就减小些。\n慢开始算法（慢启动） 慢开始的思路是从小到大以指数方式增加拥塞窗口的数值。慢开始发生在刚建立连接后的数据收发。\n一开始发送方并不清楚网络的拥塞情况，就先将cwnd初始值设置为1~2个SMSS（发送方MSS），新的RFC标准则把初始cwnd设置为2~4，至于取2还是3还是4，取决于SMSS有多大。\n在每收到⼀个对新的报⽂段的确认（重传的确认不算）后，发送方的拥塞窗⼝就增加⼀个 SMSS 的数值，因此cwnd会呈指数级别增长。\n慢启动的窗口增长速度其实不慢（指数级别），之所以叫慢启动是因为它的初始cwnd值很小。\n顺带一提 ，新建立的连接会用到慢启动，TCP 还实现了 慢启动重启 （SSR） 机制。这种机制会在持久连接空闲一定时间后重置拥塞窗口为初始cwnd值。道理很简单， 在连接空闲的同时，网络状况也可能发生了变化，为了避免拥塞，理应将拥塞窗 口重置回“安全的”默认值。\n为了不让窗口无限的指数增长，提出了慢开始门限，当窗口大小超过了慢开始门限 ssthresh 则使用拥塞避免算法线性增长窗口。\n慢开始⻔限 ssthresh ：\n当 cwnd \u0026lt; ssthresh 时，使⽤慢开始算法；\n当 cwnd \u0026gt;= ssthresh 时，停⽌使⽤慢开始算法⽽改⽤拥塞避免算法；\n拥塞避免算法 该算法是指：当cwnd超过慢开始门限后，每经过一个RTT，拥塞窗口就线性增长 cwnd = cwnd + 1。\n快速重传算法 该算法是指：如果发送方连续收到3个重复ack号的确认，说明接收方收到了乱序的报文（某个中间报文丢失或者迟到），发送方会立即进行重传，而不是等到超时时间用完才重传，避免发送方误认为发生了网络拥塞。\n中间报文的丢失或迟到极可能是意外丢失或迟到，而不是因为网络拥塞导致的丢失，但不排除拥塞的可能性。快速重传可以使网络的吞吐量提高20%。\n快恢复算法 拥塞惩罚是指端系统检测到网络拥塞时（即发生重传时），降低自己cwnd窗口的行为。拥塞惩罚按超时重传和快速重传分为两种惩罚方式：\n当发生超时重传时，发送方会认为网络出现拥塞，拥塞窗口cwnd会变成1。 当发生快速重传时，该分组很可能是意外丢失或迟到，但不排除拥塞的可能，因此cwnd会变为 cwnd/2。 快恢复算法是指当发生快速重传时，当前拥塞窗口大小减小一半，之后直接执行拥塞避免算法线性增长cwnd，而不是执行慢开始算法指数增长cwnd。\n下面是TCP拥塞控制流程图：\n拥塞惩罚机制 整个拥塞惩罚机制逻辑如下：\n超时重传的情况下：\n• 慢开始⻔限 ssthresh = max(cwnd/2，2)；\n• cwnd = 1；\n• 执⾏慢开始算法。\n快速重传的情况下（快速恢复）：\n• 慢开始⻔限 ssthresh = 当前拥塞窗⼝ cwnd / 2 ；\n• 新拥塞窗⼝ cwnd = 慢开始⻔限 ssthresh ；\n• 开始执⾏拥塞避免算法，使拥塞窗⼝缓慢地线性增⼤。\n无论是超时重传还是快速重传，都会导致慢开始门限减半，这会导致多次惩罚后，不再会执行指数增长，而是全变成线性增长。\nTCP拥塞控制动态流程图 主动队列管理 AQM 对TCP拥塞控制影响最大的网络层策略是分组丢弃策略。该策略的内容为，到达路由器的分组会按先进先出原则放入到缓存队列中，一旦队列已满，后到达的分组会被丢弃。\n这种丢弃策略会导致一连串分组的丢失和超时重传，这一方向的所有TCP连接都进入慢开始状态，这种情况叫做全局同步，全局同步会导致通信量突然下降，不一会儿通信量又突然增大（因为报文指数增长）。\n为了避免全局同步，我们可以在队列长度到达某个警戒线时主动丢弃部分分组，而不是在分组数量达到最大队列长度时被动丢弃所有分组，这就是主动队列管理AQM。\nAQM有不同的实现方式，比较主流的是随机早期检测RED。\nRED规定路由器维持一个最小门限THmin和最大门限THmax。\n队列⻓度L ⼩于最⼩⻔限 THmin，将新到达的分组放⼊队 列进⾏排队；\n队列⻓度L 超过最⼤⻔限 THmax，将新到达的分组丢弃；\n队列⻓度L 在最⼩⻔限 THmin 和最⼤⻔限 THmax 之间，按 照概率 p 丢弃新到达的分组。而且随着队列长度L的增加，p也会变大。\n连接管理 TCP连接要解决3个问题：\n使对方知道自己的存在，且确认双方能发送能接收； 允许双方协商一些参数（如最大窗口值，是否用窗口扩大选项和时间戳选项等）； 分配运输资源（缓存大小，连接表中的项目）； 三次握手建立连接 需要3个报文：\n下面是连接的状态变化\n在连接之前，A和B会先创建传输控制块TCB，存储了连接相关的重要信息如 TCP连接表，指向发送和接收缓存的指针，指向重传队列的指针，seq和ack等。\nSYN不能携带数据，但需要消耗一个序号。ACK报文是可以携带数据的。\n为什么建立连接是三次握手而不是两次？\n是为了防止已失效的连接请求报文段传到了B产生错误。具体情境如下：\nA 发送的SYN报文丢失，A又重发了一个SYN报文，并建立连接成功，后来关闭了连接，通信结束。但丢失的SYN报文此时到达B，B发送第二次握手的报文（ACK报文）后直接进入连接状态，而A没有发起建立连接的请求，不会理睬这个ACK报文，但B会一直等待A发送数据报文段过来，B的资源白白浪费。\n四次挥手断开连接 需要注意：\n通信双方都可以主动发起关闭连接的请求报文。\nFIN可以携带数据，但如果不携带数据也会消耗一个序号。\n当被动关闭者B进入CLOSE-WAIT状态时，TCP连接处于半关闭状态，此时B可以发送数据，A无法发送数据，但可以接收数据。因此这个状态下，B可能还会继续发送消息给A。\nA进入TIME-WAIT状态后，必须经过时间等待计时器设置的时间2MSL后才能进入CLOSED状态。\n一个问题：A 为什么必须等待 2MSL （MSL是最长报文段寿命）的时间后才真正关闭连接？\n1、防止第四次挥手的ACK丢失后B无法进入CLOSED状态。\n假设A在第三次挥手之后直接进入CLOSED，而且最后一个ACK丢失，B会重发第三次挥手，假设A之前的端口是X，这时有两个情况：一个是A之前的端口又开始建立新的连接，那么A收到该FIN报文之后，会回应一个RST报文给B；一个是A之前的端口没有再开启过了，那么B的FIN报文不会得到ACK回应，B会不停的重传。\n2、保证本次连接产生的所有报文（FIN、SYN和数据报文）在这2MSL内从网络中消失，不会和新连接的报文发生混淆（尤其是新连接和旧连接的客户端端口是相同的情况下）。\nTCP半关闭 半关闭是指建立连接的两端只有其中一端发送FIN报文，关闭双向连接的某一个方向。主动发送FIN的一端之后就无法向对端发送数据，只能接受对端发送的数据和发送ACK报文段。\n一端发送FIN报文之后，另一端发送FIN报文之前的连接状态称为“半关闭状态”。\n套接字的close()提供了全关闭操作，而shutdown()则提供了半关闭操作，实际应用中半关闭很少用到。\nTCP同时打开与关闭 同时打开是指通信双方A和B，A发送SYN报文给B，并在报文段到达B之前，B也发送SYN报文给A。同时打开只会出现在A和B都是服务器端的情况下。\n连接建立超时 如果一个客户端发起连接请求时，服务器是关闭的，那么客户端会在连接等待超时后再重新发送SYN报文，并且每次超时，超时时间都会翻倍。这一行为被称为指数回退。\n在Linux中net.ipv4.tcp_syn_retries参数可以配置重发SYN的次数，而net.ipv4.tcp_synack_retries则是第二次握手的SYN报文的重发次数。这两个参数通常选择一个较小值5。\nTCP有限状态机 计时器 超时重传计时器：略； 零窗口持续计时器：零窗口时发送探测报文的计时器； Time-Wait计时器：time-wait等待2MSL的计时器； 保活计时器：防止TCP连接长时间空闲； 发送报文计时器：防止发送方长时间没有发送报文； PS: 保活计时器⽤来防⽌在TCP连接出现⻓时期的空闲以及判断对方是否故障下线。\n保活计时器 通常设置为2⼩时 。若服务器过了2⼩时还没 有收到客户的信息，它就发送探测报⽂段。若发送了10个 探测报⽂段（每⼀个相隔75秒）还没有响应，就假定客户 出了故障，因⽽就终⽌该连接。\n","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/","tags":["tcp协议","udp协议"],"title":"计算机网络之运输层"},{"categories":["计算机网络"],"contents":"[toc]\n4.1、网络层概述 简介 网络层的主要任务是实现网络互连，进而实现数据包在各网络之间的传输 这些异构型网络N1~N7如果只是需要各自内部通信，他们只要实现各自的物理层和数据链路层即可\n但是如果要将这些异构型网络互连起来，形成一个更大的互联网，就需要实现网络层设备路由器\n有时为了简单起见，可以不用画出这些网络，图中N1~N7，而将他们看做是一条链路即可\n要实现网络层任务，需要解决以下主要问题：\n网络层向运输层提供怎样的服务（“可靠传输”还是“不可靠传输”）\n在数据链路层那课讲过的可靠传输，详情可以看那边的笔记：网络层对以下的分组丢失、分组失序、分组重复的传输错误采取措施，使得接收方能正确接受发送方发送的数据，就是可靠传输，反之，如果什么措施也不采取，则是不可靠传输\n网络层寻址问题\n如何对不同的主机或设备进行唯一的标识和寻址。\n路由选择问题 路由器收到数据后，是依据什么来决定将数据包从自己的哪个接口转发出去？\n依据数据包的目的地址和路由器中的路由表\n但在实际当中，路由器是怎样知道这些路由记录？\n由用户或网络管理员进行人工配置，这种方法只适用于规模较小且网络拓扑不改变的小型互联网 另一种是实现各种路由选择协议，由路由器执行路由选择协议中所规定的路由选择算法，而自动得出路由表中的路由记录，这种方法更适合规模较大且网络拓扑经常改变的大型互联网 补充\n网络层（网际层）除了 IP协议外，还有之前介绍过的地址解析协议ARP，还有网际控制报文协议ICMP，网际组管理协议IGMP ARP协议应该不是网际层，而是属于网络接口层（相当于OSI七层参考模型中的数据链路层和物理层，而ARP协议属于数据链路层） 总结 4.2、网络层提供的两种服务 在计算机网络领域，网络层应该向运输层提供怎样的服务（“面向连接”还是“无连接”）曾引起了长期的争论。 争论焦点的实质就是：在计算机通信中，可靠交付应当由谁来负责？是网络还是端系统？ 面向连接的虚电路服务 一种观点：让网络负责可靠交付\n这种观点认为，应借助于电信网的成功经验，让网络负责可靠交付，计算机网络应模仿电信网络，使用面向连接的通信方式。 通信之前先建立虚电路 (Virtual Circuit)，以保证双方通信所需的一切网络资源。 如果再使用可靠传输的网络协议，就可使所发送的分组无差错按序到达终点，不丢失、不重复。 发送方 发送给 接收方 的所有分组都沿着同一条虚电路传送\n虚电路表示这只是一条逻辑上的连接，分组都沿着这条逻辑连接按照存储转发方式传送，而并不是真正建立了一条物理连接。\n请注意，电路交换的电话通信是先建立了一条真正的物理连接。\n因此分组交换的虚连接和电路交换的连接只是类似，但并不完全一样：\n虚电路则是一种建立逻辑通信通道的方式，通信双方在通信前需要建立一条虚电路，它会预留一定的资源，但这些资源在通信过程中并不一直被占用，而是根据实际通信需要进行动态分配。因此，虚电路的通信质量较电路交换略低，但资源利用率更高，且适应通信量变化较大的场景。\n无连接的数据报服务 另一种观点：网络提供数据报服务\n如果网络层提供可靠的传输服务，也即采用第一种观点（建立面向连接的虚电路服务），会导致线路被占用（尽管虚电路建立的逻辑电路连接，不一定一直占用线路资源，但还是存在占用情况），资源利用率下降。互联网的先驱者提出了一种崭新的网络设计思路。\n网络层向上只提供简单灵活的、无连接的、尽最大努力交付的数据报服务。 网络在发送分组时不需要先建立连接。每一个分组（即 IP 数据报）独立发送，与其前后的分组无关（不进行编号）。 网络层不提供服务质量的承诺。即所传送的分组可能出错、丢失、重复和失序（不按序到达终点），当然也不保证分组传送的时限。 发送方 发送给 接收方 的分组可能沿着不同路径传送\n尽最大努力交付\n如果主机（即端系统）中的进程之间的通信需要是可靠的，那么就由网络的主机中的运输层负责可靠交付（包括差错处理、流量控制等） 。 采用这种设计思路的好处是：网络的造价大大降低，运行方式灵活，能够适应多种应用。 互联网能够发展到今日的规模，充分证明了当初采用这种设计思路的正确性。 虚电路服务与数据报服务的对比 对比的方面 虚电路服务 数据报服务 思路 可靠通信应当由网络来保证 可靠通信应当由用户主机（即端系统）来保证 连接的建立 必须有 不需要 终点地址 仅在连接建立阶段使用，后续发送的每个分组使用短的虚电路号 每个分组都有终点的完整地址 分组的转发 属于同一条虚电路的分组均按照同一路由进行转发 每个分组可以选择不同的路由进行转发 当结点出故障时 所有含有故障结点的虚电路均不能工作，因为虚电路断掉了 出故障的结点可能会丢失分组。可以选择其他节点继续转发路由 分组的顺序 总是按发送顺序到达终点 到达终点时不一定按发送顺序 端到端的差错处理和流量控制 可以由网络负责，也可以由用户主机负责 由用户主机负责，网络只是负责简单快捷的路由 4.3、IPv4 概述 分类编制的IPv4地址 简介 每一类地址都由两个固定长度的字段组成，其中一个字段是网络号 net-id，它标志主机（或路由器）所连接到的网络，而另一个字段则是主机号 host-id，它标志该主机（或路由器）。 主机号为全0的地址表示该子网的网络地址，不特指某个主机IP地址，而是指该子网的地址 主机号为全1的地址和MAC地址为全1的地址一样都是属于广播地址，不能分配给主机或路由器 主机号在它前面的网络号所指明的网络范围内必须是唯一的。 由此可见，一个 IP 地址在整个互联网范围内是唯一的。 A类地址\n本地回环地址不仅仅只是127.0.0.1，是一个范围127.0.0.1到127.255.255.254。这些本地回环地址可以被视为是代表自身主机的某个永远不会宕掉的虚拟接口，常常用来测试本机的IP协议安装和网卡是否有问题。\n所以，如果需要对本机的某些资源进行访问，我们可以直接访问127.0.0.1到127.255.255.254中的任何一个地址都可以。\n如果本机是服务器，当然也可以通过公网IP进行外部访问，这样做虽然可以，但是多了互联网上的路由成本。\nB类地址\nC类地址\n练习\n总结 IP 地址的指派范围\n实际上，128.0和192.0.0都可以指派了，这才是第一个可以指派的网络号\n一般不使用的特殊的 IP 地址\nIP 地址的一些重要特点\n(1) IP 地址是一种分等级的地址结构。分两个等级的好处是：\n第一，IP 地址管理机构在分配 IP 地址时只分配网络号，而剩下的主机号则由得到该网络号的单位自行分配。这样就方便了 IP 地址的管理。 第二，路由器仅根据目的主机所连接的网络号来转发分组（而不考虑目的主机号），这样就可以使路由表中的项目数大幅度减少，从而减小了路由表所占的存储空间。 (2) 实际上 IP 地址是标志一个主机（或路由器）和一条链路的接口。\n当一个主机同时连接到两个网络上时，该主机就必须同时具有两个相应的 IP 地址，其网络号 net-id 必须是不同的。这种主机称为多归属主机 (multihomed host)。 由于一个路由器至少应当连接到两个网络（这样它才能将 IP 数据报从一个网络转发到另一个网络），因此一个路由器至少应当有两个不同的 IP 地址。 (3) 用转发器或网桥连接起来的若干个局域网仍为一个网络，因此这些局域网都具有同样的网络号 net-id。\n(4) 所有分配到网络号 net-id 的网络，无论是范围很小的局域网，还是可能覆盖很大地理范围的广域网，都是平等的。\n划分子网的IPv4地址 为什么要划分子网 在 ARPANET 的早期，IP 地址的设计确实不够合理：\nIP 地址空间的利用率有时很低，而且划分的网络号可能支持的主机很多\n一个网络里的主机太多，就会导致广播的流量压力增大（ARP协议就需要使用广播）\n给每一个物理网络分配一个网络号会使路由表变得太大因而使网络性能变坏\n一个网络号能提供寻址的主机可能会很多，那么路由表就会较大\n两级的 IP 地址不够灵活。\n如果想要将原来的网络划分成三个独立的网路\n所以是否可以从主机号部分借用一部分作为子网号\n但是如果未在图中标记子网号部分，那么我们和计算机又如何知道分类地址中主机号有多少比特被用作子网号了呢？\n所以就有了划分子网的工具：子网掩码\n从 1985 年起在 IP 地址中又增加了一个“子网号字段”，使两级的 IP 地址变成为三级的 IP 地址。 这种做法叫做划分子网 (subnetting) 。 划分子网已成为互联网的正式标准协议。 如何划分子网 基本思路\n划分子网纯属一个单位内部的事情。单位对外仍然表现为没有划分子网的网络。 从主机号借用若干个位作为子网号 subnet-id，而主机号 host-id 也就相应减少了若干个位。 凡是从其他网络发送给本单位某个主机的 IP 数据报，仍然是根据 IP 数据报的目的网络号 net-id，先找到连接在本单位网络上的路由器。 然后此路由器在收到 IP 数据报后，再按目的网络号 net-id 和子网号 subnet-id 找到目的子网。 最后就将 IP 数据报直接交付目的主机。 划分为三个子网后对外仍是一个网络\n优点 划分子网时，减少了 IP 地址的浪费 减少广播域 控制流量 优化路由 提高安全性 使网络的组织更加灵活 更便于维护和管理 划分子网纯属一个单位内部的事情，对外部网络透明，对外仍然表现为没有划分子网的一个网络。 子网掩码 (IP 地址) AND (子网掩码) = 网络地址 重要，下面很多相关知识都会用到\n举例 例子1\n例子2\n默认子网掩码\n可以看得出：使用默认子网掩码时，也就是说没有划分子网。\n总结 子网掩码是一个网络或一个子网的重要属性。 路由器在和相邻路由器交换路由信息时，必须把自己所在网络（或子网）的子网掩码告诉相邻路由器。 路由器的路由表中的每一个项目，除了要给出目的网络地址外，还必须同时给出该网络的子网掩码。 若一个路由器连接在两个子网上，就拥有两个网络地址和两个子网掩码。 无分类编址的IPv4地址 为什么使用无分类编址 无分类域间路由选择 CIDR (Classless Inter-Domain Routing)。\nCIDR 最主要的特点\nCIDR使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号。 IP 地址从三级编址（使用子网掩码）又回到了两级编址。 如何使用无分类编址 举例 路由聚合（构造超网） 总结 IPv4地址的应用规划 给定一个IPv4地址块，如何将其划分成几个更小的地址块，并将这些地址块分配给互联网中不同网络，进而可以给各网络中的主机和路由器接口分配IPv4地址\n定长的子网掩码FLSM（Fixed Length Subnet Mask） 划分子网的IPv4就是定长的子网掩码\n举例\n通过上面步骤分析，就可以从子网1~8中任选5个分配给左图中的N1~N5\n采用定长的子网掩码划分，只能划分出2^n^个子网，其中n是从主机号部分借用的用来作为子网号的比特数量，每个子网所分配的IP地址数量相同\n但是也因为每个子网所分配的IP地址数量相同，不够灵活，容易造成IP地址的浪费\n变长的子网掩码VLSM（Variable Length Subnet Mask） 无分类编址的IPv4就是变长的子网掩码\n举例\nIPv6 4.4、IP数据报的发送和转发过程 举例 源主机如何知道目的主机是否与自己在同一个网络中，是直接交付，还是间接交付？\n可以通过目的IP地址和目的地址的子网掩码进行逻辑与运算得到目的网络地址\n如果目的网络地址和源网络地址 相同，就是在同一个网络中，属于直接交付 如果目的网络地址和源网络地址 不相同，就不在同一个网络中，属于间接交付，传输给主机所在网络的默认网关（路由器——下图会讲解）,由默认网关帮忙转发 主机C如何知道路由器R的存在？\n用户为了让本网络中的主机能和其他网络中的主机进行通信，就必须给其指定本网络的一个路由器的接口，由该路由器帮忙进行转发，所指定的路由器，也被称为默认网关\n例如：路由器的接口0的IP地址192.168.0.126做为左边网络的默认网关\n主机A会将该IP数据报传输给自己的默认网关，也就是图中所示的路由器接口0\n路由器收到IP数据报后如何转发？\n检查IP数据报首部是否出错： 若出错，则直接丢弃该IP数据报并通告源主机 若没有出错，则进行转发 根据IP数据报的目的地址在路由表中查找匹配的条目： 若找到匹配的条目，则转发给条目中指示的下一跳（也就是找到从路由器的哪个接口出去） 若找不到，则丢弃该数据报并通告源主机 假设IP数据报首部没有出错，路由器取出IP数据报首部各地址字段的值\n接下来路由器对该IP数据报进行查表转发\n逐条检查路由条目，将目的地址与路由条目中的地址掩码进行逻辑与运算得到目的网络地址，然后与路由条目中的目的网络进行比较，如果相同，则这条路由条目就是匹配的路由条目，按照它的下一条指示，图中所示的也就是接口1转发该IP数据报\n路由器是隔离广播域的\n4.5、静态路由配置及其可能产生的路由环路问题 概念 多种情况举例 静态路由配置 默认路由 IPv4地址0.0.0.0/0在路由器中的目的网络指的就是所有网络。默认路由可以被所有网络匹配，但路由匹配有优先级，默认路由是优先级最低的\n特定主机路由 有时候，我们可以给路由器添加针对某个主机的特定主机路由条目，一般用于网络管理人员对网络的管理和测试\n多条路由可选，匹配路由最具体的\n静态路由配置错误导致路由环路 假设将R2的路由表中第三条目录配置错了下一跳。这导致R2和R3之间产生了路由环路\n聚合了不存在的网络而导致路由环路 正常情况 错误情况 解决方法 黑洞路由的下一跳为null0，这是路由器内部的虚拟接口，IP数据报进入它后就被丢弃\n网络故障而导致路由环路 解决方法 添加故障的网络为黑洞路由\n假设。一段时间后故障网络恢复了\nR1又自动地得出了其接口0的直连网络的路由条目\n针对该网络的黑洞网络会自动失效\n如果又故障\n则生效该网络的黑洞网络\n总结 4.6、路由选择协议 概述 因特网所采用的路由选择协议的主要特点\n因特网采用分层次的路由选择协议\n自治系统 AS 在单一的技术管理下的一组路由器，而这些路由器使用一种 AS 内部的路由选择协议和共同的度量以确定分组在该 AS 内的路由； 同时还使用一种 AS 之间的路由选择协议用以确定分组在 AS 之间的路由。 自治系统之间的路由选择简称为域间路由选择，自治系统内部的路由选择简称为域内路由选择\n域间路由选择使用外部网关协议EGP这个类别的路由选择协议\n域内路由选择使用内部网关协议IGP这个类别的路由选择协议\n网关协议的名称可称为路由协议\n常见的路由选择协议 路由器的基本结构 路由器是一种具有多个输入输出端口的专用计算机，其任务是转发分组\n路由器结构可划分为两大部分：\n分组转发部分 由三部分构成：交换\n交换结构\n一组输入端口：\n1、信号从某个输入端口进入路由器\n2、物理层将信号转换成比特流，送交数据链路层处理\n3、数据链路层识别从比特流中识别出帧，去掉帧头和帧尾后，送交网络层处理\n4、如果送交网络层的分组是普通待转发的数据分组\n5、则根据分组首部中的目的地址进行查表转发\n6、若找不到匹配的转发条目，则丢弃该分组；否则，按照匹配条目中所指示的端口进行转发\n一组输出端口\n1、网络层更新数据分组首部中某些字段的值，例如将数据分组的生存时间减1，然后送交数据链路层进行封装\n2、数据链路层将数据分组封装成帧，交给物理层处理\n3、物理层将帧看成比特流将其变换成相应的电信号进行发送\n路由器的各端口还会有输入缓冲区和输出缓冲区\n输入缓冲区用来暂存新进入路由器但还来不及处理的分组\n输出缓冲区用来暂存已经处理完毕但还来不及发送的分组\n路由器的端口一般都具有输入和输出功能，这些实例分出了输入端口和输出端口是更好演示路由基本工作过程\n路由选择部分 路由选择部分的核心构件是路由选择处理机，它的任务是根据所使用的路由选择协议周期性地与其他路由器进行路由信息的交互，来更新路由表\n，如果送交给输入端口的网络层的分组是路由器之间交换路由信息的路由报文，则把这种分组送交给路由选择处理机\n路由选择处理机根据分组的内容来更新自己的路由表\n路由选择处理机还会周期性地给其他路由器发送自己所知道的路由信息\n路由信息协议RIP 距离向量 RIP的基本工作过程 RIP的路由条目的更新规则 举例1 路由器C的表 到达各目的网络的下一条都记为问号，可以理解为路由器D并不需要关心路由器C的这些内容\n假设路由器C的RIP更新报文发送周期到了，则路由器C将自己路由表中的相关路由信息封装到RIP更新报文中发送给路由器D\n路由器C能到达这些网络，说明路由器C的相邻路由器也能到达，只是比路由器C的距离大1，于是根据距离的对比，路由器D更新自己的路由表\n举例2 RIP存在“坏消息传播得慢”的问题 解决方法 但是，这些方法也不能完全解决“坏消息传播得慢”的问题，这是距离向量的本质决定\n总结\nRIP 协议的优缺点\n优点：\n实现简单，开销较小。 缺点：\nRIP 限制了网络的规模，它能使用的最大距离为 15（16 表示不可达）。\n路由器之间交换的路由信息是路由器中的完整路由表，因而随着网络规模的扩大，开销也就增加。\n“坏消息传播得慢”，使更新过程的收敛时间过长。\n开放最短路径优先OSPF 开放最短路径优先 OSPF (Open Shortest Path First)\n注意：OSPF 只是一个协议的名字，它并不表示其他的路由选择协议不是“最短路径优先”。\n概念 问候（Hello）分组 IP数据报首部中协议号字段的取值应为89，来表明IP数据报的数据载荷为OSPF分组.\n发送链路状态通告LSA 洪泛法有点类似于广播，就是从一个接口进来，从其他剩余所有接口出去\n链路状态数据库同步 使用SPF算法计算出各自路由器到达其他路由器的最短路径 OSPF五种分组类型 OSPF的基本工作过程 OSPF在多点接入网络中路由器邻居关系建立 如果不采用其他机制，将会产生大量的多播分组\n若DR出现问题，则由BDR顶替DR\n为了使OSPF能够用于规模很大的网络，OSPF把一个自治系统再划分为若干个更小的范围，叫做区域（Area）\n在该自治系统内，所有路由器都使用OSPF协议，OSPF将该自治系统再划分成4个更小的区域\n每个区域都有一个32比特的区域标识符\n主干区域的区域标识符必须为0，主干区域用于连通其他区域\n其他区域的区域标识符不能为0且不相同\n每个区域一般不应包含路由器超过200个\n划分区域的好处就是，利用洪泛法交换链路状态信息局限于每一个区域而不是自治系统，这样减少整个网络上的通信量\n总结\n边界网关协议BGP BGP（Border Gateway Protocol） 是不同自治系统的路由器之间交换路由信息的协议\n总结\n直接封装RIP、OSPF和BGP报文的协议 4.7、IPv4数据报的首部格式 下面只是说了IPv4的首部格式，数据载荷就是上层交付的数据\n各字段的作用 一个 IP 数据报由首部和数据两部分组成。 首部的前一部分是固定长度，共 20 字节，是所有 IP 数据报必须具有的。 在首部的固定部分的后面是一些可选部分，其长度是可变的。 图中的每一行都由32个比特（也就是4个字节）构成，每个小格子称为字段或者域，每个字段或某些字段的组合用来表达IP协议的相关功能\nIP数据报的首部长度一定是4字节的整数倍\n因为首部中的可选字段的长度从1个字节到40个字节不等，那么，当20字节的固定部分加上1到40个字节长度不等的可变部分，会造成首部长度不是4字节整数倍时，就用取值为全0的填充字段填充相应个字节，以确保IP数据报的首部长度是4字节的整数倍\n对IPv4数据报进行分片\n现在假定分片2的IP数据报经过某个网络时还需要进行分片\n总结 IPv6数据报首部格式 4.8、网际控制报文协议ICMP 概念 架构IP网络时需要特别注意两点：\n确认网络是否正常工作 遇到异常时进行问题诊断 而ICMP就是解决这些问题的协议，ICMP的主要功能包括：\n确认IP包是否成功送达目标地址 通知在发送过程当中IP包被废弃的具体原因 改善网络设置等 有了这些功能以后，就可以获得网络是否正常，设置是否有误以及设备有何异常等信息，从而便于进行网络上的问题诊断\nICMP报文的分类 ICMP报文的格式 ICMP 不是高层协议（看起来好像是高层协议，因为 ICMP 报文是装在 IP 数据报中，作为其中的数据部分），而是 IP 层的协议\nICMP差错报告报文 终点不可达 源点抑制 时间超过 参数问题 改变路由（重定向） 不应发送ICMP差错报告报文情况 ICMP应用举例 分组网间探测PING（Packet InterNet Groper） 跟踪路由（traceroute） tracert命令的实现原理 总结 4.9、虚拟专用网VPN与网络地址转换NAT 虚拟专用网VPN（Virtual Private Network） 由于 IP 地址的紧缺，一个机构能够申请到的IP地址数往往远小于本机构所拥有的主机数。 考虑到互联网并不很安全，一个机构内也并不需要把所有的主机接入到外部的互联网。 假定在一个机构内部的计算机通信也是采用 TCP/IP 协议，那么从原则上讲，对于这些仅在机构内部使用的计算机就可以由本机构自行分配其 IP 地址。 上图是因特网数字分配机构IANA官网查看IPv4地址空间中特殊地址的分配方案\n用粉红色标出来的地址就是无需申请的、可自由分配的专用地址或称私有地址\n私有地址只能用于一个机构的内部通信，而不能用于和因特网上的主机通信\n私有地址只能用作本地地址而不能用作全球地址\n因特网中所有路由器对目的地址是私有地址的IP数据报一律不进行转发\n本地地址与全球地址\n本地地址——仅在机构内部使用的 IP 地址，可以由本机构自行分配，而不需要向互联网的管理机构申请。 全球地址——全球唯一的 IP 地址，必须向互联网的管理机构申请。 问题：在内部使用的本地地址就有可能和互联网中某个 IP 地址重合，这样就会出现地址的二义性问题。 所以部门A和部门B至少需要一个 路由器具有合法的全球IP地址，这样各自的专用网才能利用公用的因特网进行通信\n部门A向部门B发送数据流程\n两个专用网内的主机间发送的数据报是通过了公用的因特网，但在效果上就好像是在本机构的专用网上传送一样\n数据报在因特网中可能要经过多个网络和路由器，但从逻辑上看，R1和R2之间好像是一条直通的点对点链路\n因此也被称为IP隧道技术\n网络地址转换NAT（Network Address Translation） 举例\n使用私有地址的主机，如何才能与因特网上使用全球IP地址的主机进行通信？\n这需要在专用网络连接到因特网的路由器上安装NAT软件\n装有NAT软件的路由器叫做NAT路由器。它至少有一个有效的外部全球IP地址\n这样，所有使用私有地址的主机在和外界通信时，都要在NAT路由器上将其私有地址转换为全球IP地址\n假设，使用私有地址的主机要给因特网上使用全球IP地址的另一台主机发送IP数据报\n因特网上的这台主机给源主机发回数据报\n当专用网中的这两台使用私有地址的主机都要给因特网使用全球地址的另一台主机发送数据报时，在NAT路由器的NAT转换表中就会产生两条记录，分别记录两个私有地址与全球地址的对应关系\n这种基本转换存在一个问题\n解决方法\n我们现在用的很多家用路由器都是这种NART路由器\n内网主机与外网主机的通信，是否能由外网主机首先发起？\n否定\n总结 ","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/","tags":["ipv4","路由选择协议","icmp协议","vpn","nat"],"title":"计算机网络之网络层"},{"categories":["计算机网络"],"contents":"[toc]\n3.1、数据链路层概述 概述 链路与数据链路 链路是从一个结点到相邻结点的一段物理线路，数据链路则是在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现）。\n网络中的主机、路由器等都必须实现数据链路层\n局域网中的主机、交换机等都必须实现数据链路层\n从层次上来看数据的流动：\n仅从数据链路层观察帧的流动：\n主机H1 到主机H2 所经过的网络可以是多种不同类型的\n注意：不同的链路层可能采用不同的数据链路层协议\n数据链路层使用的信道 数据链路层属于计算机网路的低层。数据链路层使用的信道主要有以下两种类型：\n点对点信道 广播信道 局域网属于数据链路层\n局域网虽然是个网络。但我们并不把局域网放在网络层中讨论。这是因为在网络层要讨论的是多个网络互连的问题，是讨论分组怎么从一个网络，通过路由器，转发到另一个网络。\n而在同一个局域网中，分组怎么从一台主机传送到另一台主机，但并不经过路由器转发。从整个互联网来看，局域网仍属于数据链路层的范围\n三个重要问题 数据链路层传送的协议数据单元是帧\n封装成帧 封装成帧 (framing) 就是在一段数据的前后分别添加首部和尾部，然后就构成了一个帧。 首部和尾部的一个重要作用就是进行帧定界。 数据链路层采用的点对点信道的PPP协议封装的PPP帧和采用以太网协议的MAC帧 当发送端出问题，帧没有发送完毕，那么发送端只能重发此帧；此时接收端接收到不完整的帧，会丢弃这个帧。\n差错控制 在传输过程中可能会产生比特差错：1 可能会变成 0， 而 0 也可能变成 1。\n为了保证数据传输的可靠性，在计算机网络传输数据时，必须采用各种差错检测措施。\n循环冗余检验CRC\n缺点：可能会检测不出错误，从而导致纠错能力不足\n透明传输 帧定界符：可以选用ASCII码表中的SOH(0x01)作为帧开始定界符，EOT(0x04)为帧结束定界符。\n如果数据部分出现“EOT”或“SOH”时，那么会使得数据部分拆分出帧，使得帧的解析出现问题，所以要进行字节填充。\n具体方法：发送端的数据链路层在数据中出现控制字符“EOT”、“SOH”以及“ESC”，则在前面插入一个转义字符“ESC”的编码。接收端的数据链路层在收到删除这个插入的转义字符。这样用字节填充法解决透明传输的问题。\n以上三个问题都是使用点对点信道的数据链路层来举例的\n如果使用广播信道的数据链路层除了包含上面三个问题外，还有一些问题要解决\n如图所示，主机A，B，C，D，E通过一根总线进行互连，主机A要给主机C发送数据，代表帧的信号会通过总线传输到总线上的其他各主机，那么主机B，D，E如何知道所收到的帧不是发送给她们的，主机C如何知道发送的帧是发送给自己的\n可以用编址（地址）的来解决，将帧的目的地址添加在帧中一起传输\n数据碰撞 除了上面三个问题，还有数据碰撞问题\n随着技术的发展，交换技术的成熟，在 有线（局域网）领域 使用点对点链路和链路层交换机的交换式局域网取代了共享式局域网；在无线局域网中仍然使用的是共享信道技术\n3.2、封装成帧 介绍 封装成帧是指数据链路层给上层交付的协议数据单元添加帧头和帧尾使之成为帧\n帧头和帧尾中包含有重要的控制信息 发送方的数据链路层将上层交付下来的协议数据单元封装成帧后，还要通过物理层，将构成帧的各比特，转换成电信号交给传输媒体，那么接收方的数据链路层如何从物理层交付的比特流中提取出一个个的帧？\n答：需要帧头和帧尾来做帧定界\n但并不是每一种数据链路层协议的帧都包含有帧定界标志，例如下面例子\n前导码\n前同步码：作用是使接收方的时钟同步 帧开始定界符：表明其后面紧跟着的就是MAC帧 另外以太网还规定了帧间间隔为96比特时间，因此，MAC帧不需要帧结束定界符，只需要检测当前比特是否\n透明传输 透明\n指某一个实际存在的事物看起来却好像不存在一样。\n透明传输是指数据链路层对上层交付的传输数据没有任何限制，好像数据链路层不存在一样\n帧界定标志也就是个特定数据值，如果在上层交付的协议数据单元中，恰好也包含这个特定数值，接收方就不能正确接收\n所以数据链路层应该对上层交付的数据有限制，其内容不能包含帧定界符的值\n解决透明传输问题\n解决方法：面向字节的物理链路使用字节填充 (byte stuffing) 或字符填充 (character stuffing)，面向比特的物理链路使用比特填充的方法实现透明传输 发送端的数据链路层在数据中出现控制字符“SOH”或“EOT”的前面插入一个转义字符“ESC”(其十六进制编码是1B)。 接收端的数据链路层在将数据送往网络层之前删除插入的转义字符。 如果转义字符也出现在数据当中，那么应在转义字符前面插入一个转义字符 ESC。当接收端收到连续的两个转义字符时，就删除其中前面的一个。 帧的数据部分长度\n总结 3.3、差错检测 介绍 奇偶校验 奇偶检验法只能检测出奇数个错误，不能检测出所有的错误\n循环冗余校验CRC(Cyclic Redundancy Check) 例题\n总结\n循环冗余校验 CRC 是一种检错方法，而帧校验序列 FCS 是添加在数据后面的冗余码\n3.4、可靠传输 基本概念 下面是比特差错\n其他传输差错\n分组丢失 路由器输入队列快满了，主动丢弃收到的分组\n分组失序 数据并未按照发送顺序依次到达接收端\n分组重复 由于某些原因，有些分组在网络中滞留了，没有及时到达接收端，这可能会造成发送端对该分组的重发，重发的分组到达接收端，但一段时间后，滞留在网络的分组也到达了接收端，这就造成分组重复的传输差错\n三种可靠协议 一般情况下，有线链路的不由数据链路层提供可靠传输，而交由上层解决可靠传输问题；但是无线链路的误码率较高，需要数据链路层提供可靠传输服务。有三种可靠协议实现：\n停止-等待协议SW 回退N帧协议GBN 选择重传协议SR 这三种可靠传输实现机制的基本原理并不仅限于数据链路层，可以应用到计算机网络体系结构的各层协议中\n停止-等待协议 定义 停止等待协议中，发送方每发送一个分组都必须等待接收方的确认分组ACK（Acknowled Character）才能发送下一个分组，如果是否认分组NAK（Negative Acknowledge)则重传上次发送的分组\n停止-等待协议可能遇到的问题 超时重传\n如果发送方分组丢失，那么接收方就不会发送ACK，此时发送方就会一直等待ACK而不能发送数据。为了避免这种情况，发送方主动设置一个时钟，如果分组发出后超过了一定的时间没有收到ACK就自动重发分组，这称为超时重传，超时时间一般设置为略大于一个分组往返的时间\n确认丢失\n接收方的确认（否认）分组也是可能丢失的，假设发送方发送了分组并且被接收方接收，接收方的确认分组在传输过程中丢失，那么发送方因为没收到确认分组就会触发超时重传把相同的分组再发送一遍，此时接收方会收到一个重复的分组\n为了避免分组重复，发送方需要给每个分组编号，因为发送方每次只发送一个分组就停下来等待，所以发送方只需使用一个比特进行编号就能使分组区分开来\n当接收方检查编号发现是重复的分组时就知道上个分组的确认分组丢失了，此时接收方只需丢弃收到的分组并发送ACK即可，发送方收到ACK后也不会再重发分组了\n既然数据分组需要编号，确认分组是否需要编号？\n要。如下图所示\n确认迟到\n接收方的ACK也有可能迟到，发送方发送分组0后因为ACK迟到触发超时重传再次发送分组0，再次发送分组0后收到了迟到的ACK然后发送方发送分组1。假设接收方先收到重传的分组0然后发送ACK再收到分组1，但是发送方收到重传的分组0的ACK时有可能误认为是分组1的ACK。为了避免上述情况，ACK也需要编号以便发送方确认，同样的ACK编号也只需要一个比特\n注意，图中最下面那个数据分组与之前序号为0的那个数据分组不是同一个数据分组\n注意事项\n停止-等待协议的信道利用率 假设收发双方之间是一条直通的信道\nTD：是发送方发送数据分组所耗费的发送时延 RTT：是收发双方之间的往返时间 TA：是接收方发送确认分组所耗费的发送时延 TA一般都远小于TD，可以忽略，当RTT远大于TD时，信道利用率会非常低\n像停止-等待协议这样通过确认和重传机制实现的可靠传输协议，常称为自动请求重传协议ARQ(Automatic Repeat reQuest)，意思是重传的请求是自动进行，因为不需要接收方显式地请求，发送方重传某个发送的分组\n由以上的种种情况分析，发送方每发送一个分组都需要等待接收方的回应再做下一步，因此停止等待协议信道利用率很低，大多数时间都用来等待了\n回退N帧协议GBN 为什么用回退N帧协议 在相同的时间内，使用停止-等待协议的发送方只能发送一个数据分组，而采用流水线传输的发送方，可以发送多个数据分组\n回退N帧协议在流水线传输的基础上，利用发送窗口来限制发送方可连续发送数据分组的个数\n无差错情况流程 发送方将序号落在发送窗口内的0~4号数据分组，依次连续发送出去\n他们经过互联网传输正确到达接收方，就是没有乱序和误码，接收方按序接收它们，每接收一个，接收窗口就向前滑动一个位置，并给发送方发送针对所接收分组的确认分组，在通过互联网的传输正确到达了发送方\n发送方每接收一个确认分组，发送窗口就向前滑动一个位置，这样就有新的序号落入发送窗口，发送方可以将收到确认的数据分组从缓存中删除了，而接收方可以择机将已接收的数据分组交付上层处理\n累计确认 累计确认\n优点:\n即使确认分组丢失，发送方也可能不必重传 减小接收方的开销 减小对网络资源的占用 缺点：\n不能向发送方及时反映出接收方已经正确接收的数据分组信息 有差错情况 例如\n在传输数据分组时，5号数据分组出现误码，接收方通过数据分组中的检错码发现了错误\n于是丢弃该分组，而后续到达的这剩下四个分组与接收窗口的序号不匹配\n接收同样也不能接收它们，将它们丢弃，并对之前按序接收的最后一个数据分组进行确认，发送ACK4，每丢弃一个数据分组，就发送一个ACK4\n当收到重复的ACK4时，就知道之前所发送的数据分组出现了差错，于是可以不等超时计时器超时就立刻开始重传，具体收到几个重复确认就立刻重传，根据具体实现决定\n如果收到这4个重复的确认并不会触发发送立刻重传，一段时间后。超时计时器超时，也会将发送窗口内以发送过的这些数据分组全部重传\n若WT超过取值范围，例如WT=8，会出现什么情况？\n习题\n总结\n回退N帧协议在流水线传输的基础上利用发送窗口来限制发送方连续发送数据分组的数量，是一种连续ARQ协议 在协议的工作过程中发送窗口和接收窗口不断向前滑动，因此这类协议又称为滑动窗口协议 由于回退N帧协议的特性，当通信线路质量不好时，其信道利用率并不比停止-等待协议高 选择重传协议SR 具体流程请看视频\n习题\n总结\n3.5、点对点协议PPP 点对点协议PPP（Point-to-Point Protocol）是目前使用最广泛的点对点数据链路层协议 PPP协议是因特网工程任务组IEIF在1992年制定的。经过1993年和1994年的修订，现在的PPP协议已成为因特网的正式标准[RFC1661，RFC1662] 数据链路层使用的一种协议，它的特点是： 简单； 只检测差错，而不是纠正差错（因为在差错检验时，选用的是CRC差错检验技术，CRC差错检验技术并没有纠错能力） 不使用序号，也不进行流量控制； 可同时支持多种网络层协议，例如ip、ipx等协议 PPPoE 是为宽带上网的主机使用的链路层协议 帧格式 必须规定特殊的字符作为帧定界符\n透明传输 必须保证数据传输的透明性\n实现透明传输的方法\n面向字节的异步链路：字节填充法（插入“转义字符”） 面向比特的同步链路：比特填充法（插入“比特0”） 差错检测 能够对接收端收到的帧进行检测，并立即丢弃有差错的帧。\n工作状态 当用户拨号接入 ISP 时，路由器的调制解调器对拨号做出确认，并建立一条物理连接。 PC 机向路由器发送一系列的 LCP 分组（封装成多个 PPP 帧）。 这些分组及其响应选择一些 PPP 参数，并进行网络层配置，NCP 给新接入的 PC 机分配一个临时的 IP 地址，使 PC 机成为因特网上的一个主机。 通信完毕时，NCP 释放网络层连接，收回原来分配出去的 IP 地址。接着，LCP 释放数据链路层连接。最后释放的是物理层的连接。 可见，PPP 协议已不是纯粹的数据链路层的协议，它还包含了物理层和网络层的内容。\n3.6、媒体接入控制（介质访问控制）——广播信道 媒体接入控制（介质访问控制）使用一对多的广播通信方式\nMedium Access Control翻译成媒体接入控制，有些翻译成介质访问控制\n局域网的数据链路层\n局域网最主要的特点是： 网络为一个单位所拥有； 地理范围和站点数目均有限。 局域网具有如下主要优点： 具有广播功能，从一个站点可很方便地访问全网。局域网上的主机可共享连接在局域网上的各种硬件和软件资源。 便于系统的扩展和逐渐地演变，各设备的位置可灵活调整和改变。 提高了系统的可靠性、可用性和残存性。 数据链路层的两个子层:\n为了使数据链路层能更好地适应多种局域网标准，IEEE 802 委员会就将局域网的数据链路层拆成两个子层：\n逻辑链路控制 LLC (Logical Link Control)子层； 媒体接入控制 MAC (Medium Access Control)子层。 与接入到传输媒体有关的内容都放在 MAC子层，而 LLC 子层则与传输媒体无关。不管采用何种协议的局域网，对 LLC 子层来说都是透明的。\n基本概念 为什么要媒体接入控制（介质访问控制）？\n共享信道带来的问题\n若多个设备在共享信道上同时发送数据，则会造成彼此干扰，导致发送失败。\n随着技术的发展，交换技术的成熟和成本的降低，具有更高性能的使用点对点链路和链路层交换机的交换式局域网在有线领域已完全取代了共享式局域网，但由于无线信道的广播天性，无线局域网仍然使用的是共享媒体技术\n静态划分信道 信道复用\n频分复用FDM (Frequency Division Multiplexing)\n将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。\n频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。\n时分复用TDM (Time Division Multiplexing)\n时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 波分复用 WDM(Wavelength Division Multiplexing)\n波分复用就是光的频分复用，使用一根光纤来同时传输多个光载波信号\n光信号传输一段距离后悔衰减，所以要用 掺铒光纤放大器 放大光信号\n码分复用 CDM (Code Division Multiplexing)\n动态接入控制 受控接入\n受控接入在局域网中使用得较少，本书不再讨论\n随机接入\n重点\n随机接入（CSMA/CD协议） 总线局域网使用协议：CSMA/CD媒体接入控制协议\nCSMA/CD协议曾经用于各种总线结构以太网和双绞线以太网的早起版本中。\n现在的以太网基于交换机和全双工连接，不会有碰撞，因此没有必要使用CSMA/CD协议\n基本概念 最初的以太网是将许多计算机都连接到一根总线上。易于实现广播通信。当初认为这样的连接方法既简单又可靠，因为总线上没有有源器件。\n以太网（Ethernet）是一种计算机局域网技术。IEEE组织的IEEE 802.3标准制定了以太网（Ethernet）的技术标准\n以太网采用无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢弃，其他什么也不做。\n多址接入MA 表示许多主机以多点接入的方式连接在一根总线上。\n载波监听CS 是指每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞。以太网的mac帧每隔96比特时间就会传输一个帧，所以，监听的时候只需要监听96比特时间就知道当前信道是否空闲了。\n总线上并没有什么“载波”。因此， “载波监听”就是用电子技术检测总线上有没有其他计算机发送的数据信号。\n碰撞检测CD “碰撞检测”就是计算机边发送数据边检测信道上的信号电压大小。 当几个站同时在总线上发送数据时，总线上的信号电压摆动值将会增大（互相叠加）。 当一个站检测到的信号电压摆动值超过一定的门限值时，就认为总线上至少有两个站同时在发送数据，表明产生了碰撞。 所谓“碰撞”就是发生了冲突。因此“碰撞检测”也称为“冲突检测”。 在发生碰撞时，总线上传输的信号产生了严重的失真，无法从中恢复出有用的信息来。 每一个正在发送数据的站，一旦发现总线上出现了碰撞，就要立即停止发送，免得继续浪费网络资源，然后等待一段随机时间后再次发送。 为什么要进行碰撞检测？ 因为信号传播时延对载波监听产生了影响\nA 需要单程传播时延的 2 倍的时间，才能检测到与 B 的发送产生了冲突\nCSMA/CD 协议工作流程 CSMA/CD 协议工作——争用期（碰撞窗口） CSMA/CD 协议工作——最小帧长 CSMA/CD 协议工作——最大帧长 CSMA/CD 协议工作——截断二进制指数退避算法 该算法可以用来解决以太网数据发送和传输过程中，站点间的碰撞问题。该算法是在发生碰撞过后确定随机等待时间的时候起作用。随着重传次数增加，那么，再次等待的随机时间就会更长。\nCSMA/CD 协议工作——信道利用率 在理想情况下，以太网上的各个站点数据发送与传输不发生冲突，那么计算信道利用率时，就不用考虑争用期了。\nCSMA/CD 协议工作——帧接收流程 CSMA/CD 协议的重要特性 使用 CSMA/CD 协议的以太网不能进行全双工通信而只能进行双向交替通信（半双工通信）。 每个站在发送数据之后的一小段时间内，存在着遭遇碰撞的可能性。 这种发送的不确定性使整个以太网的平均通信量远小于以太网的最高数据率。 CSMA/CD协议曾经用于各种总线结构以太网和双绞线以太网的早起版本中。\n现在的以太网基于交换机和全双工连接，不会有碰撞，因此没有必要使用CSMA/CS协议\n随机接入（CSMA/CA协议） 无线局域网使用的协议：CSMA/CA\n为什么无线局域网要使用CSMA/CA协议 帧间间隔IFS（Inter Frame Space） CSMA/CA协议的工作原理 源站为什么在检测到信道空闲后还要再等待一段时间DIFS？\n考虑到可能有其他的站有高优先级的帧要发送。若有，就要让高优先级帧先发送 目的站为什么正确接收数据帧后还要等待一段时间SIFS才能发送ACK帧？\nSIFS是最短的帧间间隔，用来分隔开属于一次对话的各帧，在这段时间内，一个站点应当能够从发送方式切换到接收方式，以便做好接受ACK帧或其他帧的准备。 信道由忙转为空闲且经过DIFS时间后，其他站还要退避一段随机时间才能使用信道？\n防止多个站点同时发送数据而产生碰撞\n使用退避算法的时机\nCSMA/CA协议的退避算法 退避算法的示例\n当A正在发送数据时，B, C和D都有数据要发送(用向上的箭头表示)。由于这三个站都检测到信道忙，因此都要执行退避算法，各自随机退避一段时间再发送数据。802. I 1标准规定，退避时间必须是整数倍的时隙时间。前面己经讲过，第i次退避是在时隙{0, 1, \u0026hellip;, 2^2^+i}中随机地选择一个。这样做是为了使不同站点选择相同退避时间的概率减少。因此，第1次退避(i=1)要推迟发送的时间是在时隙{0, 1,…，7}中(共8个时隙)随机选择一个，而第2次退避是在时隙{0, 1,…，15}中(共16个时隙)随机选择一个。当时隙编号达到255时(这对应于第6次退避)就不再增加了。这半决定退避时间的变量，称为退避变量。\n退避时间选定后，就相当于设置了一个退避计时器(backoff timer)。站点每经历一个时隙的时间就检测一次信道。这可能发生两种情况:\n若检测到信道空闲，退避计时器就继续倒计时; 若检测到信道忙，就冻结退避计时器的剩余时间，重新等待信道变为空闲并再经过时间DIFS后，从剩余时间开始继续倒计时。如果退避计时器的时间减小到零时，就开始发送整个数据帧。 C的退避计时器最先减到零，于是C立即把整个数据帧发送出去。请注意，A发送完数据后信道就变为空闲。C的退避计时器一直在倒计时。当C在发送数据的过程中，B和D检测到信道忙，就冻结各自的退避计时器的数值，重新期待信道变为空闲。正在这时E也想发送数据。由于E检测到信道忙，因此E就执行退避算法和设置退避计时器。\n当C发送完数据并经过了时间DIFS后，B和D的退避计时器又从各自的剩余时间开始倒计时。现在争用信道的除B和D外，还有E、D的退避计时器最先减到零，于是D得到了发送权。在D发送数据时，B和E都冻结其退避计时器。\n以后E的退避计时器比B先减少到零。当E发送数据时，B再次冻结其退避计时器。等到E发送完数据并经过时间DIFS后，B的退避计时器才继续工作，一直到把最后剩余的时间用完，然后就发送数据。\n冻结退避计时器剩余时间的做法是为了使协议对所有站点更加公平。\nCSMA/CA协议的信道预约和虚拟载波监听 虚拟载波监听机制能减少隐蔽站带来的碰撞问题的示例\n3.7、MAC地址、IP地址以及ARP协议 MAC地址 使用点对点信道的数据链路层不需要使用地址 使用广播信道的数据链路层必须使用地址来区分各主机 广播信道的数据链路层必须使用地址（MAC） MAC地址又称为硬件地址或物理地址。请注意：不要被 “物理” 二字误导认为物理地址属于物理层范畴，物理地址属于数据链路层范畴\nIEEE 802局域网的MAC地址格式 组织唯一标识符OUI\n生产网络设备的厂商，需要向IEEE的注册管理机构申请一个或多个OUI 网络接口标识符\n由获得OUI的厂商自行随意分配 EUI-48\n48是这个MAC地址的位数 对于使用EUI-48空间的应用程序，IEEE的目标寿命为100年（直到2080年），但是鼓励采用EUI-64作为替代\n关于无效的 MAC 帧\n数据字段的长度与长度字段的值不一致； 帧的长度不是整数个字节； 用收到的帧检验序列 FCS 查出有差错； 数据字段的长度不在 46 ~ 1500 字节之间。局域网中，MAC帧会根据以太网协议进行传输，所以最少需要64个字节，然后减去非数据字段字节数得到46 有效的 MAC 帧长度为 64 ~ 1518 字节之间。 对于检查出的无效 MAC 帧就简单地丢弃。以太网不负责重传丢弃的帧。\nIEEE 802局域网的MAC地址发送顺序 单播MAC地址举例 主机B给主机C发送单播帧，主机B首先要构建该单播帧，在帧首部中的目的地址字段填入主机C的MAC地址，源地址字段填入自己的MAC地址，再加上帧首部的其他字段、数据载荷以及帧尾部，就构成了该单播帧\n主机B将该单播帧发送出去，主机A和C都会收到该单播帧\n主机A的网卡发现该单播帧的目的MAC地址与自己的MAC地址不匹配，丢弃该帧\n主机C的网卡发现该单播帧的目的MAC地址与自己的MAC地址匹配，接受该帧\n并将该帧交给其上层处理\n广播MAC地址举例 假设主机B要发送一个广播帧，主机B首先要构建该广播帧，在帧首部中的目的地址字段填入广播地址，也就是十六进制的全F，源地址字段填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该广播帧\n主机B讲该广播帧发送出去，主机A和C都会收到该广播帧，发现该帧首部中的目的地址字段的内容是广播地址，就知道该帧是广播帧，主机A和主机C都接受该帧，并将该帧交给上层处理\n多播MAC地址举例 假设主机A要发送多播帧给该多播地址。将该多播地址的左起第一个字节写成8个比特，第一个字节的最低比特位是1，这就表明该地址是多播地址。\n快速判断地址是不是多播地址，就是上图所示箭头所指的第十六进制数不能整除2（1,3,5,7,9,B,D,F），则该地址是多播地址。理解：最低位为1表明一定是个奇数。\n假设主机B，C和D支持多播，各用户给自己的主机配置多播组列表如下所示\n主机B属于两个多播组，主机C也属于两个多播组，而主机D不属于任何多播组\n主机A首先要构建该多播帧，在帧首部中的目的地址字段填入该多播地址，源地址点填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该多播帧\n主机A将该多播帧发送出去，主机B、C、D都会收到该多播帧\n主机B和C发现该多播帧的目的MAC地址在自己的多播组列表中，主机B和C都会接受该帧\n主机D发现该多播帧的目的MAC地址不在自己的多播组列表中，则丢弃该多播帧\n给主机配置多播组列表进行私有应用时，不得使用公有的标准多播地址\nIP地址 IP地址属于网络层的范畴，不属于数据链路层的范畴\n下面内容讲的是IP地址的使用，详细的IP地址内容在网络层中介绍\n基本概念 从网络体系结构看IP地址与MAC地址 数据包转发过程中IP地址与MAC地址的变化情况 图上各主机和路由器各接口的IP地址和MAC地址用简单的标识符来表示\n如何从IP地址找出其对应的MAC地址？\nARP协议\nARP协议 如何从IP地址找出其对应的MAC地址？\nARP（地址解析协议）\n流程 ARP高速缓存表\n当主机B要给主机C发送数据包时，会首先在自己的ARP高速缓存表中查找主机C的IP地址所对应的MAC地址，但未找到，因此，主机B需要发送ARP请求报文，来获取主机C的MAC地址\nARP请求报文有具体的格式，上图的只是简单描述\nARP请求报文被封装在MAC帧中发送，目的地址为广播地址\n主机B发送封装有ARP请求报文的广播帧，总线上的其他主机都能收到该广播帧\n收到ARP请求报文的主机A和主机C会把ARP请求报文交给上层的ARP进程\n主机A发现所询问的IP地址不是自己的IP地址，因此不用理会\n主机C的发现所询问的IP地址是自己的IP地址，需要进行相应\n动态与静态的区别\nARP协议只能在一段链路或一个网络上使用，而不能跨网络使用\nARP协议的使用是逐段链路进行的\n总结 ARP表中的IP地址与MAC地址的对应关系记录，是会定期自动删除的，因为IP地址与MAC地址的对应关系不是永久性的\n3.8、集线器与交换机的区别 集线器-在物理层扩展以太网 概念 传统以太网最初是使用粗同轴电缆，后来演进到使用比较便宜的细同轴电缆，最后发展为使用更便宜和更灵活的双绞线。 采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备，叫做集线器 (hub)。 集线器是也可以看做多口中继器，每个端口都可以成为一个中继器，中继器是对减弱的信号进行放大和发送的设备 集线器的以太网在逻辑上仍是个总线网，需要使用CSMA/CD协议来协调各主机争用总线，只能工作在半双工模式，收发帧不能同时进行 集线器HUB在物理层扩展以太网 使用集线器扩展：将多个以太网段连成更大的、多级星形结构的以太网\n优点\n使原来属于不同碰撞域的以太网上的计算机能够进行跨碰撞域的通信。 扩大了以太网覆盖的地理范围。 缺点\n碰撞域增大了，但总的吞吐量并未提高，反而降低。\n假设两个主机进行数据交换，那么在整个扩展以太网下的所有主机都会收到数据，所以那么其他主机发送数据或传输数据就会发生冲突。显然，这个冲突的可能性反而提高了\n如果不同的碰撞域使用不同的数据率，那么就不能用集线器将它们互连起来。\n碰撞域\n碰撞域（collision domain）又称为冲突域，是指网络中一个站点发出的帧会与其他站点发出的帧产生碰撞或冲突的那部分网络。\n例如：几个主机接到一个集线器上，形成星型拓扑结构，那么一个主机给另外一个主机进行数据交换时，其他主机都能收到数据，因此，其他主机不能发送再发送数据了，否则就会冲突。\n所以，这也是集线器的最大问题，容易导致“广播风暴”。\n碰撞域越大，发生碰撞的概率越高。\n为了解决集线器组装时，冲突越来越大的问题，后面又出现了网桥和交换机来对以太网进行扩展的技术\n以太网交换机和网桥-在数据链路层扩展以太网 概念 扩展以太网更常用的方法是在数据链路层进行。 早期使用网桥，现在使用以太网交换机。 网桥\n网桥工作在数据链路层。 它根据 MAC 帧的目的地址对收到的帧进行转发和过滤。 当网桥收到一个帧时，并不是向所有的接口转发此帧，而是先检查此帧的目的MAC 地址，然后再确定将该帧转发到哪一个接口，或把它丢弃。 交换机\n1990 年问世的交换式集线器 (switching hub) 可明显地提高以太网的性能。 交换式集线器常称为以太网交换机 (switch) 或第二层交换机 (L2 switch)，强调这种交换机工作在数据链路层。 以太网交换机实质上就是一个多接口的网桥 网桥和交换机用户分割冲突域，就是网桥和交换机可以较少被逼的广播(hub导致的)，但不能分割广播域。不严格地说，交换机可以看作网桥的高度集成。\n集线器HUB与交换机SWITCH区别 使用集线器互连而成的共享总线式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧会通过共享总线传输到总线上的其他各个主机\n使用交换机互连而成的交换式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧进入交换机后，交换机会将该单播帧转发给目的主机，而不是网络中的其他各个主机\n这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了\n以太网交换机的交换方式\n存储转发方式 把整个数据帧先缓存后再进行处理。 直通 (cut-through) 方式 接收数据帧的同时就立即按数据帧的目的 MAC 地址决定该帧的转发接口，因而提高了帧的转发速度。 缺点是它不检查差错就直接将帧转发出去，因此有可能也将一些无效帧转发给其他的站。 这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了\n对比集线器和交换机\n多台主机同时给另一台主机发送单播帧\n集线器以太网：会产生碰撞，遭遇碰撞的帧会传播到总线上的各主机\n交换机以太网：会将它们缓存起来，然后逐个转发给目的主机，不会产生碰撞\n这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了\n集线器扩展以太网和交换机扩展以太网区别\n单播\n广播\n多个单播\n广播域（broadcast domain）：指这样一部分网络，其中任何一台设备发出的广播通信都能被该部分网络中的所有其他设备所接收。\n总结 工作在数据链路层的以太网交换机，其性能远远超过工作在物理层的集线器，而且价格并不贵，这就使得集线器逐渐被市场淘汰\n3.9、以太网交换机自学习和转发帧的流程 概念 自学习和转发帧的例子 以下例子假设各主机知道网络中其他各主机的MAC地址（无需进行ARP）\nA -\u0026gt; B\nA 先向 B 发送一帧。该帧从接口 1 进入到交换机 交换机收到帧后，先查找（图中左边）交换表。没有查到应从哪个接口转发这个帧给 B 交换机把这个帧的源地址 A 和接口 1 写入（图中左边）交换表中 交换机向除接口 1 以外的所有的接口广播这个帧 接口 4到接口 2，先查找（图中右边）交换表。没有查到应从哪个接口转发这个帧给 B 交换机把这个帧的源地址 A 和接口 2 写入（图中右边）交换表中 除B主机之外与该帧的目的地址不相符，将丢弃该帧 主机B发现是给自己的帧，接受该帧 可以看到在这个过程中，没有找到目标MAC地址对应的接口时，就将源MAC地址和源主机接接口号记录在表中，之后再将帧进行广播，如果目的MAC地址与源主机相通，则可以将帧接收到。故此，这个过程学习到了源主机MAC地址和接口。\nB -\u0026gt; A\nB 向 A 发送一帧。该帧从接口 3 进入到交换机 交换机收到帧后，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。 主机 A 发现目的地址是它，就接受该帧 交换机把这个帧的源地址 B 和接口 3 写入（图中左边）交换表中 如果交换机的帧交换表中含有目的主机MAC地址对应的接口号，那么就不需要进行广播了，直接就将帧传给指定接口。当然，源主机MAC和接口地址没有再帧交换表时，也应该学习到。\nE -\u0026gt; A\nE 向 A发送一帧 交换机收到帧后，先查找（图中右边）交换表。发现（图中右边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口2转发出去。于是就把这个帧传送到接口 2 转发给 接口 4。 交换机把这个帧的源地址 E 和接口 3 写入（图中右边）交换表中 接口 4 到 左边的交换机，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。 交换机把这个帧的源地址 E 和接口 4 写入（图中左边）交换表中 主机 A 发现目的地址是它，就接受该帧 这个过程主要是在帧的传播过程中，学习到接口和目的源主机MAC地址的关系。\nG -\u0026gt; A\n主机 A、主机 G、交换机 1的接口 1就共享同一条总线（相当于总线式网络，可以想象成用集线器连接了）\n主机 G 发送给 主机 A 一个帧 主机 A 和 交换机接口 1都能接收到 主机 A 的网卡收到后，根据帧的目的MAC地址A，就知道是发送给自己的帧，就接受该帧 交换机 1收到该帧后，首先进行登记工作 然后交换机 1对该帧进行转发，该帧的MAC地址是A，在（图中左边）交换表查找MAC 地址有 A MAC 地址为 A的接口号是1，但是该帧正是从接口 1 进入交换机的，交换机不会再从该接口 1 将帧转发出去，因为这是没有必要，于是丢弃该帧 随着网络中各主机都发送了帧后，网络中的各交换机就可以学习到各主机的MAC地址，以及它们与自己各接口的对应关系\n考虑到可能有时要在交换机的接口更换主机，或者主机要更换其网络适配器，这就需要更改交换表中的项目。为此，在交换表中每个项目都设有一定的有效时间。过期的项目就自动被删除。\n以太网交换机的这种自学习方法使得以太网交换机能够即插即用，不必人工进行配置，因此非常方便。\n总结 交换机自学习和转发帧的步骤归纳\n3.10、以太网交换机的生成树协议STP 如何提高以太网的可靠性 生成树协议STP 为什么要用STP协议？\n为了解决第二层网络环路问题而又要保证网络的稳定和健壮性，引入了链路动态管理的策略。\n首先通过阻塞某些链路避免环路的产生（避免环路） 再次当正常工作的链路由于故障断开时，阻塞的链路立刻激活，迅速取代故障链路的位置，保证网络的正常运行。 IEEE 802.1D 标准制定了一个生成树协议 STP (Spanning Tree Protocol)。\n其要点是：不改变网络的实际拓扑，但在逻辑上则切断某些链路，使得从一台主机到所有其他主机的路径是无环路的树状结构，从而消除了兜圈子现象。\nSTP中根桥，根端口，指定端口的选举规则 https://developer.aliyun.com/article/911748\n3.11、虚拟局域网VLAN LAN与VLAN LAN 表示 Local Area Network，本地局域网。 一个 LAN 表示一个广播域，含义是：LAN 中的所有成员都会收到任意一个成员发出的广播包。\n上图为最基本的LAN布局。如果设备间想要通讯，必须要获取到对方的MAC地址。\n举例：A 发信息给 C，A 并不知道 C 的 MAC 地址。此时通过 ARP 协议（Address Resolution Protocol；地址解析协议；）获取 C 的 MAC 地址，A 先要广播一个包含目标 IP 地址的 ARP 请求到链接在集线器上的所有设备上，C 接受到广播后返回 MAC 地址给 A，其他设备则丢弃信息。至此已经建立设备间通信的准备条件。\n虚拟局域网（VLAN）是在局域网（LAN）的逻辑上划分成多个广播域，每一个广播域就是一个 VLAN。\n下图为交换机划分虚拟局域网。交换机把一个广播域划分成了3个广播域，物理上这些设备在一个交换机上，但是逻辑上已经分别划分到三个交换机上，所以会有三个局域网（虚拟局域网），三个广播域。\n为什么要虚拟局域网VLAN 广播风暴\n分割广播域的方法\n为了分割广播域，所以虚拟局域网VLAN技术应运而生\n概念 利用以太网交换机可以很方便地实现虚拟局域网 VLAN (Virtual LAN)。 IEEE 802.1Q 对虚拟局域网 VLAN 的定义： 虚拟局域网 VLAN 是由一些局域网网段构成的与物理位置无关的逻辑组，而这些网段具有某些共同的需求。每一个 VLAN 的帧都有一个明确的标识符，指明发送这个帧的计算机是属于哪一个 VLAN。 同一个VLAN内部可以广播通信，不同VLAN不可以广播通信 虚拟局域网其实只是局域网给用户提供的一种服务，而并不是一种新型局域网。 由于虚拟局域网是用户和网络资源的逻辑组合，因此可按照需要将有关设备和资源非常方便地重新组合，使用户从不同的服务器或数据库中存取所需的资源。 虚拟局域网VLAN的实现机制 虚拟局域网VLAN技术是在交换机上实现的，需要交换机能够实现以下功能\n能够处理带有VLAN标记的帧——IEEE 802.1 Q帧 交换机的各端口可以支持不同的端口类型，不同端口类型的端口对帧的处理方式有所不同 Access端口\n交换机与用户计算机之间的互连\n同一个VLAN内部可以广播通信，不同VLAN不可以广播通信。\n简单来说就是：VLAN相较于LAN的实现上，就是在广播以太网MAC帧的时候，再在这个帧上添加一个4字节的VLAN标记，指明只有满足PVID==VID条件才能将广播的以太网MAC帧发送到相同VLAN的交换机的access类型端口连着的主机；其他不满足PVID==VID的条件的主机也就是不在一个VLAN下，自然就不能接收到帧。\nTruck端口\n交换机之间 或 交换机与路由器之间的互连\n小例题\n华为交换机私有的Hybrid端口类型\n总结 虚拟局域网优点\n虚拟局域网（VLAN）技术具有以下主要优点：\n改善了性能 简化了管理 降低了成本 改善了安全性 ","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/","tags":["ppp协议","以太网协议","arp协议","虚拟局域网vlan","可靠传输协议"],"title":"计算机网络之数据链路层"},{"categories":["计算机网络"],"contents":"[toc]\n2.1、物理层的基本概念 2.2、物理层下面的传输媒体 传输媒体也称为传输介质或传输媒介，他就是数据传输系统中在发送器和接收器之间的物理通路。传输媒体可分为两大类，即导引型传输媒体和非导引型传输媒体\n传输媒体不属于计算机网络体系结构的任何一层。如果非要将它添加到体系结构中，那只能将其放置到物理层之下。\n导引型传输媒体 在导引型传输媒体中，电磁波被导引沿着固体媒体传播。\n同轴电缆 双绞线 光纤 多模光纤\n可以存在多条不同角度入射的光线在一条光纤中传输。这种光纤就称为多模光纤。 单模光纤\n若光纤的直径减小到只有一个光的波长，则光纤就像一根波导那样，它可使光线一直向前传播，而不会产生多次反射。这样的光纤称为单模光纤。 电力线 非导引型传输媒体 非导引型传输媒体是指自由空间。\n无线电波 微波 红外线 可见光 LIFI\n2.3、传输方式 串行传输和并行传输 串行传输：\n数据是一个比特一个比特依次发送的，因此在发送端与接收端之间，只需要一条数据传输线路即可 并行传输：\n一次发送n个比特，因此，在发送端和接收端之间需要有n条传输线路\n并行传输的优点是比串行传输的速度n倍，但成本高\n数据在传输线路上的传输采用是串行传输，计算机内部的数据传输常用并行传输\n计算机内部往往采用一次性传输一个字长的数据。例如64位计算机，可以一次性传输64位数据。\n同步传输和异步传输 同步传输：\n数据块以稳定的比特流的形式传输。字节之间没有间隔 接收端在每个比特信号的中间时刻进行检测，以判别接收到的是比特0还是比特1 由于不同设备的时钟频率存在一定差异，不可能做到完全相同，在传输大量数据的过程中，所产生的判别时刻的累计误差，会导致接收端对比特信号的判别错位 所以要使收发双发时钟保持同步\n异步传输：\n以字节为独立的传输单位，字节之间的时间间隔不是固定 接收端仅在每个字节的起始处对字节内的比特实现同步 通常在每个字节前后分别加上起始位和结束位 单向通信（单工）、双向交替通信（半双工）和双向同时通信（全双工） 在许多情况下，我们要使用“信道（channel）”这一名词。信道和电路并不等同。\n信道一般都是用来表示向某一个方向传送信息的媒体。因此，一条通信电路往往包含一条发送信道和一条接收信道。\n从通信的双方信息交互的方式来看，可以有以下三种基本方式：\n单向通信：\n又称为单工通信，即只能有一个方向的通信而没有反方向的交互。无线电广播或有线电以及电视广播就属于这种类型\n双向交替通信：\n又称为半双工通信，即通信的双方可以发送信息，但不能双方同时发送（当然也就不能同时接收）。这种通信方式使一方发送另一方接收，过一段时间后可以再反过来\n双向同时通信：\n又称为全双工通信，即通信的双发可以同时发送和接收信息。\n单向通信只需要一条信道，而双向交替通信或双向同时通信则需要两条信道（每个方向各一条）\n双向同时通信的传输效率最高\n2.4、编码与调制 常用术语\n数据 (data) —— 运送消息的实体。\n信号 (signal) —— 数据的电气的或电磁的表现。\n模拟信号 (analogous signal) —— 代表消息的参数的取值是连续的。\n数字信号 (digital signal) —— 代表消息的参数的取值是离散的。\n码元 (code) —— 在使用时间域（或简称为时域）的波形表示数字信号时，代表不同离散数值的基本波形。\n基带信号（即基本频带信号）—— 来自信源的信号。像计算机输出的代表各种文字或图像文件的数据信号都属于基带信号。基带信号就是发出的直接表达了要传输信息的信号。\n带通信号——把基带信号经过载波调制后，把信号的频率范围搬移到较高的频段以便在信道中传输。\n因此在传输距离较近时，基带信号衰减不大，所以传输距离较近时的计算机网络都采用基带传输方式。例如：计算机到监视器、打印机，我们直接以数据线相连，无需经过调制解调器。\n但是，如果距离较远时，显然就需要调制解调器了，也就是采用带通信号。\n在计算机网络中，常见的是将数字基带信号通过编码或调制的方法在相应信道进行传输\n传输媒体与信道的关系 信道的几个基本概念\n信道 —— 一般用来表示向某一个方向传送信息的媒体。 单向通信（单工通信）——只能有一个方向的通信而没有反方向的交互。 双向交替通信（半双工通信）——通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。 双向同时通信（全双工通信）——通信的双方可以同时发送和接收信息。 严格来说，传输媒体不能和信道划等号\n对于单工传输，传输媒体只包含一个信道，要么是发送信道，要么是接收信道\n对于半双工和全双工，传输媒体中要包含两个信道，一个发送信道，另一个是接收信道\n如果使用信道复用技术，一条传输媒体还可以包含多个信道\n常用编码 不归零编码\n正电平表示比特1/0\n负电平表示比特0/1\n中间的虚线是零电平，所谓不归零编码，就是指在整个码元时间内，电平不会出现零电平\n实际比特1和比特0的表示要看现实怎么规定\n这需要发送方的发送与接收方的接收做到严格的同步\n需要额外一根传输线来传输时钟信号，使发送方和接收方同步，接收方按时钟信号的节拍来逐个接收码元 但是对于计算机网络，宁愿利用这根传输线传输数据信号，而不是传输时钟信号 由于不归零编码存在同步问题，因此计算机网络中的数据传输不采用这类编码！\n归零编码\n归零编码虽然自同步，但编码效率低\n曼彻斯特编码\n在每个码元时间的中间时刻，信号都会发生跳变\n负跳变表示比特1/0 正跳变表示比特0/1 码元中间时刻的跳变即表示时钟，又表示数据 实际比特1和比特0的表示要看现实怎么规定\n传统以太网使用的就是曼切斯特编码\n差分曼彻斯特编码\n在每个码元时间的中间时刻，信号都会发送跳变，但与曼彻斯特不同\n跳变仅表示时钟 码元开始处电平是否变换表示数据 变化表示比特1/0 不变化表示比特0/1 实际比特1和比特0的表示要看现实怎么规定\n比曼彻斯特编码变化少，更适合较高的传输速率\n总结\n调制 数字信号转换为模拟信号，在模拟信道中传输，例如WiFi，采用补码键控CCK/直接序列扩频DSSS/正交频分复用OFDM等调制方式。\n模拟信号转换为另一种模拟信号，在模拟信道中传输，例如，语音数据加载到模拟的载波信号中传输。频分复用FDM技术，充分利用带宽资源。\n基本调制方法\n调幅AM：所调制的信号由两种不同振幅的基本波形构成。每个基本波形只能表示1比特信息量。 调频FM：所调制的信号由两种不同频率的基本波形构成。每个基本波形只能表示1比特信息量。 调相PM：所调制的信号由两种不同初相位的基本波形构成。每个基本波形只能表示1比特信息量。 但是使用基本调制方法，1个码元只能包含1个比特信息\n混合调制\n上图码元所对应的4个比特是错误的，码元不能随便对应4个比特\n码元 在使用时间域的波形表示数字信号时，代表不同离散数值的基本波形。\n2.5、信道的极限容量 任何实际的信道都不是理想的，在传输信号时会产生各种失真以及带来多种干扰。 码元传输的速率越高，或信号传输的距离越远，或传输媒体质量越差，在信道的输出端的波形的失真就越严重。 失真的原因：\n码元传输的速率越高 信号传输的距离越远 噪声干扰越大 传输媒体质量越差 奈氏准则 香农公式 对比 补充：信道复用技术 本节内容视频未讲到，是《计算机网络（第7版）谢希仁》物理层的内容\n频分复用、时分复用和统计时分复用 复用 (multiplexing) 是通信技术中的基本概念。\n它允许用户使用一个共享信道进行通信，降低成本，提高利用率。\n频分复用 FDM (Frequency Division Multiplexing)\n将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。 频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。 时分复用TDM (Time Division Multiplexing)\n时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 时分复用可能会造成线路资源的浪费 使用时分复用系统传送计算机数据时，由于计算机数据的突发性质，用户对分配到的子信道的利用率一般是不高的。 统计时分复用 STDM (Statistic TDM)\n波分复用 波分复用 WDM(Wavelength Division Multiplexing)\n码分复用 码分复用 CDM (Code Division Multiplexing)\n常用的名词是码分多址 CDMA (Code Division Multiple Access)。 各用户使用经过特殊挑选的不同码型，因此彼此不会造成干扰。 这种系统发送的信号有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现。 ","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/","tags":[],"title":"计算机网络之物理层"},{"categories":["计算机网络"],"contents":"[toc]\n图示说明 代表着主机\n代表服务器\n代表着路由器\n代表着网络\n1.1、计算机网络在信息时代的作用 计算机网络已由一种通信基础设施发展成为一种重要的信息服务基础设施 计算机网络已经像水，电，煤气这些基础设施一样，成为我们生活中不可或缺的一部分 数字化 将现实世界的信息存储到计算机中，也就是01二进制来表示所有的现实世界信息。例如，我们可以将个人的身份信息存放为计算机中的一堆二进制位。\n数字化的目的是将现实世界中的事物转化为数字化的形式，以便于计算机和互联网的处理和传输。\n网络化 网络化中有三网：电信网络、计算机网络、有线电视网络。\n电信网络 电信网络是指通过电信技术实现的通信网络，它包括了固定电话网络、移动通信网络、互联网等。\n电信网络通过无线电波、光纤、铜线等传输介质，将声音、图像、数据等信息传输到用户之间。电信网络的优点是覆盖面广、传输速度快、信息容量大、可靠性高等。\n在现代社会中，电信网络已经成为人们生活、工作、学习和娱乐中不可或缺的一部分。\n计算机网络 计算机网络是指将多台计算机通过通信设备连接起来，以便它们之间可以进行数据交换和资源共享的网络系统。\n计算机网络可以通过有线或无线的方式连接计算机，使得计算机之间可以相互通信，共享数据和设备。计算机网络的目的是实现信息的共享和传递，提高计算机资源的利用率，同时也可以提高工作效率和降低成本。\n计算机网络的应用非常广泛，包括互联网（因特网）、局域网、广域网、数据中心网络等。\n有线电视网络 有线电视网络是一种通过有线电缆传输电视信号的网络。它使用同轴电缆、光纤或双绞线等传输介质，将电视信号传输到用户家中的电视机上。\n有线电视网络通常提供更多的电视频道和更高的画质，同时也可以提供互联网和电话服务。\n优点：信号稳定、画质高、可靠性强，但需要铺设大量的电缆，成本较高。\n信息化 利用信息技术和通信技术来改善生产、管理和服务等方面的工作，使得信息的获取、处理、传递和利用更加高效和便捷。\n数字化是信息化的基础和前提，而信息化则是数字化的高级应用和发展。\n我国互联网发展状况 1.2、因特网概述 1、网络、互连网（互联网）和因特网 网络：网络（Network）由若干**结点（Node）和连接这些结点的链路（Link）**组成。\n互连网（互联网）：多个网络通过路由器互连起来，这样就构成了一个覆盖范围更大的网络，即互连网（互联网）。因此，互联网又称为“网络的网络（Network of Networks）”。\n因特网：因特网（Internet）是世界上最大的互连网络（用户数以亿计，互连的网络数以百万计）。\ninternet与Internet的区别\ninternet(互联网或互连网)是一个通用名词，它泛指多个计算机网络互连而成的网络。在这些网络之间的通信协议可以是任意的。 Internet（因特网）则是一个专用名词，它指当前全球最大的、开放的、由众多网络互连而成的特定计算机网络，它采用TCP/IP协议族作为通信的规则，其前身是美国的ARPANET。 任意把几个计算机网络互连起来（不管采用什么协议），并能够相互通信，这样构成的是一个互连网(internet) ，而不是互联网(Internet)。\n2、因特网发展的三个阶段 因特网服务提供者ISP(Internet Service Provider)\n普通用户是如何接入到因特网的呢？\n答：通过ISP接入因特网\nISP可以从因特网管理机构申请到成块的IP地址，同时拥有通信线路以及路由器等联网设备。任何机构和个人只需缴纳费用，就可从ISP的得到所需要的IP地址。\n因为因特网上的主机都必须有IP地址才能进行通信，这样就可以通过该ISP接入到因特网\n中国的三大ISP：中国电信，中国联通和中国移动\n基于ISP的三层结构的因特网\n一旦某个用户能够接入到因特网，那么他也可以成为一个ISP，所需要做的就是购买一些如调制解调器或路由器这样的设备，让其他用户可以和他相连。\n3、因特网的标准化工作 因特网的标准化工作对因特网的发展起到了非常重要的作用。 因特网在指定其标准上的一个很大的特点是面向公众。 因特网所有的RFC(Request For Comments)技术文档都可从因特网上免费下载； 任何人都可以随时用电子邮件发表对某个文档的意见或建议。 因特网协会ISOC是一个国际性组织，它负责对因特网进行全面管理，以及在世界范围内促进其发展和使用。 因特网体系结构委员会IAB，负责管理因特网有关协议的开发； 因特网工程部IETF，负责研究中短期工程问题，主要针对协议的开发和标准化； 因特网研究部IRTF，从事理论方面的研究和开发一些需要长期考虑的问题。 制订因特网的正式标准要经过一下4个阶段：\n1、因特网草案（在这个阶段还不是RFC文档）\n2、建议标准（从这个阶段开始就成为RFC文档）\n3、草案标准\n4、因特网标准\n4、因特网的组成 边缘部分\n由所有连接在因特网上的主机组成（台式电脑，大型服务器，笔记本电脑，平板，智能手机等）。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。\n核心部分\n由大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。\n路由器是一种专用计算机，但我们不称它为主机，路由器是实现分组交换的关键构建，其任务是转发收到的分组，这是网络核心最重要的部分。\n处在互联网边缘的部分就是连接在互联网上的所有的主机，这些主机又称为端系统 (end system)。\n端系统在功能上可能有很大的差别：\n小的端系统可以是一台普通个人电脑，具有上网功能的智能手机，甚至是一个很小的网络摄像头。\n大的端系统则可以是一台非常昂贵的大型计算机。\n端系统的拥有者可以是个人，也可以是单位（如学校、企业、政府机关等），当然也可以是某个ISP。\n补充： 端系统之间通信的含义\n“主机 A 和主机 B 进行通信”实际上是指：运行在主机 A 上的某个程序和运行在主机 B 上的另一个程序进行通信。即“主机 A 的某个进程和主机 B 上的另一个进程进行通信”。简称为“计算机之间通信”。\n端系统之间的通信方式通常可划分为两大类：\n客户-服务器方式：\n客户 (client) 和服务器 (server) 都是指通信中所涉及的两个应用进程。 客户 - 服务器方式所描述的是进程之间服务和被服务的关系。 客户是服务的请求方，服务器是服务的提供方。 服务请求方和服务提供方都要使用网络核心部分所提供的服务。\n对等连接方式：\n对等连接 (peer-to-peer，简写为 P2P ) 是指两个主机在通信时并不区分哪一个是服务请求方还是服务提供方。 只要两个主机都运行了对等连接软件 ( P2P 软件) ，它们就可以进行平等的、对等连接通信。 双方都可以下载对方已经存储在硬盘中的共享文档。 1.3 三种交换方式 网络核心部分是互联网中最复杂的部分。\n网络中的核心部分要向网络边缘中的大量主机提供连通性，使边缘部分中的任何一个主机都能够向其他主机通信（即传送或接收各种形式的数据）。\n数据是怎么在网络中进行交换的呢？数据交换的方式有三种：\n电路交换 分组交换 报文交换 1、电路交换（Circuit Switching） 传统两两相连的方式，当电话数量很多时，电话线也很多，就很不方便\n所以要使得每一部电话能够很方便地和另一部电话进行通信，就应该使用一个中间设备将这些电话连接起来，这个中间设备就是电话交换机\n电话交换机接通电话线的方式称为电路交换； 从通信资源的分配角度来看，交换（Switching）就是按照某种方式动态地分配传输线路的资源； 电路交换的三个步骤： 建立连接：分配通信资源 通话：一直占用通信资源，也就是那些线路资源。如果有其他连接请求则会等当前连接释放。 释放连接：归还通信资源 当使用电路交换来传送计算机数据时，其线路的传输效率往往很低。这是因为计算机数据是突发式地出现在传输线路上的，而传输路线可能已经被上一个电路交换连接占有着，那么这个线路就不能被其他连接所使用。\n所以计算机通常采用的是分组交换，而不是电路交换\n优点：\n特别适合大量数据的实时交换，例如打电话、看直播等 缺点：\n缺点也很明显。一旦建立连接后就会一直占有通信资源，使得被占用的那些线路无法被其他连接使用，只能等当前连接释放 2、分组交换（Packet Switching） 在网络核心部分起特殊作用的是路由器(router)。路由器是实现分组交换 (packet switching) 的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。\n通常我们把表示该消息的整块数据成为一个报文。\n在发送报文之前，先把较长的报文划分成一个个更小的等长数据段，在每一个数据段前面。加上一些由必要的控制信息组成的首部后，就构成一个分组，也可简称为“包”，相应地，首部也可称为“包头”。\n首部包含了分组的目的地址\n分组从源主机到目的主机，可走不同的路径。\n发送方\n构造分组 发送分组 路由器\n缓存分组 转发分组 简称为“分组转发” 在路由器中的输入和输出端口之间没有直接连线。\n路由器处理分组的过程是：\n把收到的分组先放入缓存（暂时存储）； 查找转发表，找出到某个目的地址应从哪个端口转发； 把分组送到适当的端口转发出去。 接收方\n接收分组 还原报文 优点：\n相较于电路交换、报文交换而言，分组交换类似于多级流水线CPU，可以大幅提高数据交换的效率。 现代计算机网络的数据交换方式就是采用分组交换方式 缺点：\n分组交换可能会出现时延，阻塞的情况 3、报文交换（Message Switching） 报文交换中的交换结点也采用存储转发方式，但报文交换对报文的大小没有限制，这就要求交换结点需要较大的缓存空间。报文交换主要用于早期的电报通信网，现在较少使用，通常被较先进的分组交换方式所取代。\n三种交换方式的对比 假设A，B，C，D是分组传输路径所要经过的4个结点交换机，纵坐标为时间\n电路交换：\n通信之前首先要建立连接；连接建立好之后，就可以使用已建立好的连接进行数据传送；数据传送后，需释放连接，以归还之前建立连接所占用的通信线路资源。\n一旦建立连接，中间的各结点交换机就是直通形式的，比特流可以直达终点；\n一旦连接建立，所占用的线路资源不可以被其他连接所复用，只有等待当前连接资源的释放。\n报文交换：\n可以随时发送报文，而不需要事先建立连接；整个报文先传送到相邻结点交换机，全部存储下来后进行查表转发，转发到下一个结点交换机。 整个报文需要在各结点交换机上进行存储转发，由于不限制报文大小，因此需要各结点交换机都具有较大的缓存空间。 很少使用这种数据交换方式了 分组交换：\n可以随时发送分组，而不需要事先建立连接。构成原始报文的一个个分组，依次在各结点交换机上存储转发。各结点交换机在发送分组的同时，还缓存接收到的分组。 构成原始报文的一个个分组，在各结点交换机上进行存储转发，相比报文交换，减少了转发时延，还可以避免过长的报文长时间占用链路，同时也有利于进行差错控制。 某个节点转发完一个分组数据包到下一个节点时，又可以继续转发下一个分组数据包，同时下一个节点也可以转发收到分组数据包。类似于CPU多级流水线原理。 1.4 计算机网络的定义和分类 定义 计算机网络的精确定义并未统一 计算机网络的最简单的定义是：一些互相连接的、自治的计算机的集合。 互连：是指计算机之间可以通过有线或无线的方式进行数据通信； 自治：是指独立的计算机，他有自己的硬件和软件，可以单独运行使用； 集合：是指至少需要两台计算机； 计算机网络的较好的定义是：计算机网络主要是由一些通用的，可编程的硬件（一定包含有中央处理机CPU）互连而成的，而这些硬件并非专门用来实现某一特定目的（例如，传送数据或视频信号）。这些可编程的硬件能够用来传送多种不同类型的数据，并能支持广泛的和日益增长的应用。 计算机网络所连接的硬件，并不限于一般的计算机，而是包括了智能手机等智能硬件。 计算机网络并非专门用来传送数据，而是能够支持很多种的应用（包括今后可能出现的各种应用）。 分类 按交换技术分类：\n电路交换网络 报文交换网络 分组交换网络 按使用者分类：\n公用网 专用网 按传输介质分类：\n有线网络 无线网络 按覆盖范围分类：\n广域网WAN（Wide Area Network） 作用范围通常为几十到几千公里，因而有时也称为远程网（long haul network）。广域网是互联网的核心部分，其任务是通过长距离（例如跨越不同的国家）运送主机所发送的数据。\n城域网MAN 作用范围一般是一个城市，可跨越几个街区甚至整个城市\n局域网LAN 一般用微型计算机或工作站通过高速通信线路相连（速率通常在 10 Mbit/s 以上），但地理上作用范围较小（1 km 左右）\n个域网PAN 就是在个人工作的地方把个人使用的电子设备用无线技术连接起来的网络。\n以上的作用范围或距离的含义指的是：实际数据网络传输距离。\n例如：A户和B户主机，距离50m，但是两个通过互联网或者任何广域网技术进行相互地资源访问。那么显然，这属于广域网。但是如果A户和B户通过自己购买交换机搭建网络线路和路由，使得两户互通，然后通过这个线路进行资源访问，那么这就是局域网。\n按拓扑结构分类：\n总线型网络 星型网络 环形网络 网状型网络 1.5 计算机网络的性能指标 速率 带宽 吞吐量 带宽1 Gb/s的以太网，代表其额定速率是1 Gb/s，这个数值也是该以太网的吞吐量的绝对上限值。因此，对于带宽1 Gb/s的以太网，可能实际吞吐量只有 700 Mb/s，甚至更低。\n注意：吞吐量还可以用每秒传送的字节数或帧数表示\n时延 时延时指数据（一个报文或分组，甚至比特）从网络（或链路）的一端传送到另一端所需的时间。\n网络时延由几部分组成：\n发送时延 主机或路由器发送数据帧所需要的时间，也就是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需的时间。\n传播时延 电磁波在信道中传播一定的距离需要花费的时间。\n处理时延 主机或路由器在收到分组时要花费一定时间进行处理\n排队时延 分组在进过网络传输时，要经过许多路由器。但分组在进入路由器后要先在输入队列中排队等待处理。\n有时会把排队时延看成处理时延一部分\n网络时延 = 发送时延 + 传播时延 + 处理时延 （处理时延包含排队时延）\n当处理时延忽略不计时，发送时延 和 传播时延谁占主导，要具体情况具体分析\n时延带宽积 时延带宽积 = 传播时延 * 带宽\n往返时间 互联网上的信息不仅仅单方向传输而是双向交互的。因此，我们有时很需要知道双向交互一次所需的时间。\n利用率 利用率有信道利用率和网络利用率两种。\n为什么信道利用率越高反而网络时延就会越大呢？\n答案：当信道利用率越高时，网络中传输的数据量也就越大，但同时也会带来更多的数据包冲突和重传，导致网络时延增加。当网络中的数据包冲突和重传达到一定程度时，就会出现网络拥塞，导致数据传输的时延和丢包率增加。因此，在设计网络时，需要综合考虑信道利用率和网络时延之间的关系，以达到最优的性能。\n丢包率 1.6 计算机网络体系结构 1、常见的计算机网络体系结构 OSI七层模型 第1层：物理层 从OSI模型的最底层开始是物理层。物理层规定了在载体上发送和接收数据的硬件方法，包括定义电缆，网卡和物理方面。快速以太网，RS232和ATM是具有物理层组件的协议。\n它解决了网络的物理特征。这包括用于将所有物体连接在一起的电缆类型。所使用的连接器的类型，电缆的长度等。例如，用于100BaseT电缆的以太网标准规定了双绞线电缆的电气特性，连接器的尺寸和形状，电缆的最大长度。\n**物理层还规定了用于通过电缆将数据从一个网络节点传输到另一个网络的信号的电气特性。**除了‘0’或‘1’的二进制特征外，信号没有任何特殊的含义。OSI模型上层将为在物理层传输的比特分配含义。\n网络中使用的一种非常重要的物理层设备是网络TAP。网络TAP是一种硬件设备，用于复制网络链路上的流量并将副本重定向到故障排除和分析工具，即使TAP断电也不会中断流量或引入故障点。\n从图中可以看出，东向流量被引向Monitor端口A，西向流量被引向Monitor端口B。\n▣第1层物理实例包括以太网，FDDI，B8ZS，V.35，V.24，RJ45。\n第2层：数据链路层 数据链路层是我们开始对要通过网络发送的内容赋予意义或智能的地方。数据链路层上的协议解决了以下问题，例如要发送的数据包的大小，要传送的每个数据包的寻址方式，使其到达预定的接收方，以及一种确保不超过一个节点尝试同时向接收方发送数据包的方法。\n数据链路层提供了错误检测和纠正功能，以确保发送的数据与接收的数据相同。如果错误无法纠正，数据链接标准需要规定如何将错误告知节点，以便它可以重新发送出错的数据。\n每个节点(网络接口卡–NIC)在数据链路层有一个地址，称为媒体访问控制地址，通常称为MAC地址。这是实际的硬件地址，是由设备制造商分配的。您可以通过打开命令窗口并运行ipconfig / all命令来找到设备的MAC地址。\n▣ 第2层数据链路示例包括PPP，FDDI，ATM，IEEE 802.5 / 802.2，IEEE 802.3 / 802.2，HDLC，帧中继。\n第3层：网络层 **第3层负责在网络中进行网络消息的路由。网络层的一个重要功能是逻辑寻址。**每个网络设备都有一个物理地址，称为MAC地址（见第2层）。当你为电脑买了一块网卡时，该网卡的MAC地址是不能改变的。但是，如果你想使用一些其他的寻址系统，来引用你的计算机和其他设备，第3层网络层就是你可以设置所谓的 “逻辑地址 “的地方。逻辑地址为网络设备提供了一个位置，可以使用您分配的地址在网络上对其进行访问。\n逻辑地址可以由IP或IPX等网络层协议创建和使用。网络层协议将逻辑地址转换为MAC地址。\n例如，如果您使用IP作为网络层协议，则会为网络上的设备分配IP地址，例如107.210.76.30。由于IP协议在第3层上运行以实际发送数据包，因此IP需要将设备的IP地址转换为正确的MAC地址。您可以使用ipconfig / all命令查找计算机或其他设备的IP地址。\n解析IP地址后，我们现在需要设置路由，将数据包移动到目的地。当一个网络上的数据包需要发送到另一个网络上的计算机时，路由就会发挥作用。\n▣第3层网络示例包括AppleTalk DDP，IP，IPX。\n第4层：传输层 传输层是一台网络计算机与另一台网络计算机进行通信的基本层。传输层是最流行的网络协议之一传输控制协议（TCP）的地方。**传输层的主要目的是确保数据包在网络中可靠无误地移动。**传输层通过在网络设备之间建立连接，确认数据包的接收并重新发送未收到的或到达时已损坏的数据包来实现此目的。\n在许多情况下，传输层协议将大的消息分成较小的数据包，可以有效地在网络上发送（分组交换技术）。传输层协议在接收端重组消息，确保一次传输中包含的所有数据包都能收到，并且没有数据丢失。\n▣第4层传输示例包括SPX，TCP，UDP。\n第5层：会话层 会话层建立、管理和终止网络节点之间的连接。在网络上传输数据之前，必须先建立会话。会话层确保正确建立和维护这些会话。它提供全双工，半双工或单工操作，并建立检查点、延期、终止和重新启动过程。OSI模型使该层负责会话的正常关闭（这是TCP协议的一个属性），同时还负责会话检查点和恢复，这在Internet协议套件中通常不使用。会话层通常在使用远程过程调用的应用环境中显式实现。\n▣第5层会话示例包括NFS，NetBios Name，RPC，SQL。\n第6层：表示层 表示层负责将网络发送的数据从一种表示形式转换为另一种表示形式。例如，表示层可以应用复杂的压缩技术，以便在网络上发送时，需要较少的数据字节来表示信息。在传输的另一端，传输层则对数据进行解压缩。\n这一层通过从应用到网络格式的转换，提供了不受数据表示差异（如加密）影响的独立性，反之亦然。表示层可以通过压缩、编码、加密等手段将数据转换为应用层可以接受的形式。该层对要在网络上发送的数据进行格式化和加密，使数据不受兼容性问题的影响。它有时被称为语法层。\n▣第6层演示示例包括encryption，ASCII，EBCDIC，TIFF，GIF，PICT，JPEG，MPEG，MIDI。\n第7层：应用层 OSI模型的最高层-应用层，它处理应用程序用于与网络通信的技术。该层的名称是有点令人困惑，因为**应用程序（如Excel或Word）实际上不是该层的一部分。而是，应用层表示应用程序与网络交互的级别，使用编程接口请求网络服务。**HTTP是最常用的应用程序层协议之一，它代表超文本传输协议。HTTP是万维网的基础。\n▣第7层应用示例包括WWW browsers，NFS，SNMP，Telnet，HTTP，FTP。\nTCP/IP四层模型 如今用的最多的是TCP/IP体系结构，现今规模最大的、覆盖全球的、基于TCP/IP的互联网并未使用OSI标准。\nTCP/IP体系结构相当于将OSI体系结构的物理层和数据链路层合并为了网络接口层，并去掉了会话层和表示层。\nTCP/IP在网络层使用的协议是IP协议，IP协议的意思是网际协议，因此TCP/IP体系结构的网络层称为网际层\n在用户主机的操作系统中，通常都带有符合TCP/IP体系结构标准的TCP/IP协议族。\n而用于网络互连的路由器中，也带有符合TCP/IP体系结构标准的TCP/IP协议族。\n只不过路由器一般只包含网络接口层和网际层。\n网络接口层：并没有规定具体内容，这样做的目的是可以互连全世界各种不同的网络接口，例如：有线的以太网接口，无线局域网的WIFI接口等。\n网际层：它的核心协议是IP协议。\n运输层：TCP和UDP是这层的两个重要协议。\n应用层：这层包含了大量的应用层协议，如 HTTP , DNS 等。\n**IP协议（网际层）可以将不同的网络接口（网络接口层）进行互连，并向其上的TCP协议和UDP协议（运输层）**提供网络互连服务\n而TCP协议在享受IP协议提供的网络互连服务的基础上，可向应用层的相应协议提供可靠的传输服务。\nUDP协议在享受IP协议提供的网络互连服务的基础上，可向应用层的相应协议提供不可靠的传输服务。\nTCP/IP体系结构中最重要的是IP协议和TCP协议，因此用TCP和IP来表示整个协议大家族。\n教学时把TCP/IP体系结构的网络接口层分成了物理层和数据链路层\n2、计算机网络体系结构分层的必要性 物理层问题\n这图说明\n第一，严格来说，传输媒体并不属于物理层 计算机传输的信号，并不是图示的方波信号 这样举例只是让初学者容易理解\n数据链路层问题\n网络层问题\n运输层问题\n如何标识与网络通信相关的应用进程：一个分组到来，我们应该交给哪个进程处理呢？浏览器进程还是QQ进程\n应用层问题\n应用层该用什么方法（应用层协议）去解析数据\n总结\n3、计算机网络体系结构分层思想举例 例子：主机的浏览器如何与Web服务器进行通信\n解析：\n主机和Web服务器之间基于网络的通信，实际上是主机中的浏览器应用进程与Web服务器中的Web服务器应用进程之间基于网络的通信\n体系结构的各层在整个过程中起到怎样的作用？\n1、发送方发送 第一步：\n应用层按照HTTP协议的规定构建一个HTTP请求报文 应用层将HTTP请求报文交付给运输层处理 第二步：\n运输层给HTTP请求报文添加一个TCP首部，使之成为TCP报文段 TCP报文段的首部格式作用是区分应用进程以及实现可靠传输 运输层将TCP报文段交付给网络层处理 第三步：\n网络层给TCP报文段添加一个IP首部，使之成为IP数据报 IP数据报的首部格式作用是使IP数据报可以在互联网传输，也就是被路由器转发 网络层将IP数据报交付给数据链路层处理 第四步：\n数据链路层给IP数据报添加一个首部和一个尾部，使之成为帧（图示右边为首部，左边为尾部） 该首部的作用主要是为了让帧能够在一段链路上或一个网络上传输，能够被相应的目的主机接收 该尾部的作用是让目的主机检查所接收到的帧是否有误码 数据链路层将帧交付给物理层 第五步：\n物理层先将帧看做是比特流，这里的网络N1假设是以太网，所以物理层还会给该比特流前面添加前导码 前导码的作用是为了让目的主机做好接收帧的准备 物理层将装有前导码的比特流变换成相应的信号发送给传输媒体 第六步：\n信号通过传输媒体到达路由器 2、路由器转发 在路由器中\n物理层将信号变为比特流，然后去掉前导码后，将其交付给数据链路层 数据链路层将帧的首部和尾部去掉后，将其交付给网络层，这实际交付的是IP数据报 网络层解析IP数据报的首部，从中提取目的网络地址 在路由器中\n提取目的网络地址后查找自身路由表。确定转发端口，以便进行转发 网络层将IP数据报交付给数据链路层 数据链路层给IP数据报添加一个首部和一个尾部，使之成为帧 数据链路层将帧交付给物理层 物理层先将帧看成比特流，这里的网络N2假设是以太网，所以物理层还会给该比特流前面添加前导码 物理层将装有前导码的比特流变换成相应的信号发送给传输媒体，信号通过传输媒体到达Web服务器 可以看得出来：路由器的工作主要是将在物理线路里的信号，按照OSI七层模型进行解析，直到得到网络层的IP数据报的目的网络地址，这样就可以根据路由器的路由表和当前数据包需要转发的目的网络地址，选择一个最佳的路径将数据报发送出去，直到数据报到达目的主机。\n3、接收方接收 和发送方（主机）发送过程的封装正好是反着来\n在Web 服务器上\n物理层将信号变换为比特流，然后去掉前导码后成为帧，交付给数据链路层 数据链路层将帧的首部和尾部去掉后成为IP数据报，将其交付给网络层 网络层将IP数据报的首部去掉后成为TCP报文段，将其交付给运输层 运输层将TCP报文段的首部去掉后成为HTTP请求报文，将其交付给应用层 应用层对HTTP请求报文进行解析，然后给主机发回响应报文 发回响应报文的步骤和之前过程类似\n4、计算机网络体系结构中的专用术语 以下介绍的专用术语来源于OSI的七层协议体系结构，但也适用于TCP/IP的四层体系结构和五层协议体系结构\n实体\n协议\n协议：控制两个对等实体进行逻辑通信的规则的集合\n协议三要素：\n语法：定义所交换信息的格式 语义：定义收发双方所要完成的操作 同步：定义收发双发的时序关系 服务\n简单来说，PDU是在网络协议中传输的数据单元，而SDU是上层协议向下层协议传递的数据单元。在数据传输过程中，SDU会被分割成多个PDU进行传输，而接收方会将接收到的PDU重新组装成SDU。\n","permalink":"https://cold-bin.github.io/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/","tags":["osi七层模型"],"title":"计算机网络之概述"},{"categories":["缓存系统"],"contents":"[toc]\n缓存淘汰 背景 不论是进程内缓存，还是分布式缓存，都无法避免这样一个问题：当我们需要缓存的数据大于物理内存时，那么就需要通过操作系统虚拟内存管理不断地在硬盘和内存中进行页面置换。设想一下：缓存数据越大，那么缺页中断的就越多，导致页面置换就越多，从而磁盘IO增加，从而大大拖垮缓存的性能。所以，虽然操作系统允许我们运行比物理内存更大更多的进程，但是为了兼具缓存性能，我们依然需要适当的控制，来保证我们缓存数据的大小不能超过物理内存，一旦超过就可能会发生页面置换中的磁盘IO\n所以，我们可以采取如下一些策略。\n内存足够时，我们也可以进一步减轻内存负担，例如过期删除、主动清理等 内存不足够时，需要通过一些好的策略来做内存数据的淘汰，避免物理内存不足后发生页面置换现象；或者我们直接加内存 在缓存数据的淘汰中，我们需要将数据分为两类：高频热数据和低频冷数据。我们应该尽可能地避免高频热数据被缓存淘汰掉，这样会降低缓存的命中率。所以，我们需要采取一些好的缓存策略来保证我们的缓存命中率。\n几种缓存淘汰策略及实现 FIFO先进先出淘汰 缓存满了之后，自动删除较早放入的缓存数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 // @author cold bin // @date 2023/5/18 package cache import ( \u0026#34;container/list\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;sync\u0026#34; ) type fifoCache struct { maxCap uint32 // 缓存允许的最大容量，这里就暂时用个粗略地数字单表允许地容量 list *list.List // 保证fifo cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026amp;fifoCache{} func NewFifoCache(maxCap uint32) Cache { return \u0026amp;fifoCache{ maxCap: maxCap, list: \u0026amp;list.List{}, cacheMap: make(map[string]*list.Element, maxCap), } } func (c *fifoCache) Get(key string) (any, error) { if v, ok := c.cacheMap[key]; ok { return v, nil } return nil, errors.New(\u0026#34;key不存在\u0026#34;) } // Set 当容量足够时，就可以继续放；当容量不足够时，就按照fifo策略淘汰 func (c *fifoCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() // 存在即更新 if v, ok := c.cacheMap[key]; ok { v.Value = value c.cacheMap[key] = v return nil } // 是否触发缓存淘汰，这里容量判断（实际上精确地内存占用判断较为困难，因为操作系统不只有物理内存，还有虚拟的内存，只能粗略估计） if c.list.Len()*10 \u0026gt;= int(c.maxCap) \u0026amp;\u0026amp; c.list.Len() \u0026gt; 0 { // 淘汰最早节点 k := c.list.Back() delete(c.cacheMap, k.Value.(string)) c.list.Remove(k) } // 不存在即添加 c.cacheMap[key] = \u0026amp;list.Element{Value: value} c.list.PushFront(key) return nil } func (c *fifoCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() if e, ok := c.cacheMap[key]; ok { delete(c.cacheMap, key) c.list.Remove(e) return nil } return errors.New(\u0026#34;key不存在\u0026#34;) } 优点：实现很简单 缺点：先进来的数据可能是热点数据，淘汰掉热点数据，会导致缓存的命中率降低。 LFU最不经常使用 LFU最不经常使用算法，是基于数据的访问频次来对缓存数据进行淘汰的算法。\n其淘汰数据依据两点：访问频次\n核心：如果数据过去被访问多次，那么将来被访问的频率也就更高，当内存超过最大容量时，淘汰掉访问次数较少的数据。\n依然使用队列实现，淘汰部分淘汰掉访问次数最低即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 // @author cold bin // @date 2023/5/18 package cache import ( \u0026#34;errors\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;sync\u0026#34; ) type lfuCache struct { maxCap uint32 // 缓存允许的最大容量 counts counts // 记录访问频次的队列 cacheMap map[string]*entry // 缓存 lock sync.RWMutex } var _ Cache = \u0026amp;lfuCache{} type counts []*entry func (c counts) find(key string) int { for i, e := range c { if e.k == key { return i } } return -1 } // 更新频次，需要重新调整序列 func (c *counts) update(en *entry, v any, weight int) { en.v = v en.weight = weight // 调整排序 sort.Sort(c) } func (c counts) Len() int { return len(c) } func (c counts) Less(i, j int) bool { return c[i].weight \u0026lt; c[j].weight } func (c counts) Swap(i, j int) { c[i], c[j] = c[j], c[i] c[i].idx = i c[j].idx = j } type entry struct { k string v any weight int idx int } func NewLfuCache(maxCap uint32) Cache { return \u0026amp;lfuCache{ maxCap: maxCap, counts: make([]*entry, 0, 1024), cacheMap: make(map[string]*entry, maxCap), } } func (c *lfuCache) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() if v, ok := c.cacheMap[key]; ok { // 找key并更新访问频次 c.counts.update(v, v.v, v.weight+1) return v, nil } return nil, errors.New(\u0026#34;key不存在\u0026#34;) } func (c *lfuCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() // 存在即更新 if e, ok := c.cacheMap[key]; ok { c.counts.update(e, value, e.weight+1) return nil } // 不存在即新增 e := \u0026amp;entry{ k: key, v: value, weight: 0, idx: -1, } c.cacheMap[key] = e c.counts = append(c.counts, e) // 容量满了，触发内存淘汰 if c.counts.Len()*10 \u0026gt;= int(c.maxCap) \u0026amp;\u0026amp; len(c.counts) \u0026gt; 0 { idx := len(c.counts) - 1 e := c.counts[idx] c.counts = c.counts[:idx] delete(c.cacheMap, e.k) } return nil } func (c *lfuCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() if _, ok := c.cacheMap[key]; ok { i := c.counts.find(key) c.counts = append(c.counts[:i], c.counts[i+1:]...) delete(c.cacheMap, key) return nil } return errors.New(\u0026#34;key不存在\u0026#34;) } 优点：命中率较高\n缺点：\n实现较复杂，内存占用较高\n另一种缓存污染问题\n越到后期，数据就越难缓存，因为前期缓存数据积累的访问次数太大，会导致后期热点数据存不到缓存里\nLRU最近最少使用 最近最少使用缓存淘汰算法，其淘汰最近一段时间最少被访问的缓存数据。我们使用队列可以实现这种淘汰算法，对于访问的元素，移动到链表尾，这样链表头为较旧的元素，当容量满时，淘汰掉链表头元素即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 // @author cold bin // @date 2023/5/18 package cache import ( \u0026#34;container/list\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;sync\u0026#34; ) type lruCache struct { maxCap uint32 // 缓存允许的最大容量 list []*list.Element // 记录访问顺序 cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026amp;lruCache{} func NewLruCache(maxCap uint32) Cache { return \u0026amp;lruCache{ maxCap: maxCap, list: make([]*list.Element, 0, 1024), cacheMap: make(map[string]*list.Element, maxCap), } } func (c *lruCache) find(key string) int { for i, k := range c.list { if k.Value == key { return i } } return -1 } // lru核心 func (c *lruCache) swapLast(key string) { i := c.find(key) if i == -1 { panic(\u0026#34;i==-1\u0026#34;) } e := c.list[i] c.list = append(c.list[:i], c.list[i+1:]...) c.list = append(c.list, e) } func (c *lruCache) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() if v, ok := c.cacheMap[key]; ok { // 元素key移到尾部 c.swapLast(key) return v, nil } return nil, errors.New(\u0026#34;not found\u0026#34;) } func (c *lruCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() // 存在则更新 if v, ok := c.cacheMap[key]; ok { v.Value = value // c.cacheMap[key] = v c.swapLast(key) return nil } // 容量满时则淘汰头部元素即可 if len(c.list)*10 \u0026gt;= int(c.maxCap) \u0026amp;\u0026amp; len(c.list) \u0026gt; 0 { tmpKey := c.list[0] c.list = c.list[1:] delete(c.cacheMap, tmpKey.Value.(string)) } // 不存在则添加 v := \u0026amp;list.Element{Value: value} k := \u0026amp;list.Element{Value: key} c.cacheMap[key] = v c.list = append(c.list, k) return nil } func (c *lruCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() i := c.find(key) c.list = append(c.list[:i], c.list[i+1:]...) delete(c.cacheMap, key) return nil } 上面lru实现的时间复杂度较高。其实可以采用双向链表+哈希表的方式实现O(1)级别的时间复杂度\n优点：缓存命中率较高\n缺点：\n内存占用高\n缓存污染\n当我们在批量读取数据的时候，由于数据被访问了一次，这些大量数据都会被加入到「活跃 LRU 链表」里，然后之前缓存在活跃 LRU 链表（或者 young 区域）里的热点数据全部都被淘汰了，如果这些大量的数据在很长一段时间都不会被访问的话，那么整个 LRU 链表就被污染了。\n预读失效\n操作系统在访问内存数据的时候，如果数据不在内存里，会从磁盘里去加载数据，又因为空间局部性原理，会加载连续的多个块，此时，只有一个块里有热点数据，其余块都没有热点数据，然而还会把lru链表最后的热点数据挤出去。另外几个没有热点数据的块，当然也就预读失效了。\n如何解决lru的缓存污染和预读污染问题？\nLRU-K lru-1\n数据第一次被访问，加入到访问历史列表；\n如果数据在访问历史列表里后没有达到K次访问，则按照LRU淘汰；\n当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；\n缓存数据队列中被再次访问后，重新排序；\n需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即淘汰\u0026quot;倒数第K次访问离现在最久\u0026quot;的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 // @author cold bin // @date 2023/5/19 package cache import ( \u0026#34;container/list\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;sync\u0026#34; ) type lrukCache struct { maxCap uint32 // 缓存允许的最大容量 k int // 指定多少次访问后移入缓存队列，一般推荐为两次 history []*entry1 // 访问历史记录，只有访问次数大于等于k的数据才会被缓存，其余数据按照lru淘汰。采用链表更好点 list []*list.Element // 缓存里的key顺序，最近访问的元素放到队列的头部，越往后就是越久的元素，按照lru方式淘汰 cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026amp;lrukCache{} type entry1 struct { k string v any count int // 访问次数 } // 判断key在哪个队列里：-1-\u0026gt;history;1-\u0026gt;list func (c *lrukCache) judge(key string) (idx int, ans int) { for i, e := range c.history { if e.k == key { return i, -1 } } for i, e := range c.list { if e.Value.(string) == key { return i, 1 } } return -1, 0 } // list里最近访问的元素放到list后面 // 切片这里时间复杂度较高，采用链表的话，更好点 func (c *lrukCache) placeLruk(idx int) { e := c.list[idx] c.list = append(c.list[:idx], c.list[idx+1:]...) c.list = append(c.list, e) } // history里最近访问的元素放到history后面 func (c *lrukCache) placeHistory(idx int) { e := c.history[idx] c.history = append(c.history[:idx], c.history[idx+1:]...) c.history = append(c.history, e) } // 访问次数达到k的元素从history移到list，并将存入缓存 func (c *lrukCache) historyMoveToList(hidx int) { e := c.history[hidx] // 删除 c.history = append(c.history[:hidx], c.history[hidx+1:]...) // 放到list末尾 c.list = append(c.list, \u0026amp;list.Element{Value: e.k}) // 放入缓存 c.cacheMap[e.k] = \u0026amp;list.Element{Value: e.v} } func (c *lrukCache) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() idx, ans := c.judge(key) switch ans { case 1: // list // 将新访问的缓存移到lru后面 c.placeLruk(idx) return c.cacheMap[key].Value, nil case -1: // history // 访问history，如果访问次数达到k，移到list；否则按照lru规则 c.history[idx].count++ // 先记录值 v := c.history[idx].v if c.history[idx].count == c.k { // 可以放list和缓存了 c.historyMoveToList(idx) } else { // lru规则 c.placeHistory(idx) } return v, nil case 0: return nil, errors.New(\u0026#34;not found\u0026#34;) default: return nil, errors.New(\u0026#34;not known error\u0026#34;) } } func (c *lrukCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() idx, ans := c.judge(key) switch ans { case 1: // list // 更新缓存和list c.cacheMap[key].Value = value c.placeLruk(idx) return nil case -1: // history // 更新history c.history[idx].count++ if c.history[idx].count == c.k { // 可以放list和缓存了 c.historyMoveToList(idx) } else { // lru规则 c.placeHistory(idx) } return nil case 0: // 不存在即添加到history后，并按照lru淘汰 // 是否触发缓存淘汰 if (len(c.history)+len(c.list))*10 \u0026gt;= int(c.maxCap) { c.history = c.history[1:] // 没有缓存 } // 添加 e := \u0026amp;entry1{k: key, v: value} c.history = append(c.history, e) return nil default: return errors.New(\u0026#34;not known error\u0026#34;) } } func (c *lrukCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() idx, ans := c.judge(key) switch ans { case 1: // list // 删除list c.list = append(c.list[:idx], c.list[idx+1:]...) // 删除缓存 delete(c.cacheMap, key) return nil case -1: // history c.history = append(c.history[:idx], c.history[idx+1:]...) return nil default: return errors.New(\u0026#34;not found\u0026#34;) } } LRU其实就是LRU-1，意思就是最近使用过1次的数据，就加入缓存。这样做可能会导致这样的情况发生：近期前有大量访问，但是近期没有访问的数据，那么这些数据就可能会被LRU规则淘汰掉，但这些数据还是有可能在近期后被访问。这样就造成某些偶尔发生的缓存命中率低的现象。\nLRU-K策略指的是，只有数据被访问k次后，才能被加入缓存里，这样可以降低缓存污染（减少冷数据加入缓存的情况），从而提高缓存命中率。\n其实linux操作系统中page cache的缓存淘汰策略里也是通过将将lru算法改进为lru-2算法提高进入active list的lru缓存的门槛，从而降低了缓存污染严重性。\n2Q 2Q（Two Queue）缓存淘汰算法是一种基于 LRU（Least Recently Used）和 FIFO（First In First Out）算法的混合算法。其主要思想是将缓存分为两个队列：Q1 和 Q2。\n当数据第一次访问时，2Q算法将数据缓存在FIFO队列里面，当数据第二次被访问时，则将数据从FIFO队列移到LRU队列里面，两个队列各自按照自己的方法淘汰数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 // @author cold bin // @date 2023/5/19 package cache import ( \u0026#34;container/list\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;sync\u0026#34; ) type twoQueue struct { maxCap uint32 // 缓存允许的最大容量 fifoList []*list.Element // fifo lruList []*list.Element // lru cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026amp;twoQueue{} func NewTwoQueue(maxCap uint32) Cache { return \u0026amp;twoQueue{ maxCap: maxCap, fifoList: make([]*list.Element, 0, 1024), lruList: make([]*list.Element, 0, 1024), cacheMap: make(map[string]*list.Element, 1024), } } func (c *twoQueue) placeLru(idx int) { e := c.lruList[idx] c.lruList = append(c.lruList[:idx], c.lruList[idx+1:]...) c.lruList = append(c.lruList, e) } // 判断key存在与哪个队列。-1-\u0026gt;fifo;1-\u0026gt;lru func (c *twoQueue) judge(key string) (idx int, ans int) { for i, k := range c.fifoList { if k.Value.(string) == key { return i, -1 } } for i, k := range c.lruList { if k.Value.(string) == key { return i, 1 } } return -1, 0 } func (c *twoQueue) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() if _, ok := c.cacheMap[key]; ok { // 如果访问数据在fifo队列里，就移到lru队列；如果在lfu队列里，就移到lru尾部 idx, ans := c.judge(key) switch ans { case 1: // lru c.placeLru(idx) case -1: // fifo // 删除 k := c.fifoList[idx] c.fifoList[idx] = nil c.fifoList = append(c.fifoList[:idx], c.fifoList[idx+1:]...) // 移动到lru尾部 c.lruList = append(c.lruList, k) // 是否触发lru内存淘汰 if (len(c.lruList)+len(c.fifoList))*10 \u0026gt;= int(c.maxCap) \u0026amp;\u0026amp; len(c.lruList) \u0026gt; 0 { tmpKey := c.lruList[0] c.lruList = c.lruList[1:] delete(c.cacheMap, tmpKey.Value.(string)) } case 0: return nil, errors.New(\u0026#34;not exist\u0026#34;) default: return nil, errors.New(\u0026#34;not known error\u0026#34;) } } return nil, errors.New(\u0026#34;not exist\u0026#34;) } func (c *twoQueue) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() if _, ok := c.cacheMap[key]; ok { // 存在就更新：如果访问数据在fifo队列里，就移到lru队列；如果在lru队列里，就移到lru尾部 idx, ans := c.judge(key) switch ans { case 1: // lru c.placeLru(idx) case -1: // fifo // 删除 k := c.fifoList[idx] c.fifoList[idx] = nil c.fifoList = append(c.fifoList[:idx], c.fifoList[idx+1:]...) // 是否触发lru内存淘汰 if (len(c.lruList)+len(c.fifoList))*10 \u0026gt;= int(c.maxCap) \u0026amp;\u0026amp; len(c.lruList) \u0026gt; 0 { tmpKey := c.lruList[0] c.lruList = c.lruList[1:] delete(c.cacheMap, tmpKey.Value.(string)) } // 移动到lru尾部 c.lruList = append(c.lruList, k) case 0: return errors.New(\u0026#34;not exist\u0026#34;) default: return errors.New(\u0026#34;not known error\u0026#34;) } } // fifo是否触发缓存淘汰 if (len(c.fifoList)+len(c.lruList))*10 \u0026gt;= int(c.maxCap) \u0026amp;\u0026amp; len(c.fifoList) \u0026gt; 0 { // 淘汰最早节点 k := c.fifoList[0] delete(c.cacheMap, k.Value.(string)) c.fifoList = c.fifoList[1:] } // 新增则加入fifo队列 c.cacheMap[key] = \u0026amp;list.Element{Value: value} c.fifoList = append([]*list.Element{\u0026amp;list.Element{Value: key}}, c.fifoList...) return nil } func (c *twoQueue) Del(key string) error { //TODO implement me panic(\u0026#34;implement me\u0026#34;) } 优点：更快，一般来说可以实现O(1)级别的淘汰 缺点：belady现象：物理内存增加反而导致缓存命中率下降 应用与举例 redis中的近似LRU策略 当实际内存超出 maxmemory 时，就会执行一次内存淘汰策略，具体何种内存淘汰策略，可在maxmemory-policy 中进行配置。Redis 提供了如下几种策略：\nnoeviction：不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru：尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl：跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random：跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru：区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random：跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。\n如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。\n如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。\nredis中的lru：\n在 volatile-lru 策略中，Redis 并没有采用经典的 LRU 算法，而是采用了一种近似 LRU 算法。至于为什么会不是。主要有以下几点：\n内存占用高（除了缓存数据，需要额外引入链表来记录key的访问顺序，以实现lru策略） 大量节点的访问，会带来链表或队列中节点频繁的移动，较为损耗性能 传统lru会出现缓存污染问题和预读失效问题（当然，redis并没有预读缓存） 那么针对以上两个问题，近似lru策略这样解决：\n近似 LRU 算法中，是在现有数据结构的基础上，使用随机采样法来淘汰元素，避免额外的内存开销。（就是每次淘汰的数据并不是全部的key，而是采取的样本）\n近似 LRU 算法中，Redis 给每个 key 增加了一个额外的小字段（长度为 24 个 bit），存储了元素最后一次次被访问的时间戳。Redis 会对少量的 key 进行采样，并淘汰采样的数据中最久没被访问过的 key。这也就意味着 Redis 无法淘汰数据库最久访问的数据。\nRedis LRU 算法有一个重要的点在于，它可以更改样本数量（修改 maxmemory-samples 属性值）来调整算法的精度，使其近似接近真实的 LRU 算法，同时又避免了内存的消耗，因为每次只需要采样少量样本，而不是全部数据。\nbigcache中的fifo策略 完完全全的fifo策略，淘汰的时候会淘汰掉最早的数据，但是容易出现缓存命中率不高的问题。总而言之，fifo只考虑了时间维度因素，并没有考虑到访问频率因素。\n当然选取fifo策略，相较于lru、lfu等策略实现起来更简单，而且也更快，内存占用也更低\nmemcached中的lru策略 memcached的缓存淘汰策略是受2Q设计影响的修改LRU（Segmented LRU，分段LRU）。主要是借鉴了2q策略中的数据分类，memcached将数据分为以下几类：\nHOT queue：如果一个 item 的过期时间（TTL）很短，会进入该队列，在 HOT queue 中不会发生 bump，如果一个 item 到达了 queue 的 tail，那么会进入到 WARM 队列（如果 item 是 ACTIVE 状态）或者 COLD 队列（如果 item 处于不活跃状态）。 WARM queue：如果一个 item 不是 FETCHED，永远不会进入这个队列，该队列里面的 item TTL 时间相对较长，这个队列的 lock 竞争会很少。该队列 tail 处的一个 item 如果再一次被访问，会 bump 回到 head，否则移动到 COLD 队列。 COLD queue：包含了最不活跃的 item，一旦该队列内存满了，该队列 tail 处的 item 会被 清除。如果一个 item 被激活了，那么会异步移动到 WARM 队列。 TEMP queue：该队列中的 item TTL 通常只有几秒，该列队中的 item 永远不会发生 bump，也不会进入其他队列，节省了 CPU 时间，也避免了 lock 竞争。 “bump”是memcached中的一个标记，用于标记某个键值对不应该被淘汰。\n总结 根据业务需求选择合适的算法。不同的业务场景对缓存的需求不同，因此需要根据实际情况选择合适的缓存淘汰算法。 避免频繁清除缓存。频繁清除缓存会导致缓存命中率降低，从而影响系统的性能。因此，在选择缓存淘汰算法时，需要考虑清除缓存的频率。 配置合适的缓存容量。缓存容量过小会导致缓存命中率降低，而缓存容量过大会浪费系统资源。因此，需要根据实际情况配置合适的缓存容量。 考虑多级缓存。多级缓存可以提高缓存命中率，从而提高系统的性能。例如，可以比如bigcache+redis的方式构建多级缓存。 使用缓存预热技术。缓存预热技术可以在系统启动时将常用的数据预先加载到缓存中，从而提高缓存命中率。 ","permalink":"https://cold-bin.github.io/post/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/","tags":["fifo","lfu","lru","2q","lru-k"],"title":"缓存淘汰策略"},{"categories":["操作系统"],"contents":"[toc]\nIO管理概述 IO设备基本概念 IO设备管理是操作系统设计中最凌乱也最具挑战性的部分。由于它包含了很多领域的不同设备以及与设备相关的应用程序，因此很难有一个通用且一直的设计方案。所以在理解设备管理之前，应该先了解具体的IO设备类型。\nIO设备可以根据不同的方式进行分类。\n按使用特性分类IO设备 人机交互类外部设备，又称慢速IO设备，用于桶计算机用户之间交互的设备，如打印机、显示器、鼠标、键盘等。这类设备数据交换速度相对较慢，通常是以字节为单位进行数据交换。 存储设备，用于存储程序和数据的设备，如磁盘、磁带、光盘等。这类设备用于数据交换，速度较快，通常以多字节组成的块为单位进行数据交换。 网络通信设备，用于与远程设备通信的设备，如各种网络接口、调制解调器等。其数据交换速度介于外部设备与存储设备之间。网络通信设备在使用和管理上与前两者设备有很大的不同。 按传输速率分类IO设备 低速设备，传输速率仅为每秒钟几个字节至数百个字节的一类设备，如键盘、鼠标等。 中速设备，传输速率在每秒数千个字节至数万个字节的一类设备，如行式打印机、激光打印机等。 高速设备，传输速率在数百个千字节至千兆字节的一类设备，如磁带机、磁盘机、光盘机等。 按信息交换单位分类IO设备 块设备\n由于信息的存取总是以数据块为单位，所以存储信息的设备称为块设备。它属于有结构设备，如磁盘等。磁盘设备的基本特征是传输速率高，以及可寻址，即对他可随机地读写任意块。\n字符设备\n用于数据输入输出的设备为字符设备，因为其传输的基本单位是字符。它属于无结构类型，如交互式终端机、打印机等。他们的传输速率低、不可寻址、并且在输入输出时常采用中断驱动方式。\nIO设备的使用方式 对于IO设备，有以下三种不同类型的使用方式：独占式使用设备。独占式使用设备是指在申请设备是，如果设备空闲，就将其独占，不再允许其他进程申请使用，一直等到该设备被释放才允许其他进程申请使用。例如：打印机。\n分时式共享使用设备。独占式使用设备时，设备利用率低，当设备没有独占使用的要求时，可以通过分时共享使用，提高利用率。例如：对磁盘设备的IO操作，各进程每次IO操作请求可以通过分时来交替进行。\n以SPOOLing方式使用外部设备。SPOOLing技术是在批处理操作系统时代引入的，即假脱机IO技术。这种技术用于对设备的操作，实质上就是对IO操作进行批处理。具体的内容后面有单独讲解。\n采用上面三种使用方式的设备分别称为独占设备、共享设备和虚拟设备。\nIO设备的构成 IO设备通常包括一个机械部件和一个电子部件。为了达到设计的模块性和通用性，一般将其分开。\n电子部件成为IO控制器，在个人计算机中，通常是一块插入主板扩充槽的印制电路板。\n机械部件即设备本身。例如鼠标、键盘等。\nIO控制器 由于具体的设备操作涉及硬件接口，且不同的设备有不同的硬件特性和参数，所以这些复杂的操作交由操作系统用户编写程序来操作是不实际的。引入控制器后，系统可以通过几个简单的参数完成对控制器的操作，而具体的硬件操作则由控制器调用相应的设备接口完成。\n设备控制器的引入大大简化了操作系统的设计，特别是有利于计算机系统和操作系统对各类控制器和设备的兼容；同时也实现了主存和设备之间的数据传输操作，使CPU从繁重的设备控制操作中解放出来。\nIO控制器优点：\n简化了操作系统的设计 将CPU从复杂的设备控制操作中解放出来 设备（IO）控制器的主要功能\n接收和识别CPU或通道发来的命令，如磁盘控制器能接收读、写、查找、搜索等命令。 数据交换。CPU和IO设备之间的数据交换，IO控制器为了和高速CPU进行数据交换，往往需要数据寄存器来做缓存层。 向CPU报告设备状态。 设备地址识别。 IO控制器的组成 IO控制方式 设备管理的主要任务之一是控制设备和内存或处理器之间的数据传送，外围设备和内存之间的输入输出控制方式有四种\n程序直接控制方式 计算机从外部设备读取数据到存储器，每次读一个字的数据。对读入的每个字，CPU需要对状态循环检查，知道确定该字已经在IO控制器的数据寄存器中。在程序IO方式中，由于CPU的高速型和IO设备的低速性，致使CPU的绝大部分时间都处于等待IO设备完成数据IO的循环测试中，造成CPU的极大浪费。CPU之所以要不断地测试IO设备的状态，就是因为在CPU中无中断机构，使IO设备无法向CPU报告它已完成了一个字符的输入操作。\n缺点\n采用这种IO控制方式会导致CPU和IO设备只能串行工作，导致CPU的利用率降低。\n中断驱动方式 中断驱动方式的思想是：允许IO设备主动打断CPU的运行并请求服务，从而“解放”CPU，使得其向IO控制器发送命令后可以继续做其他有用的工作。\n我们从IO控制器和CPU两个角度分别来看中断驱动方式的工作过程：\n从IO控制器的角度来看，IO控制器从CPU接受一个读命令，然后从外围设备读数据。一旦数据读入到该IO控制器的数据寄存器，便通过控制线给CPU发出一个中断信号，表示数据已准备好，然后等待CPU请求该数据。IO控制器收到CPU发出的取数据请求后，将数据放到数据总线上，传到CPU的寄存器中。至此，本次IO操作完成，IO控制器又可以开始下一次IO操作。 从CPU的角度来看，CPU发送读命令，然后保存当前运行程序的上下文（现场，包括程序计数器及处理器寄存器），转去执行其他程序。在每个指令周期的末尾，CPU检查中断。当有来自IO控制器的中断时，CPU保存当前正在运行程序的上下文，转去执行中断处理程序处理该中断。这时，CPU从IO控制器读一个字的数据传送到寄存器，并存入主存。接着，CPU恢复发出IO命令的程序（或其他程序）的上下文，然后继续运行。 缺点：\n中断驱动方式依然需要花费较多的CPU时间。因为数据中的每个字在存储器与IO控制器之间的传输都必须通过CPU处理。 优点：\n相比程序直接控制方式，中断驱动方式可以很好的会减少CPU等待IO设备完成IO的时间。 DMA方式 中断驱动方式中，CPU仍然需要主动处理在存储器和IO设备之间的数据传送，所以速度还是受限，而直接内存存取（DMA）方式的基本思想是在外围设备和内存之间开辟直接的数据交换通路，彻底解放CPU。\n该方式的特点是：\n基本单位是数据块 所传诵的数据，是从设备直接送入内存的或者相反，中间不需要经过CPU的内存拷贝 仅在传送一个或多个数据块的开始和结束时，才需CPU干预，整块数据的传送是在DMA控制器的控制下完成的。 DMA的工作过程是：\nCPU读写数据时，他给IO控制器发出一条命令，启动DMA控制器，然后继续其他工作。 之后CPU就把这个操作委托给DMA控制器，由该控制器负责处理。DMA控制器直接与存储器交互，传送整个数据块，这个过程不需要CPU参与。 当传送完成后，DMA控制器发送一个中断信号给处理器。因此，只有在传送开始和结束时才需要CPU的参与。 通道控制方式 DMA方式在工作流程上大幅度释放CPU，但是这种方式还需要CPU来控制传输的数据块大小、传输的内存位置，为了将CPU从这些繁琐的工作中释放处理，有了通道方式。\nIO通道是专门负责输入输出的处理机，当CPU要完成一组读写操作时，只需要给I/O通道发出一条I/O指令，给出要执行的通道程序的首地址和要访问的I/O设备，通道接收到该指令后，执行完I/O任务，也就是完成数据传输，才给CPU发送中断请求。\nIO软件层次结构 IO软件层次中，其中设备独立性软件、设备驱动程序、中断处理程序这三部分属于操作系统内核部分，即“IO系统”或称“IO核心子系统”\n用户层软件 用户层软件实现了与用户交互的接口，用户可直接使用该层提供的、与I/O操作相关的库函数对设备进行操作。\nEg：printf(“hello, world!\u0026quot;);\n==用户层软件将用户请求翻译成格式化的I/O请求，并通过“系统调用”请求操作系统内核的服务。==\nEg：printf(“hello, world!”); 会被翻译成等价的 write 系统调用，当然，用户层软件也会在系统调用时填入相应参数。\nWindows 操作系统向外提供的一系列系统调用，但是由于系统调用的格式严格，使用麻烦，因此在用户层上封装了一系列更方便的库函数接口供用户使用（Windows API）。\n设备独立性软件 设备独立性软件，又称设备无关性软件。==与设备的硬件特性无关的功能几乎都在这一层实现。==\n主要实现的功能：\n向上层提供统一的调用接口（如 read/write 系统调用）\n设备的保护\n原理类似与文件保护。==设备被看做是一种特殊的文件==，不同用户对各个文件的访问权限是不一样的，同理，对设备的访问权限也不一样。\n差错处理\n设备独立性软件需要对一些设备的错误进行处理\n设备的分配与回收\n数据缓冲区管理\n可以通过缓冲技术屏蔽设备之间数据交换单位大小和传输速度的差异\n建立逻辑设备名到物理设备名的映射关系；根据设备类型选择调用相应的驱动程序\n用户或用户层软件发出I/O操作相关系统调用的系统调用时，需要指明此次要操作的I/O设备的逻辑设备名（eg：去学校打印店打印时，需要选择 打印机1/打印机2/打印机3 ，其实这些都是逻辑设备名） 设备独立性软件需要通过“逻辑设备表（LUT，Logical UnitTable）”来确定逻辑设备对应的物理设备，并找到该设备对应的设备驱动程序\n操作系统系统可以采用两种方式管理逻辑设备表（LUT）：\n第一种方式，整个系统只设置一张LUT，这就意味着所有用户不能使用相同的逻辑设备名，因此这种方式只适用于单用户操作系统。 第二种方式，为每个用户设置一张LUT，各个用户使用的逻辑设备名可以重复，适用于多用户操作系统。系统会在用户登录时为其建立一个用户管理进程，而LUT就存放在用户管理进程的PCB中 设备驱动程序 各式各样的设备，外形不同，其内部的电子部件（I/O控制器）也有可能不同。驱动程序是一个进程。\n==为何不同的设备需要不同的设备驱动程序？== 佳能打印机的厂家规定状态寄存器为 0 代表空闲，1代表忙碌。有两个数据寄存器\n惠普打印机的厂家规定状态寄存器为 1代表空闲，0代表忙碌。有一个数据寄存器\n不同设备的内部硬件特性也不同，这些特性只有厂家才知道，因此厂家须提供与设备相对应的驱动程序，CPU执行驱动程序的指令序列，来完成设置设备寄存器，检查设备状态等工作\n==主要负责对硬件设备的具体控制，将上层发出的一系列命令（如read/write）转化成特定设备“能听得懂”的一系列操作。包括设置设备寄存器；检查设备状态等。==\n不同的I/O设备有不同的硬件特性，具体细节只有设备的厂家才知道。因此厂家需要根据设备的硬件特性设计并提供相应的驱动程序。\n中断处理程序 当I/O任务完成时，I/O控制器会发送一个中断信号，系统会根据中断信号类型找到相应的中断处理程序并执行。中断处理程序的处理流程如下：\n总结 IO核心子系统 IO核心子系统概述 概念 设备独立性软件 已在前文说明\n设备驱动软件 已在前文说明\n中断处理程序 已在前文说明\nIO调度与设备保护 IO调度 调度一组IO请求就是确定确定一个好的顺序来执行这些请求。应用程序所发布的系统调用的顺序不一定总是最佳选择，所以需要调度来改善系统整体性能，是进程之间公平的共享设备访问，减少IO完成所需要的平均等待时间。\n操作系统开发人员通过为每个设备维护一个请求队列来实现调度。当一个应用程序执行阻塞IO系统调用时，该请求就加到相应设备的队列上。IO调度会重新安排队列顺序以改善系统总体效率和应用程序的平均响应时间。\nIO子系统还可以使用主存或磁盘上的存储空间的技术，如缓冲、高速缓冲、假脱机等。\n设备保护 操作系统中，设备被视为文件。文件保护机制也就是设备保护机制，例如口令或密码保护、访问控制等机制。\n详见文件管理章节。\nSPOOLing技术 为了缓和CPU的高速型与IO设备低速性之间的矛盾而引入了脱机输入、脱机输出技术。该技术是利用专门的外围控制机，将低速IO设备上的数据传送到高速磁盘上；或者相反。\nSPOOLing的意思是外部设备同时联机操作，又称为假脱机输入输出操作，是操作系统中采用的一项将独占设备改造成共享设备的技术。\n在磁盘上开辟出的两个存储区域：\n输入井模拟脱机输入时的磁盘，用于收容IO设备输入的数据。 输出井模拟脱机输出的磁盘，用于收容用户程序的输出数据。 在内存中开辟的两个缓冲区:\n输入缓冲区用于暂存由输入设备送来的数据，以后再传送到输入井。 输出缓冲区用于暂存从输出井送来的数据，以后再传送到输出设备。 过程：\n输入：输入进程模拟脱机输入时的外围控制机，将用户要求的数据从输入设备通过输入缓冲区再送到输入井。当CPU需要输入数据时，直接将数据从输入井读入内存。\n输出：输出进程模拟脱机输出时的外围控制机，把用户要求输出的数据先从内存送到输出井，待输出设备空闲时，再将输出井中的数据经过输出缓冲区送到输出设备。\n综上，实质上就是利用输入井作为输入时的缓存区；输出井作为输出时的缓存区。\n为什么说是缓存区呢？其实，输入输出井就是在磁盘的缓存中开辟的区域，属于高速辅存。\nSPOOLing技术举例 共享打印机是使用SPOOLing技术的一个实例，这项技术已被广泛的用于多用户系统和局域网络中。当用户进程请求打印输出时，SPOOLing系统同意为它打印输出，但并不真正立即把打印机分配给该用户进程，而只为她做两件事：\n1）由输出进程在输出井中为之申请一个空闲磁盘块区，并将要打印的数据送入其中。\n2）输出进程再为用户进程申请一张空白的用户请求打印表，并将用户的打印要求填入其中，再将该表挂到请求打印队列中。\nSPOOLing系统的特点是：提高了IO速度；将独占设备改造为共享设备；实现了虚拟设备功能。\n设备分配与回收 设备分配的基本任务是根据用户的IO请求，为他们分配所需的设备。设备分配的总原则是充分发挥设备的使用效率，尽可能地让设备忙碌，又要避免由于不合理的分配方法造成进程死锁。\n从设备的特性来看，可以把设备分成独占设备、共享设备和虚拟设备三类。\n对于独立设备，将一个设备分配给某进程后，便有该进程独占，直至该进程完成或释放该设备。\n对于共享设备，可以同时分配给多个进程使用，但需要对这些进程访问该设备的先后次序进行合理的调度。\n虚拟设备属于可共享设备，可以将它同时分配给多个进程使用。\n设备管理的数据结构 设备分配依据的主要数据结构有设备控制表（DCT）、控制器控制表（COCT）、通道控制表（CHCT）和系统设备表（SDT），各数据结构功能如下：\n设备控制表：系统为每一个设备配置一张DCT，它用于记录设备的特性以及与IO控制器连接的情况。DCT包括设备标示符、设备类型、设备状态、指向COUCT的指针等。其中，设备队列指针指向等待使用该设备的进程组成的等待队列，控制器表指针指向于该设备相连接的设备控制器。 控制器控制表：每个控制器都配有一张COCT，它反应设备控制器的使用状态以及和通道的连接情况等。 通道控制表：每个通道配有一张CHCT。 系统设备表：整个系统只有一张SDT，它记录已连接到系统中的所有物理设备的情况，每个物理设备占一个表目。 由于在多道程序系统中，进程数多于资源数，会引起资源的竞争。因此，要有一套合理的分配原则，主要考虑的因素有：IO设备的固有属性，IO设备的分配算法，设备分配的安全性，以及设备独立性。\n设备分配原则与算法 设备分配原则。设备的分配原则应根据设备特性、用户要求和系统配置的情况来决定。设备分配的总原则既要充分发挥设备的使用效率，又要避免造成进程死锁，还要将用户程序和具体设备隔离开。\n设备分配方式。设备分配方式有静态分配和动态分配两种。\n静态分配\n主要用于对独占设备的分配，它在用户作业开始执行前，有系统一次性分配该作业所要求的全部设备、控制器（和通道）。一旦分配后，这些设备、控制器（和通道）就一直为高作业所占用，直到该作业被撤销。静态分配方式不会出现死锁，但设备的使用效率较低。因此，静态分配方式并不符合分配的总原则。\n动态分配\n是在进程执行过程中，根据执行需要进行分配。当进程需要设备时，通过系统调用命令向系统提出设备请求，由系统按照事先规定的策略给进程分配所需要的设备、IO控制器，一旦用完之后，便立即释放。动态分配方式有利于提高设备的利用率，但如果分配算法使用不当，则有可能造成进程死锁。\n设备分配算法。常用的动态设备分配算法有先请求先分配、优先级高者优先等。\n对于独占设备，即可以采用动态分配方式也可以静态分配方式，往往采用静态分配方式，即在作业执行前，将作业所要用到的这一类设备分配给它。\n共享设备可被多个进程所共享，一般采用动态分配方式，但在每个IO传输的单位时间内只被一个进程所占有，通常采用先请求先分配和优先级高者先分的分配算法。\n设备分配安全性和独立性 设备分配的安全性是指设备分配中应防止发生进程死锁。\n安全分配方式。每当进程发出IO请求后便进入阻塞状态，直到其IO操作完成时才被唤醒。这样，一旦进程已经获得某种设备后便阻塞，不能再请求任何资源，而且在它阻塞时也不保持任何资源。\n优点是设备分配安全；缺点是CPU和IO设备是串行工作的。\n不安全分配方式。进程在发出IO请求后继续运行，需要时发出第二个、第三个IO请求等。仅当进程所请求的设备已被另一进程占用时，才进入阻塞状态。\n优点是一个进程可以同时操作几个设备，从而使进程推进迅速；缺点是这种设备分配有可能产生死锁。\n为了提高设备分配的灵活性和设备的利用率、方便实现IO重定向，因此引入了设备独立性。设备独立性是指应用程序独立于具体使用的物理设备。\n为了实现设备独立性，在应用程序中使用逻辑设备名来请求使用某类设备，在系统中设置一张逻辑设备表（LUT），用于将逻辑设备名映射为物理设备名。LUT表项包括逻辑设备名、物理设备名和设备驱动程序入口地址；当进程用逻辑设备名来请求分配设备时，系统为他分配相应的物理设备，并在LUT中建立一个表项，以后进程再利用逻辑设备名请求IO操作时，系统通过查找LUT来寻找相应的物理设备和驱动程序。\n在系统中可采取两种方式建立逻辑设备表：\n在整个系统中只设置一张LUT表。这样，所有进程的设备分配情况都记录在这张表中，故不允许有相同的逻辑设备名，主要适用于单用户系统中。\n为每个用户设置一张LUT。当用户登录时，系统便为用户建立一个进程，同时也位置建立一张LUT，并肩改变放入进程的PCB中。\n设备分配步骤 根据进程请求的逻辑设备名查找SDT；（用户编程时提供的逻辑设备名就是“设备名”） 查找SDT，找到用户进程指定类型的、并且空闲的设备，将其分配给该进程。操作系统在逻辑设备表LUT中新增一个表项。 根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列，不忙碌则将控制器分配给进程； 根据COCT找到CHCT，若通道忙碌则将进程挂到通道等待队列中，不忙碌则将通道分配给进程。 设备回收步骤 设备回收就是设备分配的逆操作。\n缓冲区的管理 操作系统总是用磁盘高速缓存技术来提高磁盘的IO速度，对高速缓存复制的访问要比原始数据访问更为高效。例如，正在运行的进程的指令即存储在磁盘上，也存储在物理内存上，也被复制到CPU的二级和一级高速缓存中。\n不过，磁盘高速缓存技术不同于通常意义下的介于CPU与内存之间的小容量高速存储器，而是利用内存中的存储空间来暂存从磁盘中读出的一系列盘块中的信息，因此，磁盘高速缓存在逻辑上属于磁盘，物理上则是驻留在内存中的盘块。\n高速缓存在内存中分为两种形式：一种是在内存中开辟一个单独的存储空间作为磁盘高速缓存，大小固定；另一种是把未利用的内存空间作为一个缓冲池，共请求分页系统和磁盘IO时共享。\n在设备管理子系统中，引入缓冲区的目的有：\n1）缓和CPU与IO设备间速度不匹配的矛盾。\n2）减少对CPU的中断频率，放宽对CPU 中断响应时间的限制。\n3）解决基本数据单元大小不匹配的问题。\n4）提高CPU和IO设备之间的并行性。\n其实现方法有：\n1）采用硬件缓冲器，但由于成本太高，出一些关键部位外，一般情况下不采用硬件缓冲器。\n2）采用缓冲区（位于内存区域）\n根据系统设置缓冲器的个数，缓冲技术可以分为以下几种\n单缓冲 在设备和处理器之间设置一个缓冲区。设备和处理器交换数据时，先把被交换数据写入缓冲区，然后把需要数据的设备或处理器从缓冲区取走数据。\n写入数据和取走数据不能同时进行，只能串行执行，所以原则是\n非空不写 未满不读 在块设备输入时，假定从磁盘把一块数据输入到缓冲区的时间为T，操作系统将该缓冲区中的数据局传送到用户区的时间为M，而CPU对这一块数据处理的时间为C。由于T和C是可以并行的，所以可把系统对每一块数据的处理时间表示为Max（C,T）+M。\n双缓冲 双缓冲区机制又称缓冲对换。IO设备输入数据时先输入到缓冲区1，直到缓冲区1满后才输入到缓冲区2，此时操作系统可以从缓冲区1中取出数据放入用户进程，并由CPU计算。\n双缓冲的使用提高了处理器和输入设备的并行操作的程度。\n系统处理一块数据的时间可以粗略地认为是Max（C,T）。如果CT，则可使CPU不必等待设备输入。对于字符设备，若采用行输入方式，则采用双缓冲可使用户再输入完第一行之后，在CPU执行第一行中的命令的同事，用户可继续向第二缓冲区输入下一行数据。而单缓冲情况下则必须等待一行数据被提取完毕才可输入下一行的数据。\n如果两台机器之间通信仅配置了单缓冲，那么，他们在任意时刻都只能实现单方向的数据传输。为了实现双向数据传输，必须在两台机器中都设置两个缓冲区，一个用作发送缓冲区，另一个用作接收缓冲区。\n循环缓冲 包含多个大小相等的缓冲区，每个缓冲区中有一个缓冲区，最后一个缓冲区指针指向第一个缓冲区，多个缓冲区构成一个环形。用于输入输出时，还需要有两个指针in和out。对输入而言，首先要从设备接收数据到缓冲区中，in指针指向可以输入数据的第一个空缓冲区；当运行进程需要数据时，从循环缓冲去中去一个装满数据的缓冲区，并从此缓冲区中提取数据，out指针指向可以提取数据的第一个满缓冲区。输出正好相反。\n缓冲池 由多个系统共用的缓冲区组成，缓冲区按其使用状况可以形成三个队列：空缓冲队列、装满输入数据的缓冲队列（输入队列）和装满输出数据的缓冲队列（输出队列）。还应具有四种缓冲区：用于收容输入数据的工作缓冲区、用于提取输入数据的工作缓冲区、用于收容输出数据的工作缓冲区、用于提取输出数据的工作缓冲区。\n收容输入。在输入进程需要输入数据时，便调用Getbuf(emq)过程，从空缓冲队列emq的队首摘下一空缓冲区，把它作为收容输入工作缓冲区hin。然后，把数据输入其中，装满后再调用Putbuf(inq，hin)过程，将该缓冲区挂在输入队列 inq 上。 提取输入。当计算进程需要输入数据时，调用 Getbuf(inq)过程，从输入队列 inq 的队首取得一个缓冲区，作为提取输入工作缓冲区(sin)，计算进程从中提取数据。计算进程用完该数据后，再调用 Putbuf(emq，sin)过程，将该缓冲区挂到空缓冲队列 emq 上。 收容输出。当计算进程需要输出时，调用 Getbuf(emq)过程从空缓冲队列 emq 的队首取得一个空缓冲区，作为收容输出工作缓冲区 hout。当其中装满输出数据后，又调用Putbuf(outq，hout)过程，将该缓冲区挂在 outq 末尾。 提取输出。由输出进程调用 Getbuf(outq)过程，从输出队列的队首取得一装满输出数据的缓冲区，作为提取输出工作缓冲区 sout。在数据提取完后，再调用 Putbuf(emq，sout)过程，将该缓冲区挂在空缓冲队列末尾。 缓存（cache）与缓冲（buffer）的对比 缓存是指在计算机系统中，为了提高数据读写效率而设置的高速缓存存储器。缓存通常被用来存储经常被访问的数据，以减少对主存储器的访问次数，从而提高读写速度。缓存的数据可以是来自外部存储器的数据，也可以是来自CPU内部的数据。\n缓冲是指在计算机系统中，为了解决数据传输速度不匹配而设置的临时存储区。缓冲通常被用来存储数据，以平衡数据的输入和输出速度。缓冲的数据通常是来自输入输出设备的数据，例如磁盘、网络等，缓冲可以将这些数据暂时存储起来，等待处理器或者其他设备的处理。\n简单来说，缓存是为了提高数据读写效率而设置的高速存储器，缓冲是为了解决数据传输速度不匹配而设置的临时存储区。\n出错处理 操作系统可以采用内存保护，这样一来就可以预防许多硬件和应用程序的错误，即便有一些设备硬件上的适龄也不回导致系统的完全崩溃。\nIO设备传输中出现的错误很多，如网络上的堵塞和传输过载等。操作系统可以对一些短暂的出错进行处理，比如读取磁盘出错，那么可以选择重新对磁盘进行read操作；再比如在网络上发送数据出错，那么只要网络通信协议允许，就可以做resend操作。但是，如果计算机系统中的重要组件出现了永久性错误，那么操作系统将无法恢复。\n作为一个规则，IO系统调用通常返回一位调用状态信息，以表示成功或失败。在UNIX系统中，用一个名为errno的全局变量来表示出错代码，以表示出错原因。\n注意：read、send和resend都是操作系统的基本输入输出命令，分别用来读、发送和重发数据。\n","permalink":"https://cold-bin.github.io/post/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/","tags":["dma","spooling技术","io控制方式"],"title":"操作系统之输入输出管理"},{"categories":["操作系统"],"contents":"[toc]\n文件系统基础 文件的概念 由于系统的内存有限并且不能长期保存，故平时总是把它们以文件的形式存放在外存中，需要时再将它们调入内存。如何高效的对文件进行管理是操作系统实现的目标。\n文件和文件系统 现代OS几乎都是通过文件系统来组织和管理在计算机中所存储的大量程序和数据的。文件系统的管理功能是通过把它所管理的程序和数据组织成一系列文件的方法来实现的。而文件则是指具有文件名的若干相关元素的集合。元素通常是记录，而记录是一组有意义的数据项的集合。可以把数据组成分为数据项、记录、文件。\n数据项，数据项是最低级数据组织形式。分为基本数据项（用于描述一个对象某种属性的字符集，是数据组织中可以明明的最小逻辑数据单位，即原子数据，又称为数据元素或字段）和组合数据项（由若干个基本数据项组成） 记录，是一组相关数据项的集合，用于描述一个对象在某方面的属性，为了能够唯一标识一个记录，需要在记录中确定一个或集合数据项，把他们的集合称为关键字，关键字是能够唯一标识一个记录的数据项。 文件，文件是具有文件名的一组相关元素的集合，分为有结构文件和无结构文件。有结构文件由若干个相关记录组成，无结构文件则被看成一个字符流。文件是文件系统的最大数据单位。文件应该具有自己的属性，包括文件类型（如源文件、目标文件、可执行文件等），文件长度（文件的当前长度，也可能是最大允许长度），文件的物理位置（指示文件在哪一个设备上及在该设备的哪个位置的指针），文件的建立时间（文件最后一次修改时间）。 一个文件可对应若干个记录，一个记录可对应若干个数据项。\n文件系统管理的对象有：文件（作为文件管理的直接对象），目录（为了方便用户对文件的存取和检索，在文件系统中配置目录，每个目录项中，必须含有文件名及该文件所在的物理地址，对目录的组织和管理是方便和提高对文件存取速度的关键），磁盘（磁盘）存储空间（文件和目录必定占用存储空间，对这部分空间的有效管理，不仅能提高外存的利用率，而且能提高对文件的存取速度）。\n文件操作 创建文件，在创建一个新文件时，系统首先要为新文件分配必要的外存空间，并在文件系统的目录中，为之建立一个目录项，目录项中应该记录新文件的文件名及其在外存的地址等属性。 删除文件，当已不再需要某文件时，可将其从文件系统中删除。在删除时，系统应先从目录中找到要删除文件的目录项，使之成为空项，然后回收该文件所占用的存储空间。 读文件，读文件时，须在相应系统调用中给出文件名和应读入的内存目标地址。此时，系统要查找目录，找到指定目录项，从中得到被读文件在外存中的位置。在目录项中，还有一个指针用于对文件进行读/写。 写文件，写文件时，须在相应系统调用中给出文件名和其在内存源地址。此时，系统要查找目录，找到指定目录项，从再利用目录中的写指针进行写操作。 截断文件，如果一个文件的内容已经陈旧而需要全部更新时，一种方法是将此文件删除，再重新创建一个新文件，但如果文件名和属性均无改变，则可采取截断文件的方法，其将原有的文件长度设置为0，放弃原有文件的内容，再将新内容读入。 设置文件的读/写位置，用于设置文件读/写指针的位置，以便每次读/写文件时，不需要从始端开始而是从所设置的位置开始操作。可以改顺序存取为随机存取。 当前OS所提供的大多数对文件的操作，其过程大致都是这样两步：\n首先，检索文件目录来找到指定文件的属性及其在外存上的位置；\n然后，对文件实施相应的操作，如读/写文件等。\n当用户要求对一个文件实施多次读/写或其他操作时，每次都要从检索目录开始，为了避免多次重复地检索目录，在大多数OS中都引入了打开这一文件系统调用，当用户第一次请求对某文件系统进行操作时，先利用open系统调用将该文件打开。打开是指系统将指名文件的属性（包括该文件在外存上的物理位置）从外存拷贝到内存打开文件表的一个表目中，并将该表目的编号（索引号）返回给用户，以后，当用户再要求对该文件进行操作时，便可利用系统所返回的索引号向系统提出操作请求，系统便可直接利用该索引号到打开文件表中去查找，从而避免了对该文件的再次检索，如果用户不再需要对该文件实施操作，可利用关闭系统调用来关闭此文件，OS将会把该文件从打开文件表中的表目上删除掉。\n小tips：如果没有调用close系统调用来关闭文件资源，操作系统会在程序结束时自动关闭文件资源。但是，如果程序在关闭文件之前崩溃或意外退出，文件资源可能会一直处于打开状态，导致资源泄漏。此外，如果程序打开了大量文件而没有关闭它们，可能会导致系统资源不足，从而影响其他程序的运行。因此，为了保证程序的稳定性和性能，建议在读写文件完毕后及时调用close系统调用来关闭文件资源。\n文件的结构 对任何的文件，都存在以下两种形式的结构\n文件的逻辑结构，这是从用户观点出发所观察到的文件组织形式，是用户可以直接处理的数据及其结构，独立于文件的物理特性，又称为文件组织。\n文件的物理结构，又称为文件的存储结构，是指文件在外存上的存储组织形式，不仅与存储介质有关，还与外存分配方式有关。\n文件物理结构涉及硬件，不是操作系统关注的重点\n文件逻辑结构的类型 文件的逻辑结构可分为两大类，一类是有结构文件，这是指由一个以上的记录构成的文件，故把他称为记录式文件，另一类是无结构文件，这是指由字符流构成的文件，又称为流式文件。\n有结构文件（记录式文件）\n每个记录都用于描述实体集中的一个实体，各记录有着相同或不同数目的数据项，记录分为定长记录（文件中所有记录的长度都是相同的，所有记录中的各数据项都处在记录中相同的位置，具有相同的顺序和长度）和变长记录（文件中个记录的长度不相同，可能由于一个记录中所包含的数据项目并不相同）。\n根据用户和系统的需要，可采用多种方式来组织这些记录，如顺序文件（记录按照某种顺序排列所形成的文件，记录通常是定长的，能较快查找到文件中的记录），索引文件（记录为可变长度时，通常建立一张索引表，并为每个记录设置一个表项，加快对记录检索的速度），索引顺序文件（为文件建立一张索引表，为每一组记录中的第一个记录设置一个表项）。\n无结构文件（流式文件）\n对于源程序、可执行文件、库函数等通常采用的是无结构文件形式，即流式文件，其长度以字节为单位。\n库函数文件通常是库函数文件通常是二进制文件，其文件类型取决于库函数的编译方式和目标平台。在Unix/Linux系统中，常用的库函数文件扩展名为.so（共享对象）或.a（静态库），在Windows系统中则通常是.dll（动态链接库）或.lib（静态库）。这些库文件包含了编译好的函数和代码，可以被程序调用和链接，以实现特定的功能。库函数文件可以由开发者自己编写，也可以使用第三方库，如标准C库或开源库等。\n顺序文件 顺序文件是有结构文件。文件是记录的集合，它可以按照各种不同的顺序进行排列，一般地，可归纳为以下两种情况。\n链表结构（变长），各记录之间的顺序与关键字无关，通常按照时间先后排序，最先存入的记录作为第一个记录，其次，为第二个记录，以此类推。链表结构存储检索效率低下，所以定位删除、更新、查询的效率都较为低下。 顺序结构（定长），文件中所有记录按照关键字排列，可以按照关键词长度从大到小排列。顺序结构的检索效率更高，但增加删除效率低下，需要删除原分配空间，另外分配一个更小，将所有没有删除的数据拷贝到新分配好的空间中去，可以类比二维数组实现。 顺序文件的最佳应用场合是在对诸记录进行批量存取时，即每次要读或写一大批记录时，此时，对顺序文件的存取效率是所有逻辑文件中最高的，此外，只有顺序文件才能存储在磁带上，并能有效工作。但是想要增加或删除一个文件比较困难。\n索引文件 对于定长记录文件，顺序文件的实现，可以方便的实现顺序存取和直接存取。然而，顺序文件对于变长记录就很难实现。为了解决变长记录检索问题，可为变长记录文件建立一张索引表，对主文件中的每个记录，在索引表中设有一个相应的表项，用于记录该记录的长度L及指向该记录的指针（指向该记录在逻辑地址空间的首址），由于索引表示按记录键排序的，因此，索引表本身是一个定长记录的顺序文件。从而可以方便实现直接存取。\n在对索引文件进行检索时，首先根据用户（程序)提供的关键字，并利用折半查找检索索引表，从中找到相应的表项，再利用该表项给出的指向记录的指针值，去访问所需的记录。每当要向索引文件中增加一个新纪录时，便须对索引表进行修改。索引表的问题在于除了有主文件外，还需要配置一张索引表，每个记录需要有一个索引项，因此提高了存储费用。\n索引顺序文件 其有效克服了变长记录不便于直接存取的缺点，而且所付出的代价也不算太大，它是顺序文件和索引文件相结合的产物，它将顺序文件中的所有记录分为若干个组，为顺序文件建立一张索引表，在索引表中为每组中的第一个记录建立一个索引项，其中含有该记录的键值和指向记录的指针。\n在对索引顺序文件进行检索时，首先利用用户（程序）所提供的关键字及某种查找算法去检索索引表，找到该记录组中的第一个记录的表项，从中得到该记录组第一个记录在主文件中的位置，然后，再利用顺序查找法去查找主文件，从中找出所要求的记录。\n类似于字典查找，先找拼音或部首缩小范围，再找具体的字。\n直接文件 对于直接文件，则根据给定的记录键值，直接获得指定记录的物理地址，换言之，记录键值本身就决定了记录的物理地址，这种由记录键值到记录物理地址的转换被称为键值转换。\n哈希文件 利用Hash函数可将记录键值转换为相应记录的地址，为了能实现文件存储空间的动态分配，通常由Hash函数所求得的并非是相应记录的地址，而是指向一目录表相应表目的指针，该表目的内容指向相应记录所在的物理块。\n文件的目录结构 为了能够对文件实施有效的管理，必须对它们加以妥善组织，这主要是通过文件目录实现的，文件目录也是一种数据结构，用于标识系统中的文件及其物理地址，供检索时使用，对目录的管理要求如下：\n实现按名存取，即用户只须向系统提供所需访问的文件的名字，便能够快速准确地找到指定文件在外存上的存储位置，这是目录管理中最基本的功能。 提高对目录检索速度，通过合理地组织目录结构的方法，可加快对目录的检索速度，从而提高对文件的存取速度。 文件共享，在多用户系统中，应该允许用户共享一个文件。 允许文件重名，系统应允许不同用户对不同文件采用相同的名字，以便用户按照自己的习惯给文件命名和使用文件。 文件控制块（FCB） 一个FCB条目可能是文件，也可能是目录，但不能即是文件，又是目录。因为目录也是文件的一种。FCB的有序集合称为文件目录，一个FCB就是一个文件目录项。为了创建一个新文件，系统将分配一个FCB并存放在文件目录中，称为目录项。\n值得注意的是，FCB是存储在目录中，而并不是目录项中。也就是说，目录文件会存储当前目录里的FCB，而当前目录的FCB是在上级目录中存放\n基本信息：文件名、类型、物理位置 存取控制权限信息：文件读写权限 使用信息：创建时间等 索引节点index node 理解inode，要从文件储存说起。文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点” 。\ninode包含内容 Linux中目录的数据块中的每一项中都包含了文件名和其对应的inode。inode记录了文件的属性以及该文件实际存储位置，即数据块号（block number），每一个block（常见大小4KB），通过inode可以实现文件的查找定位。inode是Linux中的，Unix中是vnode。\n基本上，inode包含的信息至少有如下这些：（1）文件的类型 （2）文件访问权限； （3）文件的所有者与组； （4）文件的大小； （5）链接数，即指向该inode的文件名总数； （6）文件的状态改变时间（ctime）、最近访问时间（atime）和最近修改时间（mtime）； （7）文件特殊属性，SUID、SGID和SBIT； （8）文件内容的真正指向（pointer）。\n可以用stat命令，查看某个文件的inode信息。\n每个文件都只占用一个inode。因此，文件系统能够建立的文件数量与inode数量有关。系统读取档案时需要先找到inode，并分析inode所记录的权限与用户是否符合，若符合才能够开始实际读取block的内容。\n操作系统读取磁盘文件的流程 根据给定的文件的所在目录，获取该目录的数据实体。再根据数据实体中的数据项，找到对应文件的inode（拿到了inode地址）； 根据文件inode，找到inodeTable； 根据inodeTable中的对应关系，找到对应的block； 读取文件。 inode的优点 对于有些无法删除的文件可以通过删除inode节点来删除； 移动或者重命名文件，只是改变了目录下的文件名到inode的映射，并不需要实际对硬盘操作； 删除文件的时候，只需要删除inode，不需要实际清空那块硬盘，只需要在下次写入的时候覆盖即可（这也是为什么删除了数据可以进行数据恢复的原因之一）； 打开一个文件后，只需要通过inode来识别文件。 inode和FCB的关系 FCB = 文件名 + inode的所有内容 FCB是在文件打开时创建的，而inode是在文件创建时就被创建的 FCB随着文件关闭而释放，inode并不会 寻找文件时，需要通过inode包含的文件物理地址查询 目录结构 目录结构的组织，关系到文件系统的存取速度，也关系到文件的共享性和安全性，目前常用的目录结构形式有单级目录、两级目录、多级目录。\n单级目录 在整个系统中只建立一张目录表，每个文件占一个目录项，目录项中含文件名、文件扩展名、文件长度、文件类型、文件物理地址、状态位（表示目录项是否空闲）等。\n缺点：\n查找速度慢：所有文件都在同一个目录下，文件越多，查询越慢 无法实现文件重名：在同一目录下，没有办法实现重名，只能每个文件都取不同名字 不便于实现多用户的文件共享：单级目录下，多个用户使用相同的文件名对一个文件进行访问，但不能实现多个用户使用不同文件名对一个文件进行访问。 两级目录 为每个用户建立一个单独的用户文件目录UFD（User File Directory），这些文件目录具有相似的结构，由用户所有文件的文件控制块组成。此外，系统中还有一个主文件目录MFD（Master File Directory），在主文件目录中，每个用户目录文件都占有一个目录项，其目录项包括用户名和指向用户目录文件的指针。\n优点：\n相对于单级目录，提高了检索速度 相对于单级目录，不同用户可以使用相同的文件名 不同用户还可以使用不同文件名访问系统中的同一个共享文件 缺点：\n但在多个用户需要合作完成一个大任务时，不便于用户之间共享文件 多级目录 对于大型文件系统，通常采用三级或三级以上的目录结构，以提高对目录的检索速度和文件系统的性能。多级目录结构又称为树形目录结构，主目录被称为根目录，把数据文件称为树叶，其他的目录均作为树的结点。\n说明：方框代表目录文件，圆圈代表数据文件，主目录中有是哪个用户总目录A、B、C，在B用户的总目录B中，又包括三个分目录F、E、D，其中每个分目录中又包含多个文件，为提高系统的灵活性，应该允许在一个目录文件中的目录项既是作为目录文件的FCB，又是数据文件的FCB，这一信息可用目录项中的一位来指示。如用户A总目录中，目录项A是目录文件FCB，而目录项B和D则是数据文件的FCB。\n在树形目录结构中，从根目录到任何数据文件，都只有一条唯一的通路，在该路径上从树的根开始，把全部目录文件名和数据文件名依次用\u0026quot;/\u0026ldquo;连接起来，即构成该数据文件的路径名。系统中的每个文件都有唯一的路径名。例如，用户B访问文件J，则使用路径名/B/F/J来访问。\n当一个文件系统含有很多级时，每访问一个文件，都要使用从树根开始直到树叶（数据文件）为止的、包含各中间节点（目录）的全路径名，这非常麻烦，可为每个进程设置一个当前目录，又称为工作目录，进程对各文件的访问都相对于当前目录而进行的。把从当前目录开始值得数据文件为止所构成的路径名称为相对路径名，而把从树根开始的路径名称为绝对路径名。\n增加或删除目录 增加目录没啥好说的。\n删除目录为空：简单地将其删除，使它在其上一级目录中所对应的目录项为空 不删除非空目录：当目录不为空时，为了删除一个非空目录，必须先删除目录中所有的文件，使之称为空目录，然后再删除，如果目录中包含有子目录，则应该递归调用方式删除 删除非空目录：将目录中的所有文件和子目录同时删除 文件的共享 文件共享使多个用户（进程）共享同一份文件，系统中只需保留该文件的一份副本。如果系统不能提供共享功能，那么每个需要该文件的用户都要有各自的副本，会造成对存储空间的极大浪费。随着计算机技术的发展，文件共享的范围已由单机系统发展到多机系统，进而通过网络扩展到全球。\n这些文件的分享是通过分布式文件系统、远程文件系统、分布式信息系统实现的。这些系统允许多个客户通过C/S模型共享网络中的服务器文件。\n现代常用的两种文件共享方法有：硬链接和软链接\n硬链接：基于索引结点的共享方式 在树形结构的目录中，当有两个或多个用户要共享一个子目录或文件时，必须将共享文件或子目录链接到两个或多个用户的目录中，才能方便地找到该文件，如图所示\n在这种共享方式中引用索引结点，即诸如文件的物理地址及其他的文件属性等信息，不再是放在目录项中，而是放在索引结点中。\n在文件目录中只设置文件名及指向相应索引结点的指针。在索引结点中还应有一个链接计数count,用于表示链接到本索引结点（亦即文件）上的用户目录项的数目。只有当count=0时，也就是文件没有其他用户被硬链接，此时可以删除掉文件；count\u0026gt;0时，不能删除文件。\n优点：\n读取效率高：多个用户访问共享文件时，无需经过路径名，只需要找到当前用户的共享文件名，即可拿到索引节点； 存储开销小：共享文件只有一个索引节点，没有额外存储开销。 缺点：\n可能会出现访问共享文件异常空指针地情况：如果删除掉索引节点时，还有用户保留了共享文件。那么这个共享文件的inode指针将会为空，访问会出错。 不好实现网络文件地共享 示例说明 当count=2时，表示有两个用户目录项链接到本文件上，或者说是有两个用户共享此文件。\n当用户A创建一个新文件时，它便是该文件的所有者，此时将count置为1。\n当有用户 B要共享此文件时，在用户B的目录中增加一个目录项，并设置一指针指向该文件的索引结点。\n此时，文件主仍然是用户A，count=2。如果用户A不再需要此文件，不能将文件直接删除。\n因为，若删除了该文件，也必然删除了该文件的索引结点，这样便会便用户B的指针悬空，而用户B则可能正在此文件上执行写操作，此时用户B会无法访问到文件。\n因此用户A不能删除此文件，只是将该文件的count减1，然后删除自己目录中的相应目录项。用户B仍可以使用该文件。\n当count=0时，表示没有用户使用该文件，系统将负责删除该文件。\n软链接：利用符号链实现文件共享 为使用户B能共享用户A的一个文件F,可以由系统创建一个LINK类型的新文件，也取名为F，并将文件F写入用户B的目录中，以实现用户B的目录与文件F的链接。\n在新文件中只包含被链接文件F的路径名或URL。这样的链接方法被称为符号链接。\n值得注意的是：在利用符号链方式实现文件共享时，只有文件的拥有者才拥有指向其索引结点的指针；而共享该文件的其他用户则只有该文件的路径名，并不拥有指向其索引结点的指针，也就是说，通过符号链的方式共享文件时，需要通过路径名来查找索引节点，显然比较慢。而且，每个软链接文件都需要创建一个link类型文件的索引节点，增加了存储开销。\n优点：\n不存在异常悬空指针的情况：当文件的拥有者把一个共享文件删除后，其他用户通过符号链去访问它时，会出现访问失败，于是将符号链删除，此时不会产生任何影响。 方便地获取网络文件：网络共享只需提供该文件所在机器的网络地址以及该机器中的文件路径 缺点：\n读取效率较低：当其他用户读共享文件时，需要根据文件路径名逐个地查找目录，直至找到该文件的索引结点。因此，每次访问时，都可能要多次地读盘，使得访问文件的开销变大并增加了启动磁盘的频率； 存储开销更大：符号链文件共享就是创建一个link类型的文件，所以也需要创建一个索引节点来保存link类型文件的元信息； 文件的保护 为了防止文件共享可能会导致文件被破坏或未经核准的用户修改文件，文件系统必须控制用户对文件的存取，即解决对文件的读、写、执行的许可问题。\n为此，必须在文件系统中建立相应的文件保护机制。\n文件保护通过口令保护、加密保护和访问控制等方式实现。其中，口令保护和加密保护是为了防止用户文件被他人存取或窃取，而访问控制则用于控制用户对文件的访问方式。\n口令保护与加密保护 口令保护：用户需要输入对的口令，才能拿到索引节点，之后再通过索引节点中文件物理地址去访问磁盘上的文件。但是口令保护有个无法忽视的缺点，那就是如果某用户越过了索引节点，直接拿到文件物理地址，口令保护就会失去作用； 加密保护：对存储的文件进行可靠加密，只有解密才能拿到文件内容，可以很好的起到文件保护作用。 访问控制 控制的含义就是控制不同文件的操作权限。解决访问控制最常用的方法是根据用户身份进行文件操作控制。\n而实现基于身份访问的最为普通的方法是为每个文件和目录增加一个访问控制列表(Access-Control List, ACL)，以规定每个用户名及其所允许的访问类型。\n这种方法的优点是可以使用复杂的访同方法。其缺点是长度无法预期并且可能导致复杂的空间管理，使用精简的访问列表可以解决这个问题。\n精简的访问列表釆用拥有者、组和其他三种用户类型。\n拥有者：创建文件的用户。 组：一组需要共享文件且具有类似访问的用户。 其他：系统内的所有其他用户。 这样只需用三个域列出访问表中这三类用户的访问权限即可。\n文件拥有者在创建文件时，说明创建者用户名及所在的组名，系统在创建文件时也将文件主的名字、所属组名列在该文件的FCB中。\n用户访问该文件时，按照拥有者所拥有的权限访问文件，如果用户和拥有者在同一个用户组则按照同组权限访问，否则只能按其他用户权限访问。UNIX操作系统即釆用此种方法。\n文件系统实现 文件系统层次结构 文件系统接口。文件系统为用户提供与文件及目录有关的调用，如新建、打开、读写、关闭、删除文件，建立、删除目录等。此层由若干程序模块组成，每一模块对应一条系统调用，用户发出系统调用时，控制即转入相应的模块。 文件目录系统的主要功能是管理文件目录，其任务有管理活跃文件目录表、管理读写状态信息表、管理用户进程的打开文件表、管理与组织在存储设备上的文件目录结构、调用下一级存取控制模块。 实现文件保护主要由存取控制模块完成，它把用户的访问要求与FCB中指示的访问控制权限进行比较，以确认访问的合法性。 逻辑文件系统与文件信息缓冲区的主要功能是根据文件的逻辑结构将用户要读写的逻辑记录转换成文件的逻辑结构内的相应块号。 物理文件系统的主要功能是把逻辑记录所在的相对块号转换成实际的物理地址。 辅助分配模块的主要功能是管理辅存空间，即负责分配辅存空闲空间和回收辅存空间。 设备管理程序模块的主要功能是分配设备、分配设备读写缓冲区、磁盘调度、启动设备、处理设备中断、释放设备读写缓冲区、释放设备等。 目录实现 在读文件前，必须先打开文件。打开文件时，操作系统利用路径名找到相应目录项，目录项中提供了查找文件磁盘块所需要的信息，目录实现的基本方法有线性列表和哈希表两种方法。\n目录就是FCB的集合\n线性表 线性表的项是由文件名和数据块指针组成。（数据块指针指向的可能是子目录，也可能是一个具体的文件）\n下图的画法是链表实现，当然，也可以是数组实现。\n**优点：**实现较为简单\n**缺点：**线性表的增删操作较为复杂耗时，而且查询也比较耗时(O(n))\n哈希表 哈希表根据文件名得到一个值，并返回一个指向线性列表中元素的指针。\n**优点：**查询目录速度更快\n**缺点：**可能会发生哈希冲突，导致文件可能会被覆盖；增删文件时，会触发重哈希，较为耗损性能。\n目录查询必须通过在磁盘上反复搜索完成，需要不断的进行IO操作，开销较大。所以如前面所述，为了减少IO操作，把当前使用的文件目录复制到内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数。\n文件实现 文件分配对应于文件的物理结构，是指如何为文件分配磁盘块。常用的磁盘空间分配方式有三种：连续分配、链接分配和索引分配。有的系统对三种方式都支持，但是更普遍的是一个系统只提供一种方法支持。\n文件分配方式 连续分配 如果将块分配给文件，使得文件的所有逻辑块都得到硬盘中的连续物理块，则这种分配方案被称为连续分配。\n优点：\n实现简单。只需要在目录项中记录文件名、起始地址和文件长度即可。 可以获得优秀的读取性能。通过连续分配磁盘空间块的方式，作业访问磁盘时需要的寻道数和寻道时间最小。 支持随机访问文件。可以通过指针指定当前的读写位置，后续就可以直接再这个指针上进行偏移读取文件任何部分内容。（因为文件在磁盘上的数据是连续的，所以可以不断地偏移指针） 缺点：\n文件长度不能随便动态增加。因为一个文件末尾后的盘块可能已经分配给其他文件，一旦需要增加，就需要大量移动盘块。 外部碎片增加。反复增删文件后会产生外部碎片。所以给文件连续分配磁盘空间块时，比较适合文件长度固定的文件。 链接分配 隐式链接分配 链接分配解决了连续分配的所有问题。也就是说，链接分配可以实现文件长度动态增加，而且不存在外部碎片。\n在链接分配中，分配给特定文件的磁盘块不需要在磁盘上连续存在。分配给文件的每个磁盘块都包含一个指向分配给同一文件的下一个磁盘块的指针。\n优点\n链接分配没有外部碎片。通过链接的方式，所有的磁盘空闲块都能得到利用，所以也就没有外部碎片。 可以使用任何空闲块来满足文件块请求。只要是空闲块都可以被链接写入文件。 只要空闲块可用，文件可以继续增长。 目录条目将仅包含起始块地址。 缺点\n随机（直接）访问不提供。因为每个块直接并不是地址连续的，而是一个块包含下一个块的指针。所以，扫描一块后，就需要取出下一个块的地址，无法通过指针直接偏移到文件的任何位置。这个过程都是在磁盘里的。 额外的空间占用。指针在磁盘块中需要一些空间。 会有数据丢失风险。链接列表中的任何指针都不能被破坏，否则文件将被损坏。 文件访问性能较低。需要遍历每个块。 显式链接分配 链接分配的显式实现——文件分配表FAT。\n隐式链表分配的主要缺点是它不提供对特定块的随机访问。要访问一个块，我们还需要访问它之前的所有块。\n文件分配表克服了链表分配的缺点。在这个方案中，维护一个文件分配表，它收集所有的磁盘块链接。该表对每个磁盘块都有一个条目，并按块编号进行索引。\n优点\n使用整个磁盘块获取数据。 坏磁盘块不会导致所有连续的块丢失。FAT将指向下一个磁盘的指针和磁盘数据分离了。 提供随机访问，尽管它不太快。为什么说隐式链接分配提供了随机访问呢？因为通过FAT的记录，我们也可以做到指针确切偏移到文件存储磁盘的任意一个位置，只是相比于连续分配方式，隐式链接分配的指针偏移要更加复杂一点，需要在FAT中取出下一个要偏移的块。这个过程都是在内存进行的。 每个文件操作中只需要遍历FAT。 缺点\n每个磁盘块都需要一个FAT条目。 根据FAT条目的数量，FAT大小可能非常大。 可以通过增加块大小来减少FAT条目的数量，但也会增加内部碎片。 区别 显式链接分配方式，就是数据块和指向下一个数据块的指针都存于同一个磁盘块； 而隐式连接分配方式，就是将数据块和指向下一个数据块的指针进行分离，所有的指针存于FAT中，而FAT需要加载到内存里。 索引分配 FAT的限制：\n文件分配表尽量解决尽可能多的问题，但会导致一个缺点。 块的数量越多，FAT的大小就越大。\n因此，我们需要为文件分配表分配更多空间。 由于文件分配表需要被缓存，因此不可能在缓存中具有尽可能多的空间。 在这里我们需要一种可以解决这些问题的新技术。\n索引分配方案不是维护所有磁盘指针的文件分配表，而是将一个文件中所有磁盘指针存储在一个称为索引块的磁盘块中。 索引块不包含文件数据，但它保存指向分配给该特定文件的所有磁盘块的指针。目录条目将只包含索引块地址。\n优点\n支持直接访问。 坏数据块会导致只有该块的丢失。 缺点\n坏索引块可能导致整个文件丢失。 文件的大小取决于数据块的数量，索引块可以容纳。 文件较小时，会造成索引块的浪费。 更多的指针开销 如果文件太大，一个索引块无法存下时，那么就需要多个索引块进行索引，如何组织多个索引块呢？\n单级索引分配 在索引分配中，文件大小取决于磁盘块的大小。要允许大文件，我们必须将几个索引块链接在一起。在链接索引分配中，\n提供文件名称的小标题 前100个块地址的集合 指向另一个索引块的指针 对于较大的文件，索引块的最后一个条目是一个指向另一个索引块的指针。 这也被称为链接模式。\n优点: 它消除了文件大小限制 缺点: 随机访问变得有点困难\n多级索引分配 在多级指数分配中，有各种索引级别。 有外层索引块包含指向内层索引块的指针，内层索引块包含指向文件数据的指针。\n外层索引用于查找内层索引。 内层索引用于查找所需的数据块。 优点: 随机访问变得更好，更高效。 缺点: 文件的访问时间会更长；文件最大大小有限。\n值得注意的是，多级索引的索引层数，将决定能存储文件的容量上限，不能无休止的增加文件长度。\n文件存储空间管理 由于文件存储设备是分成若干个大小相等的物理块，并以块为单位来交换信息的，因此，文件存储空间的管理实质上是一个空闲块的组织和管理问题，它包括空闲块组织，空闲块的分配和空闲块的回收等几个问题。\n空闲表法 空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：\n当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。\n这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。\n空闲链表法 我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：\n当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。\n这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。\n空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。\n成组链表法 在 UNIX 系统中采用的是成组链接法，这是将上述两种方法相结合而形成的一种空闲盘块管理方法，它兼备了上述两种方法的优点而克服了两种方法均有的表太长的缺点。\n其大致思想是：把顺序的n个空闲扇区地址保存在第一个空闲扇区内，其后一个空闲扇区则保存另一顺序空闲扇区的地址和空闲块数，如此继续直至所有空闲扇区均予以链接。系统只需要保存一个指向第一个空闲扇区的指针。\n位示图法 位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。\n当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：\n1 1111110011111110001110110111111100111 ... 盘块分配与回收过程如下：\n在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。\n","permalink":"https://cold-bin.github.io/post/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/","tags":["链接"],"title":"操作系统之文件管理"},{"categories":["操作系统"],"contents":"[toc]\n内存管理概念 内存管理的基本原理和要求 存储器结构 上图展示了一个典型的存储器层次结构。一般而言，从高层往底层走，存储设备变得更慢、更便宜和更大。在最高层是少量快速的CPU 寄存器，CPU 可以再一个时钟周期内访问它们。接下来是一个或者多个小型到中型的基于 SRAM 的高速缓存存储器，可以再几个 CPU 时钟周期内访问它们。然后是一个大的基于 DRAM 的主存，可以在几十或者几百个时钟周期内访问它们。接下来是慢速但是容量很大的本地磁盘。最后有些系统甚至包括了一层附加的远程服务器上的磁盘，要通过网络来访问它们，例如网络文件系统（Network File System,NFS）这样的分布式文件系统，允许程序访问存储在远程的网络服务器上的文件。\n存储器层次结构的核心是，对于每个 k , 位于 k 层的更快更小的存储设备作为位于 k+1 层的更大更慢的存储设备的缓存。也就是说，层次结构中的每一层都缓存来自较低一层的数据对象。例如，本地磁盘作为通过网络从远程磁盘取出文件的缓存，以此类推知道 CPU 寄存器作为L1的缓存。\n进程运行原理 用户程序-\u0026gt;进程的过程\n创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤：\n编译：由编译程序将用户源代码编译成若干个目标模块（也就是机器码）。\n注意！！！\n编译并不能生成可执行的程序，仅仅只是将用户源代码编译成了若干个目标模块（机器码），需要链接后才能拿到可执行文件。\n链接：由链接程序将编译后形成的一组目标模块以及所需库函数链接在一起，形成一个完整的装入模块。\n生成了可执行文件，也就是装入模块.\n显然实际开发中引入他人编写好的库文件可以省略某些功能的开发环节，提高项目的开发效率。但遗憾的是，“开源”的库文件很难找到，多数程序员并不会直接分享源代码，他们更愿意分享库文件的二进制版本——链接库。\n所谓链接库（库函数文件），其实就是将开源的库文件进行编译、打包操作后得到的二进制文件。\n库函数文件通常是库函数文件通常是二进制文件，其文件类型取决于库函数的编译方式和目标平台。在Unix/Linux系统中，常用的库函数文件扩展名为.so（共享对象）或.a（静态库），在Windows系统中则通常是.dll（动态链接库）或.lib（静态库）。这些库文件包含了编译好的函数和代码，可以被程序调用和链接，以实现特定的功能。库函数文件可以由开发者自己编写，也可以使用第三方库，如标准C库或开源库等。\n虽然链接库是二进制文件，但无法独立运行，必须等待其它程序调用，才会被载入内存，但是已经编译成机器码了，只需等待调用即可运行了。\n装入：由装入程序将装入模块装入内存运行。\n编译 这部分并不是操作系统的工作，是由编程语言编译器完成。\n链接 程序的链接有以下三种方式：\n静态链接：在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开。\n缺点\n首先，可执行文件内部拷贝了所有目标文件和静态链接库的指令和数据，文件本身的体积会很大。 当系统中存在多个链接同一个静态库的可执行文件时，每个可执行文件中都存有一份静态库的指令和数据，就会造成内存空间的极大浪费。 优点\n动态链接库形成的可执行文件，可以放到其他机子上运行，前提是目标机器上有与该可执行文件所需的动态链接库版本兼容的库文件并且目标操作系统与编译时使用的操作系统兼容。而静态链接库，只有编译时使用的操作系统兼容这一条限制。 装入时动态链接：将用户源程序编译后所得到的一组目标模块，在装入内存时，釆用边装入边链接的链接方式。\n运行时动态链接：对某些目标模块的链接，是在程序执行中需要该目标模块时，才对它进行的链接。其优点是便于修改和更新，便于实现对目标模块的共享。\n装入 绝对装入\n在编译时，如果知道程序将驻留在内存的某个位置，编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同，故不需对程序和数据的地址进行修改。\n绝对装入方式只适用于单道程序环境。另外，程序中所使用的绝对地址,可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中釆用的是符号地址，编译或汇编时再转换为绝对地址。\n可重定位装入\n在多道程序环境下，多个目标模块的起始地址通常都是从0开始，程序中的其他地址都是相对于起始地址的,此时应釆用可重定位装入方式。根据内存的当前情况，将装入模块装入到内存的适当位置。装入时对目标程序中指令和数据的修改过程称为重定位，地址变换通常是在装入时一次完成的，所以又称为静态重定位，如下图（a）所示。\n静态重定位的特点是在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。此外，作业一旦进入内存后，在整个运行期间不能在内存中移动，也不能再申请内存空间。\n动态运行时装入，也称为动态重定位\n程序在内存中如果发生移动，就需要釆用动态的装入方式。装入程序在把装入模块装入内存后，并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器的支持，如上图（b）所示。\n动态重定位的特点是可以将程序分配到不连续的存储区中；在程序运行之前可以只装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。\n逻辑地址空间与物理地址空间 编译后，每个目标模块都是从0号单元开始编址，称为该目标模块的相对地址（或逻辑地址)。\n当链接程序将各个模块链接成一个完整的可执行目标程序时，链接程序顺序依次按各个模块的相对地址构成统一的从0号单元开始编址的逻辑地址空间。用户程序和程序员只需知道逻辑地址，而内存管理的具体机制则是完全透明的，它们只有系统编程人员才会涉及。不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置。\n物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址，进程在运行时执行指令和访问数据最后都要通过物理地址从主存中存取。当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成物理地址，这个过程称为地址重定位。\n内存保护 内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。通过釆用重定位寄存器和界地址寄存器来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址值。每个逻辑地址值必须小于界地址寄存器；内存管理机构动态地将逻辑地址与界地址寄存器进行比较，如果未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元，如图所示。\n当CPU调度程序选择进程执行时，派遣程序会初始化重定位寄存器和界地址寄存器。每一个逻辑地址都需要与这两个寄存器进行核对，以保证操作系统和其他用户程序及数据不被该进程的运行所影响。\n内存扩充 覆盖与交换技术是在多道程序环境下用来扩充内存的两种方法。\n内存覆盖 早期的计算机系统中，主存容量很小，虽然主存中仅存放一道用户程序，但是存储空间放不下用户进程的现象也经常发生，这一矛盾可以用覆盖技术来解决。\n覆盖的基本思想是：由于程序运行时并非任何时候都要访问程序及数据的各个部分（尤其是大程序），因此可以把用户空间分成一个固定区和若干个覆盖区。将经常活跃的部分放在固定区，其余部分按调用关系分段。首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。\n外存就是磁盘缓存，进程被suspend之后存储的地方\n覆盖技术的特点是打破了必须将一个进程的全部信息装入主存后才能运行的限制，但当同时运行程序的代码量大于主存时仍不能运行。\n内存交换 交换（对换）的基本思想是：把处于等待状态（或在CPU调度原则下被剥夺运行权利）的程序从内存移到辅（外）存，把内存空间腾出来，这一过程又叫换出；把准备好竞争CPU运行的程序从辅（外）存移到内存，这一过程又称为换入。中级调度就是釆用交换技术。\n例如，有一个CPU釆用时间片轮转调度算法的多道程序环境。时间片到，内存管理器将刚刚执行过的进程换出，将另一进程换入到刚刚释放的内存空间中。同时，CPU调度器可以将时间片分配给其他已在内存中的进程。每个进程用完时间片都与另一进程交换。理想情况下，内存管理器的交换过程速度足够快，总有进程在内存中可以执行。\n有关交换需要注意以下几个问题：\n交换需要备份存储，通常是快速磁盘。它必须足够大，并且提供对这些内存映像的直接访问。 为了有效使用CPU，需要每个进程的执行时间比交换时间长，而影响交换时间的主要是转移时间。转移时间与所交换的内存空间成正比。 如果换出进程，必须确保该进程是完全处于空闲状态。 交换空间通常作为磁盘的一整块，且独立于文件系统，因此使用就可能很快。(外存，磁盘的缓存) 交换通常在有许多进程运行且内存空间吃紧时开始启动，而系统负荷降低就暂停。 普通的交换使用不多，但交换策略的某些变种在许多系统中（如UNIX系统）仍发挥作用。 交换技术主要是在不同进程（或作业）之间进行，而覆盖则用于同一个程序或进程中。由于覆盖技术要求给出程序段之间的覆盖结构，使得其对用户和程序员不透明，所以对于主存无法存放用户程序的矛盾，现代操作系统是通过虚拟内存技术来解决的，覆盖技术则已成为历史；而交换技术在现代操作系统中仍具有较强的生命力。\n连续分配管理方式 连续分配方式，是指为一个用户程序分配一个连续的内存空间。它主要包括单一连续分配、固定分区分配和动态分区分配。\n单一连续分配 内存在此方式下分为系统区和用户区，系统区仅提供给操作系统使用，通常在低地址部分；用户区是为用户提供的、除系统区之外的内存空间。这种方式无需进行内存保护。\n这种方式的优点是实现简单、无外部碎片，可以釆用覆盖技术，不需要额外的技术支持\n内存被分为系统区和用户区，没有任何外部的内存空间。\n缺点是只能用于单用户、单任务的操作系统中，有内部碎片，存储器的利用率极低\n用户区只能存放一个用户进程，但是用户区的剩余内存肯定大于等于0，也就是内存碎片\n固定分区分配 固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干个固定大小的区域，每个分区只装入一道作业。当有空闲分区时，便可以再从外存的后备作业队列中,选择适当大小的作业装入该分区，如此循环。\n固定分区分配在划分分区时，有两种不同的方法，如上图所示\n分区大小相等：用于利用一台计算机去控制多个相同对象的场合，缺乏灵活性。 分区大小不等：划分为含有多个较小的分区、适量的中等分区及少量的大分区。 为便于内存分配，通常将分区按大小排队，并为之建立一张分区说明表，其中各表项包括每个分区的起始地址、大小及状态（是否已分配），如图(a)所示。当有用户程序要装入时，便检索该表，以找到合适的分区给予分配并将其状态置为”已分配”；未找到合适分区则拒绝为该用户程序分配内存。存储空间的分配情况如图(b)所示。\n这种分区方式存在两个问题：\n一是程序可能太大而放不进任何一个分区中，这时用户不得不使用覆盖技术来使用内存空间； 二是主存利用率低，当程序小于固定分区大小时，也占用了一个完整的内存分区空间，这样分区内部有空间浪费，这种现象称为内部碎片。 固定分区是可用于多道程序设计最简单的存储分配，无外部碎片，但不能实现多进程共享一个主存区，所以存储空间利用率低。固定分区分配很少用于现在通用的操作系统中，但在某些用于控制多个相同对象的控制系统中仍发挥着一定的作用。\n动态分区分配 动态分区分配又称为可变分区分配，是一种动态划分内存的分区方法。这种分区方法不预先将内存划分，而是在进程装入内存时，根据进程的大小动态地建立分区，并使分区的大小正好适合进程的需要。因此系统中分区的大小和数目是可变的。\n如上图所示，系统有64MB内存空间，其中低8MB固定分配给操作系统，其余为用户可用内存。开始时装入前三个进程，在它们分别分配到所需空间后，内存只剩下4MB，进程4无法装入。在某个时刻，内存中没有一个就绪进程，CPU出现空闲，操作系统就换出进程2，换入进程4。由于进程4比进程2小，这样在主存中就产生了一个6MB的内存块。之后CPU又出现空闲，而主存无法容纳进程2,操作系统就换出进程1，换入进程2。\n动态分区在开始分配时是很好的，但是之后会导致内存中出现许多小的内存块。随着时间的推移，内存中会产生越来越多的碎片（图中最后的4MB和中间的6MB，且随着进程的换入/换出，很可能会出现更多更小的内存块)，内存的利用率随之下降。\n这些小的内存块称为外部碎片(指在所有分区外的存储空间会变成越来越多的碎片)，这与固定分区中的内部碎片正好相对。克服外部碎片可以通过紧凑（Compaction)技术来解决，就是操作系统不时地对进程进行移动和整理。但是这需要动态重定位寄存器的支持，且相对费时。紧凑的过程实际上类似于Windows系统中的磁盘整理程序，只不过后者是对外存空间的紧凑。\n在进程装入或换入主存时，如果内存中有多个足够大的空闲块，操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略，考虑以下几种算法：\n首次适应(First Fit)算法：空闲分区以地址递增的次序链接。分配内存时顺序查找，找到大小能满足要求的第一个空闲分区。 最佳适应(Best Fit)算法：空闲分区按容量递增形成分区链，找到第一个能满足要求的空闲分区。 最坏适应(Worst Fit)算法：又称最大适应(Largest Fit)算法，空闲分区以容量递减的次序链接。找到第一个能满足要求的空闲分区，也就是挑选出最大的分区。 邻近适应(Next Fit)算法：又称循环首次适应算法，由首次适应算法演变而成。不同之处是分配内存时从上次查找结束的位置开始继续查找。 在这几种方法中，首次适应算法不仅是最简单的，而且通常也是最好和最快的。在UNIX 系统的最初版本中，就是使用首次适应算法为进程分配内存空间，其中使用数组的数据结构 (而非链表）来实现。不过，首次适应算法会使得内存的低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。\n邻近适应算法试图解决这个问题，但实际上，它常常会导致在内存的末尾分配空间（因为在一遍扫描中，内存前面部分使用后再释放时，不会参与分配)，分裂成小碎片。它通常比首次适应算法的结果要差。\n最佳适应算法虽然称为“最佳”，但是性能通常很差，因为每次最佳的分配会留下很小的难以利用的内存块，它会产生最多的外部碎片。\n最坏适应算法与最佳适应算法相反，选择最大的可用块，这看起来最不容易产生碎片，但是却把最大的连续内存划分开，会很快导致没有可用的大的内存块，因此性能也非常差。\nKunth和Shore分别就前三种方法对内存空间的利用情况做了模拟实验，结果表明：\n首次适应算法可能比最佳适应法效果好，而它们两者一定比最大适应法效果好。另外注意,在算法实现时,分配操作中，最佳适应法和最大适应法需要对可用块进行排序或遍历查找，而首次适应法和邻近适应法只需要简单查找；回收操作中，当回收的块与原来的空闲块相邻时（有三种相邻的情况，比较复杂)，需要将这些块合并。在算法实现时，使用数组或链表进行管理。除了内存的利用率，这里的算法开销也是操作系统设计需要考虑的一个因素。\n以上三种内存分区管理方法有一共同特点，即用户进程（或作业）在主存中都是连续存放的。这里对它们进行比较和总结，见上表图。\n非连续分配与管理方式 非连续分配允许一个程序分散地装入到不相邻的内存分区中，根据分区的大小是否固定分为分页存储管理方式和分段存储管理方式。\n分页存储管理方式中，又根据运行作业时是否要把作业的所有页面都装入内存才能运行分为基本分页存储管理方式和请求分页存储管理方式。下面介绍基本分页存储管理方式。\n基本分页存储管理 固定分区会产生内部碎片，动态分区会产生外部碎片，这两种技术对内存的利用率都比较低。我们希望内存的使用能尽量避免碎片的产生。\n这就引入了分页的思想：把主存空间划分为大小相等且固定的块，块相对较小，作为主存的基本单位。每个进程也以块为单位进行划分，进程在执行时，以块为单位逐个申请主存中的块空间。\n分页的方法从形式上看，像分区相等的固定分区技术，分页管理不会产生外部碎片。但它又有本质的不同点：块的大小相对分区要小很多，而且进程也按照块进行划分，进程运行时按块申请主存可用空间并执行。这样，进程只会在为最后一个不完整的块申请一个主存块空间时，才产生主存碎片，所以尽管会产生内部碎片，但是这种碎片相对于进程来说也是很小的，每个进程平均只产生半个块大小的内部碎片（也称页内碎片）。\n分页存储几个概念 页和页框\n进程中的块称为页(Page)，内存中的块称为页框（Page Frame，或页帧）。外存也以同样的单位进行划分，直接称为块(Block)。进程在执行时需要申请主存空间，就是要为每个页分配主存中的可用页框，这就产生了页和页框的一一对应。\n地址结构\n分页存储管理的逻辑地址结构如图所示\n地址结构包含两部分：前一部分为页号P，后一部分为页内偏移量W。地址长度为32 位，其中011位为页内地址，即每页大小为4KB；12-31位为页号，地址空间最多允许有2^20页。\n页表\n为了便于在内存中找到进程的每个页面所对应的物理块，系统为每个进程建立一张页表，记录页面在内存中对应的物理块号，页表一般存放在内存中。\n在配置了页表后，进程执行时，通过查找该表，即可找到每页在内存中的物理块号。\n可见，页表的作用是实现从页号到物理块号的地址映射，如图所示\n基本地址变换机构 在系统中通常设置一个页表寄存器(PTR)，存放页表在内存的始址F和页表长度M。进程未执行时，页表的始址和长度存放在进程控制块中，当进程执行时，才将页表始址和长度存入页表寄存器。设页面大小为L，逻辑地址A到物理地址E的变换过程如下：\n计算页号P(P=A/L)和页内偏移量W (W=A%L)。 比较页号P和页表长度M，若P \u0026gt;= M，则产生越界中断，否则继续执行。 页表中页号P对应的页表项地址 = 页表起始地址F + 页号P * 页表项长度，取出该页表项内容b，即为物理块号。 计算E=b*L+W，用得到的物理地址E去访问内存。 以上整个地址变换过程均是由硬件自动完成的。\n例如，若页面大小L为1K字节，页号2对应的物理块为b=8，计算逻辑地址A=2500 的物理地址E的过程如下：P=2500/1K=2，W=2500%1K=452，查找得到页号2对应的物理块的块号为 8，E=8*1024+452=8644。\n下面讨论分页管理方式存在的两个主要问题：\n每次访存操作都需要进行逻辑地址到物理地址的转换，地址转换过程必须足够快，否则访存速度会降低； 每个进程引入了页表，用于存储映射机制，页表不能太大，否则内存利用率会降低。 为解决以上问题，引出了后面的“具有快表的地址变换机构”和“两级页表”\n具有快表的地址变换机构 由上面介绍的地址变换过程可知，若页表全部放在内存中，则存取一个数据或一条指令至少要访问两次内存：一次是访问页表，确定所存取的数据或指令的物理地址，第二次才根据该地址存取数据或指令。显然，这种方法比通常执行指令的速度慢了一半。\n为此，在地址变换机构中增设了一个具有并行查找能力的高速缓冲存储器——快表，又称联想寄存器(TLB)，用来存放当前访问的若干页表项，以加速地址变换的过程。与此对应，主存中的页表也常称为慢表，配有快表的地址变换机构如图所示。\n在具有快表的分页机制中，地址的变换过程：\nCPU给出逻辑地址后，由硬件进行地址转换并将页号送入高速缓存寄存器，并将此页号与快表中的所有页号进行比较。 如果找到匹配的页号，说明所要访问的页表项在快表中，则直接从中取出该页对应的页框号，与页内偏移量拼接形成物理地址。这样，存取数据仅一次访存便可实现。 如果没有找到，则需要访问主存中的页表，在读出页表项后，应同时将其存入快表，以便后面可能的再次访问。但若快表已满，则必须按照一定的算法对旧的页表项进行替换。 注意：有些处理机设计为快表和慢表同时查找，如果在快表中查找成功则终止慢表的查找。\n一般快表的命中率可以达到90%以上，这样，分页带来的速度损失就降低到10%以下。快表的有效性是基于著名的局部性原理，这在后面的虚拟内存中将会具体讨论。\n两级页表 问题一: 根据页号查询页表的方法:K 号页对应的页表项存放位置 = 页表始址 + K * 4 ，页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框；\n问题二: 没有必要让整个页表常驻内存，因为进程在一段时间内可能只需要访问某几个特定的页面。\n解决办法：把页表再分页并离散存储，然后再建立一张页表记录页表各个部分的存放位置，称为页目录表，或称外层页表，或称顶层页表。\n多级页表 基本分段存储管理方式 分页管理方式是从计算机的角度考虑设计的，以提高内存的利用率，提升计算机的性能, 且分页通过硬件机制实现，对用户完全透明；而分段管理方式的提出则是考虑了用户和程序员，以满足方便编程、信息保护和共享、动态增长及动态链接等多方面的需要。\n分段 段式管理方式按照用户进程中的自然段划分逻辑空间。例如，用户进程由主程序、两个子程序、栈和一段数据组成，于是可以把这个用户进程划分为5个段，每段从0 开始编址，并分配一段连续的地址空间（段内要求连续，段间不要求连续，因此整个作业的地址空间是二维的）。其逻辑地址由段号S与段内偏移量W两部分组成。\n在下图中，段号为16位，段内偏移量为16位，则一个作业最多可有2^16=65536个段，最大段长为64KB。\n在页式系统中，逻辑地址的页号和页内偏移量对用户是透明的，用户不可以更改，是由操作系统计算出来的；但在段式系统中，段号和段内偏移量必须由用户显示提供，在髙级程序设计语言中，这个工作由编译程序完成。\n段表 每个进程都有一张逻辑空间与内存空间映射的段表，其中每一个段表项对应进程的一个段，段表项记录该段在内存中的起始地址和段的长度。段表的内容如图所示。\n在配置了段表后，执行中的进程可通过查找段表，找到每个段所对应的内存区。可见，段表用于实现从逻辑段到物理内存区的映射，如图所示。\n地址变换机构 分段系统的地址变换过程如图所示。为了实现进程从逻辑地址到物理地址的变换功能，在系统中设置了段表寄存器，用于存放段表始址F和段表长度M。其从逻辑地址A到物理地址E之间的地址变换过程如下：\n从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W。 比较段号S和段表长度M，若S多M，则产生越界中断，否则继续执行。 段表中段号S对应的段表项地址 = 段表起始地址F + 段号S * 段表项长度，取出该段表项的前几位得到段长C。若段内偏移量\u0026gt;=C，则产生越界中断，否则继续执行。 取出段表项中该段的起始地址b，计算 E = b + W，用得到的物理地址E去访问内存。 段的共享与保护 在分段系统中，段的共享是通过两个作业的段表中相应表项指向被共享的段的同一个物理副本来实现的。当一个作业正从共享段中读取数据时，必须防止另一个作业修改此共享段中的数据。不能修改的代码称为纯代码或可重入代码（它不属于临界资源)，这样的代码和不能修改的数据是可以共享的，而可修改的代码和数据则不能共享。\n与分页管理类似，分段管理的保护方法主要有两种：一种是存取控制保护，另一种是地址越界保护。地址越界保护是利用段表寄存器中的段表长度与逻辑地址中的段号比较，若段号大于段表长度则产生越界中断；再利用段表项中的段长和逻辑地址中的段内位移进行比较，若段内位移大于段长，也会产生越界中断。\n段页管理方式 页式存储管理能有效地提高内存利用率，而分段存储管理能反映程序的逻辑结构并有利于段的共享。如果将这两种存储管理方法结合起来，就形成了段页式存储管理方式。\n在段页式系统中，作业的地址空间首先被分成若干个逻辑段，每段都有自己的段号，然后再将每一段分成若干个大小固定的页。对内存空间的管理仍然和分页存储管理一样，将其分成若干个和页面大小相同的存储块，对内存的分配以存储块为单位，如图所示。\n在段页式系统中，作业的逻辑地址分为三部分：段号、页号和页内偏移量，如图所示。\n为了实现地址变换，系统为每个进程建立一张段表，而每个分段有一张页表。段表表项中至少包括段号、页表长度和页表起始地址，页表表项中至少包括页号和块号。此外，系统中还应有一个段表寄存器，指出作业的段表起始地址和段表长度。\n注意：在一个进程中，段表只有一个，而页表可能有多个。\n在进行地址变换时，首先通过段表查到页表起始地址，然后通过页表找到页帧号，最后形成物理地址。如图所示，进行一次访问实际需要三次访问主存，这里同样可以使用快表以加快查找速度，其关键字由段号、页号组成，值是对应的页帧号和保护码。\n分页 VS 分段 页是信息的物理单位。分页的主要目的是为了实现离散分配，提高内存利用率。分页仅仅是系统管理上的需要，完全是系统行为，对用户是不可见的。 段是信息的逻辑单位。分段的主要目的是更好地满足用户需求。一个段通常包含着一组属于一个逻辑模块的信息。 分段对用户是可见的，用户编程时需要显式地给出段名。 页的大小固定且由系统决定。段的长度却不固定，决定于用户编写的程序。 分页的用户进程地址空间是一维的，程序员只需给出一个记忆符即可表示一个地址。 分段的用户进程地址空间是二维的，程序员在标识一个地址时，既要给出段名，也要给出段内地址。 内存回收* 操作系统层面的内存回收主要是通过虚拟内存管理来实现的\n虚拟内存管理 虚拟内存基本概念 虚拟内存：具有请求调入和置换功能，从逻辑上对内存容量加以扩充的一种存储系统，也就是内存+外存 局部性原理： 时间局部性原理：如果执行了程序中的某条指令，那么不久后这条指令很有可能再次执行;如果某个数据被访问过，不久之后该数据很可能再次被访问。(因为程序中存在大量的循环) 空间局部性原理：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也很有可能被访问。(因为很多数据在内存中都是连续存放的，并且程序的指令也是顺序地在内存中存放的) 虚拟内存的特征： 多次性：前面内存基本管理方式就是一次性将整个进程装入内存运行，但是现在很多应用都远远大于内存大小，显而易见，内存并不能完整装下现在的应用了。一次装不下，我们可以分多次装入内存，每次只运行一部分需要的程序和加载需要的数据，不需要的数据和程序可以换出到硬盘缓存中，从而腾出空间存放要调入内存的信息 对换性：是指无需在作业运行时一直常驻内存，而是允许在作业的运行过程中，进行换进和换出，例如，当进程被suspend后就剋以换出，ready后就换进。 虚拟性：是指从逻辑上扩充内存的容量，使用户所看到的内存容量，远大于实际的内存容量。（不断换入换出，内存就可以容纳比实际内存容量更大的容量） 虚拟内存实现 请求分页存储管理 请求分页系统建立在基本分页系统基础之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。\n请求分页是目前最常用的一种实现虚拟存储器的方法。\n在请求分页系统中，只要求将当前需要的一部分页面装入内存，便可以启动作业运行。\n在作业执行过程中，当所要访问的页面不在内存时，再通过调页功能将其从外存（硬盘，但不是硬盘缓存）调入，同时还可以通过置换功能将暂时不用的页面换出到外存上，以便腾出内存空间。\n为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存的计算机系统，还需要有页表机制、缺页中断机构和地址变换机构。\n页表机制 请求分页系统的页表机制不同于基本分页系统，请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存的情况，如何发现和处理这种情况是请求分页系统必须解决的两个基本问题。\n为此，在请求页表项中增加了四个字段，如图所示。\n相较于基本分页存储管理，页表项新增四列：\n状态位P：用于指示该页是否已调入内存，供程序访问时参考。 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近己有多长时间未被访问，供置换算法换出页面时参考。 修改位M：标识该页在调入内存后是否被修改过。 外存地址：用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。 缺页中断机构 在请求分页系统中，每当所要访问的页面不在内存时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。\n此时应将缺页的进程阻塞（调页完成唤醒)，如果内存中有空闲块，则分配一个块，将要调入的页装入该块，并修改页表中相应页表项，若此时内存中没有空闲块，则要淘汰某页（若被淘汰页在内存期间被修改过，则要将其写回外存)。\n缺页中断作为中断同样要经历，诸如保存CPU环境、分析中断原因、转入缺页中断处理程序、恢复CPU环境等几个步骤。\n但与一般的中断相比，它有以下两个明显的区别：\n在指令执行期间产生和处理中断信号，而非一条指令执行完后，属于内部中断。\n如果该指令访问的地址仍然没有在内存中，就会再次触发缺页中断，重复上述操作，直到该指令能够正常执行。\n因此，一条指令在执行期间可能会产生多次缺页中断，直到它所需要的所有页面都在内存中。\n地址变换机构 请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存，又增加了某些功能而形成的。\n如图所示，在进行地址变换时，先检索快表：\n若找到要访问的页，便修改页表项中的访问位（写指令则还须重置修改位)，然后利用页表项中给出的物理块号和页内地址形成物理地址。 若未找到该页的页表项，应到内存中去查找页表，再对比页表项中的状态位P，看该页是否已调入内存，未调入则产生缺页中断，请求从外存把该页调入内存。 可能需要页面置换。存在这种情况：当内存已经满了，但是此时CPU发生缺页中断，需要请求外存的缺页。那么只有将已经满了的内存，换出页，再调入所需的页。 请求分段存储管理 略\n请求段页式存储管理 略\n页面置换算法 页面的换入、换出需要磁盘I/O，会有较大的开销，因此好的页面置换算法应该追求更少的缺页率。\n最佳置换算法(OPT)\n先进先出置换算法(FIFO)\n最近最久未使用置换算法(LRU)\n时钟置换算法(CLOCK)\n当采用简单Clock算法时，只需为每页设置一位访问位，再将内存中的所有页面都通过链接指针链接成一个循环队列。\n当某页被访问时，其访问位被置为1。置换算法在选择一页淘汰时，只需检查页的访问位，如果是0，就选择将该页换出；若为1，则重新将它置0，暂不换出，而给该页第二次驻留内存的机会，再按照FIFO算法检查下一个页面。当检查到队列中的最后一个页面时，若其访问位仍为1，则再返回到队首去检查第一个页面。由于该算法是循环地检查各页面的使用情况，故称为Clock算法。\n因该算法只有一位访问位，只能用它表示该页是否已经使用过，而置换时是将未使用过的页面换出去，又称为最近未用算法NRU(Not recently used)。\n改进型的时钟置换算法\n在将一个页面换出时，如果该页已被修改过，需将该页重新写回到磁盘上；但如果该页未被修改过，则不必将它拷回磁盘。\n在改进型Clock算法中，除需考虑页面的使用情况外，需再增加一个因素，即置换代价。\n这样，在选择页面换出时，既要是未使用过的页面，又要是未被修改过的页面。把同时满足这两个条件的页面作为首选淘汰的页面。\n由访问位A和修改位M可以组合成下面四种类型的页面：\n1类（A=0,M=0）：表示该页最近既未被访问，又未被修改，是最佳淘汰页，可以换出到外存上。\n2类（A=0,M=1）：表示该页最近未被访问，但已被修改，并不是很好的淘汰页，可以考虑换出到外存（硬盘）。\n3类（A=1,M=0）：表示该页最近已被访问，但未被修改，该页有可能再被访问，需要保留在内存里。\n4类（A=1,M=1）：表示该页最近已被访问且被修改，该页很可能再被访问，需要保留在内存里。\n在内存中的每个页必定是这四类页面之一，在进行页面置换时，可采用与简单Clock算法相类似的算法，其差别在于该算法需同时检查访问位与修改位，以确定该页是四类页面中的哪一种。其执行过程可分为以下三步：\n第一步：从指针所指示的当前位置开始，扫描循环队列，寻找A=0且M=0的第一类页面，将所遇到的第一个页面作为所选中的淘汰页。在第一次扫描期间不改变访问位A。\n第二步：如果第一步失败，即扫描一次后未遇到第一类页面，则开始第二轮扫描，寻找A=0且M=1的第二类页面，将所遇到的第一个这类\n页面作为淘汰页。在第二轮扫描期间，将所有扫描过的页面的访问位都置0.\n第三步：如果第二步也失败，即也未找到第二类页面，则将指针返回到开始的位置。然后重复第一步，如果仍失败，必要时再重复第二步，此时就一定能找到被淘汰的页。\n该算法与简单Clock算法相比，可减少磁盘的I/O操作次数。但为了找到一个可置换的页，可能需经过几轮扫描。换言之，实现该算法本身的开销将有所增加。\n上图的NRU算法有错误。\nBelady异常：在某些情况下，分配的物理页越多，产生缺页中断的次数反而越多。\nBelady现象演示 假定给某进程分为5页（page），但是它在内存中只分配到3个页帧（page frame），现在有一访问串：1,2,3,4,1,2,5,1,2,3,4,5，表示依次访问第1页、第2页……\n刚开始时，进程页还在虚存（磁盘）中，尚未缓存到内存中，所以第一次要访问第1页时发生一次缺页故障，此时调入第1页到内存中，占一个页帧\n此时还剩下两个页帧未分配，由于接下来依次访问第2、3页，同理会触发两次缺页故障，在此之后，第1、2、3页都已经缓存在内存中\n接下来要访问第四页，由于在此之前第1,2,3页已经缓存在内存中，该进程所分配到的3个页帧已满，为此必须替换掉一页，才能把第四页加载进来，此时又发生一次缺页故障。由于采用FIFO替换算法，因为第一页是最先进来，所以它会被替换出去\n接下来又要访问第一页，由于当前缓存页时第4、2、3页，从而根据FIFO，要将第2页替换为第1页，这就又发生一次缺页中断，调入第1页后，此时存在于内存中的是第4、1、3页。同理，接下来要访问第2页，发生一次缺页中断，将第3页替换为第2页，此时存在于内存中的是第4、1、2页。\n在接下来的访问中，如果第K页已经存在内存中，则直接使用，所以此时不会发生缺页故障，重复按照上述过程，我们可以得到如下示例图表\n红色标识出的是发生缺页故障后调入的页，可以看见共发生9次缺页异常，而从访问串可知访问12次，所以缺页率为9/12=0.75。\n现在，该进程在上述3页帧的基础上多分配一页帧，也就是变成四页帧，则仿照上述分析过程，可画出如下图表\n红色标识出的是发生缺页故障后调入的页，蓝色标识的是之前调入的页面，可以看见共发生10次缺页异常，而从访问串可知访问12次，所以缺页率为10/12=0.833。\nFIFO替换算法产生该现象的原因是它没有考虑到程序执行的动态特征。\n页面分配策略 先简述几个名词：\n驻留集：一个进程的驻留集指当前在主存中的这个进程的页的集合。\n由于采用虚拟存储技术，驻留集的大小（实际内存大小）一般小于进程的大小（实际占有内存再加上进程存储在外存上的资源）。若驻留集太小，会导致频繁缺页；太大会导致多道程序并发度降低，资源利用率下降。\n工作集：一个进程的工作集指这个进程最近被使用过的页的集合。\n抖动：又称颠簸，指刚被调出去的页又马上被调回，调回不久后又被调出。\n置换策略：固定分配局部置换、可变分配全部置换、可变分配局部置换。\n页面分配与置换策略 固定分配：操作系统为每个进程分配一组固定数目大小的物理块。在程序运行过程中，不允许改变。即驻留集大小固定不变。 可变分配：先为每个进程分配一定大小的物理块，在程序运行过程中，可以动态改变物理块的大小。即驻留集大小可变。 局部置换：进程发生缺页时，只能选择当前进程中的物理块进行置换。 全局置换：可以将操作系统进程中保留的空闲物理块分配给缺页进程，还可以将别的进程持有的物理块置换到外存，再将这个物理块分配给缺页的进程。 固定分配局部置换 系统为每个进程分配一定数量的内存块（物理块），在整个运行期都不改变。若进程在运行过程中发生了缺页，则只能在本进程的内存页面中选出一个进行调出，再调回需要的页面。\n缺点：不好确定一个进程到底应该分配多大的实际内存才合理。 可变分配全局置换 系统为每个进程分配一定数量的内存块。操作系统还会保持一个空闲物理块的队列。若某个进程发生缺页，可以从空闲物理块中取出一块分配给该进程。如果空闲物理块没有了，那么会选择一个未锁定（不那么重要的，可能是其它进程的）的页面换出到外存，再将物理块分配给缺页的进程。\n缺点：在空闲物理块没有的情况下，如果将其它进程的页面调出外存，那么这个进程就会拥有较小的驻留集，如此会导致该进程的缺页率上升。 可变分配局部置换 刚开始为每个进程分配一定数量的物理块。当进程发生缺页时，只允许从当前进程的物理块中选出一个换出内存。如果当前进程在运行的时候频繁缺页，系统会为该进程动态增加一些物理块，直到该进程缺页率趋于适中程度；如果说一个进程在运行过程中缺页率很低或者不缺页，则可以适当减少该进程分配的物理块。通过这些操作可以保持多道程序的并发度较高。\n与 可变分配全局置换 的区别：\n可变分配全局置换：只要发生缺页，就会分配新的物理块 可变分配局部置换：根据缺页率动态增加或者减少物理块的数量。 ","permalink":"https://cold-bin.github.io/post/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","tags":["虚拟内存","内存分配与回收"],"title":"操作系统之内存管理"},{"categories":["拷贝"],"contents":"[toc]\n拷贝 说起拷贝，语文上的联想词大概就是“复制”。那么在计算机范围内，二者定义是否一样呢？答案是不一样的。\n拷贝：是指在计算机程序中，将一个对象的值或内容复制到另一个对象中的操作。拷贝的目的通常是为了在程序中创建一个新的、独立的对象，该对象与原始对象具有相同的值或内容，但在内存中占据不同的空间，因此它们可以被独立地修改，而互相不会影响。拷贝可以应用于各种类型的对象，包括基本类型、复合类型和自定义类型等。通常，拷贝的方式有浅拷贝和深拷贝两种，具体的拷贝方式取决于被拷贝对象的类型和需要实现的功能。 复制：是指在内存中创建一个新的对象，该对象是原始对象的完整副本，包括所有嵌套对象。因此，如果更改了原始对象中的可变元素，则不会影响复制对象中的相应元素，因为它们是不同的对象。复制有时也被称为深拷贝，因为它复制了对象的所有嵌套层次结构。 浅拷贝 在各大编程语言中，浅拷贝往往就是复制对象的地址\n显然，浅拷贝对象之后，拷贝对象或原对象地改变将会引起另一方内容的改变。\n实现 取引用或地址即可\n1 2 int a = 1 int *b = \u0026amp;a 意义 保存内存空间：浅拷贝可以避免不必要的内存使用，因为它只是复制对象的一层，而不是复制对象内部的所有嵌套对象。因此，当需要创建一个新对象但又不需要完全复制原始对象时，可以使用浅拷贝。 提高效率：在某些情况下，使用浅拷贝可以提高代码的执行效率，因为它只复制对象的一层，而不是复制对象内部的所有嵌套对象。这意味着在处理大型数据集合时，使用浅拷贝可以提高代码的运行速度。 保留原始数据：浅拷贝可以在不修改原始对象的情况下创建一个新的对象，因此可以在需要保留原始数据的情况下使用。 深拷贝 相较于浅拷贝而言，深拷贝的拷贝不仅仅只是停留在引用或指针类型的表面，需要递归地获取将所有引用或指针类型的值，直到非引用或指针类型。\n这样深拷贝后的对象与原对象之间没有任何指向关系，修改任意一方数据，并不会引起另一方的改变。\n实现 深拷贝的定义很简单。依据定义实现的方式就是递归到非基本类型时，再拷贝这个基本值。由于不同编程语言的引用类型结构不一样，实现方式不一样。以go语言为例实现一个深拷贝示例（除了下面利用反射机制实现比较通用的深拷贝外，还可以自己手动深拷贝）\n下面的实现针对有环路的数据结构会陷入无限递归的情况。需要做判环检测\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 // @author cold bin // @date 2022/10/24 package dcopy import ( \u0026#34;reflect\u0026#34; \u0026#34;time\u0026#34; ) func DeepCopy(src any) any { if src == nil { return nil } original := reflect.ValueOf(src) cpy := reflect.New(original.Type()).Elem() copyRecursive(original, cpy) return cpy.Interface() } // 递归深拷贝，直至拷贝到基础类型为止，基础类型的赋值一定是 func copyRecursive(src, dst reflect.Value) { switch src.Kind() { case reflect.Ptr: // 这里主要是取出指针的实际数据，然后根据数据在反射创建一个新的指针变量， // 更改这个指针变量的元素值，从而实现指针的深拷贝 originalValue := src.Elem() if !originalValue.IsValid() { return } dst.Set(reflect.New(originalValue.Type())) copyRecursive(originalValue, dst.Elem()) case reflect.Interface: // 先判断指针是否为nil，为nil就没必要继续了，直接返回到上层递归 if src.IsNil() { return } originalValue := src.Elem() copyValue := reflect.New(originalValue.Type()).Elem() copyRecursive(originalValue, copyValue) dst.Set(copyValue) case reflect.Struct: // 使用时间的程序通常应该将它们作为值，而不是指针来存储和传递。不能深度拷贝时间，貌似拷贝出来的时间都是utc t, ok := src.Interface().(time.Time) if ok { dst.Set(reflect.ValueOf(t)) return } for i := 0; i \u0026lt; src.NumField(); i++ { // TODO 跳过未导出字段，后期迭代考虑迁移到可配置的选项里 if src.Type().Field(i).PkgPath != \u0026#34;\u0026#34; { continue } // 递归深拷贝直到未配置类型的非基础类型 copyRecursive(src.Field(i), dst.Field(i)) } case reflect.Slice: if src.IsNil() { return } // 反射创建切片，从而初始化 dst.Set(reflect.MakeSlice(src.Type(), src.Len(), src.Cap())) // 递归深拷贝切片的每个元素 for i := 0; i \u0026lt; src.Len(); i++ { copyRecursive(src.Index(i), dst.Index(i)) } case reflect.Array: dst.Set(reflect.New(reflect.ArrayOf(src.Len(), src.Type())).Elem()) for i := 0; i \u0026lt; src.Len(); i++ { copyRecursive(src.Index(i), dst.Index(i)) } case reflect.Map: if src.IsNil() { return } dst.Set(reflect.MakeMap(src.Type())) for _, originalKey := range src.MapKeys() { // 取值 originalValue := src.MapIndex(originalKey) // 复制 copyValue := reflect.New(originalValue.Type()).Elem() // 首先对map的值递归，直到基础类型 copyRecursive(originalValue, copyValue) // 然后再递归map的键递归，直到基础类型 copyKey := reflect.New(originalKey.Type()).Elem() copyRecursive(originalKey, copyKey) dst.SetMapIndex(copyKey, copyValue) } //\tTODO: 函数貌似深拷贝也没啥用 //case reflect.Func: //\tif src.IsNil() { //\treturn //\t} //\treflect.MakeFunc(src.Type(), func(in []reflect.Value) (out []reflect.Value) { // //\t}) // TODO: 反射提供的api不足以深拷贝一个 chan 好像，不知道为啥... 那暂时默认使用浅拷贝吧 //case reflect.Chan: //if src.IsNil() { //\treturn //} //newChanV := reflect.MakeChan(src.Type(), src.Cap()) //dst.Set(newChanV) default: // 递归结束条件，直到未配置的类型才会被直接赋值 dst.Set(src) } } test\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // @author cold bin // @date 2022/10/24 package dcopy import ( \u0026#34;reflect\u0026#34; \u0026#34;testing\u0026#34; ) func TestCopy(t *testing.T) { type Struct struct { Int int Map map[int]string } s := Struct{ Int: 1, Map: map[int]string{1: \u0026#34;1\u0026#34;}, } newS := DeepCopy(s) s.Map[1] = \u0026#34;2\u0026#34; if reflect.DeepEqual(s, newS) { t.Errorf(\u0026#34;TestCopy()=相等, want=不相等\u0026#34;) } } 意义 避免对象共享：通过深拷贝，每个对象都被复制到新的内存地址中，对象之间不存在共享关系，因此对其中一个对象的修改不会影响其他对象。 确保拷贝对象的独立性：通过深拷贝，拷贝对象与原始对象之间完全隔离，任何一方的修改都不会影响另一方，这可以确保拷贝对象的独立性。 浅拷贝与深拷贝 区别 浅拷贝取得是地址，深拷贝拿得是内容\n选择 浅拷贝得优势在于拷贝得仅仅只是对象得地址，一般来说，64位计算机里地址的大小就是8bytes大小。那么相较于深拷贝大于8字节的变量，从内存开销上来说浅拷贝更划算一点。\n那么是否可以为了节约内存，我们将所有大于8字节的变量都采用浅拷贝的方式传递呢？\n答案是：不可以。理由有以下几点\n额外的计算和性能开销：因为在进行浅拷贝时，拷贝对象只是复制了原始对象的指针或引用，而其具体内容。当我们通过拷贝对象访问原始对象时，需要先访问原始对象的引用，再访问原始对象的内容，这就需要进行额外的计算开销。\n举个例子，如果原始对象有一个属性名为a，我们进行了浅拷贝，得到了一个拷贝对象。当我们通过拷贝对象访问属性a时，需要先访问原始对象的引用，再访问属性a，这就需要进行额外的计算。相反，如果进行深拷贝，拷贝对象会完全复制一份，不需要额外的计算就可以访问对象。\n对象间的相互影响：由于浅拷贝只是复制了原始对象的引用，所以当拷贝对象修改了某个属性或方法时，原始对象也会受到影响。这种相互影响可能导致程序出现不可预期的行为。\n并发访问问题：当多个线程或者协程同时访问同一个对象的拷贝时，可能会导致数据竞争或者锁争用等并发问题。\n增加GC的压力：指针可以导致内存泄漏。如果一个指针指向的对象没有被及时释放，就会一直占用内存，造成内存泄漏。这个问题可以通过使用垃圾回收来解决，但是如果过度使用指针，就会导致垃圾回收器无法及时回收内存，从而增加内存使用量和垃圾回收时间。\n一方面，针对一些比较大的数据，我们可以进行浅拷贝，以避免深拷贝带来的性能耗损问题；另一方面，大规模使用浅拷贝也会带来一系列问题，应当避免大规模的浅拷贝使用。\n当然，原对象和拷贝对象是否相互关联，依据业务而定来选择深浅拷贝\n零拷贝 零拷贝是指计算机执行I/O操作时，CPU不需要将数据从一个存储区域复制到另一个存储区域，从而可以减少上下文切换以及CPU的拷贝时间。它是一种I/O操作优化技术。\nDMA DMA，英文全称是Direct Memory Access，即直接内存访问。DMA允许外设设备和内存存储器之间直接进行IO数据传输，其过程不需要CPU的参与。\n主要干的事情就是：转发CPU的IO请求以及从外设设备哪里拷贝数据到内存里\n相当于DMA在IO数据传送上，可以去分担CPU的活儿，让CPU可以去干其他事情，变相地提高了CPU的工作效率。暂时可以把DMA当作一个小CPU来看待\n传统的I/O 过程\n以文件下载服务为例\nread：把数据从磁盘读取到内核缓冲区，再拷贝到用户缓冲区 write：先把用户缓冲区数据写入到内核socket缓冲区，最后写入网卡设备。 流程如下图：\n依据上图我们可以得知：传统IO的读写流程，包括了4次上下文切换，4次数据拷贝\n零拷贝的几种实现 传统的IO操作具有4次上下文切换，4次数据拷贝。我们可以使用零拷贝的方式来减少上下文的切换和数据拷贝的次数，从而大大提高IO的效率\nmmap+write 什么是mmap？\nmmap在linux和windows均已实现\nmmap是一种将文件/设备映射到内存的方法，实现文件的磁盘地址和进程虚拟地址空间中的一段虚拟地址的一一映射关系。也就是说，可以在某个进程中通过操作这一段映射的内存，实现对文件的读写等操作。修改了这一段内存的内容，文件对应位置的内容也会同步修改，而读取这一段内存的内容，相当于读取文件对应位置的内容。\n先看过程图\n依据上图我们可以得到：mmap+write的方式可以减少一次CPU拷贝。但是上下文的切换依然是4次\nmmap就是利用虚拟内存，可以把内核空间和用户空间的虚拟地址映射到同一个物理地址，从而减少数据拷贝次数！mmap就是用了虚拟内存这个特点，它将内核中的读缓冲区与用户空间的缓冲区进行映射，所有的IO都在内核中完成\n实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 // windows下 package main import ( \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;unsafe\u0026#34; ) const defaultMaxFileSize = 1 \u0026lt;\u0026lt; 20 // 假设文件最大为 1M const defaultMemMapSize = 1 \u0026lt;\u0026lt; 20 // 假设映射的内存大小为 1M type Mmap struct { file *os.File // file 即文件描述符 data *[defaultMaxFileSize]byte // data 是映射内存的起始地址 } func (demo *Mmap) mmap() { // 需要 CreateFileMapping 和 MapViewOfFile 两步才能完成内存和文件的映射 h, err := syscall.CreateFileMapping(syscall.Handle(demo.file.Fd()), nil, syscall.PAGE_READWRITE, 0, defaultMemMapSize, nil) if err != nil { panic(err) } addr, err := syscall.MapViewOfFile(h, syscall.FILE_MAP_WRITE, 0, 0, uintptr(defaultMemMapSize)) if err != nil { panic(err) } err = syscall.CloseHandle(syscall.Handle(h)) if err != nil { panic(err) } // 这里将映射的内存地址直接转化为byte数组的地址，这样就形成了完整的映射关系：修改映射区域的内存就会引起文件内容修改 demo.data = (*[defaultMaxFileSize]byte)(unsafe.Pointer(addr)) } // 取消映射 func (demo *Mmap) munmap() { // 先拿一下映射的data地址值 addr := (uintptr)(unsafe.Pointer(\u0026amp;demo.data[0])) // 再调一下api取消这个地址的映射 if err := syscall.UnmapViewOfFile(addr); err != nil { panic(err) } } func main() { f, _ := os.OpenFile(\u0026#34;tmp.txt\u0026#34;, os.O_CREATE|os.O_RDWR, 0644) demo := \u0026amp;Mmap{file: f} demo.mmap() defer demo.munmap() msg := \u0026#34;tmp.txt你好呀 ^% *sda ..a\u0026#34; for i := 0; i \u0026lt; len(msg); i++ { demo.data[i] = msg[i] } } sendfile sendfile是linux下的api，windows上没有\nsendfile表示在两个文件描述符之间传输数据，它是在操作系统内核中操作的，避免了数据从内核缓冲区和用户缓冲区之间的拷贝操作，因此可以使用它来实现零拷贝。\n过程如下图\n可以看出，上下文切换只有2次，CPU的数据拷贝是1次\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // 在linux系统下运行 package main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; ) func main() { srcFile, err := os.Open(\u0026#34;src.txt\u0026#34;) if err != nil { log.Fatal(err) } defer srcFile.Close() dstFile, err := os.Create(\u0026#34;dst.txt\u0026#34;) if err != nil { log.Fatal(err) } defer dstFile.Close() src := int(srcFile.Fd()) dst := int(dstFile.Fd()) for { // 将 src 文件描述符中的数据传输到 dst 文件描述符中、 // linux系统下，sendfile有限制，需要批量sendfile n, err := syscall.Sendfile(dst, src, nil, 4096) if err != nil { log.Fatal(err) } if n == 0 { break } } log.Println(\u0026#34;Data transfer completed.\u0026#34;) } ","permalink":"https://cold-bin.github.io/post/%E6%8B%B7%E8%B4%9D/","tags":["mmap","sendfile","dma","零拷贝"],"title":"拷贝"},{"categories":["操作系统"],"contents":"[TOC]\n什么是进程？ 概念 进程（process）：是一个具有一定独立功能的程序，关于某个数据集合的一次运行活动，是操作系统资源分配的基本单位\n要点\n进程是程序的正在被执行的实例\n躺在磁盘里的程序不是进程，而是程序被加载到内存正在运行的过程才是进程\n进程是程序在一个数据集合上运行的过程\n进程是操作系统进行资源分配的基本单位\n结构和特征 进程的结构 控制块（Process Control Block, PCB）：进程的唯一标识，用以操作系统来辨别不同的进程，包括如下几部分：\nOS根据PCB来对进程进行控制和管理，进程的创建和撤销都是根据PCB进行的 存储：程序计数器、进程状态、CPU暂存器、存储器管理、输入输出状态等信息 PCB在新建进程时创建，并常驻内存，是进程实体的一部分，也是进程的唯一标识 数据段：原始数据及进程开启后产生的各种数据\n数据区域（data region）：存储（全局和静态）变量和进程执行期间使用的动态分配的内存 堆栈区域（stack region）：存储活动过程调用的指令和本地变量、中间数据 程序段：存放在文本区域（text region），被多个进程共享\n存储处理器执行的代码，二进制格式\n程序的一次执行过程就是一个进程，程序实体就是进程的映像\n进程的特征（理解） 动态性：由创建而生，由撤销而亡 并发性：多个进程可以同时运行（相互之间互不干扰耶） 独立性：独立资源分配（这样就可以保证更好的保证并发性） 异步性：相互独立，互不干扰，各自以不可预知的速度向前推进或者说实体按异步方式运行 进程与线程 什么是线程？ 线程（thread）是一系列活动按事先设定好的顺序一次执行的过程，是一系列指令的集合 是一条执行路径，不能单独存在，必须包含在进程中 可以并发执行 是程序执行的基本单位，每个线程可以独立执行，并共享进程的资源和内存空间 线程是OS运算调度的最小单位 区别 进程是操作系统资源分配和调度的基本单位，也就是说，进程创建的目的其实就是操作系统向程序运行的那个过程分配资源；而线程是OS运算调度的最小单位，也就是说，线程才是真正使用CPU运算的，才是真正干活的。\n而且，进程里的多个线程是共享进程资源的\n调度：OS中进程是资源分配的基本单位，线程是运算调度的最小单位 资源：进程才拥有资源，多个线程可以共享进程资源（进程申请资源，线程只有资源的使用权） 地址空间或其他资源：不同进程之间相互独立，但是同一进程里的不同线程是共用进程资源的，如内存等 通信：进程通信需要其他手段辅助，线程通信就更为容易一点 线程相对于进程，大大降低了创建、撤销和切换可执行实体的成本和难度\n为什么要引入线程？ 节省资源：线程比进程更容易创建和撤销，引入线程后，降低开销 共享进程资源：线程可以共享进程的资源和内存空间，这使得线程间的通信更加容易 提高响应能力：通过多线程技术，可以在后台执行耗时的任务，同时保持前台界面的响应性能 提高系统的并发性：多个线程可以并行执行，从而提高系统的并发性和吞吐量。如java的多线程编程 线程的实现方式 用户级线程（ULT） 在用户空间实现的线程，叫用户级线程。OS无法感知用户级线程的存在，此时线程的创建、调度、切换、运行、销毁都是用户空间实现的，内核空间并没有参与，OS只是在起初给多个用户级线程所在的进程分配资源。剩下的线程调度和切换等线程的管理工作都是由应用程序完成的，OS并没有参与其中。\nCPU分配到程序P，由程序P完成用户线程调度，分配资源\n优点：\n线程的切换不需要转到内核空间，节省了模式切换的开销 调度算法可以由进程根据需要，对自己的线程选择调度算法进行管理和调度，而与OS的低级调度算法无关 用户级线程的实现与OS平台无关，因为对于线程管理的代码是属于用户程序的一部分，因此用户级线程甚至可以在不支持线程机制的操作平台上实现 缺点：\n系统调用阻塞问题，再基于进程机制的OS中，大多数系统调用将使进程阻塞，因此当线程执行一个系统调用时，不仅该线程被阻塞，而且进程内的所有线程会被阻塞。而内核支持线程中的其他线程依然能运行 不能利用多处理机进行多重处理的优点，内核每次分配给一个进程的仅有一个CPU，因此，进程中仅有一个线程能执行，在该线程放弃CPU前，其他线程只能等待 内核级线程（KLT） 在内核空间实现的线程，叫内核级线程。进程里的所有线程的管理工作都交由内核态来完成。当然，每个内核线程都会设置一个内核线程控制块（类似于PCB的作用）用以内核的感知和控制。\n进程P通过轻量级进程LWP调用内核KLT，内核线程通过调度器Thread Scheduler调度将任务映射到相应的CPU上\n但是，程序一般不会直接去使用内核线程，而是通过去使用内核线程的一种高级接口叫轻量级进程（Light Weight Process）去操作内核线程（官方定义名称，在大多数系统中，LWP与普通进程的区别也在于它只有一个最小的执行上下文和调度程序所需的统计信息，而这也是它之所以被称为轻量级的原因）。并且它们之间的关系是1:1存在。需要注意的是只有先支持内核线程，才能有轻量级进程。\n优点：\n在多处理机系统中，内核能够同时调度同一进程中的多个线程并行执行 如果进程中的一个线程被阻塞了，内核可以调度该进程中的其他线程占有处理机运行，也可以运行其他进程中的线程。也就是说，不会出现一个线程阻塞，进程内的其他线程无法执行的情况 内核支持线程具有很小的数据结构和堆栈，线程的切换比较快，切换开销小 内核本身也可以采用多线程技术，可以提高系统的执行速度和效率 缺点：\n对于用户的线程切换而言，其模式切换的开销比较大，在同一个进程中，从一个线程切换到另一个线程时，需要从用户态转到核心态进行，这是因为用户进程的线程在用户态运行，而线程的调度和管理是在内核实现的，系统开销较大 相较于用户级线程而言，内核对内核级线程的支持是有限的 混合实现 使用用户线程和内核级线程混合实现这种方式，分别使用了用户线程和内核级线程的优点。在这种混合模式中，用户线程与内核级线程的数量比是不定的，即为N:M的关系。\n进程P多个用户级线程开启，线程的创建、销毁、切换都是有自己来，但是线程调度、处理器映射通过分配轻量级进程来做系统调用。算是一种折中的做法吧\n总结 进程\n作为系统资源分配的基本单位 可以包括多个线程，一般来说，至少有一个线程 进程不是一个可执行的实体，真正执行的是线程。进程只是负责应用程序实体的资源管理 线程\n是运行和调度的基本单位\n目的\n引入进程的目的在于，使多道程序并发执行而互不影响，提高系统的资源利用率和吞吐量 引入线程的目的是为了减少程序在开发时的时空开销，提高系统的并发性。相较于进程而言，大大降低 进程是怎么运行的？ 进程状态 进程阻塞原因以I/O请求为例\n五种基本状态 就绪（Ready） 进程已经准备好，已分配到所需资源，只要分配到CPU就能够立即运行\n执行（Running） 进程处于就绪状态被调度后，进程进入执行状态\n阻塞（Blocking） 正在执行的进程由于某些事件（I/O请求，申请缓存区失败）而暂时无法继续运行，进程受到阻塞。在满足请求时进入就绪状态等待进程调度\n何时会发生？\n例如，应用程序发出I/O请求时，需要等待系统调用的结果返回，那么这段等待时间是阻塞的，就可以将阻塞的进程的CPU使用权夺走，交给已经就绪的进程使用\n创建（New） 进程在创建时需要申请一个空白PCB，向其中填写控制和管理进程的信息，完成资源分配。如果创建工作无法完成，比如资源无法满足，就无法被调度运行，把此时进程所处状态称为创建状态；当进程创建完成后，进程就会进入就绪态。\n何时会发生？\n例如：用户启动应用，应用开始加载到内存里，创建进程work\n终止（Terminated） 进程结束，或出现错误，或被系统终止，进入终止状态，进行进程资源释放和回收。\n何时会发生？\n例如：正常结束、异常结束、外界干预\n进程控制 即OS对进程实现有效的管理，包括创建新进程、撤销已有进程、挂起、阻塞和唤醒、进程切换等操作。OS通过原语操作实现进程控制\n原语 定义：由若干条指令组成，完成特定的功能，是一种原子操作\n其实就是将多条指令封装成函数并形成原子操作，用以进程控制\n特点\n原子性：要么全做，要么都不做，执行过程不会被中断 在内核态（也叫管态、系统态、特权态、核心态）下执行，常驻内存 是内核的三大支撑功能之一（中断处理、时钟管理、原语操作） 原语图\n创建原语（create）、阻塞原语（block）、唤醒原语（wakeup）、撤销原语（destroy）\n创建 父子进程 在OS中，允许一个进程创建另一个进程，创建者称为父进程，被创建者称为子进程，子进程也能创建更多孙进程。结构像树一样。如在UNIX中，由父子进程组成一个进程组。 子进程可以继承父进程所拥有的的资源。当子进程被撤销时，应将从父进程那里获得的资源归还给父进程。此外，在撤销父进程的时候，也必须同时撤销其所有子进程。 注意：在Windows中不存在任何进程层次结构，所有进程具有相同的地位。但当进程创建了一个进程时，进程创建者会获得一个句柄（令牌），可以用来被控制被创建的进程，但是这个句柄是可以传递的。也就是说，获得了句柄的进程就拥有控制该进程的权利，因此进程之间不是层次关系，而是是否获得句柄，控制与被控制的关系。 引进创建进程的典型事件 用户登录：在分时系统中，用户在终端键入登录命令后，若登录成功，系统将为该用户建立一个进程。 作业调度：在多道批处理系统中，当作业调度程序按一定算法调度到某些作业时，便将它们存入内存，为它们创建进程。 提供服务：当运行中用户程序提出某种请求，系统将专门创建一个进程来提供用户所需要的服务，如需要打印文件时，创建打印进程。 应用请求：用户进程自己创建新进程，与创建者进程以并发的形式完成特定任务。 进程创建过程 OS调用进程创建原语Create，该原语按照以下步骤创建一个新进程：\n申请空白PCB，为新进程从申请获得唯一的数字标识符。 为新进程分配其运行所需资源，包括各种物理资源和逻辑资源，如CPU时间、所需内存大小、I/O设备等。 初始化PCB：①初始化标识信息（数字标识符和父进程标识符）②初始化处理机状态（程序计数器指向程序入口地址，栈指针指向栈顶）③初始化处理机控制信息（设置进程状态、优先级等） 如果进程就绪队列能够接纳新进程，便将新进程插入就绪队列。 终止 引起进程终止的事件 正常结束：表示进程的任务已经完成，准备退出运行。在批处理系统中，通常会在程序的最后安排一条Halt指令，用于向OS表示运行已结束。 异常结束：指进程在运行时发生了某种异常事件，使程序无法继续运行。常见的有： ① 越界：程序访问的存储区越出该进程的区域。 ② 保护错：进程视图访问一个不存在或不允许访问的资源或文件。 ③ 非法指令：程序试图去执行一条不存在的指令。 ④ 特权指令错：用户没有执行当前指令的权限。 ⑤ 运行超时：执行时间超过允许执行的最大值。 ⑥ 等待超时：进程等待某事件的时间超过规定最大值。 ⑦ 算术运算错：进程试图执行一个被进制的运算。 ⑧ I/O故障：在I/O过程发送了错误 外界干预：指进程应外界的请求而终止运行。这些干预有：程序员或操作系统干预；父进程请求；父进程终止。 进程终止过程 OS调用进程终止原语，按下述过程终止指定的进程：\n根据被终止进程的标识符，从PCB集合中检索出该进程的PCB，读取进程的状态。 若进程处于执行状态，立即终止进程，并置调度标志位真，在进程终止后重新进程CPU调度。 若进程还有子孙进程，将它们也终止，防止它们称为不可控的进程。 将终止进程所拥有的全部资源归还给父进程，或归还给系统。 将终止进程的PCB从所在队列中移除，操作系统记录等待需要搜集信息的其它程序搜集信息。 阻塞和唤醒 在使用阻塞和唤醒时，必须成对使用，如果在某进程中引起了进程阻塞，那么与之合作的、或相关进程中必须安排一条该进程的唤醒原语，保证进程不会永久的处于阻塞状态，无法再运行。\n引起进程的阻塞与唤醒的事件 向系统请求共享资源失败，请求者进程只能被阻塞，只有在共享资源空闲时，才会被唤醒。 等待某种操作的完成，在等待过程中应该是阻塞状态，等操作完成后，才会唤醒。 新数据尚未到达，进程只能阻塞，直到数据到达被唤醒。 等待新任务的到达，进程会阻塞，直到新任务到达会被唤醒，处理完成后又把自己阻塞起来。 进程阻塞过程 当需要阻塞进程时，进程调用阻塞原语block将自己阻塞，是一种主动行为。进入block状态后，进程先立即停止执行，然后将PCB的状态改为阻塞状态，并将PCB插入相应的阻塞队列。最后，重新进行CPU调度，切换前在PCB中保留当前进程的CPU状态，然后将CPU状态按新进程的PCB设置。\n进程唤醒过程 当被阻塞进程所期待的事情发生，有关进程调用唤醒原语wakeup将等待该事件的进程唤醒。 执行过程：将阻塞进程从阻塞队列中移出，将其PCB的状态改为就绪状态，然后将PCB插入到就绪队列中。\n挂起与激活 进程的挂起与激活操作分别通过suspend原语和active原语实现。值得注意的是：静止阻塞和静止就绪不是进程的五大基本状态之一。之后，直接在外存里放着，需要用的时候再加载到内存里\n进程的挂起 当系统中出现了引起进程挂起的事件，OS利用挂起原语suspend将指定进程或处于阻塞状态的进程挂起。 执行过程：首先检查被挂起进程的状态，如果是就绪状态，就改为就绪挂起状态；如果是阻塞状态，就改为阻塞挂起状态；为了方便用户或父进程考查该进程的运行情况，把该进程的PCB复制到某指定的内存区域（一般挂起状态的进程存在于外存）。如果被挂起进程正在执行，就重新进行调度。（也就是说，正在执行的进程不能被突然挂起，需要等到该进程调度到活动阻塞或活动就绪状态才行）\n进程的激活 当系统中发生激活进程的时间时，OS将利用激活原语active将制定进程激活。 激活过程：先将进程（PCB）从外存调入内存，检查该进程现在的状态，若是就绪挂起，则改为就绪状态；若是阻塞挂起，则改为阻塞状态。加入采用抢占式调度策略，则每当有就绪挂起状态的进程被激活，就要检查是否需要进行重新调度（如使用优先级的话，检查激活进程的优先级是否大于当前正在运行的进程，如果低则不用重新调度；如果高则剥夺当前进程的运行，把CPU分配给刚激活的进程）。\n进程切换 状态转换 进程调度 调度的概念 **定义：**根据一定的算法和规则将处理机资源进行重新分配的过程\n**前提：**作业/进程数远大于处理机数目\n**目的：**提高资源利用率，减少处理机的空闲时间\n**调度程序：**一方面要满足特定系统用户的需求（快速响应），另一方面要考虑系统整体效率（系统平均周转时间）和调度算法本身的开销\n调度层次：\n高级调度（作业调度） 将外存里的后背作业队列调入内存，并筛入就绪队列 只调入一次，调出一次 中级调度（内存调度） 将进程调至外村，条件合适再调回内存 在内、外村对换取进行进程对换 低级调度（进程调度） 从就绪队列选取进程分配给处理机 最基本的调度，频率非常高 进程调度的时机 什么时候会发生进程调度呢？引起进程调度的因素有哪些？这些也就是进程的调度时机。\n正在执行的进程执行完毕 执行中的进程因提出I/O请求或发生等事件而暂停执行。 时间片完成 在进程通信或同步过程中执行了某种原语操作，如P操作（wait操作）阻塞 高优先者进入（适用于剥夺式调度的系统） 进程调度的方式 抢占式调度（剥夺式调度） 定义：立即暂停当前进程，分配处理机的时间片给另一个进程\n**原则：**优先权/短进程优先/时间片原则\n优先级更高的进程出现在就绪队列，那么当前进程直接让出CPU时间片给这个更高优先级的进程；\n短进程优先：短进程执行时间很短，那么可以直接去执行这个短进程\n时间片优先：当前进程的时间片用完后，需要将处理机的使用权移交给其他就绪态进程\n**优点：**具有作业优先级，适合分时/实时操作系统\n非抢占式调度（非剥夺式调度） **定义：**若有进程请求执行，需要等待处理机执行完当前进程或当前进程阻塞 **缺点：**适合批处理系统，不适合分时/实时系统 进程调度的过程 保存镜像：记录进程的CPU现场 调度算法：确定分配处理机的原则 进程切换：分配处理机给其他进程 处理机回收：从进程收回处理机 进程调度的切换具体过程 检查是否允许上下文切换，有可能某进程处于原语操作中，不允许切换 保存当前进程的上下文（程序计数器PC指针，寄存器） 更新PCB信息 把此进程的PCB移入队列（可能是就绪队列，也可能是阻塞队列） 选择另一个就绪态队列执行，并更新PCB 更新内存管理的数据结构 恢复所选进程的上下文，将CPU执行权交给所选进程 典型的调度算法 FCFS（先来先服务） First Come First Served\n定义：分配处理机给作业队列或就绪队列中队头的作业或就绪态进程 原则：按照作业/就绪态进程到达顺序服务 **调度方式：**非抢占式调度 **适用场景：**作业调度、进程调度 **优点：**有利于CPU繁忙型作业，充分利用CPU资源 **缺点：**I/O繁忙型作业，操作耗时，CPU比较浪费 SJF（短作业优先） Shortest Job First\n定义：所需服务时间最短的作业/就绪态进程优先分配处理机使用权\n原则：追求最少的平均（带权）周转时间\n**调度方式：**SJF/SRTN非抢占式调度\n由于短作业优先调度算法里还是存在进程阻塞后，再加入就绪队列，此时进程已经进行了一部分工作，那么，再进行短作业调度时，看的就是剩余执行时间了。这种调度也被称为SRTN（Shortest Remaining Time Next）\n**适用场景：**作业调度、进程调度\n**优点：**平均等待/周转时间最少\n**缺点：**长作业周转时间会增加或饥饿；估计时间不准确，不能保证紧迫任务计时处理\nHRRN（高响应比优先调度） HRRN（Highest Response Ratio Next）调度算法是介于先来先服务算法与最短进程优先算法之间的一种折中算法。先来先服务算法只考虑进程的等待时间而忽视了进程的执行时间，而最短进程优先调度算法只考虑用户估计的进程的执行时间而忽视了就绪进程的等待时间。\n为此需要定义响应比Rp:\n1 Rp=（等待时间+预计执行时间）/执行时间=响应时间/执行时间 定义： Rp较大的进程获得处理机使用权 原则：综合考虑作业或进程的等待时间和执行时间 **调度方式：**非抢占式 **适用场景：**作业调度、进程调度 **优点：**优点很明显，权衡了进程等待时间和执行时间，不会使得某些进程等待太长时间 **缺点：**一旦当前进程放弃执行权（完成或阻塞）时，那么就会重新计算所有进程的响应比； PSA（优先级调度） **定义：**又叫优先级调度，按作业/进程的优先级（紧迫程度）进行调度\n**原则：**优先级最高（最紧迫）的作业/进程先调度\n**调度方式：**抢占式/非抢占式（并不能获得即使执行）\n抢占式：高优先级的立即执行\n非抢占式：高优先级等待当前进程让出处理机后执行\n**适用场景：**作业调度/进程调度\n优先级设置原则：\n静态/动态优先级 系统\u0026gt;用户；交互型\u0026gt;非交互型;I/O型\u0026gt;计算型 低优先级进程可能会产生”饥饿“，也就是低优先级进程的等待时间会大大延长 RR（时间片轮转调度） **定义：**按进程到达就绪队列的顺序，轮流分配一个时间片去执行，时间用完则剥夺\n原则：公平、轮流为每个进程服务，进程在一定时间内都能得到响应\n**调度方式：**抢占式，由时钟确定时间，时间一到，就会产生中断\n时间片的决定因素\n系统的响应时间、就绪队列的进程数量、系统的处理能力 **适用场景：**进程调度\n**优点：**公平，响应快，适用于分时系统\n**缺点：**时间片太大，相当于FCFS；太小，处理机切换频繁，开销增大\nMFQ（多级反馈队列调度） 定义： 设置多个按优先级排序的就绪队列。优先级从高到低，时间片从小到大。新进程采用队列降级法\n新来的进程会进入第一季队列，然后分时间片来获得处理机；\n没有执行完，将会移动到下一级队列，前面的队列不为空，不执行后续队列进程\n原则：集前几种调度算法的有点，相当于PSA+RR\n**调度方式：**抢占式\n**适用场景：**进程调度\n优缺点：\n对各类型的相对公平：快速响应。\n新来的进程都是进入第一级队列，都会得到CPU时间片执行一会儿，没执行完会扔到下一级队列，等到上一级队列进程执行完毕，下一级的队列里的进程再次得到执行\n终端型作业用户：短作业优先\n作业越短，那么最终完成进程所在队列级数就会越小，所以整体来看，作业越短，进程完成时间就越短\n批处理作业用户：周转时间短\n当批处理的大量进程都需要执行时，使用MFQ算法调度，让每个进程都可以公平得到CPU时间片，这样算的话\n长批处理作业用户：每个进程都会被依次执行，当前进程没执行完毕就会塞入下一级队列等待执行。所以长批处理作业，也是会得到CPU时间片的，不会发生“饥饿”现象\n在多级反馈队列进程调度算法里，低优先级队列里的进程会发生”饥饿“吗？\n答：不会。因为MFQ调度算法里前面队列不为空，后面的队列就不会得到执行，所以低优先级的队列一定是被执行过至少一个CPU时间片的\n进程之间是怎么协作的？ 进程通信 每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。\n进程间通信目的一般有共享数据，数据传输，消息通知，进程控制等。以 Unix/Linux 为例，介绍几种重要的进程间通信方式：共享内存，管道，消息队列，信号量，信号，套接字\n定义\n进程间通信就是在不同进程之间传播或交换信息，那么不同进程之间存在着什么双方都可以访问的介质呢？进程的用户空间是互相独立的，一般而言是不能互相访问的，唯一的例外是共享内存区。另外，系统空间是“公共场所”，各进程均可以访问，所以内核也可以提供这样的条件。此外，还有双方都可以访问的外设。在这个意义上，两个进程当然也可以通过磁盘上的普通文件交换信息，或者通过“注册表”或其它数据库中的某些表项和记录交换信息。广义上这也是进程间通信的手段，但是一般都不把这算作“进程间通信”。进程间通信（IPC，Inter Process Communication）是一组编程接口，让程序员能够协调不同的进程，使之能在一个操作系统里同时运行，并相互传递、交换信息。IPC方法包括管道（PIPE）、消息队列、信号、共享存储以及套接字（Socket）。\n为什么需要进程间通信呢？\n因为有些复杂程序或者是系统需要多个进程或者线程共同完成某个具体的任务，那么也就需要进程之间通信和数据访问。整个系统以进程粒度运行可以进一步提高系统整体并行性能和内存访问安全，每个进程可以有各自的分工。所以多个进程共同完成一个大的系统是比单个进程多线程要有很大的优势。\n管道 管道在Linux中就是文件（本质上就是内存里固定大小的缓冲区），而且大小固定，是以流的形式读写的：未满不读，已满不写，未空不写，已空不写。\n例如：有个4kb大小的管道，那么写的时候，就是\n如果你学过 Linux 命令，那你肯定很熟悉|这个竖线。\n1 ps auxf | grep mysql 上面命令行里的|竖线就是一个管道，它的功能是将前一个命令ps auxf的输出，作为后一个命令grep mysql的输入，从这功能描述，可以看出管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行.\n同时，我们得知上面这种管道是没有名字，所以|表示的管道称为匿名管道，用完了就销毁。\n管道还有另外一个类型是命名管道，也被叫做 FIFO，因为数据是先进先出的传输方式。\n在使用命名管道前，先需要通过 mkfifo 命令来创建，并且指定管道名字：\n1 mkfifo myPipe myPipe就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用ls看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思：\n1 2 $ ls -l prw-r--r--. 1 root root 0 Jul 17 02:45 myPipe 接下来，我们往myPipe这个管道写入数据：\n1 2 echo \u0026#34;hello\u0026#34; \u0026gt; myPipe # 将数据写进管道 # 停住了 ... 你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。\n于是，我们执行另外一个命令来读取这个管道里的数据：\n1 2 cat \u0026lt; myPipe # 读取管道里的数据 hello 可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。\n我们可以看出，管道这种通信方式效率低，不适合进程间频繁地交换数据。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。\n我们可以得知，对于匿名管道，它的通信范围是存在父子关系的进程。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通的目的。\n在 shell 里面执行 A | B命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。\n另外，对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。\n消息队列 前面说到管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据。对于这个问题，消息队列的通信模式就可以解决。\n比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。\n再来，消息队列是保存在内核中的消息链表，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。\n消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。\n缺点 消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。\n共享内存 消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那共享内存的方式，就很好的解决了这一问题。\n现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。\n共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。\nLinux系统mmap的实现机制\n信号量 用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。（脏写）\n为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，信号量就实现了这一保护机制。\n信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据。\n信号量表示资源的数量，控制信号量的方式有两种原子操作：\n一个是 P 操作，这个操作会把信号量减去 1，相减后如果信号量 \u0026lt; 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 \u0026gt;= 0，则表明还有资源可使用，进程可正常继续执行。 另一个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 \u0026lt;= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 \u0026gt; 0，则表明当前没有阻塞中的进程； P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。 接下来，举个例子，如果要使得两个进程互斥访问共享内存，我们可以初始化信号量为 1。\n具体的过程如下：\n进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。\n若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。\n直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。\n可以发现，信号初始化为 1，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。 另外，在多进程里，每个进程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个进程能密切合作，以实现一个共同的任务。\n例如，进程 A 是负责生产数据，而进程 B 是负责读取数据，这两个进程是相互合作、相互依赖的，进程 A 必须先生产了数据，进程 B 才能读取到数据，所以执行是有前后顺序的。\n那么这时候，就可以用信号量来实现多进程同步的方式，我们可以初始化信号量为 0。\n具体过程：\n如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没生产数据，于是进程 B 就阻塞等待；\n接着，当进程 A 生产完数据后，执行了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B；\n最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，于是进程 B 就可以正常读取数据了。\n可以发现，信号初始化为 0，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执行。\n信号 信号一般用于一些异常情况下的进程间通信，是一种异步通信，它的数据结构一般就是一个数字.\n在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 kill -l 命令，查看所有的信号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如\nCtrl+C 产生 SIGINT 信号，表示终止该进程；\nCtrl+Z 产生 SIGTSTP 信号，表示停止该进程，但还未结束；\n如果进程在后台运行，可以通过 kill 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：\nkill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，用来立即结束该进程； 所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。\n信号是进程间通信机制中唯一的异步通信机制\n进程需要为信号设置相应的监听处理，当收到特定信号时，执行相应的操作，类似很多编程语言里的通知机制。\nSocket 前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。\n实际上，Socket 通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。​\n总结 进程同步 概念 进程同步概念\n主要任务是使并发执行的诸进程之间能有效地共享资源和相互合作，使执行的结果具有可再现性。\n进程之间的两种制约关系\n间接相互制约关系（互斥）：互斥地访问系统共享资源\n直接相互制约关系（同步）：进程间合作，比如进程A、B，进程B是对进程A的数据进行处理，那么进程B就一定要在进程A之后执行。\n例如管道，发送者先发满管道后，接收者才可以去读管道。\n互斥的访问过程与原则 临界资源 一段时间仅允许一个进程访问的资源\n临界资源可能是硬件，也可能是软件：变量，数据，表格，队列等。 并发进程对临界资源的访问必须做某种限制，否则就可能出现与时间有关的错误\n临界资源访问过程 **进入区：**尝试进入临界区，成功就上锁（lock） **临界区：**访问共享资源 **退出区：**解锁（unlock），唤醒其他进程 **剩余区：**剩余代码 1 2 3 4 5 6 7 8 //一个访问临界资源的循环进程描述伪代码如下： while(true) { 进入区 临界区 退出区 剩余区 } 临界资源访问原则 空闲让进：临界区空闲，允许一个进程进入 忙则等待：临界区已有进程，其他进程进入阻塞状态 有限等待：处于等待的进程，等待的时间有限 让权等待：等待时应让出CPU执行权，防止”忙等待“ 互斥软件实现基本方法* while代码就是自旋锁\n单标志法 算法思想：两个进程在访问完临界区后会把临界区的权限转交给另一个进程。也就是说每个进程进入临界区的权限只能被另一个进程赋予。\nturn的初始值为0，即刚开始值允许0号进程进入临界区。\n若P1先上执行机运行，则会一直卡在⑤。直到P1的时间片用完，发生调度，切换P0上处理机运行。代码①不会卡住P0，P0可以正常访问临界区，在P0访问临界区期间即使切换会P1，P1依然会被卡在⑤。只有P0在退出区将turn改为1后，P1才能进入临界区。\n因此，该算法可以实现“同一时刻最多只允许一个进程访问临界区”。\n但是只能按照P0 -\u0026gt; P1 -\u0026gt; P0 -\u0026gt; P1\u0026hellip;这样轮流访问。如果P0访问临界区完毕后，不再访问临界区，但是P1需要再次访问临界区时turn!=1，进程不能获得锁，一直阻塞，这样就违背“空闲让进”原则。\n因此，单标记法存在的主要问题是：违背“空闲让进”原则和“让权等待”。\n双标志先检查法 算法思想：设置一个布尔型数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿，比如“falg[0]=ture”意味着0号进程P0现在向进入临界区。每个进程在进入临界区之前先检查当前有没有别的进程想进入临界区，如果没有，则把自身对应的标志flag[i]设置为true，之后开始访问临界区。\n在高并发的情况下，有可能按照①⑤②⑥③\u0026hellip;的顺序执行，那么P0和P1将会同时访问临界区。因此，双标志先检查法的主要问题是:违反“忙则等待”原则和“让权等待”。\n原因在于进入区的“检查”和“上锁”不是原子的。\u0026ldquo;检查” 后，“上锁”前可能已经发生进程切换。\n双标志后检查法 算法思想:双标志先检查法的改版。前一个算法的问题是先“检查”后“上锁”，但是这两个操作不是原子的，因此导致了两个进程同时进入临界区的问题。因此，人们又想到先“上锁”后“检查”的方法，来避免上述问题。\n高并发下，按照①⑤②\u0026hellip;的顺序执行，PO 和P1将都无法进入临界区。因此，双标志后检查法虽然解决了“忙则等待”的问题，但是又违背了“空闲让进”、“让权等待”和“有限等待”原则，会因各进程都长期无法访问临界资源而产生“饥饿”现象。两个进程都争着想进入临界区，但是谁也不让谁，最后谁都无法进入临界区。\nPeterson 算法 算法思想：结合双标记法、单标记法的思想。如果双方都争着想进入临界区，那可以让进程尝试“孔融让梨”（谦让）。做一个有礼貌的进程。\nPeterson算法用软件方法解决了进程互斥问题，遵循了空闲让进、忙则等待、有限等待三个原则，但是依然未遵循让权等待的原则。\nPeterson算法相较于之前三种软件解决方案来说，是最好的，但依然不够好。\n互斥硬件实现基本方法 中断屏蔽方法：关中断/开中断 禁止一切中断，CPU执行完临界区之前，不会发生进程切换的中断响应。 关中断时间长会影响效率 不适用于多处理机，无法防止其他处理机调度其他进程访问临界区。因为关中断只对当前处理机生效，不会对其他处理机是否响应中断请求造成影响 只是用于内核进程（该指令运行于内核态） Test-And-Set指令 简称TS指令，也有些地方称为TestAndSetLock指令，或者TSL指令。TSL指令是用硬件实现的，执行的过程不允许被中断，属于原子操作。以下是用C语言描述的逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //bool lock表示当前临界区是否被加锁 //true标识已加锁，false表示未加锁 bool TestAndSet(bool *lock){ bool old; old=*lock;\t//old用来存放lock原来的值 *lock=true;\t//无论之前是否加锁，都将lock设为true return old;\t//返回lock原来的值 } //以下是使用TSL指令实现互斥的算法逻辑 while(TestAndSet(\u0026amp;lock));\t//“上锁”并“检查”是一个原子操作 critical section;\t//临界区代码 lock=false;\t//解锁 remainder section;\t//剩余区代码 优点：实现简单，把“上锁”和“检查“通过硬件的方式变成原子操作；适用于多处理机环境。\n缺点：仍然不满足让权等待原则（无法进入临界区时释放处理机）。 原因：暂时无法进入临界区的进程会占用CPU并循环执行TSL指令，从而导致忙等。\nswap指令 Swap指令，又叫XCHG指令，是用硬件实现的原子操作，执行的过程中不允许被中断。以下是C语言描述的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Swap(bool *a,bool *b){ bool tmp; tmp=*a; *a=*b; *b=tmp; } //以下是用Swap指令实现互斥的算法逻辑 //lock表示当前临界区是否被加锁 bool old=true; while(old==true) Swap(\u0026amp;lock,\u0026amp;old); //临界区代码... lock=false; //剩余区代码... 逻辑上和TSL一样，优缺点和TSL也一样，不满足让权等待。\n信号量 信号量其实就是一个变量(可以是一个整数， 也可以是更复杂的记录型变量)，可以用一个信号量来表示系统中某种资源的数量，比如:系统中只有一台打印机，就可以设置一一个初值为1的信号量。\n用户进程可以通过使用操作系统提供的一对原语来对信号量进行操作，从而很方便的实现了进程互斥、进程同步。\n原语是一种特殊的程序段，是原子操作。**原语是由关中断/开中断指令实现的。**软件解决方案的主要问题是由“进入区的各种操作不是原子的”，因此如果能把进入区、退出区的操作都用“原语”实现，使这些操作步骤都成为一个原子操作就能避免问题。\nwait、signal 原语常简称为P、V操作(来自荷兰语proberen和verhogen)。因此，做题的时候常把wait(S)、signal(S) 两个操作分别写为P(S)、V(S)\n整形信号量 简而言之，就是使用整型变量作为信号量，用以表示系统中某种资源的数量。\n记录型信号量 整型信号量的缺陷是存在“忙等”问题（自旋），因此人们又提出了“记录型信号量”，即用记录型数据结构表示的信号量。\n信号量实现进程的互斥 分析并发进程的关键活动，划定临界区(如:对临界资源打印机的访问就应放在临界区) 设置互斥信号量mutex，初值为1 在临界区之前执行P(mutex) 在临界区之后执行V(mutex) 注意:对不同的临界资源需要设置不同的互斥信号量。 P、V操作必须成对出现。 缺少P(mutex)就不能保证临界资源的互斥访问。缺少V(mutex)会导致资源永不被释放，等待进程永不被唤醒。\n信号量实现进程的同步 分析什么地方需要实现“同步关系”，即必须保证“一前一后”执行的两个操作(或两句代码) 设置同步信号量S,初始为0 在“前操作”之后执行V(S) 在“后操作”之前执行P(S) 具体实现\n若先执行到V(S)操作，则S++后S=1。之后当执行到P(S)操作时，由于S=1，表示有可用资源，会执行S–， S的值变回0，P2进程不会执行block原语，而是继续往下执行代码4。\n若先执行到P(S)操作，由于S=0，S\u0026ndash; 后S=-1，表示此时没有可用资源，因此P操作中会执行block原语，主动请求阻塞。之后当执行完代码2，继而执行V(S)操作，S++， 使S变回0。由于此时有进程在该信号量对应的阻塞队列中，因此会在V操作中执行wakeup原语，唤醒P2进程。这样P2就可以继续执行代码4了\n信号量机制实现进程的前驱关系（多个进程的同步） 进程P1中有句代码S1，P2中有句代码S2…P6中有句代码S6。这些代码要求按如下前驱图所 示的顺序来执行：其实每一对前驱关系都是一个进程同步问题(需要保证一前一后的操作)\n因此：\n要为每一对前驱关系各设置一个同步变量 在“前操作”之后对相应的同步变量执行V操作 在“后操作”之前对相应的同步变量执行P操作 管程 前面的信号量机制实现的进程同步与互斥，会使大量的同步操作分散在各个进程中，为了减少这一现象，我们就要引入——管程。\n一个管程包含一个数据结构和能为并发进程所执行（在该操作系统上）的一组操作，这组操作能同步进程和改变管程中的数据。\n管程的组成 一组局部变量 对局部变量操作的一组过程 对局部变量进行初始化的语句。 管程的特点 局部数据变量只能被管程的过程访问，外部过程不可被访问 一个进程通过调用管程的一个过程进入管程 在任何时候都只能有一个进程在管程中执行，调用管程的任何其他进程都被挂起，等待管程变成可用的 条件变量\n它是管程内部的一种同步机制。每个条件变量都表示一种等待原因，对应着一个等待队列。（这里这个等待队列中存的就是因为这个等待原因而阻塞的PCB\n他可以执行wait和signal操作，并且应该置于wait和signal之前，就比如：x.wait, x.signal\nWait() 将自己阻塞在等待队列中 唤醒一个等待者或释放管程的互斥访问 Signal() 将等待队列中的一个线程唤醒 如果等待队列为空，则等同空操作 经典同步问题 生产者-消费者问题 单生产者-单消费者问题 问题描述\n系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区；消费者进程每次从临界缓冲区中取出一个产品并使用。生产者和消费者共享一个初始为空、大小为n的缓冲区。\n只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待。 只有缓冲区不空时，消费者才能从缓冲区中取出产品，否则必须等待。 缓冲区是临界资源，各进程必须互斥的访问 问题分析\n由于缓冲区是临界资源必须互斥使用，因此需要设置一个互斥信号量mutex，缓冲区有两种状态：进程正在访问和没有进程正在访问，因此可以给mutex赋初值为1。\n缓冲区中的大小是生产者和消费者都可以进行操作的，当缓冲区满时，生产者要等待消费者取走产品，按一定次序访问这属于同步关系；当缓冲区空时，消费者要等待生产者生产产品，按一定次序访问这属于同步关系，因此需要设置两个同步信号量，full和empty;\n1 2 3 Semaphore mutex = 1; //互斥信号量实现对缓冲区的互斥访问 Semaphore empty = n; //同步信号量，表示空闲缓冲区的数量，初值为有界缓冲区大小n Semaphore full = 0; //同步信号量，表示产品数量，也是非空缓冲区数量，初值为0 伪代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Producer(){ while (1) { 生产一个产品; P(empty); //消耗一个空闲缓冲区,后操作前进行p操作 P(mutex); 把产品放入缓冲区; V(mutex); V(full); //增加一个产品，前操作后进行v操作 } } Consumer(){ while (1) { P(full); //消耗一个产品 P(mutex); 从缓冲区取出一个产品; V(mutex); V(empty); //增加一个空闲缓冲区，前操作后进行v操作 使用产品; } } 不能交换P操作顺序，否则会出现死锁\n多生产者-多消费者问题 问题描述\n桌子上有一个盘子，每次只能向其中放入一个水果。爸爸只向盘子中放苹果，妈妈只向盘子中放橘子，儿子专等着吃盘子中的橘子，女儿专等着吃盘子中的苹果。只有盘子为空时，爸爸妈妈才能往盘子里放一个水果。仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出水果。\n问题分析\n盘子相当于一个初始为空，大小为1的缓冲区。爸爸妈妈分别可以看作生产者进程1、生产者进程2，儿子可以看作消费者进程1，女儿可以看作消费者进程2\n互斥关系：(mutex = 1) 对缓冲区（盘子）的访问要互斥地进行\n同步关系（一前一后）：\n父亲将苹果放入盘子后，女儿才能取苹果 母亲将橘子放入盘子后，儿子才能取橘子 只有盘子为空时，父亲或母亲才能放入水果 伪代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 semaphore mutex = 1; //实现互斥访问盘子(缓冲区) semaphore apple = 0; //盘子中有几个苹果 semaphore orange = 0; //盘子中有几个橘子 semaphore plate = 1; //盘子中还可以放多少个水果 dad(){ while(1){ 准备一个苹果; P(plate); P(mutex); 把苹果放入盘子; V(mutex); V(apple); } } mom(){ while(1){ 准备一个橘子; P(plate); P(mutex); 把橘子放入盘子; V(mutex); V(orange); } } daughter(){ while(1){ P(apple); P(mutex); 从盘子中取出苹果; V(mutex); V(plate); 吃苹果 } } son(){ while(1){ P(orange); P(mutex); 从盘子中取出橘子; V(mutex); V(plate); 吃掉橘子 } } 读者-写者问题 问题描述\n有两组并发进程: 读者和写者,共享一组数据区。 读者/写者问题是指，保证一个写者进程必须与其他进程互斥访问共享对象的同步问题\n允许多个读者同时执行读操作 不允许读者、写者同时操作 不允许多个写者同时操作 问题分析\n两类进程：写进程、读进程\n互斥关系：写进程——写进程、写进程——读进程。读进程与读进程不存在互斥关系。\n写进程和任何进程都要互斥，设置一个互斥信号量rw，在写者访问共享文件前后分别执行P、V操作，读者进程和写者进程也要互斥，因此读者访问共享文件前后也要对rw进行P、V操作，但是如果所有读者进程在访问共享文件之前都进行P(rw)操作，那么会导致各个读进程之间也无法同时访问文件。该如何解决这个问题？\nP(rw)和V(rw)其实就是对共享文件的加锁和解锁，既然各个读进程可以同时访问文件，而读进程和写进程需要互斥访问文件，那么就让第一个读进程对文件进行加锁，最后一个读进程对文件解锁，如何知道是还有几个读进程呢？就需要设置一个整形变量count来记录当前有几个读进程在在访问文件。\n解决上述问题后，我们再来看这一种情况:\n当两个读进程并发来访问文件时，有可能第一个进程还没来的及进行count++，第二个进程就就已经过了判断count是否为0的操作了，这个时候第二个进程也被阻塞在P(rw)，再次出现了上述问题，该如何解决？其实仔细分析可知道会出现这种情况在于对count的判断和赋值没办法保证一致性，即不是原语操作，为了解决这个问题我们要引入一个互斥信号量mutex来保证对count的判断和赋值是一个互斥的操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 semaphore rw = 1; //用于实现对文件的互斥访问，表示当前是否有进程在访问共享文件 int count = 0; // 记录当前有几个读进程在访问文件 semaphore mutex = 1; //用于保证对count变量的互斥访问 writer(){ while(1){ P(rw); //写之前\u0026#34;加锁\u0026#34; 写文件 V(rw); //写之后\u0026#34;解锁\u0026#34; } } reader(){ while(1){ P(mutex); //各进程互斥访问count if(count==0) P(rw); //第一个读进程负责加锁 count++; //读进程+1 V(mutex); 读文件 P(mutex); //各进程互斥访问count count--; //访问文件的读进程数-1 if(count == 0) V(rw); //最后一个读进程负责解锁 V(mutex); } } 在这个算法中，读进程是优先的，如果有一个读进程正在访问文件，这个时候来了一堆读进程和一个写进程，那么读进程一直在访问文件，写进程可能一直等待，发生”饿死“，如何解决这个问题？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 semaphore rw = 1; //用于实现对文件的互斥访问，表示当前是否有进程在访问共享文件 int count = 0; // 记录当前有几个读进程在访问文件 semaphore mutex = 1; //用于保证对count变量的互斥访问 semaphore w = 1 // 实现写者优先 writer(){ while(1){ P(w); P(rw); //写之前\u0026#34;加锁\u0026#34; 写文件 V(rw); //写之后\u0026#34;解锁\u0026#34; V(w); } } reader(){ while(1){ p(w); P(mutex); //各进程互斥访问count if(count==0) P(rw); //第一个读进程负责加锁 count++; //读进程+1 V(mutex); V(w); 读文件 P(mutex); //各进程互斥访问count count--; // 访问文件的读进程数-1 if(count == 0) V(rw); //最后一个读进程负责解锁 V(mutex); } } 加入互斥信号量w就可以解决写者饿死的问题了，当一个读者在读文件时此时w已经被解锁了，这个时候一个写者进程尝试访问文件，先对w进行加锁，然后阻塞在rw，此时读者进程再来就会被阻塞在w，等待写进程执行.\n哲学家进餐问题 问题描述\n一张圆桌上坐着5名哲学家，每两个哲学家之间的桌上摆一根筷子，桌子的中间是一碗米饭。哲学家们倾注毕生的精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子(一根一根地拿起)。如果筷子已在他人手上，则需等待。饥饿的哲学家只有同时拿起两根筷子才可以开始进餐，当进餐完毕后，放下筷子继续思考。\n问题分析\n五名哲学家相当于五个进程，每个进程只能访问“左右两边”的资源，且相邻的进程访问相同的资源，形成互斥关系。\n关系分析。系统中有5个哲学家进程，5位哲学家与左右邻居对其中间筷子的访问是互斥关系。 整理思路。这个问题中只有互斥关系，但与之前遇到的问题不同的是，每个哲学家进程需要同时持有两个临界资源才能开始吃饭。如何避免临界资源分配不当造成的死锁现象，是哲学家问题的精髓。 信号量设置。定义互斥信号量数组chopstick[5]={1,1,1,1,1}用于实现对5个筷子的互斥访问。并对哲学家按0~4编号，哲学家i左边的筷子编号为i，右边的筷子编号为(i+1)%5。 伪代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 semaphore chopstick[5] = {1,1,1,1,1}; Philosopher i： while (1) { 思考; P(chopstick[i]); // 拿左边的筷子 P(chopstick[(i+1) % 5]); // 再拿右边的筷子 进食; V(chopstick[i]); // 吃完后，放下左筷子 V(chopstick[(i+1) % 5]);// 放下右筷子 } 这种解法会导致死锁，每个哲学家都拿一只筷子然后等待其他人放下筷子\n为防止死锁发生还可采取的措施：\n最多允许4个哲学家同时去拿左边的筷子； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 semaphore chopstick[5] = {1,1,1,1,1}; semaphore count = 4; Philosopher i： while (1) { 思考; P(count); // 抢锁，直到第5个进程拿左筷子时阻塞 P(chopstick[i]); // 拿左边的筷子 P(chopstick[(i+1) % 5]); // 再拿右边的筷子 进食; V(chopstick[i]); // 吃完后，放下左筷子 V(chopstick[(i+1) % 5]);// 放下右筷子 V(count); } 仅当一个哲学家左右两边的筷子都可用时，才允许他拿筷子； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 记录型信号量法 semaphore chopstick[5] = {1,1,1,1,1}; semaphore mutex = 1; //互斥的取筷子 Philosopher i： while (1) { 思考; p(mutex); P(chopstick[i]); //取左 P(chopstick[(i+1) % 5]); //取右 V(mutex); 进餐; V(fchopstick[i]); //放左 V(chopstick[(i+1) % 5]); //放右 } 1 2 3 4 5 6 7 8 9 10 11 //AND信号量法 semaphore chopstick[5] = {1,1,1,1,1}; Philosopher i; while (1) { 思考; Wait(chopstick[(i+1)%5]，chopstick[i]); 进餐; Signal(chopstick[(i+1) % 5],chopstick[i]); } 给所有哲学家编号，奇数号的哲学家必须首先拿左边的筷子，偶数号的哲学家则反之 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 记录型信号量法 semaphore chopstick[5] = {1,1,1,1,1}; Philosopher i： while (1) { 思考; if(i % 2 == 1) { P(chopstick[i]); //取左 P(chopstick[(i+1) % 5]); // 取右 }else{ P(chopstick[(i+1) % 5]); // 取右 P(chopstick[i]); //取左 } 进餐; V(chopstick[i]); //放左 V(chopstick[(i+1) % 5]); // 放右 } 如何处理死锁问题？ 死锁概念 定义 所谓死锁是指多个进程因竞争资源而造成的一种僵局，若无外力作用，这些进程都将无法向前推进。\n相似概念“饥饿”：在操作系统中，饥饿问题是指由于高优先级请求的不断涌入导致低优先级进程长时间被停滞，无法获得处理器或资源。通常情况下，当一个任务被无限期地推迟时，就会出现饥饿问题。\n与死锁得区别在于：\n死锁发生后操作系统若不干预，那么陷入死锁的所有进程永远被阻塞； 而饥饿发生，仅仅只是长时间没有被分配到处理机资源，但是还是有可能后面一段时间会被执行到，并没有永久阻塞。 死锁产生原因 系统资源的竞争\n通常系统中拥有的不可剥夺资源，其数量不足以满足多个进程运行的需要，多个进程在运行过程中会因争夺资源而陷入僵局。只有对不可剥夺资源的竞争才可能产生死锁，对可剥夺资源的竞争是不会引起死锁的。\n进程推进顺序非法\n进程在运行过程中，请求和释放资源的顺序不当，同样会导致死锁。信号量使用不当也会造成死锁。进程间相互等待对方发来的消息，结果也会造成某些进程间无法继续向前推进。\n死锁产生得必要条件 产生死锁必须同时满足以下四个条件，只要其中任一个条件不成立，死锁就不会发生。\n注意！！！\n此处是必要条件，而非充要条件。四个条件都满足，但不一定发生死锁。\n互斥条件：进程要求对所分配的资源进行排他性控制，即在一段时间内某资源仅为一个进程所占用。此时若有其他进程请求该资源，则请求进程只能等待。 不可剥夺条件：进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放。 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。 循环等待条件：存在一种进程资源的循环等待链，连中每一个进程已获得的资源同时被链中下一个进程所请求。 死锁处理策略 预防策略 所有进程运行起来之前采取的措施\n破环互斥条件 方法\n将只能互斥访问的资源改为同时共享访问，也就是将独占锁改为共享锁\n注: 不是所有资源都能改成可共享的，某些资源无法通过这种方式达到解除死锁的目的\n破坏不可剥夺条件 方法\n请求新资源无法满足时，必须释放已有资源。由OS协助强制剥夺某进程持有的资源。\n实现复杂代价高 此操作过多会导致原进程任务无法推进 破坏请求和保持条件 方法\n进程开始运行时，一次性申请所有所需的资源，这样自身不会阻塞但是会：\n造成资源浪费 其他进程会更加饥饿 阶段性请求和释放资源\n相较于前面一次性申请所有资源更好，每使用完一个资源，就释放它，可以避免长时间暂用而导致其他进程的饥饿问题\n破坏循环等待条件 方法\n避免出现资源申请环路，即对资源事先分类编号，按号分配。\n这种方式可以有效提高资源的利用率和系统吞吐量，但是增加了系统开销，增大了进程对资源的占用时间。 对资源的编号应相对稳定，限制了新设备增加 避免策略 死锁避免：在使用前进行判断，只允许不会产生死锁的进程申请资源；\n死锁避免是利用额外的检验信息，在分配资源时判断是否会出现死锁，只在不会出现死锁的情况下才分配资源。 两种避免办法：\n如果一个进程的请求会导致死锁，则不启动该进程 如果一个进程的增加资源请求会导致死锁，则拒绝该申请。 银行家算法 避免死锁的具体实现通常利用银行家算法\n银行家算法的实质就是**要设法保证系统动态分配资源后不进入不安全状态，以避免可能产生的死锁。**即没当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞。\n银行家算法的执行有个前提条件，即要求进程预先提出自己的最大资源请求，并假设系统拥有固定的资源总量。下面介绍银行家算法所用的主要的数据结构。\n死锁检测和解除 死锁检测 为了能对系统是否已发生了死锁进行检测，必须:\n用某种数据结构来保存资源的请求和分配信息: 提供一种算法， 利用上述信息来检测系统是否已进入死锁状态。 如果系统中剩余的可用资源数足够满足进程的需求，那么这个进程暂时是不会阻塞的，可以顺利地执行下去。如果这个进程执行结束了把资源归还系统，就可能使某些正在等待资源的进程被激活，并顺利地执行下去。相应的，这些被激活的进程执行完了之后又会归还一些资源，这样可能又会激活另外一 些阻塞的进程。\n如果按上述过程分析，最终能消除所有边， 就称这个图是可完全简化的。此时一定没有发生死锁(相当于能找到一个安全序列)\n如果最终不能消除所有边，那么此时就是发生了死锁。最终还连着边的那些进程就是处于死锁状态的进程。\n死锁检测算法\n在资源分配图中，找出既不阻塞又不是孤点的进程Pi。即找出一条有向边与它相连，且该有向边对应资源的申请数量小于等于系统中已有空闲资源数量。如上图中，R1没有空闲资源，R2有一个空闲资源。若所有的连接该进程的边均满足上述条件，则这个进程能继续运行直至完成，然后释放它所占有的所有资源。消去它所有的请求边和分配边，使之称为孤立的结点。在下图中，P1是满足这一条件的进程结点，于是将P1的所有边消去。\n进程Pi所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可能变为非阻塞进程。在下图中，P2 就满足这样的条件。根据中的方法进行一系列简化后，若能消去图中所有的边，则称该图是可完全简化的。\n死锁定理:如果某时刻系统的资源分配图是不可完全简化的，那么此时系统死锁\n死锁解除 一旦检测出死锁的发生，就应该立即解除死锁。\n补充:并不是系统中所有的进程都是死锁状态，用死锁检测算法化简资源分配图后，还连着边的那些进程就是死锁进程\n解除死锁的主要方法有:\n资源剥夺法。挂起(新时放到外存上)某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但是应防止被挂起的进程长时间得不到资源而饥饿。 撤销进程法(或称终止进程法)。强制撤销部分、甚至全部死锁进程，并剥夺这些进程的资源.这种方式的优点是实现简单，但所付出的代价可能会很大。因为有些进程可能已经运行了很长时间，已经接近结束了，一旦被终止可谓功亏一篑，以后还得从头再来。 进程回退法。让一个或多个死锁进程回退到足以避免死锁的地步。这就要求系统要记录进程的历史信息，设置还原点。 ","permalink":"https://cold-bin.github.io/post/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/","tags":["死锁","进程同步与调度"],"title":"操作系统之进程管理"},{"categories":["操作系统"],"contents":"[toc]\n操作系统概念 操作系统（operation system，简称OS），是管理计算机硬件与软件资源的计算机程序。\n例如，在windows上win+r快捷键，然后在弹出输入框输入CMD，显示的黑框框也是一个操作系统。（在这个窗口里，可以控制计算机的各种资源）\n计算机系统构成 由用户、应用程序、操作系统、硬件这四部分组成。\n操作系统向用户和应用程序提供接口。\n用户或应用程序可以通过这些接口访问计算机的硬件资源，如：用户开启或关闭声音、应用程序向操作系统申请磁盘空间或内存等行为。也就是说，操作系统通过封装具体的硬件细节形成接口，对于用户或应用程序而言是个黑盒，但是强大的封装性更易于被使用。\nOS是一种系统软件，担任以下角色\n与硬件交互 对资源共享进行调度管理 解决并发操作处理中存在的协调问题 数据结构复杂，外部接口多样化，便于用户反复使用 OS主要作用\n管理与配置内存 决定系统资源供需的优先次序 控制输入设备与输出设备 操作网络与管理文件系统等基本事务 提供一个用户与系统交互的界面 =\u0026gt; GUI OS的目标和功能 目标\n有效性：提高系统资源利用率、提高系统的吞吐量 方便性：方便用户使用 可扩充性：硬件不断发展，依然适配OS 开放性：兼容不同软件和硬件 功能\n作为计算机系统资源的管理者\n作为用户与计算机硬件系统之间的接口\n接口主要体现在三个方面：操作系统提供的程序接口（也就是系统调用）、操作系统提供的命令接口、操作系统的图形用户接口（GUI）\n实现了对计算机资源的抽象\n抽象的含义就是：将复杂的硬件资源抽象成软件资源，从而提供对外接口的方式，也就是封装\n举个例子\n浏览器搜索：浏览器会将需要将发送的内容组装成报文，再通过系统调用需要使用浏览器所在设备的网络设备进行远程通信\nOS的特征 并发特征是共享、虚拟和异步的前提，其中共享和并发互为前提\n并发是共享的前提\n如果只有单道程序运行，则不存在共享的可能。\n共享是并发的前提\n如果多道程序无法共享部分资源（比如磁盘、网卡等），显然就没有并发这一说（因为相当于只有单道程序访问资源，自然无需调度和管理啦）\n并发是虚拟的前提\n虚拟的目的就是表达出多道程序并发时的运行态。如果没有并发，自然虚拟就没有意义了\n并发是异步的前提\n如果没有并发的多道程序，那么单处理机上的多道程序的分时占用处理机的情况就不会发生，自然就谈不上异步了。\n并发 指的是同一时间间隔内执行和调度多个程序的能力\n宏观上，处理机同时执行多道程序 微观上，处理机在多道程序间高速切换（分时交替执行）（毕竟操作系统的核是有限的，然后程序的数量显然要大于核的数量） 主要关注**单个处理机（单个核）**在同一时间段内处理任务数量的能力\n并行与并发的区别：\n并行指的是同一时刻发生，并发指的是同一时间段内发生\n共享 指的是资源共享，系统中的资源供多个并发执行的应用程序共同使用（显然，共享的前提是并发）\n同时访问方式：同一时段，允许多个程序同时访问共享资源 互斥共享方式：也叫独占式。允许多个应用程序在同一个共享资源上独立而互不干扰地工作 虚拟 指的是使用某种技术把一个物理实体变成多个逻辑上的对应物。\n时分复用技术\n虚拟处理机技术、虚拟设备技术\n多道程序宏观上同时运行在多个核上，微观上是占用核的时间片之后，就会切换下一个应用程序来占用核的时间片，这样轮流工作就保障了多道程序同时运行。\n空分复用技术\n虚拟磁盘技术（磁盘分区）、虚拟存储器技术\n异步 指的是：多道程序环境下，允许多个程序并发执行；单个处理机环境下，多个程序分时交替运行；\n加之程序运行的不可预知性（运行的时机、程序暂停、程序的运行时间、程序的性能）就会导致这样的状况：\n宏观上，所有的程序好像是一直在一起在运行，但是实际上由于多道程序的存在，那么单个处理机就一定会出现多道程序的分时占用处理机运行，也就是说微观上其实是”走走停停“，也就是说多道程序不是同步完成的，而是异步完成的。 显而易见，只有并发出现的多道程序，才会导致分时占用处理机的异步情况发生\n操作系统的发展与分类 1、手工操作阶段 在计算机发展的初期，没有操作系统，人们只能依靠手工操作来使用计算机。\n举个栗子，很多年前，人先手工把专门的 “打孔纸” 打好孔，交给机器计算。因为计算机算的快，人手工打孔慢，这就造成了资源的浪费。如下图所示：\n2、批处理阶段 单道批处理系统 单道批处理系统：一种引入了脱机输入/输出技术 ，并由监督程序负责控制作业输入和输出，但在工作期间只能对单份作业进行处理的系统。\n接着举个栗子，这次我们把很多人手工打好的 “打孔纸” 先通过某种手段(卫星机/外围机)包装好，假设我们把它们包装到一盘“磁带”里。接着我们在把一盘接着一盘包装好的 “磁带” 独立地送入到计算机进行计算。如下图所示：\n主要优点：缓解了一定程度的人机速度矛盾，资源利用率有所提升。 主要缺点：内存中仅能有一道程序运行，只有该程序运行结束之后才能调入下一道程序。CPU有大量的时间是在空闲等待I/O完成。资源利用率依然很低。\n多道批处理系统 多道批处理系统：也是一种引入了脱机输入/输出的技术 ，并由监督程序负责控制作业输入和输出，在工作期间能对多份作业同时进行处理的系统。\n它就解决了 单道批处理系统 资源利用率依然很低的缺点\n在接着举个栗子，在 “磁带” 都准备好后，我们在把一盘接着一盘包装好的 “磁带” 耦合地送入到计算机进行计算。如下图所示：\n主要优点：多道程序并发执行，共享计算机资源。资源利用率大幅提升，CPU和其他资源更能保持“忙碌”状态，系统吞吐量增大。 主要缺点：用户响应时间长，没有人机交互功能（用户提交自己的作业之后就只能等待计算机处理完成，中间不能控制自己的作业执行。比如：无法调试程序/无法在程序运行过程中输入一些参数）。\n3、分时操作系统 分时操作系统：计算机以时间片为单位，轮流为各个用户/作业服务，各个用户可通过终端与计算机进行交互。\n它就解决了 多道批处理系统 不能人机交互的缺点\n比如，由4个人在玩电脑，但他们用的是同一个分时操作系统。那么该系统就会按照 “①②③④①②③…” 的顺序轮流分 50ms 给每个用户使用。\n主要优点：用户请求可以被即时响应，解决了人机交互问题。允许多个用户同时使用一台计算机，并且用户对计算机的操作相互独立，感受不到别人的存在。 主要缺点：不能优先处理一些紧急任务。操作系统对各个用户/作业都是完全公平的，循环地为每个用户/作业服务一个时间片，不区分任务的紧急性。\n4、实时操作系统 实时操作系统：实时反映的一种操作系统，不像分时操作系统那样呆板。在实时操作系统的控制下，计算机系统接收到外部信号后及时进行处理，并且要在严格的时限内处理完事件。实时操作系统的主要特点是及时性和可靠性。\n主要优点：能够优先响应一些紧急任务，某些紧急任务不需时间片排队。\n实时操作系统有时还可以被分为：硬实时操作系统和软实时操作系统。\n5、其他几种操作系统 网络操作系统：是伴随着计算机网络的发展而诞生的，能把网络中各个计算机有机地结合起来，实现数据传送等功能，实现网络中各种资源的共享（如文件共享）和各台计算机之间的通信。（如：Windows NT 就是一种典型的网络操作系统，网站服务器就可以使用）\n分布式操作系统：主要特点是分布性和并行性。系统中的各台计算机地位相同，任何工作都可以分布在这些计算机上，由它们并行、协同完成这个任务。\n个人计算机操作系统：如 Windows XP、MacOS，方便个人使用。\n这些操作系统的每一个抠出来单独讲的话，都是一门学问。我们在这里只了初步的了解。\n操作系统的运行环境 操作系统的运行机制 操作系统其实也是一种程序，程序的执行过程也就是CPU执行一行行机器指令的过程。区别于一般程序员开发的应用程序，实现操作系统的程序就是内核程序。\n1、内核程序\u0026amp;应用程序 内核程序：实现操作系统的程序称为内核程序，许多内核程序结合在一起便组成了操作系统内核。\n应用程序：普通程序员借助编程工具以及高级语言所完成的程序叫做应用程序。\n2、特权指令\u0026amp;非特权指令 **特权指令：**作为系统资源的管理者，操作系统可以使用一些直接关系重大的指令（内存清零等），这些指令被称为特权指令，而且应用程序没有办法直接使用特权指令，这保证了操作系统的安全。\n**非特权指令：**应用程序可以使用的指令，如加减乘除指令等。\nCPU在设计的时候便已经划分出了特权指令以及非特权指令，因此在执行这条命令前CPU就可以先判断指令的内容。\n3、内核态\u0026amp;用户态 CPU有两种状态：内核态（或称核心态，管态）以及用户态（目态）。\n当CPU处于内核态时，说明此时正在运行的是内核程序（CPU上加载的是内核程序，是操作系统这个管理者角色在发挥作用），此时可以执行特权指令；\n当CPU处于用户态时，说明此时正在运行的是应用程序，此时只能执行非特权指令。在CPU中有一个寄存器叫做程序状态寄存器（PSW），该寄存器的01状态来表示此时处于内核态还是用户态。\n下面我们来讲解一下，用户态与内核态之间是如何相互进行变化的。\n内核态\u0026ndash;\u0026gt;用户态：需要执行一条特权指令（指令内容是修改PSW寄存器状态），此时内核态转变为用户态，CPU主动让出使用权，CPU会去加载用户态。\n用户态\u0026ndash;\u0026gt;内核态：由“中断”引发，当CPU检测到中断时，操作系统会强制将CPU变为内核态夺回CPU使用权，再执行与中断有关的一系列操作。\n下面我们用一个小栗子来解释一下这个过程：\n刚开机时，CPU 为“内核态”，操作系统内核程序先上CPU运行 开机完成后，用户可以启动某个应用程序 操作系统内核程序在合适的时候主动让出 CPU，让该应用程序上CPU运行 应用程序运行在“用户态” 此时，一位黑客在应用程序中植入了一条特权指令，企图破坏系统… CPU发现接下来要执行的这条指令是特权指令，但是自己又处于“用户态” 这个非法事件会引发一个中断信号 “中断”使操作系统再次夺回CPU的控制权 操作系统会对引发中断的事件进行处理，处理完了再把CPU使用权交给别的应用程序 中断是操作系统夺回CPU使用权的唯一方式（也是由用户态切换到内核态的唯一方式）\n4、操作系统的内核 当我们刚开始使用操作系统的时候，会发现一些程序并不是必需的（记事本等），这些程序称为操作系统的非内核功能，下图是将操作系统进行了更进一步的划分。\n有人将内核划分为大内核以及微内核，这两种划分在不同的时期各有优势。下面用企业来类比一下操作系统：\n内核就是企业的管理层，负责一些重要的工作。只有管理层（内核程序）才能执行特权指令，普通员工（应用程序）只能执行非特权指令。用户态、核心态之间的切换相当于普通员工和管理层之间的工作交接。\n大内核：企业初创时体量不大，管理层的人会负责大部分的事情。优点是效率高；缺点是组织结构混乱，难以维护。\n微内核：随着企业体量越来越大，管理层只负责最核心的一些工作。优点是组织结构清晰，方便维护；缺点是效率低。\n5、小结 需要注意的是：特权指令只能在核心态下执行； 内核程序只能在核心态下执行。\n操作系统的重要部件和机制 0、时钟 操作系统的时钟是一个很重要的核心部件。CPU时间片的计时、系统本身计时等\n1、中断 操作系统会响应中断信号强制夺回CPU使用权，使用户态转换为内核态。**中断是操作系统夺回CPU使用权的唯一方式，**如果没有“中断”机制，那么一旦应用程序上CPU运行，CPU就会一直运行这个应用程序，其他程序就没有办法再使用CPU，这样的话就不存在并发机制了。一般来说，中断分为两种情况即内中断和外中断。\n内中断 包括：陷入（由应用程序主动引发，如应用程序的系统调用）、故障（由错误条件引发）、终止（由致命错误引发）\n内中断（也称异常）与当前执行的指令有关，中断信号来源于CPU内部。比如，当用户妄图执行特权指令时，CPU便会产生中断，由用户态变为内核态；再比如，当进行除法运算除数为0时同样会发生错误，产生中断。总的来说，就是当前指令是非法的指令，那么一定会引起中断。\n当应用程序想要进行一次系统调用使用内核的服务时，会执行一条**陷入（trap）**指令，陷入指令会引发一条中断信号，操作系统就会夺回CPU的使用权，也就是说，执行“陷入指令”，意味着应用程序主动地将CPU控制权还给操作系统内核。\n例如，一个读文件的应用程序开始运行时，并不是由应用应用程序来读取系统文件，而是该应用程序执行到读文件这一段程序时，自己会发生中断，之后操作系统产生陷入指令，再之后执行操作系统的特权指令进行读取\n外中断 外中断与当前执行的指令无关，中断信号来源于CPU外部。每次在一条指令结束之后CPU都会检查是否有外中断信号。比如时钟信号，或者I/O设备都可以发出外中断信号。\n中断分类 中断的原理：不同的中断信号，需要用不同的中断处理程序来处理。当CPU检测到中断信号后，会根据中断信号的类型去查询“中断向量表”，以此来找到相应的中断处理程序在内存中的存放位置。所以，显然中断处理程序一定是内核程序，工作在内核态。\n中断处理过程 关中断\nCPU不再响应高级中断的请求\n保存断点\n断点指的是PC（或叫IP，也就是执行程序的地址）\n引出中断程序\n每个中断都会有一个中断程序，拿到中断程序的地址，但并没有开始执行中断程序\n保存现场和屏蔽字\n将中断前CPU的所有状态，包括：寄存器的值等现场数据进行保存，方便后续中断执行完毕，CPU现场的恢复\n开中断\n可以响应其他中断请求\n执行中断程序\n再关中断\n恢复现场和屏蔽字\n开中断\n在上面的中断处理过程中，只有执行中断程序的过程里CPU才能去响应其他中断信号，其余阶段都不能响应。\n2、原语 由若干条指令组成的一个程序段，用以完成某个特定的功能，且执行过程中不会被中断，也就是说具备原子性\n本质上就是多条指令的封装，通过中断机制里的开关中断来实现\n3、系统数据结构 进程管理：作业控制块、进程控制块 存储器管理：存储器分配与回收 设备管理：缓冲区、设备控制块 4、系统调用 有操作系统实现，给应用程序调用 是一套接口的集合 应用程序访问内核服务的方式 会导致内核从用户态切换到内核态 如拷贝问题：我如果频繁的读取某项数据，会导致用户态和内核态频繁的切换，就比较消耗资源。可以通过零拷贝的方式来避免这种情况的发生\n虽然出现了用户态和内核态，比较消耗资源，但是更加安全，避免用户态的肆意妄为导致系统崩溃掉\n操作系统的结构设计 大内核OS结构-传统的操作系统结构 无结构OS\n设计基于功能实现和获得高的效率 OS是为数众多的一组过程的集合 OS整体无结构（程序之间） 程序内部代码无结构（goto语句非常多） 模块化OS\n模块的独立性\n模块大小的划分 衡量模块独立性的指标 内聚性：模块内部各部分间联系的紧密程度 耦合度：模块间相互联系和相互影响的程度 模块化OS的优点\n提高OS设计的正确性、可理解性和可维护性 增强OS的可适应性 加速OS的开发过程 模块化OS的缺点\n设计时对模块的划分与接口的规定不精确 模块间存在复杂依赖关系 分层式OS\n分层设计的基本原则：每一层都仅使用其底层所提供的功能和服务。\n分层设计考虑的因素\n程序嵌套 作业调度/进程控制/内存分配 运行频率 频率高-\u0026gt; 低层（速度快）时钟管理 公共模块 用户接口 最高层 微内核OS结构 微内核技术：精心设计的、能实现现代OS核心功能的小型内核，它与一般OS不同，更小更精练，运行于核心态，开机后 常驻内存。但是微内核并不是完成的操作系统，只是抽出了操作系统最核心部分的功能\n微内核OS结构的特征\n以微内核为OS核心 以客户/服务器为基础 采用面向对象的程序设计方法 ","permalink":"https://cold-bin.github.io/post/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/","tags":["中断"],"title":"操作系统之概述"},{"categories":[],"contents":"Hi there 👋 I\u0026rsquo;m a computer science student who is passionate about writing code, solving problems and building software.\n🔭 I’m currently a CS student at cqupt. 📚 I’m currently learning backend development using Golang and how to construct a distributed system. 👯 I’m looking for a backend development internship. 📫 How to reach me: qq-mail or leave message in my blog. 🖋 Writer at blog. ♟ I’m a Genshin Gamer to like HuTao and Shenhe. friend\u0026rsquo;s links kun-kun\n","permalink":"https://cold-bin.github.io/about/","tags":[],"title":"关于我"},{"categories":[],"contents":" 记录个人觉得有意义的各种博客、公众号页面、视频等。\n博客园 地址 内容 微信公众号 程序员自由之路 主要是JavaEE开发，关于Java框架的文章很多 Michael翔 主要是JavaEE开发 Coder 魔法院 秋风飒飒吹 软件开发的各方面均有涉及 Ccww 主要是Java和Spring框架 Throwable 主要是JavaEE开发 ChokCoco 某个CSS大佬，国服第一切图仔，有很多关于CSS炫酷特性的文章 CODE WITH ARHO 一个芬兰码农，博客内容大部分是JUnit测试相关 cloudchewie.com 一个华中科技大学学生的个人博客，样式很炫 Why技术-博客园Why技术-个人网站 里面有很多高质量的关于Java源码阅读分析的博文💚 why技术 Hello 算法 关于算法学习的网站 小林coding 关于计算机网络、计算机基础等的网站，质量很高💚 小林coding 单个博文 docker安装Elasticsearch7.6集群并设置密码\n我试图通过这篇文章，教会你一种阅读源码的方式\nLGTM? 那些迷之缩写\nOracle官方的JDK各版本特性说明\n斯坦福大学关于位运算技巧的总结\nVim快捷键集合\n其它\nicon stop the war ","permalink":"https://cold-bin.github.io/links/","tags":[],"title":"各种外链"},{"categories":["汇编"],"contents":"指令系统总结 我们对8086CPU的指令系统进行一下总结。读者若要详细了解8086指令系统中的各个指令的用，可以查看有关的指令手册。\n8086CPU提供以下几大类指令。\n数据传送指令 mov、push、pop、pushf、popf、xchg 等都是数据传送指令，这些指令实现寄存器和内存、寄器和寄存器之间的单个数据传送。 算术运算指令 add、sub、adc、sbb、inc、dec、cmp、imul、idiv、aaa等都是算术运算指令，这些指令实现存器和内存中的数据的算数运算。它们的执行结果影响标志寄存器的sf、zf、of、cf、pf、af位。 逻辑指令 and、or、not、xor、test、shl、shr、sal、sar、rol、ror、rcl、rcr等都是逻辑指令。除了not指外，它们的执行结果都影响标志寄存器的相关标志位。 转移指令 可以修改IP，或同时修改CS和IP的指令统称为转移指令。转移指令分为以下几类。 （1）无条件转移指令，比如，jmp； （2）条件转移指令，比如，jcxz、je、jb、ja、jnb、jna等； （3）循环指令，比如，loop； （4）过程，比如，call、ret、retf； （5）中断，比如，int、iret。 处理机控制指令 对标志寄存器或其他处理机状态进行设置，cld、std、cli、sti、nop、clc、cmc、stc、hlt、wait、esc、lock等都是处理机控制指令。 串处理指令 对内存中的批量数据进行处理，movsb、movsw、cmps、scas、lods、stos等。若要使用这些指令方便地进行批量数据的处理，则需要和rep、repe、repne 等前缀指令配合使用。 ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%80%BB%E7%BB%93/","tags":[],"title":"汇编之总结"},{"categories":["汇编"],"contents":"外中断 1、外中断 CPU在计算机系统中，除了能够执行指令，进行运算以外，还应该能够对外部设备进行控制，接收它们的输入，向它们进行输出（I/O能力）\nPC系统的接口卡和主板上，装有各种接口芯片。这些外设接口芯片的内部有若干寄存器，CPU将这些寄存器当作端口来访问\n外设的输入不直接送入内存和CPU，而是送入相关的接口芯片的端口中； CPU向外设的输出也不是直接送入外设，而是先送入端口中，再由相关的芯片送到外设。 CPU还可以向外设输出控制命令，而这些控制命令也是先送到相关芯片的端口中，然后再由相关的芯片根据命令对外设实施控制。\n即：CPU通过端口和外部设备进行联系\n当CPU外部有需要处理的事情发生的时候，比如说，外设的输入到达，相关芯片将向CPU发出相应的中断信息。CPU在执行完当前指令后，可以检测到发送过来的中断信息，引发中断过程，处理外设的输入。\nPC系统中，外中断源有两类\n1、可屏蔽中断\n可屏蔽中断是CPU可以不响应的外中断。CPU是否响应可屏蔽中断，要看标志寄存器的IF位的设置。 当CPU检测到可屏蔽中断信息时，如果IF=1，则CPU在执行完当前指令后响应中断，引发中断过程；如果IF=0，则不响应可屏蔽中断。\n可屏蔽中断信息来自于CPU外部，中断类型码是通过数据总线送入CPU的；而内中断的中断类型码是在CPU内部产生的。\n中断过程中将IF置0的原因就是，在进入中断处理程序后，禁止其他的可屏蔽中断。 如果在中断处理程序中需要处理可屏蔽中断，可以用指令将IF置1。\n8086CPU提供的设置IF的指令：sti，设置IF=1；cli，设置IF=0。\n2、不可屏蔽中断\n不可屏蔽中断是CPU必须响应的外中断。当CPU检测到不可屏蔽中断信息时，则在执行完当前指令后，立即响应，引发中断过程。\n对于8086CPU，不可屏蔽中断的中断类型码固定为2，所以中断过程中，不需要取中断类型码。则不可屏蔽中断的中断过程为：①标志寄存器入栈，IF=0，TF=0；②CS、IP入栈；③（IP）=（8），（CS）=（0AH）。\n几乎所有由外设引发的外中断，都是可屏蔽中断。当外设有需要处理的事件（比如说键盘输入）发生时，相关芯片向CPU发出可屏蔽中断信息。不可屏蔽中断是在系统中有必须处理的紧急情况发生时用来通知CPU的中断信息。\n2、PC机键盘的处理过程 键盘中有一个芯片对键盘上的每一个键的开关状态进行扫描。按下一个键时，开关接通，该芯片就产生一个扫描码，扫描码说明了按下的键在键盘上的位置。扫描码被送入主板上的相关接口芯片的寄存器中，该寄存器的端口地址为60h。松开按下的键时，也产生一个扫描码，扫描码说明了松开的键在键盘上的位置。松开按键时产生的扫描码也被送入60h端口中。\n一般将按下一个键时产生的扫描码称为通码，松开一个键产生的扫描码称为断码。\n扫描码长度为一个字节，通码的第7位为0，断码的第7位为1 即：断码 = 通码 + 80h。比如，g键的通码为22h，断码为a2h\n键盘的输入到达60h端口时，相关的芯片就会向CPU发出中断类型码为9的可屏蔽中断信息。CPU检测到该中断信息后，如果IF=1，则响应中断，引发中断过程，转去执行int 9中断例程。\nBIOS提供了int 9中断例程，用来进行基本的键盘输入处理，主要的工作如下： （1）读出60h端口中的扫描码； （2）如果是字符键的扫描码，将该扫描码和它所对应的字符码（即ASCII码）送入内存中的BIOS键盘缓冲区； 如果是控制键（比如Ctrl）和切换键（比如CapsLock）的扫描码，则将其转变为状态字节写入内存中存储状态字节的单元； （3）对键盘系统进行相关的控制，比如说，向相关芯片发出应答信息。\nBIOS键盘缓冲区可以存储15个键盘输入，一个键盘输入用一个字单元存放，高位字节存放扫描码，低位字节存放字符码。\n0040:17单元存储键盘状态字节，该字节记录了控制键和切换键的状态。键盘状态字节各位记录的信息如下。\n0 右shift状态 置1表示按下右shift键 1 左shift状态 置1表示按下左shift键 2 Ctrl状态 置1表示按下Ctrl键 3 Alt状态 置1表示按下Alt键 4 ScrollLock状态 置1表示Scroll指示灯亮 5 NumLock状态 置1表示小键盘输入的是数字 6 CapsLock状态 置1表示输入大写字母 7 Insert状态 置1表示处于删除态 编写int 9中断例程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 ;编程：在屏幕中间依次显示“a”~“z”，并可以让人看清。在显示的过程中，按下\u0026#39;Esc\u0026#39;键后，改变显示的颜色。 ;完整功能代码： assume cs:code stack segment db 128 dup (0) stack ends data segment dw 0,0 data ends code segment start:\tmov ax,stack mov ss,ax mov sp,128 mov ax,data mov ds,ax mov ax,0 mov es,ax push es:[9*4] pop ds:[0] push es:[9*4+2] pop ds:[2]\t;将原来的int 9中断例程的入口地址保存在ds:0、ds:2单元中 mov word ptr es:[9*4], offset int9 mov es:[9*4+2], cs\t;在中断向量表中设置新的int 9中断例程的入口地址 ;显示字符串 mov ax, 0b800h mov es, ax mov ah, \u0026#39;a\u0026#39; s:\tmov es:[160*12+40*2], ah call delay inc ah cmp ah, \u0026#39;z\u0026#39; jna s mov ax,0 mov es,ax push ds:[0] pop es:[9*4] push ds;[2] pop es;[9*4+2] ;将中断向量表中int 9中断例程的入口恢复为原来的地址 mov ax,4c00h int 21h ;将循环延时的程序段写为一个子程序 delay:\tpush ax push dx mov dx, 2000h ;用两个16位寄存器来存放32位的循环次数 mov ax, 0 s1: sub ax, 1 sbb dx, 0 cmp ax, 0 jne s1 cmp dx, 0 jne s1 pop dx pop ax ret ;------以下为新的int 9中断例程-------------------- int9:\tpush ax push bx push es in al, 60h;从端口60h读出键盘的输入 pushf ;标志寄存器入栈 pushf pop bx and bh,11111100b push bx popf\t;TF=0,IF=0 call dword ptr ds:[0] ;对int指令进行模拟，调用原来的int 9中断例程 cmp al,1 jne int9ret mov ax,0b800h mov es,ax inc byte ptr es:[160*12+40*2+1] ;属性增加1，改变颜色 int9ret: pop es pop bx pop ax iret code ends end start CPU对外设输入的通常处理方法 （1）外设的输入送入端口； （2）向CPU发出外中断（可屏蔽中断）信息； （3）CPU检测到可屏蔽中断信息，如果IF=1，CPU在执行完当前指令后响应中断，执行相应的中断例程； （4）可在中断例程中实现对外设输入的处理。\n端口和中断机制，是CPU进行I/O的基础。\n","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/","tags":[],"title":"汇编之外中断"},{"categories":["汇编"],"contents":"[toc]\n端口 在PC机系统中，和CPU通过总线相连的芯片除各种存储器外，还有以下3种芯片。\n各种接口卡（比如，网卡、显卡）上的接口芯片，它们控制接口卡进行工作； 主板上的接口芯片，CPU通过它们对部分外设进行访问； 其他芯片，用来存储相关的系统信息，或进行相关的输入输出处理。 在这些芯片中，都有一组可以由CPU读写的寄存器。这些寄存器，它们在物理上可能处于不同的芯片中， 但是它们在以下两点上相同。\n都和CPU的总线相连，这种连接是通过它们所在的芯片进行的； CPU对它们（芯片上的寄存器）进行读或写的时候都通过控制线向它们所在的芯片发出端口读写命令。 从CPU的角度，将这些寄存器都当作端口，对它们进行统一编址，从而建立了一个统一的端口地址空间。 每一个端口在地址空间中都有一个地址。在访问端口的时候，CPU通过端口地址来定位端口。因为端口所在的芯片和CPU通过总线相连，\nCPU可以直接读写以下3个地方的数据。\nCPU内部的寄存器 内存单元 端口 1、端口的读写 端口地址和内存地址一样，通过地址总线来传送。在PC系统中，CPU最多可以定位64KB个不同的端口。则端口地址的范围为0-65535。\n端口的读写指令只有两条：in和out，分别用于从端口读取数据和往端口写入数据。\n在in和out指令中，只能使用ax或al来存放从端口中读入的数据或要发送到端口中的数据。\n1 2 3 4 5 6 7 8 ;对0~255以内的端口进行读写时： in al, 20h ;从20h端口读入一个字节 out 20h, al ;往20h端口写入一个字节 ;对256~65535的端口进行读写时，端口号放在dx中： mov dx, 3f8h ;将端口号3f8h送入dx in al, dx ;从3f8h端口读入一个字节 out dx, al ;向3f8h端口写入一个字节 2、CMOS RAM芯片 PC机中，有一个CMOS RAM芯片，一般简称为CMOS。此芯片的特征如下\n包含一个实时钟和一个有128个存储单元的RAM存储器 该芯片靠电池供电。关机后内部的实时钟正常工作，RAM中的信息不丢失 128个字节的RAM中，内部实时钟占用0~0dh单元来保存时间信息，其余大部分单元用于保存系统配置信息，供系统启动时BIOS程序读取。BIOS也提供了相关的程序，使我们可以在开机的时候配置CMOS RAM中的系统信息。 该芯片内部有两个端口，端口地址为70h和71h。CPU通过这两个端口来读写CMOS RAM 70h为地址端口，存放要访问的CMOS RAM单元的地址；71h为数据端口，存放从选定的CMOS RAM单元中读取的数据，或要写入到其中的数据。 可见，CPU对CMOS RAM的读写分两步进行，比如，读CMOS RAM的2号单元： ①将2送入端口70h； ②从端口71h读出2号单元的内容。 CMOS RAM中存储的时间信息\n在CMOS RAM中，存放着当前的时间：年、月、日、时、分、秒。长度都为1个字节， 存放单元为：\n9 8 7 6 5 4 3 2 1 0 年 月 日 时 分 秒 BCD码是以4位二进制数表示十进制数码的编码方法 4 == 0100B\n一个字节可表示两个BCD码。则CMOS RAM存储时间信息的单元中，存储了用两个BCD码表示的两位十进制数，高4位的BCD码表示十位，低4位的BCD码表示个位。比如，00010100b表示14。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ;编程，在屏幕中间显示当前的月份。 assume cs:code code segment start:\tmov al，8 ;从CMOS RAM的8号单元读出当前月份的BCD码。 out 70h，al in al, 71h ;从数据端口71h中取得指定单元中的数据： mov ah, al ;al中为从CMOS RAM的8号单元中读出的数据 mov cl, 4 shr ah, cl ;ah中为月份的十位数码值,左移四位空出四位 and al, 00001111b ;al中为月份的个位数码值 add ah, 30h ;BCD码值+30h=十进制数对应的ASCII add al, 30h mov bx, 0b800h mov es, bx mov byte ptr es:[160*12+40*2], ah ;显示月份的十位数码 mov byte ptr es:[160*12+40*2+2], al ;接着显示月份的个位数码 mov ax，4c00h int 21h code ends end start 3、shl和shr指令 shl和shr是逻辑移位指令\nshl是逻辑左移指令，它的功能为：\n将一个寄存器或内存单元中的数据向左移位； 将最后移出的一位写入CF中； 最低位用0补充。 shr是逻辑右移指令，同理\n1 2 3 4 5 6 7 8 9 mov al, 01001000b shl al, 1 ;将a1中的数据左移一位执行后（al）=10010000b，CF=0。 mov al, 01010001b mov cl, 3 ;如果移动位数大于1时，必须将移动位数放在cl中 shl al, c1 mov al, 10000001b shr al, 1 ;将al中的数据右移一位执行后（al）=01000000b，CF=1。 将X逻辑左移一位，相当于执行X=X*2 将X逻辑右移一位，相当于执行X=X/2\n4、总结 什么是端口？\nCPU通过总线连着很多芯片（网卡、显卡、某些外设）。CPU和这些芯片“沟通”的渠道就是端口。例如使用cmos ram芯片时，CPU通过地址总线将地址信息传输到这个芯片接收地址信号的端口上，这样就可以指定芯片内部的CPU指令的操作对象地址，之后，数据则会通过芯片的另一个端口进行读或写。\n对端口的读写实际上就是遵照“先传地址，再操作数据”的原则，本质上就是对和CPU相联的芯片里面的寄存器进行读写。\n","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/","tags":["汇编里的端口"],"title":"汇编之端口"},{"categories":["redis"],"contents":"[toc]\nRedis高级篇之最佳实践 今日内容\nRedis键值设计 批处理优化 服务端优化 集群最佳实践 1、Redis键值设计 1.1、优雅的key结构 Redis的Key虽然可以自定义，但最好遵循下面的几个最佳实践约定：\n遵循基本格式：[业务名称]:[数据名]:[id] 长度不超过44字节 不包含特殊字符 例如：我们的登录业务，保存用户信息，其key可以设计成如下格式：\n这样设计的好处：\n可读性强 避免key冲突 方便管理 更节省内存： key是string类型，底层编码包含int、embstr和raw三种。embstr在小于44字节使用，采用连续内存空间，内存占用更小。当字节数大于44字节时，会转为raw模式存储，在raw模式下，内存空间不是连续的，而是采用一个指针指向了另外一段内存空间，在这段空间里存储SDS内容，这样空间不连续，访问的时候性能也就会收到影响，还有可能产生内存碎片 1.2、拒绝BigKey BigKey通常以Key的大小和Key中成员的数量来综合判定，例如：\nKey本身的数据量过大：一个String类型的Key，它的值为 5 MB Key中的成员数过多：一个ZSET类型的Key，它的成员数量为10,000个 Key中成员的数据量过大：一个Hash类型的Key，它的成员数量虽然只有1,000个但这些成员的Value（值）总大小为100 MB 那么如何判断元素的大小呢？redis也给我们提供了命令\n推荐值：\n单个key的value小于10KB 对于集合类型的key，建议元素数量小于1000 1.2.1、BigKey的危害 网络阻塞 对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢 数据倾斜 BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡 Redis阻塞 对元素较多的hash、list、zset等做运算会耗时较旧，使主线程被阻塞 CPU压力 对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本机其它应用 1.2.2、如何发现BigKey ①redis-cli \u0026ndash;bigkeys 利用redis-cli提供的\u0026ndash;bigkeys参数，可以遍历分析所有key，并返回Key的整体统计信息与每个数据的Top1的big key\n命令：redis-cli -a 密码 --bigkeys\n②scan扫描 自己编程，利用scan扫描Redis中的所有key，利用strlen、hlen等命令判断key的长度（此处不建议使用MEMORY USAGE）\nscan 命令调用完后每次会返回2个元素，第一个是下一次迭代的光标，第一次光标会设置为0，当最后一次scan 返回的光标等于0时，表示整个scan遍历结束了，第二个返回的是List，一个匹配的key的数组\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 import com.heima.jedis.util.JedisConnectionFactory; import org.junit.jupiter.api.AfterEach; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import redis.clients.jedis.Jedis; import redis.clients.jedis.ScanResult; import java.util.HashMap; import java.util.List; import java.util.Map; public class JedisTest { private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\u0026#34;192.168.150.101\u0026#34;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\u0026#34;123321\u0026#34;); // 3.选择库 jedis.select(0); } final static int STR_MAX_LEN = 10 * 1024; final static int HASH_MAX_LEN = 500; @Test void testScan() { int maxLen = 0; long len = 0; String cursor = \u0026#34;0\u0026#34;; do { // 扫描并获取一部分key ScanResult\u0026lt;String\u0026gt; result = jedis.scan(cursor); // 记录cursor cursor = result.getCursor(); List\u0026lt;String\u0026gt; list = result.getResult(); if (list == null || list.isEmpty()) { break; } // 遍历 for (String key : list) { // 判断key的类型 String type = jedis.type(key); switch (type) { case \u0026#34;string\u0026#34;: len = jedis.strlen(key); maxLen = STR_MAX_LEN; break; case \u0026#34;hash\u0026#34;: len = jedis.hlen(key); maxLen = HASH_MAX_LEN; break; case \u0026#34;list\u0026#34;: len = jedis.llen(key); maxLen = HASH_MAX_LEN; break; case \u0026#34;set\u0026#34;: len = jedis.scard(key); maxLen = HASH_MAX_LEN; break; case \u0026#34;zset\u0026#34;: len = jedis.zcard(key); maxLen = HASH_MAX_LEN; break; default: break; } if (len \u0026gt;= maxLen) { System.out.printf(\u0026#34;Found big key : %s, type: %s, length or size: %d %n\u0026#34;, key, type, len); } } } while (!cursor.equals(\u0026#34;0\u0026#34;)); } @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } } ③第三方工具 利用第三方工具，如 Redis-Rdb-Tools 分析RDB快照文件，全面分析内存使用情况 https://github.com/sripathikrishnan/redis-rdb-tools ④网络监控 自定义工具，监控进出Redis的网络数据，超出预警值时主动告警 一般阿里云搭建的云服务器就有相关监控页面 1.2.3、如何删除BigKey BigKey内存占用较多，即便时删除这样的key也需要耗费很长时间，导致Redis主线程阻塞，引发一系列问题。\nredis 3.0 及以下版本 如果是集合类型，则遍历BigKey的元素，先逐个删除子元素，最后删除BigKey Redis 4.0以后 Redis在4.0后提供了异步删除的命令：unlink 1.3、恰当的数据类型 例1 比如存储一个User对象，我们有三种存储方式：\n①方式一：json字符串 user:1 {\u0026ldquo;name\u0026rdquo;: \u0026ldquo;Jack\u0026rdquo;, \u0026ldquo;age\u0026rdquo;: 21} 优点：实现简单粗暴\n缺点：数据耦合，不够灵活\n②方式二：字段打散 user:1:name Jack user:1:age 21 优点：可以灵活访问对象任意字段\n缺点：占用空间大、没办法做统一控制\n③方式三：hash（推荐） user:1\rname\rjack\rage\r21\r优点：底层使用ziplist，空间占用小，可以灵活访问对象的任意字段\n缺点：代码相对复杂\n例2 假如有hash类型的key，其中有100万对field和value，field是自增id，这个key存在什么问题？如何优化？\nkey\rfield\rvalue\rsomeKey\rid:0\rvalue0\r.....\r.....\rid:999999\rvalue999999\r存在的问题：\nhash的entry数量超过500时，会使用哈希表而不是ZipList，内存占用较多\n可以通过hash-max-ziplist-entries配置entry上限。但是如果entry过多就会导致BigKey问题\n方案一 拆分为string类型\nkey\rvalue\rid:0\rvalue0\r.....\r.....\rid:999999\rvalue999999\r存在的问题：\nstring结构底层没有太多内存优化，内存占用较多 想要批量获取这些数据比较麻烦 方案二 拆分为小的hash，将 id / 100 作为key， 将id % 100 作为field，这样每100个元素为一个Hash\nkey\rfield\rvalue\rkey:0\rid:00\rvalue0\r.....\r.....\rid:99\rvalue99\rkey:1\rid:00\rvalue100\r.....\r.....\rid:99\rvalue199\r....\rkey:9999\rid:00\rvalue999900\r.....\r.....\rid:99\rvalue999999\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 package com.heima.test; import com.heima.jedis.util.JedisConnectionFactory; import org.junit.jupiter.api.AfterEach; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import redis.clients.jedis.Jedis; import redis.clients.jedis.Pipeline; import redis.clients.jedis.ScanResult; import java.util.HashMap; import java.util.List; import java.util.Map; public class JedisTest { private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\u0026#34;192.168.150.101\u0026#34;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\u0026#34;123321\u0026#34;); // 3.选择库 jedis.select(0); } @Test void testSetBigKey() { Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 1; i \u0026lt;= 650; i++) { map.put(\u0026#34;hello_\u0026#34; + i, \u0026#34;world!\u0026#34;); } jedis.hmset(\u0026#34;m2\u0026#34;, map); } @Test void testBigHash() { Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 1; i \u0026lt;= 100000; i++) { map.put(\u0026#34;key_\u0026#34; + i, \u0026#34;value_\u0026#34; + i); } jedis.hmset(\u0026#34;test:big:hash\u0026#34;, map); } @Test void testBigString() { for (int i = 1; i \u0026lt;= 100000; i++) { jedis.set(\u0026#34;test:str:key_\u0026#34; + i, \u0026#34;value_\u0026#34; + i); } } @Test void testSmallHash() { int hashSize = 100; Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(hashSize); for (int i = 1; i \u0026lt;= 100000; i++) { int k = (i - 1) / hashSize; int v = i % hashSize; map.put(\u0026#34;key_\u0026#34; + v, \u0026#34;value_\u0026#34; + v); if (v == 0) { jedis.hmset(\u0026#34;test:small:hash_\u0026#34; + k, map); } } } @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } } 1.4、总结 Key的最佳实践 固定格式：[业务名]:[数据名]:[id] 足够简短：不超过44字节 不包含特殊字符 Value的最佳实践： 合理的拆分数据，拒绝BigKey 选择合适数据结构 Hash结构的entry数量不要超过1000 设置合理的超时时间 2、批处理优化 2.1、Pipeline 2.1.1、我们的客户端与redis服务器是这样交互的 单个命令的执行流程\nN条命令的执行流程\nredis处理指令是很快的，主要花费的时候在于网络传输。于是乎很容易想到将多条指令批量的传输给redis\n2.1.2、MSet Redis提供了很多Mxxx这样的命令，可以实现批量插入数据，例如：\nmset hmset 利用mset批量插入10万条数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Test void testMxx() { String[] arr = new String[2000]; int j; long b = System.currentTimeMillis(); for (int i = 1; i \u0026lt;= 100000; i++) { j = (i % 1000) \u0026lt;\u0026lt; 1; arr[j] = \u0026#34;test:key_\u0026#34; + i; arr[j + 1] = \u0026#34;value_\u0026#34; + i; if (j == 0) { jedis.mset(arr); } } long e = System.currentTimeMillis(); System.out.println(\u0026#34;time: \u0026#34; + (e - b)); } 2.1.3、Pipeline MSET虽然可以批处理，但是却只能操作部分数据类型，因此如果有对复杂数据类型的批处理需要，建议使用Pipeline\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Test void testPipeline() { // 创建管道 Pipeline pipeline = jedis.pipelined(); long b = System.currentTimeMillis(); for (int i = 1; i \u0026lt;= 100000; i++) { // 放入命令到管道 pipeline.set(\u0026#34;test:key_\u0026#34; + i, \u0026#34;value_\u0026#34; + i); if (i % 1000 == 0) { // 每放入1000条命令，批量执行 pipeline.sync(); } } long e = System.currentTimeMillis(); System.out.println(\u0026#34;time: \u0026#34; + (e - b)); } 2.2、集群下的批处理 Redis Cluster及hash slot 算法\n如MSET或Pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中(否则redis会直接报错)，否则就会导致执行失败。大家可以想一想这样的要求其实很难实现，因为我们在批处理时，可能一次要插入很多条数据，这些数据很有可能不会都落在相同的节点上，这就会导致报错了\n这个时候，我们可以找到4种解决方案\n第一种方案：串行执行，所以这种方式没有什么意义，当然，执行起来就很简单了，缺点就是耗时过久。\n第二种方案：串行slot，简单来说，就是执行前，客户端先计算一下对应的key的slot，一样slot的key就放到一个组里边，不同的，就放到不同的组里边，然后对每个组执行pipeline的批处理，他就能串行执行各个组的命令，这种做法比第一种方法耗时要少，但是缺点呢，相对来说复杂一点，所以这种方案还需要优化一下\n第三种方案：并行slot，相较于第二种方案，在分组完成后串行执行，第三种方案，就变成了并行执行各个命令，所以他的耗时就非常短，但是实现呢，也更加复杂。\n第四种：hash_tag，redis计算key的slot的时候，其实是根据key的有效部分来计算的，通过这种方式就能一次处理所有的key，这种方式耗时最短，实现也简单，但是如果通过操作key的有效部分，那么就会导致所有的key都落在一个节点上，产生数据倾斜的问题，所以我们推荐使用第三种方式。\n我们可以在redis的key字符串前添加{}符号，这样可以将大括号里面的字符内容作为有效部分，让redis计算key，寻找slot的有效值，从而找到对应master节点。这样，我们可以做到：将批处理命令的某一部分放到同一个节点时，我们需要在大括号里分配相同的字符值，这样在计算查找slot的对应master节点才会相同。\n2.2.1 串行化执行代码实践 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 public class JedisClusterTest { private JedisCluster jedisCluster; @BeforeEach void setUp() { // 配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); HashSet\u0026lt;HostAndPort\u0026gt; nodes = new HashSet\u0026lt;\u0026gt;(); nodes.add(new HostAndPort(\u0026#34;192.168.150.101\u0026#34;, 7001)); nodes.add(new HostAndPort(\u0026#34;192.168.150.101\u0026#34;, 7002)); nodes.add(new HostAndPort(\u0026#34;192.168.150.101\u0026#34;, 7003)); nodes.add(new HostAndPort(\u0026#34;192.168.150.101\u0026#34;, 8001)); nodes.add(new HostAndPort(\u0026#34;192.168.150.101\u0026#34;, 8002)); nodes.add(new HostAndPort(\u0026#34;192.168.150.101\u0026#34;, 8003)); jedisCluster = new JedisCluster(nodes, poolConfig); } @Test void testMSet() { jedisCluster.mset(\u0026#34;name\u0026#34;, \u0026#34;Jack\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;, \u0026#34;sex\u0026#34;, \u0026#34;male\u0026#34;); } @Test void testMSet2() { Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(3); map.put(\u0026#34;name\u0026#34;, \u0026#34;Jack\u0026#34;); map.put(\u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;); map.put(\u0026#34;sex\u0026#34;, \u0026#34;Male\u0026#34;); //对Map数据进行分组。根据相同的slot放在一个分组 //key就是slot，value就是一个组 Map\u0026lt;Integer, List\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt;\u0026gt; result = map.entrySet() .stream() .collect(Collectors.groupingBy( entry -\u0026gt; ClusterSlotHashUtil.calculateSlot(entry.getKey())) ); //串行的去执行mset的逻辑 for (List\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; list : result.values()) { String[] arr = new String[list.size() * 2]; int j = 0; for (int i = 0; i \u0026lt; list.size(); i++) { j = i\u0026lt;\u0026lt;2; Map.Entry\u0026lt;String, String\u0026gt; e = list.get(0); arr[j] = e.getKey(); arr[j + 1] = e.getValue(); } jedisCluster.mset(arr); } } @AfterEach void tearDown() { if (jedisCluster != null) { jedisCluster.close(); } } } 2.2.2 Spring集群环境下批处理代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Test void testMSetInCluster() { Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(3); map.put(\u0026#34;name\u0026#34;, \u0026#34;Rose\u0026#34;); map.put(\u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;); map.put(\u0026#34;sex\u0026#34;, \u0026#34;Female\u0026#34;); stringRedisTemplate.opsForValue().multiSet(map); List\u0026lt;String\u0026gt; strings = stringRedisTemplate.opsForValue().multiGet(Arrays.asList(\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;sex\u0026#34;)); strings.forEach(System.out::println); } 原理分析\n在RedisAdvancedClusterAsyncCommandsImpl 类中\n首先根据slotHash算出来一个partitioned的map，map中的key就是slot，而他的value就是对应的对应相同slot的key对应的数据\n通过 RedisFuture mset = super.mset(op);进行异步的消息发送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Override public RedisFuture\u0026lt;String\u0026gt; mset(Map\u0026lt;K, V\u0026gt; map) { Map\u0026lt;Integer, List\u0026lt;K\u0026gt;\u0026gt; partitioned = SlotHash.partition(codec, map.keySet()); if (partitioned.size() \u0026lt; 2) { return super.mset(map); } Map\u0026lt;Integer, RedisFuture\u0026lt;String\u0026gt;\u0026gt; executions = new HashMap\u0026lt;\u0026gt;(); for (Map.Entry\u0026lt;Integer, List\u0026lt;K\u0026gt;\u0026gt; entry : partitioned.entrySet()) { Map\u0026lt;K, V\u0026gt; op = new HashMap\u0026lt;\u0026gt;(); entry.getValue().forEach(k -\u0026gt; op.put(k, map.get(k))); RedisFuture\u0026lt;String\u0026gt; mset = super.mset(op); executions.put(entry.getKey(), mset); } return MultiNodeExecution.firstOfAsync(executions); } 3、服务器端优化-持久化配置 Redis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议：\n用来做缓存的Redis实例尽量不要开启持久化功能 建议关闭RDB持久化功能，使用AOF持久化 利用脚本定期在slave节点做RDB，实现数据备份 设置合理的rewrite阈值，避免频繁的bgrewrite 配置no-appendfsync-on-rewrite = yes，禁止在rewrite期间做aof，避免因AOF引起的阻塞 部署有关建议： Redis实例的物理机要预留足够内存，应对fork和rewrite 单个Redis实例内存上限不要太大，例如4G或8G。可以加快fork的速度、减少主从同步、数据迁移压力 不要与CPU密集型应用部署在一起 不要与高硬盘负载应用一起部署。例如：数据库、消息队列 4、服务器端优化-慢查询优化 4.1 什么是慢查询 并不是很慢的查询才是慢查询，而是：在Redis执行时耗时超过某个阈值的命令，称为慢查询。\n慢查询的危害：由于Redis是单线程的，所以当客户端发出指令后，他们都会进入到redis底层的queue来执行，如果此时有一些慢查询的数据，就会导致大量请求阻塞，从而引起报错，所以我们需要解决慢查询问题。\n慢查询的阈值可以通过配置指定：\nslowlog-log-slower-than：慢查询阈值，单位是微秒。默认是10000，建议1000\n慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定：\nslowlog-max-len：慢查询日志（本质是一个队列）的长度。默认是128，建议1000\n修改这两个配置可以使用：config set命令：\n4.2 如何查看慢查询 知道了以上内容之后，那么咱们如何去查看慢查询日志列表呢：\nslowlog len：查询慢查询日志长度 slowlog get [n]：读取n条慢查询日志 slowlog reset：清空慢查询列表 5、服务器端优化-命令及安全配置 安全可以说是服务器端一个非常重要的话题，如果安全出现了问题，那么一旦这个漏洞被一些坏人知道了之后，并且进行攻击，那么这就会给咱们的系统带来很多的损失，所以我们这节课就来解决这个问题。\nRedis会绑定在0.0.0.0:6379，这样将会将Redis服务暴露到公网上，而Redis如果没有做身份认证，会出现严重的安全漏洞. 漏洞重现方式：https://cloud.tencent.com/developer/article/1039000\n为什么会出现不需要密码也能够登录呢，主要是Redis考虑到每次登录都比较麻烦，所以Redis就有一种ssh免秘钥登录的方式，生成一对公钥和私钥，私钥放在本地，公钥放在redis端，当我们登录时服务器，再登录时候，他会去解析公钥和私钥，如果没有问题，则不需要利用redis的登录也能访问，这种做法本身也很常见，但是这里有一个前提，前提就是公钥必须保存在服务器上，才行，但是Redis的漏洞在于在不登录的情况下，也能把秘钥送到Linux服务器，从而产生漏洞\n漏洞出现的核心的原因有以下几点：\nRedis未设置密码 利用了Redis的config set命令动态修改Redis配置 使用了Root账号权限启动Redis 所以：如何解决呢？我们可以采用如下几种方案\n为了避免这样的漏洞，这里给出一些建议：\nRedis一定要设置密码 禁止线上使用下面命令：keys、flushall、flushdb、config set等命令。可以利用rename-command禁用。 bind：限制网卡，禁止外网网卡访问 开启防火墙 不要使用Root账户启动Redis 尽量不是有默认的端口 6、服务器端优化-Redis内存划分和内存配置 当Redis内存不足时，可能导致Key频繁被删除、响应时间变长、QPS不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因。\n有关碎片问题分析\nRedis底层分配并不是这个key有多大，他就会分配多大，而是有他自己的分配策略，比如8,16,20等等，假定当前key只需要10个字节，此时分配8肯定不够，那么他就会分配16个字节，多出来的6个字节就不能被使用，这就是我们常说的碎片问题\n进程内存问题分析：\n这片内存，通常我们都可以忽略不计\n缓冲区内存问题分析：\n一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，所以这片内存也是我们需要重点分析的内存问题。\n内存占用 说明 数据内存 是Redis最主要的部分，存储Redis的键值信息。主要问题是BigKey问题、内存碎片问题 进程内存 Redis主进程本身运⾏肯定需要占⽤内存，如代码、常量池等等；这部分内存⼤约⼏兆，在⼤多数⽣产环境中与Redis数据占⽤的内存相⽐可以忽略。 缓冲区内存 一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用BigKey，可能导致内存溢出。 于是我们就需要通过一些命令，可以查看到Redis目前的内存分配状态：\ninfo memory：查看内存分配的情况 memory xxx：查看key的主要占用情况 接下来我们看到了这些配置，最关键的缓存区内存如何定位和解决呢？\n内存缓冲区常见的有三种：\n复制缓冲区：主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过replbacklog-size来设置，默认1mb AOF缓冲区：AOF刷盘之前的缓存区域，AOF执行rewrite的缓冲区。无法设置容量上限 客户端缓冲区：分为输入缓冲区和输出缓冲区，输入缓冲区最大1G且不能设置。输出缓冲区可以设置 以上复制缓冲区和AOF缓冲区 不会有问题，最关键就是客户端缓冲区的问题\n客户端缓冲区：指的就是我们发送命令时，客户端用来缓存命令的一个缓冲区，也就是我们向redis输入数据的输入端缓冲区和redis向客户端返回数据的响应缓存区，输入缓冲区最大1G且不能设置，所以这一块我们根本不用担心，如果超过了这个空间，redis会直接断开，因为本来此时此刻就代表着redis处理不过来了，我们需要担心的就是输出端缓冲区\n我们在使用redis过程中，处理大量的big value，那么会导致我们的输出结果过多，如果输出缓存区过大，会导致redis直接断开，而默认配置的情况下， 其实他是没有大小的，这就比较坑了，内存可能一下子被占满，会直接导致咱们的redis断开，所以解决方案有两个\n设置一个大小 增加我们带宽的大小，避免我们出现大量数据从而直接超过了redis的承受能力 7、服务器端集群优化-集群还是主从 集群虽然具备高可用特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题：\n集群完整性问题 集群带宽问题 数据倾斜问题 客户端性能问题 命令的集群兼容性问题 lua和事务问题 问题1、在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务。\n大家可以设想一下，如果有几个slot不能使用，那么此时整个集群都不能用了，我们在开发中，其实最重要的是可用性，所以需要把如下配置修改成no，即有slot不能使用时，我们的redis集群还是可以对外提供服务\n问题2、集群带宽问题\n集群节点之间会不断的互相Ping来确定集群中其它节点的状态。每次Ping携带的信息至少包括：\n插槽信息 集群状态信息 集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1kb，此时每次集群互通需要的带宽会非常高，这样会导致集群中大量的带宽都会被ping信息所占用，这是一个非常可怕的问题，所以我们需要去解决这样的问题\n解决途径：\n避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群。 避免在单个物理机中运行太多Redis实例 配置合适的cluster-node-timeout值 问题3、命令的集群兼容性问题\n有关这个问题咱们已经探讨过了，当我们使用批处理的命令时，redis要求我们的key必须落在相同的slot上，然后大量的key同时操作时，是无法完成的，所以客户端必须要对这样的数据进行处理，这些方案我们之前已经探讨过了，所以不再这个地方赘述了。\n问题4、lua和事务的问题\nlua和事务都是要保证原子性问题，如果你的key不在一个节点，那么是无法保证lua的执行和事务的特性的，所以在集群模式是没有办法执行lua和事务的\n那我们到底是集群还是主从\n单体Redis（主从Redis）已经能达到万级别的QPS，并且也具备很强的高可用特性。如果主从能满足业务需求的情况下，所以如果不是在万不得已的情况下，尽量不搭建Redis集群\nRedis主从就是常见的主从模式，从节点自动同步主节点数据，实现数据的热备份。 Redis哨兵就是在Redis主从上添加了一个监控系统（Redis Sentinel系统），实现故障转移，Redis哨兵会监控Redis主从节点运行状态，当主节点故障下线后，Redis哨兵会选择一个从节点充当新的主节点，继续提供服务。 Redis集群在Redis主从上添加了监控机制和数据分片机制（Redis中是分槽位），实现故障转移和数据水平扩展，Redis集群中组合了多个Redis主从，并且每个Redis主节点都负责存储集群中的一部分数据，当某个主节点故障下线后，Redis集群会选择该节点的一个从节点充当新的主节点，继续提供服务。 生产环境应该很少使用单纯的Redis主从吧，如果数据量比较少，可以使用哨兵模式，但Redis集群的稳定性、可扩展性都优于哨兵模式，所以使用Redis集群的场景应该是最多的吧。 ","permalink":"https://cold-bin.github.io/post/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","tags":["redis最佳实践及优化指南"],"title":"Redis高级篇之最佳实践"},{"categories":["汇编"],"contents":"内中断 1、内中断的产生 任何一个通用的CPU，都具备一种能力，可以在执行完当前正在执行的指令之后，检测到从CPU外部发送过来的或内部产生的一种特殊信息，并且可以立即对所接收到的信息进行处理。这种特殊的信息，我们可以称其为：中断信息。中断的意思是指，CPU不再接着（刚执行完的指令）向下执行，而是转去处理这个特殊信息。\n中断信息可以来自CPU的内部和外部（内中断，外中断）\n内中断：当CPU的内部有需要处理的事情发生的时候，将产生中断信息，引发中断过程。这种中断信息来自CPU的内部\n8086CPU的内中断（下面四种情况将产生中断信息）\n除法错误，比如，执行div指令产生的除法溢出； 单步执行； 执行 into指令； 执行 int指令。 中断信息中包含中断类型码，中断类型码为一个字节型数据，可以表示256种中断信息的来源（中断源）\n上述的4种中断源，在8086CPU中的中断类型码如下。\n除法错误：0 单步执行：1 执行into指令：4 执行int指令，该指令的格式为int n，指令中的n为字节型立即数，是提供给CPU的中断类型码。 2、中断处理程序、中断向量表、中断过程 中断处理程序\n用来处理中断信息的程序被称为中断处理程序。\n根据CPU的设计，中断类型码的作用就是用来定位中断处理程序。比如CPU根据中断类型码4，就可以找到4号中断的处理程序\n中断向量表\n中断向量就是中断处理程序的入口地址。中断向量表就是中断处理程序入口地址的列表\nCPU用8位的中断类型码通过中断向量表找到相应的中断处理程序的入口地址 中断过程\n中断过程的主要任务就是用中断类型码在中断向量表中找到中断处理程序的入口地址，设置CS和IP\n简要描述如下\n取得中断类型码N； pushf TF=0，IF=0 （为什么这样参考单步中断） push CS , push IP （IP）=（N * 4），（CS）=（N * 4 + 2） 硬件在完成中断过程后，CS:IP将指向中断处理程序的入口，CPU开始执行中断处理程序。\n3、iret指令 CPU随时都可能执行中断处理程序，中断处理程序必须一直存储在内存某段空间之中 而中断处理程序的入口地址，即中断向量，必须存储在对应的中断向量表表项中。\n中断处理程序的常规编写步骤：\n保存用到的寄存器； 处理中断； 恢复用到的寄存器； 用iret指令返回。 iret 指令描述为：pop IP pop CS popf\niret 指令执行后，CPU回到执行中断处理程序前的执行点继续执行程序\n4、除法错误中断的处理 1 2 3 mov ax, 1000h mov bh, 1 div bh ;除法溢出错误 1、当CPU执行div bh时，发生了除法溢出错误，产生0号中断信息，从而引发中断过程，\n2、CPU执行0号中断处理程序\n3、系统中的0号中断处理程序的功能：显示提示信息“Divide overflow”后，返回到操作系统中。\n编程实验\n编程：编写0号中断处理程序do0，当发生除法溢出时，在屏幕中间显示“overflow！”，返回DOS。\n1、0000:0200至0000:02FF的256个字节的空间所对应的中断向量表项都是空的，可以将中断处理程序do0传送到内存0000:0200处。\n2、中断处理程序do0放到0000:0200,再将其地址登记在中断向量表对应表项\n0号表项的地址0:0。0:0字单元存放偏移地址，0:2字单元存放段地址 将do0的段地址0存放在0000:0002字单元中，将偏移地址200H存放在0000:0000字单元 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 assume cs:code code segment start:\tmov ax, cs mov ds, ax mov si, offset do0\t;设置ds:si指向源地址 mov ax, 0 mov es, ax mov di, 200h\t;设置es:di指向目的地址0000:0200 mov cx, offset do0end - offset do0\t;设置cx为传输长度 编译时给出do0部分代码长度 cld\t;设置传输方向为正 rep movsb ;将do0的代码送入0:200处 mov ax, 0 ;设置中断向量表 mov es, ax mov word ptr es:[0*4], 200h mov word ptr es:[0*4+2], 0 mov ax,4c00h int 21h ;do0程序的主要任务是显示字符串 do0:\tjmp short do0 start db \u0026#34;overflow!\u0026#34; do0start: mov ax, cs mov ds, ax mov si, 202h\t;设置ds:si指向字符串 mov ax, 0b800h mov es, ax mov di, 12*160+36*2\t;设置es:di指向显存空间的中间位置 mov cx, 9\t;设置cx为字符串长度 s:\tmov al, [si] mov es:[di], al inc si add di, 1 mov al, 02h ;设置颜色 mov es:[di], al add di, 1 loop s mov ax, 4c00h int 21h do0end:\tnop code ends end start 5、单步中断 CPU在执行完一条指令之后，如果检测到标志寄存器的TF位为1，则产生单步中断，引发中断过程。单步中断的中断类型码为1\nDebug是如何利用CPU所提供的单步中断的功能进行调试？如使用t命令查看寄存器状态\nDebug提供了单步中断的中断处理程序，功能为显示所有寄存器中的内容后等待输入命令\n在使用t命令执行指令时，Debug将TF设置为1，在CPU执行完这条指令后就引发单步中断，执行单步中断的中断处理程序，所有寄存器中的内容被显示在屏幕上，并且等待输入命令。\n在进入中断处理程序之前，设置TF=0。从而避免CPU在执行中断处理程序的时候发生单步中断\n6、int指令 int指令的格式为：int n ，n为中断类型码，它的功能是引发中断过程。\nCPU执行int n指令，相当于引发一个n号中断的中断过程\n在程序中使用int指令调用任何一个中断的中断处理程序(中断例程)\n编写供应用程序调用的中断例程\n实验1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ;求2 * 3456^2 assume cs:code code segment start: mov ax, 3456 ;(ax)=3456 int 7ch ; 调用中断7ch的中断例程，计算ax中的数据的平方 add ax, ax adc dx, dx ;存放结果，将结果乘以2 mov ax,4c00h int 21h code ends end start ;编程：安装中断7ch的中断例程 ;功能：求一word型数据的平方。 ;参数：(ax) = 要计算的数据。 ;返回值：dx、ax中存放结果的高16位和低16位。 assume cs:code code segment start: mov ax,cs mov ds,ax mov si,offset sqr\t;设置ds:si指向源地址 mov ax,0 mov es,ax mov di,200h\t;设置es:di指向目的地址 mov cx,offset sqrend - offset sqr\t;设置cx为传输长度 cld\t;设置传输方向为正 rep movsb mov ax,0 mov es,ax mov word ptr es:[7ch*4], 200h mov word ptr es:[7ch*4+2], 0 mov ax,4c00h int 21h sqr: mul ax iret ;CPU执行int 7ch指令进入中断例程之前，标志寄存器、当前的CS和IP被压入栈 ;在执行完中断例程后，应该用iret 指令恢复int 7ch执行前的标志寄存器和CS、IP的 sqrend:\tnop code ends end start 实验2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ;功能：将一个全是字母，以0结尾的字符串，转化为大写。 ;参数：ds:si指向字符串的首地址。 ;应用举例：将data段中的字符串转化为大写。 assume cs:code data segment db \u0026#39;conversation\u0026#39;,0 data ends code segment start: mov ax, data mov ds, ax mov si, 0 int 7ch mov ax,4c00h int 21h code ends end start 12345678910111213141516171819 assume cs:code code segment start: mov ax,cs mov ds,ax mov si,offset capital mov ax,0 mov es,ax mov di,200h mov cx,offset capitalend - offset capital cld rep movsb mov ax,0 mov es,ax mov word ptr es:[7ch*4],200h mov word ptr es:[7ch*4+2],0 mov ax,4c00h int 21h capital: push cx push si change: mov cl,[si] mov ch,0 jcxz ok and byte ptr [si],11011111b inc si jmp short change ok:\tpop si pop cx iret capitalend:nop code ends end start 7、BIOS和DOS所提供的中断例程 在系统板的ROM中存放着一套程序，称为BIOS（基本输入输出系统）\nBIOS中主要包含以下几部分内容\n硬件系统的检测和初始化程序； 外部中断和内部中断的中断例程； 用于对硬件设备进行I/O操作的中断例程； 其他和硬件系统相关的中断例程。 程序员在编程的时候，可以用int 指令直接调用BIOS和DOS系统提供的中断例程，来完成某些工作。 和硬件设备相关的DOS中断例程中，一般都调用了BIOS的中断例程。\nBIOS和DOS中断例程的安装过程\nBIOS和DOS提供的中断例程是如何安装到内存中的呢？\n1、开机后，CPU一加电，初始化（CS）= 0FFFFH，（IP）= 0，自动从FFFF:0单元开始执行程序。FFFF:0处有一条转跳指令，CPU执行该指令后，转去执行BIOS中的硬件系统检测和初始化程序。\n2、初始化程序将建立BIOS所支持的中断向量，即将BIOS提供的中断例程的入口地址登记在中断向量表中。 注意，对于BIOS所提供的中断例程，只需将入口地址登记在中断向量表中即可，因为它们是固化到ROM中的程序，一直在内存中存在。\n3、硬件系统检测和初始化完成后，调用int 19h进行操作系统的引导。从此将计算机交由操作系统控制。\n4、DOS启动后，除完成其他工作外，还将它所提供的中断例程装入内存，并建立相应的中断向量。\nBIOS中断例程应用\n一般来说，一个供程序员调用的中断例程中往往包括多个子程序，中断例程内部用传递进来的参数来决定执行哪一个子程序。\nBIOS和DOS提供的中断例程，都用 ah 来传递内部子程序的编号。\n编程：在屏幕的5行12列显示3个红底高亮闪烁绿色的“al。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 assume cs:code code segment ;int 10h中断例程的\u0026#34;设置光标位置\u0026#34;功能 mov ah, 2;设置光标调用第10h号中断例程的2号子程序，功能为设置光标位置(可以提供光标所在的行号、列号和页号作为参数) ;设置光标到第0页，第5行，第12列 mov bh, 0；第0页 mov dh, 5；dh中放行号 mov dl, 12；dl中放列号 int 10h ;int10h中断例程的\u0026#34;在光标位置显示字符\u0026#34;功能。 mov ah，9 ;调用第10h号中断例程的9号子程序，功能为在光标位置显示字符 ;提供要显示的字符、颜色属性、页号、字符重复个数作为参数 mov al，\u0026#39;a\u0026#39; ;字符 mov b1，11001010b ;颜色属性 mov bh，0 ;第0页 mov cx，3 ;字符重复个数 int 10h code ends end bh中页号的含义：内存地址空间中，B8000H~BFFFFH共32kB的空间，为80*25彩色字符模式的显示缓冲区。 一屏的内容在显示缓冲区中共占4000个字节。显示缓冲区分为8页，每页4KB（约4000B），显示器可以显示任意一页的内容。一般情况下，显示第0页的内容。也就是说，通常情况下，B8000H~B8F9FH中的4000个字节的内容将出现在显示器上。\nDOS中断例程应用 int 21h中断例程是DOS提供的中断例程，4ch号功能，即程序返回功能\n1 2 3 mov ah, 4ch ;调用第21h号中断例程的4ch号子程序，功能为程序返回,可以提供返回值作为参数 mov al, 0 ;返回值 int 21h 编程：在屏幕的5行12列显示字符串“Welcome to masm！”。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 assume cs:code data segment db\t\u0026#39;Welcome to masm\u0026#39;, \u0026#39;$\u0026#39; ;“$”本身并不显示，只起到边界的作用 data ends code segment start:\tmov ah, 2 ;10号中断设置光标位置功能 mov bh, 0 ;第0页 mov dh, 5；dh中放行号 mov dl, 12 ;dl中放列号 int 10h mov ax, data mov ds, ax mov dx, 0 ;ds:dx指向字符串的首地址data:0 （参数） mov ah, 9 ;调用第21h号中断例程的9号子程序，功能为在光标位置显示字符串，可以提供要显示字符串的地址作为参数 int 21h mov ax, 4c00h ;21号中断程序返回功能 int 21h code ends end start ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/","tags":["汇编里的中断"],"title":"汇编之内中断"},{"categories":["redis","缓存"],"contents":"[toc]\n多级缓存 1.什么是多级缓存 传统的缓存策略一般是请求到达Tomcat后，先查询Redis，如果未命中则查询数据库，如图：\n存在下面的问题：\n请求要经过Tomcat处理，Tomcat的性能成为整个系统的瓶颈 Redis缓存失效时，会对数据库产生冲击 多级缓存就是充分利用请求处理的每个环节，分别添加缓存，减轻Tomcat压力，提升服务性能：\n浏览器访问静态资源时，优先读取浏览器本地缓存 访问非静态资源（ajax查询数据）时，访问服务端 请求到达Nginx后，优先读取Nginx本地缓存 如果Nginx本地缓存未命中，则去直接查询Redis（不经过Tomcat） 如果Redis查询未命中，则查询Tomcat 请求进入Tomcat后，优先查询JVM进程缓存 如果JVM进程缓存未命中，则查询数据库 在多级缓存架构中，Nginx内部需要编写本地缓存查询、Redis查询、Tomcat查询的业务逻辑，因此这样的nginx服务不再是一个反向代理服务器，而是一个编写业务的Web服务器了。\n因此这样的业务Nginx服务也需要搭建集群来提高并发，再有专门的nginx服务来做反向代理，如图：\n另外，我们的Tomcat服务将来也会部署为集群模式：\n可见，多级缓存的关键有两个：\n一个是在nginx中编写业务，实现nginx本地缓存、Redis、Tomcat的查询\n另一个就是在Tomcat中实现JVM进程缓存\n其中Nginx编程则会用到OpenResty框架结合Lua这样的语言。\n2.JVM进程缓存 为了演示多级缓存的案例，我们先准备一个商品查询的业务。\n2.1.导入案例 2.2.初识Caffeine 缓存在日常开发中启动至关重要的作用，由于是存储在内存中，数据的读取速度是非常快的，能大量减少对数据库的访问，减少数据库的压力。我们把缓存分为两类：\n分布式缓存，例如Redis： 优点：存储容量更大、可靠性更好、RDB\\AOF持久化、可以在集群间共享 缺点：访问缓存有网络开销 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享 进程本地缓存，例如HashMap、GuavaCache： 优点：读取本地内存，没有网络开销，速度更快 缺点：存储容量有限、可靠性较低、无法共享 场景：性能要求较高，缓存数据量较小 我们今天会利用Caffeine框架来实现JVM进程缓存。\nCaffeine是一个基于Java8开发的，提供了近乎最佳命中率的高性能的本地缓存库。目前Spring内部的缓存使用的就是Caffeine。GitHub地址：https://github.com/ben-manes/caffeine\nCaffeine的性能非常好，下图是官方给出的性能对比：\n可以看到Caffeine的性能遥遥领先！\n缓存使用的基本API：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Test void testBasicOps() { // 构建cache对象 Cache\u0026lt;String, String\u0026gt; cache = Caffeine.newBuilder().build(); // 存数据 cache.put(\u0026#34;gf\u0026#34;, \u0026#34;迪丽热巴\u0026#34;); // 取数据 String gf = cache.getIfPresent(\u0026#34;gf\u0026#34;); System.out.println(\u0026#34;gf = \u0026#34; + gf); // 取数据，包含两个参数： // 参数一：缓存的key // 参数二：Lambda表达式，表达式参数就是缓存的key，方法体是查询数据库的逻辑 // 优先根据key查询JVM缓存，如果未命中，则执行参数二的Lambda表达式 String defaultGF = cache.get(\u0026#34;defaultGF\u0026#34;, key -\u0026gt; { // 根据key去数据库查询数据 return \u0026#34;柳岩\u0026#34;; }); System.out.println(\u0026#34;defaultGF = \u0026#34; + defaultGF); } Caffeine既然是缓存的一种，肯定需要有缓存的清除策略，不然的话内存总会有耗尽的时候。\nCaffeine提供了三种缓存驱逐策略：\n基于容量：设置缓存的数量上限\n1 2 3 4 // 创建缓存对象 Cache\u0026lt;String, String\u0026gt; cache = Caffeine.newBuilder() .maximumSize(1) // 设置缓存大小上限为 1 .build(); 基于时间：设置缓存的有效时间\n1 2 3 4 5 // 创建缓存对象 Cache\u0026lt;String, String\u0026gt; cache = Caffeine.newBuilder() // 设置缓存有效期为 10 秒，从最后一次写入开始计时 .expireAfterWrite(Duration.ofSeconds(10)) .build(); 基于引用：设置缓存为软引用或弱引用，利用GC来回收缓存数据。性能较差，不建议使用。\n注意：在默认情况下，当一个缓存元素过期的时候，Caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。\n2.3.实现JVM进程缓存 2.3.1.需求 利用Caffeine实现下列需求：\n给根据id查询商品的业务添加缓存，缓存未命中时查询数据库 给根据id查询商品库存的业务添加缓存，缓存未命中时查询数据库 缓存初始大小为100 缓存上限为10000 2.3.2.实现 首先，我们需要定义两个Caffeine的缓存对象，分别保存商品、库存的缓存数据。\n在item-service的com.heima.item.config包下定义CaffeineConfig类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package com.heima.item.config; import com.github.benmanes.caffeine.cache.Cache; import com.github.benmanes.caffeine.cache.Caffeine; import com.heima.item.pojo.Item; import com.heima.item.pojo.ItemStock; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class CaffeineConfig { @Bean public Cache\u0026lt;Long, Item\u0026gt; itemCache(){ return Caffeine.newBuilder() .initialCapacity(100) .maximumSize(10_000) .build(); } @Bean public Cache\u0026lt;Long, ItemStock\u0026gt; stockCache(){ return Caffeine.newBuilder() .initialCapacity(100) .maximumSize(10_000) .build(); } } 然后，修改item-service中的com.heima.item.web包下的ItemController类，添加缓存逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @RestController @RequestMapping(\u0026#34;item\u0026#34;) public class ItemController { @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; @Autowired private Cache\u0026lt;Long, Item\u0026gt; itemCache; @Autowired private Cache\u0026lt;Long, ItemStock\u0026gt; stockCache; // ...其它略 @GetMapping(\u0026#34;/{id}\u0026#34;) public Item findById(@PathVariable(\u0026#34;id\u0026#34;) Long id) { return itemCache.get(id, key -\u0026gt; itemService.query() .ne(\u0026#34;status\u0026#34;, 3).eq(\u0026#34;id\u0026#34;, key) .one() ); } @GetMapping(\u0026#34;/stock/{id}\u0026#34;) public ItemStock findStockById(@PathVariable(\u0026#34;id\u0026#34;) Long id) { return stockCache.get(id, key -\u0026gt; stockService.getById(key)); } } 3.Lua语法入门 Nginx编程需要用到Lua语言，因此我们必须先入门Lua的基本语法。\n3.1.初识Lua Lua 是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。官网：https://www.lua.org/\nLua经常嵌入到C语言开发的程序中，例如游戏开发、游戏插件等。\nNginx本身也是C语言开发，因此也允许基于Lua做拓展。\n3.1.HelloWorld CentOS7默认已经安装了Lua语言环境，所以可以直接运行Lua代码。\n1）在Linux虚拟机的任意目录下，新建一个hello.lua文件\n2）添加下面的内容\n1 print(\u0026#34;Hello World!\u0026#34;) 3）运行\n3.2.变量和循环 学习任何语言必然离不开变量，而变量的声明必须先知道数据的类型。\n3.2.1.Lua的数据类型 Lua中支持的常见数据类型包括：\n另外，Lua提供了type()函数来判断一个变量的数据类型：\n3.2.2.声明变量 Lua声明变量的时候无需指定数据类型，而是用local来声明变量为局部变量：\n1 2 3 4 5 6 7 8 -- 声明字符串，可以用单引号或双引号， local str = \u0026#39;hello\u0026#39; -- 字符串拼接可以使用 .. local str2 = \u0026#39;hello\u0026#39; .. \u0026#39;world\u0026#39; -- 声明数字 local num = 21 -- 声明布尔类型 local flag = true Lua中的table类型既可以作为数组，又可以作为Java中的map来使用。数组就是特殊的table，key是数组角标而已：\n1 2 3 4 -- 声明数组 ，key为角标的 table local arr = {\u0026#39;java\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;lua\u0026#39;} -- 声明table，类似java的map local map = {name=\u0026#39;Jack\u0026#39;, age=21} Lua中的数组角标是从1开始，访问的时候与Java中类似：\n1 2 -- 访问数组，lua数组的角标从1开始 print(arr[1]) Lua中的table可以用key来访问：\n1 2 3 -- 访问table print(map[\u0026#39;name\u0026#39;]) print(map.name) 3.2.3.循环 对于table，我们可以利用for循环来遍历。不过数组和普通table遍历略有差异。\n遍历数组：\n1 2 3 4 5 6 -- 声明数组 key为索引的 table local arr = {\u0026#39;java\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;lua\u0026#39;} -- 遍历数组 for index,value in ipairs(arr) do print(index, value) end 遍历普通table\n1 2 3 4 5 6 -- 声明map，也就是table local map = {name=\u0026#39;Jack\u0026#39;, age=21} -- 遍历table for key,value in pairs(map) do print(key, value) end 3.3.条件控制、函数 Lua中的条件控制和函数声明与Java类似。\n3.3.1.函数 定义函数的语法：\n1 2 3 4 function 函数名( argument1, argument2..., argumentn) -- 函数体 return 返回值 end 例如，定义一个函数，用来打印数组：\n1 2 3 4 5 function printArr(arr) for index, value in ipairs(arr) do print(value) end end 3.3.2.条件控制 类似Java的条件控制，例如if、else语法：\n1 2 3 4 5 6 if(布尔表达式) then --[ 布尔表达式为 true 时执行该语句块 --] else --[ 布尔表达式为 false 时执行该语句块 --] end 与java不同，布尔表达式中的逻辑运算是基于英文单词：\n3.3.3.案例 需求：自定义一个函数，可以打印table，当参数为nil时，打印错误信息\n1 2 3 4 5 6 7 8 function printArr(arr) if not arr then print(\u0026#39;数组不能为空！\u0026#39;) end for index, value in ipairs(arr) do print(value) end end 4.实现多级缓存 多级缓存的实现离不开Nginx编程，而Nginx编程又离不开OpenResty。\n4.1.安装OpenResty OpenResty® 是一个基于 Nginx的高性能 Web 平台，用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。具备下列特点：\n具备Nginx的完整功能 基于Lua语言进行扩展，集成了大量精良的 Lua 库、第三方模块 允许使用Lua自定义业务逻辑、自定义库 官方网站： https://openresty.org/cn/\n4.2.OpenResty快速入门 我们希望达到的多级缓存架构如图：\n其中：\nwindows上的nginx用来做反向代理服务，将前端的查询商品的ajax请求代理到OpenResty集群\nOpenResty集群用来编写多级缓存业务\n4.2.1.反向代理流程 现在，商品详情页使用的是假的商品数据。不过在浏览器中，可以看到页面有发起ajax请求查询真实商品数据。\n这个请求如下：\n请求地址是localhost，端口是80，就被windows上安装的Nginx服务给接收到了。然后代理给了OpenResty集群：\n我们需要在OpenResty中编写业务，查询商品数据并返回到浏览器。\n但是这次，我们先在OpenResty接收请求，返回假的商品数据。\n4.2.2.OpenResty监听请求 OpenResty的很多功能都依赖于其目录下的Lua库，需要在nginx.conf中指定依赖库的目录，并导入依赖：\n1）添加对OpenResty的Lua模块的加载\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，在其中的http下面，添加下面代码：\n1 2 3 4 # lua模块 lua_package_path \u0026#34;/usr/local/openresty/lualib/?.lua;;\u0026#34;; # c模块 lua_package_cpath \u0026#34;/usr/local/openresty/lualib/?.so;;\u0026#34;; 2）监听/api/item路径\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，在nginx.conf的server下面，添加对/api/item这个路径的监听：\n1 2 3 4 5 6 location /api/item { # 默认的响应类型 default_type application/json; # 响应结果由lua/item.lua文件来决定 content_by_lua_file lua/item.lua; } 这个监听，就类似于SpringMVC中的@GetMapping(\u0026quot;/api/item\u0026quot;)做路径映射。\n而content_by_lua_file lua/item.lua则相当于调用item.lua这个文件，执行其中的业务，把结果返回给用户。相当于java中调用service。\n4.2.3.编写item.lua 1）在/usr/loca/openresty/nginx目录创建文件夹：lua\n2）在/usr/loca/openresty/nginx/lua文件夹下，新建文件：item.lua\n3）编写item.lua，返回假数据\nitem.lua中，利用ngx.say()函数返回数据到Response中\n1 ngx.say(\u0026#39;{\u0026#34;id\u0026#34;:10001,\u0026#34;name\u0026#34;:\u0026#34;SALSA AIR\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;RIMOWA 21寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4\u0026#34;,\u0026#34;price\u0026#34;:17900,\u0026#34;image\u0026#34;:\u0026#34;https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;拉杆箱\u0026#34;,\u0026#34;brand\u0026#34;:\u0026#34;RIMOWA\u0026#34;,\u0026#34;spec\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;status\u0026#34;:1,\u0026#34;createTime\u0026#34;:\u0026#34;2019-04-30T16:00:00.000+00:00\u0026#34;,\u0026#34;updateTime\u0026#34;:\u0026#34;2019-04-30T16:00:00.000+00:00\u0026#34;,\u0026#34;stock\u0026#34;:2999,\u0026#34;sold\u0026#34;:31290}\u0026#39;) 4）重新加载配置\n1 nginx -s reload 刷新商品页面：http://localhost/item.html?id=1001，即可看到效果：\n4.3.请求参数处理 上一节中，我们在OpenResty接收前端请求，但是返回的是假数据。\n要返回真实数据，必须根据前端传递来的商品id，查询商品信息才可以。\n那么如何获取前端传递的商品参数呢？\n4.3.1.获取参数的API OpenResty中提供了一些API用来获取不同类型的前端请求参数：\n4.3.2.获取参数并返回 在前端发起的ajax请求如图：\n可以看到商品id是以路径占位符方式传递的，因此可以利用正则表达式匹配的方式来获取ID\n1）获取商品id\n修改/usr/loca/openresty/nginx/nginx.conf文件中监听/api/item的代码，利用正则表达式获取ID：\n1 2 3 4 5 6 location ~ /api/item/(\\d+) { # 默认的响应类型 default_type application/json; # 响应结果由lua/item.lua文件来决定 content_by_lua_file lua/item.lua; } 2）拼接ID并返回\n修改/usr/loca/openresty/nginx/lua/item.lua文件，获取id并拼接到结果中返回：\n1 2 3 4 -- 获取商品id local id = ngx.var[1] -- 拼接并返回 ngx.say(\u0026#39;{\u0026#34;id\u0026#34;:\u0026#39; .. id .. \u0026#39;,\u0026#34;name\u0026#34;:\u0026#34;SALSA AIR\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;RIMOWA 21寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4\u0026#34;,\u0026#34;price\u0026#34;:17900,\u0026#34;image\u0026#34;:\u0026#34;https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;拉杆箱\u0026#34;,\u0026#34;brand\u0026#34;:\u0026#34;RIMOWA\u0026#34;,\u0026#34;spec\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;status\u0026#34;:1,\u0026#34;createTime\u0026#34;:\u0026#34;2019-04-30T16:00:00.000+00:00\u0026#34;,\u0026#34;updateTime\u0026#34;:\u0026#34;2019-04-30T16:00:00.000+00:00\u0026#34;,\u0026#34;stock\u0026#34;:2999,\u0026#34;sold\u0026#34;:31290}\u0026#39;) 3）重新加载并测试\n运行命令以重新加载OpenResty配置：\n1 nginx -s reload 刷新页面可以看到结果中已经带上了ID：\n4.4.查询Tomcat 拿到商品ID后，本应去缓存中查询商品信息，不过目前我们还未建立nginx、redis缓存。因此，这里我们先根据商品id去tomcat查询商品信息。我们实现如图部分：\n需要注意的是，我们的OpenResty是在虚拟机，Tomcat是在Windows电脑上。两者IP一定不要搞错了。\n4.4.1.发送http请求的API nginx提供了内部API用以发送http请求：\n1 2 3 4 local resp = ngx.location.capture(\u0026#34;/path\u0026#34;,{ method = ngx.HTTP_GET, -- 请求方式 args = {a=1,b=2}, -- get方式传参数 }) 返回的响应内容包括：\nresp.status：响应状态码 resp.header：响应头，是一个table resp.body：响应体，就是响应数据 注意：这里的path是路径，并不包含IP和端口。这个请求会被nginx内部的server监听并处理。\n但是我们希望这个请求发送到Tomcat服务器，所以还需要编写一个server来对这个路径做反向代理：\n1 2 3 4 location /path { # 这里是windows电脑的ip和Java服务端口，需要确保windows防火墙处于关闭状态 proxy_pass http://192.168.150.1:8081; } 原理如图：\n4.4.2.封装http工具 下面，我们封装一个发送Http请求的工具，基于ngx.location.capture来实现查询tomcat。\n1）添加反向代理，到windows的Java服务\n因为item-service中的接口都是/item开头，所以我们监听/item路径，代理到windows上的tomcat服务。\n修改 /usr/local/openresty/nginx/conf/nginx.conf文件，添加一个location：\n1 2 3 location /item { proxy_pass http://192.168.150.1:8081; } 以后，只要我们调用ngx.location.capture(\u0026quot;/item\u0026quot;)，就一定能发送请求到windows的tomcat服务。\n2）封装工具类\n之前我们说过，OpenResty启动时会加载以下两个目录中的工具文件：\n所以，自定义的http工具也需要放到这个目录下。\n在/usr/local/openresty/lualib目录下，新建一个common.lua文件：\n1 vi /usr/local/openresty/lualib/common.lua 内容如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 -- 封装函数，发送http请求，并解析响应 local function read_http(path, params) local resp = ngx.location.capture(path,{ method = ngx.HTTP_GET, args = params, }) if not resp then -- 记录错误信息，返回404 ngx.log(ngx.ERR, \u0026#34;http请求查询失败, path: \u0026#34;, path , \u0026#34;, args: \u0026#34;, args) ngx.exit(404) end return resp.body end -- 将方法导出 local _M = { read_http = read_http } return _M 这个工具将read_http函数封装到_M这个table类型的变量中，并且返回，这类似于导出。\n使用的时候，可以利用require('common')来导入该函数库，这里的common是函数库的文件名。\n3）实现商品查询\n最后，我们修改/usr/local/openresty/lua/item.lua文件，利用刚刚封装的函数库实现对tomcat的查询：\n1 2 3 4 5 6 7 8 9 10 -- 引入自定义common工具模块，返回值是common中返回的 _M local common = require(\u0026#34;common\u0026#34;) -- 从common中获取read_http这个函数 local read_http = common.read_http -- 获取路径参数 local id = ngx.var[1] -- 根据id查询商品 local itemJSON = read_http(\u0026#34;/item/\u0026#34;.. id, nil) -- 根据id查询商品库存 local itemStockJSON = read_http(\u0026#34;/item/stock/\u0026#34;.. id, nil) 这里查询到的结果是json字符串，并且包含商品、库存两个json字符串，页面最终需要的是把两个json拼接为一个json：\n这就需要我们先把JSON变为lua的table，完成数据整合后，再转为JSON。\n4.4.3.CJSON工具类 OpenResty提供了一个cjson的模块用来处理JSON的序列化和反序列化。\n官方地址： https://github.com/openresty/lua-cjson/\n1）引入cjson模块：\n1 local cjson = require \u0026#34;cjson\u0026#34; 2）序列化：\n1 2 3 4 5 6 local obj = { name = \u0026#39;jack\u0026#39;, age = 21 } -- 把 table 序列化为 json local json = cjson.encode(obj) 3）反序列化：\n1 2 3 4 local json = \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;jack\u0026#34;, \u0026#34;age\u0026#34;: 21}\u0026#39; -- 反序列化 json为 table local obj = cjson.decode(json); print(obj.name) 4.4.4.实现Tomcat查询 下面，我们修改之前的item.lua中的业务，添加json处理功能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 -- 导入common函数库 local common = require(\u0026#39;common\u0026#39;) local read_http = common.read_http -- 导入cjson库 local cjson = require(\u0026#39;cjson\u0026#39;) -- 获取路径参数 local id = ngx.var[1] -- 根据id查询商品 local itemJSON = read_http(\u0026#34;/item/\u0026#34;.. id, nil) -- 根据id查询商品库存 local itemStockJSON = read_http(\u0026#34;/item/stock/\u0026#34;.. id, nil) -- JSON转化为lua的table local item = cjson.decode(itemJSON) local stock = cjson.decode(stockJSON) -- 组合数据 item.stock = stock.stock item.sold = stock.sold -- 把item序列化为json返回结果 ngx.say(cjson.encode(item)) 4.4.5.基于ID负载均衡 刚才的代码中，我们的tomcat是单机部署。而实际开发中，tomcat一定是集群模式：\n因此，OpenResty需要对tomcat集群做负载均衡。\n而默认的负载均衡规则是轮询模式，当我们查询/item/10001时：\n第一次会访问8081端口的tomcat服务，在该服务内部就形成了JVM进程缓存 第二次会访问8082端口的tomcat服务，该服务内部没有JVM缓存（因为JVM缓存无法共享），会查询数据库 \u0026hellip; 你看，因为轮询的原因，第一次查询8081形成的JVM缓存并未生效，直到下一次再次访问到8081时才可以生效，缓存命中率太低了。\n怎么办？\n如果能让同一个商品，每次查询时都访问同一个tomcat服务，那么JVM缓存就一定能生效了。\n也就是说，我们需要根据商品id做负载均衡，而不是轮询。\n1）原理 nginx提供了基于请求路径做负载均衡的算法：\nnginx根据请求路径做hash运算，把得到的数值对tomcat服务的数量取余，余数是几，就访问第几个服务，实现负载均衡。\n例如：\n我们的请求路径是 /item/10001 tomcat总数为2台（8081、8082） 对请求路径/item/1001做hash运算求余的结果为1 则访问第一个tomcat服务，也就是8081 只要id不变，每次hash运算结果也不会变，那就可以保证同一个商品，一直访问同一个tomcat服务，确保JVM缓存生效。\n2）实现 修改/usr/local/openresty/nginx/conf/nginx.conf文件，实现基于ID做负载均衡。\n首先，定义tomcat集群，并设置基于路径做负载均衡：\n1 2 3 4 5 upstream tomcat-cluster { hash $request_uri; server 192.168.150.1:8081; server 192.168.150.1:8082; } 然后，修改对tomcat服务的反向代理，目标指向tomcat集群：\n1 2 3 location /item { proxy_pass http://tomcat-cluster; } 重新加载OpenResty\n1 nginx -s reload 3）测试 启动两台tomcat服务：\n同时启动：\n清空日志后，再次访问页面，可以看到不同id的商品，访问到了不同的tomcat服务：\n4.5.Redis缓存预热 Redis缓存会面临冷启动问题：\n冷启动：服务刚刚启动时，Redis中并没有缓存，如果所有商品数据都在第一次查询时添加缓存，可能会给数据库带来较大压力。\n缓存预热：在实际开发中，我们可以利用大数据统计用户访问的热点数据，在项目启动时将这些热点数据提前查询并保存到Redis中。\n我们数据量较少，并且没有数据统计相关功能，目前可以在启动时将所有数据都放入缓存中。\n1）利用Docker安装Redis\n1 docker run --name redis -p 6379:6379 -d redis redis-server --appendonly yes 2）在item-service服务中引入Redis依赖\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 3）配置Redis地址\n1 2 3 spring: redis: host: 192.168.150.101 4）编写初始化类\n缓存预热需要在项目启动时完成，并且必须是拿到RedisTemplate之后。\n这里我们利用InitializingBean接口来实现，因为InitializingBean可以在对象被Spring创建并且成员变量全部注入后执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package com.heima.item.config; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.heima.item.pojo.Item; import com.heima.item.pojo.ItemStock; import com.heima.item.service.IItemService; import com.heima.item.service.IItemStockService; import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.util.List; @Component public class RedisHandler implements InitializingBean { @Autowired private StringRedisTemplate redisTemplate; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; private static final ObjectMapper MAPPER = new ObjectMapper(); @Override public void afterPropertiesSet() throws Exception { // 初始化缓存 // 1.查询商品信息 List\u0026lt;Item\u0026gt; itemList = itemService.list(); // 2.放入缓存 for (Item item : itemList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(item); // 2.2.存入redis redisTemplate.opsForValue().set(\u0026#34;item🆔\u0026#34; + item.getId(), json); } // 3.查询商品库存信息 List\u0026lt;ItemStock\u0026gt; stockList = stockService.list(); // 4.放入缓存 for (ItemStock stock : stockList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(stock); // 2.2.存入redis redisTemplate.opsForValue().set(\u0026#34;item:stock:id:\u0026#34; + stock.getId(), json); } } } 4.6.查询Redis缓存 现在，Redis缓存已经准备就绪，我们可以再OpenResty中实现查询Redis的逻辑了。如下图红框所示：\n当请求进入OpenResty之后：\n优先查询Redis缓存 如果Redis缓存未命中，再查询Tomcat 4.6.1.封装Redis工具 OpenResty提供了操作Redis的模块，我们只要引入该模块就能直接使用。但是为了方便，我们将Redis操作封装到之前的common.lua工具库中。\n修改/usr/local/openresty/lualib/common.lua文件：\n1）引入Redis模块，并初始化Redis对象\n1 2 3 4 5 -- 导入redis local redis = require(\u0026#39;resty.redis\u0026#39;) -- 初始化redis local red = redis:new() red:set_timeouts(1000, 1000, 1000) 2）封装函数，用来释放Redis连接，其实是放入连接池\n1 2 3 4 5 6 7 8 9 -- 关闭redis连接的工具方法，其实是放入连接池 local function close_redis(red) local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒 local pool_size = 100 --连接池大小 local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.log(ngx.ERR, \u0026#34;放入redis连接池失败: \u0026#34;, err) end end 3）封装函数，根据key查询Redis数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 -- 查询redis的方法 ip和port是redis地址，key是查询的key local function read_redis(ip, port, key) -- 获取一个连接 local ok, err = red:connect(ip, port) if not ok then ngx.log(ngx.ERR, \u0026#34;连接redis失败 : \u0026#34;, err) return nil end -- 查询redis local resp, err = red:get(key) -- 查询失败处理 if not resp then ngx.log(ngx.ERR, \u0026#34;查询Redis失败: \u0026#34;, err, \u0026#34;, key = \u0026#34; , key) end --得到的数据为空处理 if resp == ngx.null then resp = nil ngx.log(ngx.ERR, \u0026#34;查询Redis数据为空, key = \u0026#34;, key) end close_redis(red) return resp end 4）导出\n1 2 3 4 5 6 -- 将方法导出 local _M = { read_http = read_http, read_redis = read_redis } return _M 完整的common.lua：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 -- 导入redis local redis = require(\u0026#39;resty.redis\u0026#39;) -- 初始化redis local red = redis:new() red:set_timeouts(1000, 1000, 1000) -- 关闭redis连接的工具方法，其实是放入连接池 local function close_redis(red) local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒 local pool_size = 100 --连接池大小 local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.log(ngx.ERR, \u0026#34;放入redis连接池失败: \u0026#34;, err) end end -- 查询redis的方法 ip和port是redis地址，key是查询的key local function read_redis(ip, port, key) -- 获取一个连接 local ok, err = red:connect(ip, port) if not ok then ngx.log(ngx.ERR, \u0026#34;连接redis失败 : \u0026#34;, err) return nil end -- 查询redis local resp, err = red:get(key) -- 查询失败处理 if not resp then ngx.log(ngx.ERR, \u0026#34;查询Redis失败: \u0026#34;, err, \u0026#34;, key = \u0026#34; , key) end --得到的数据为空处理 if resp == ngx.null then resp = nil ngx.log(ngx.ERR, \u0026#34;查询Redis数据为空, key = \u0026#34;, key) end close_redis(red) return resp end -- 封装函数，发送http请求，并解析响应 local function read_http(path, params) local resp = ngx.location.capture(path,{ method = ngx.HTTP_GET, args = params, }) if not resp then -- 记录错误信息，返回404 ngx.log(ngx.ERR, \u0026#34;http查询失败, path: \u0026#34;, path , \u0026#34;, args: \u0026#34;, args) ngx.exit(404) end return resp.body end -- 将方法导出 local _M = { read_http = read_http, read_redis = read_redis } return _M 4.6.2.实现Redis查询 接下来，我们就可以去修改item.lua文件，实现对Redis的查询了。\n查询逻辑是：\n根据id查询Redis 如果查询失败则继续查询Tomcat 将查询结果返回 1）修改/usr/local/openresty/lua/item.lua文件，添加一个查询函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -- 导入common函数库 local common = require(\u0026#39;common\u0026#39;) local read_http = common.read_http local read_redis = common.read_redis -- 封装查询函数 function read_data(key, path, params) -- 查询本地缓存 local val = read_redis(\u0026#34;127.0.0.1\u0026#34;, 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \u0026#34;redis查询失败，尝试查询http， key: \u0026#34;, key) -- redis查询失败，去查询http val = read_http(path, params) end -- 返回数据 return val end 2）而后修改商品查询、库存查询的业务：\n3）完整的item.lua代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 -- 导入common函数库 local common = require(\u0026#39;common\u0026#39;) local read_http = common.read_http local read_redis = common.read_redis -- 导入cjson库 local cjson = require(\u0026#39;cjson\u0026#39;) -- 封装查询函数 function read_data(key, path, params) -- 查询本地缓存 local val = read_redis(\u0026#34;127.0.0.1\u0026#34;, 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \u0026#34;redis查询失败，尝试查询http， key: \u0026#34;, key) -- redis查询失败，去查询http val = read_http(path, params) end -- 返回数据 return val end -- 获取路径参数 local id = ngx.var[1] -- 查询商品信息 local itemJSON = read_data(\u0026#34;item🆔\u0026#34; .. id, \u0026#34;/item/\u0026#34; .. id, nil) -- 查询库存信息 local stockJSON = read_data(\u0026#34;item:stock:id:\u0026#34; .. id, \u0026#34;/item/stock/\u0026#34; .. id, nil) -- JSON转化为lua的table local item = cjson.decode(itemJSON) local stock = cjson.decode(stockJSON) -- 组合数据 item.stock = stock.stock item.sold = stock.sold -- 把item序列化为json 返回结果 ngx.say(cjson.encode(item)) 4.7.Nginx本地缓存 现在，整个多级缓存中只差最后一环，也就是nginx的本地缓存了。如图：\n4.7.1.本地缓存API OpenResty为Nginx提供了shard dict的功能，可以在nginx的多个worker之间共享数据，实现缓存功能。\n1）开启共享字典，在nginx.conf的http下添加配置：\n1 2 # 共享字典，也就是本地缓存，名称叫做：item_cache，大小150m lua_shared_dict item_cache 150m; 2）操作共享字典：\n1 2 3 4 5 6 -- 获取本地缓存对象 local item_cache = ngx.shared.item_cache -- 存储, 指定key、value、过期时间，单位s，默认为0代表永不过期 item_cache:set(\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;, 1000) -- 读取 local val = item_cache:get(\u0026#39;key\u0026#39;) 4.7.2.实现本地缓存查询 1）修改/usr/local/openresty/lua/item.lua文件，修改read_data查询函数，添加本地缓存逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 -- 导入共享词典，本地缓存 local item_cache = ngx.shared.item_cache -- 封装查询函数 function read_data(key, expire, path, params) -- 查询本地缓存 local val = item_cache:get(key) if not val then ngx.log(ngx.ERR, \u0026#34;本地缓存查询失败，尝试查询Redis， key: \u0026#34;, key) -- 查询redis val = read_redis(\u0026#34;127.0.0.1\u0026#34;, 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \u0026#34;redis查询失败，尝试查询http， key: \u0026#34;, key) -- redis查询失败，去查询http val = read_http(path, params) end end -- 查询成功，把数据写入本地缓存 item_cache:set(key, val, expire) -- 返回数据 return val end 2）修改item.lua中查询商品和库存的业务，实现最新的read_data函数：\n其实就是多了缓存时间参数，过期后nginx缓存会自动删除，下次访问即可更新缓存。\n这里给商品基本信息设置超时时间为30分钟，库存为1分钟。\n因为库存更新频率较高，如果缓存时间过长，可能与数据库差异较大。\n3）完整的item.lua文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 -- 导入common函数库 local common = require(\u0026#39;common\u0026#39;) local read_http = common.read_http local read_redis = common.read_redis -- 导入cjson库 local cjson = require(\u0026#39;cjson\u0026#39;) -- 导入共享词典，本地缓存 local item_cache = ngx.shared.item_cache -- 封装查询函数 function read_data(key, expire, path, params) -- 查询本地缓存 local val = item_cache:get(key) if not val then ngx.log(ngx.ERR, \u0026#34;本地缓存查询失败，尝试查询Redis， key: \u0026#34;, key) -- 查询redis val = read_redis(\u0026#34;127.0.0.1\u0026#34;, 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \u0026#34;redis查询失败，尝试查询http， key: \u0026#34;, key) -- redis查询失败，去查询http val = read_http(path, params) end end -- 查询成功，把数据写入本地缓存 item_cache:set(key, val, expire) -- 返回数据 return val end -- 获取路径参数 local id = ngx.var[1] -- 查询商品信息 local itemJSON = read_data(\u0026#34;item🆔\u0026#34; .. id, 1800, \u0026#34;/item/\u0026#34; .. id, nil) -- 查询库存信息 local stockJSON = read_data(\u0026#34;item:stock:id:\u0026#34; .. id, 60, \u0026#34;/item/stock/\u0026#34; .. id, nil) -- JSON转化为lua的table local item = cjson.decode(itemJSON) local stock = cjson.decode(stockJSON) -- 组合数据 item.stock = stock.stock item.sold = stock.sold -- 把item序列化为json 返回结果 ngx.say(cjson.encode(item)) 5.缓存同步 大多数情况下，浏览器查询到的都是缓存数据，如果缓存数据与数据库数据存在较大差异，可能会产生比较严重的后果。\n所以我们必须保证数据库数据、缓存数据的一致性，这就是缓存与数据库的同步。\n5.1.数据同步策略 缓存数据同步的常见方式有三种：\n设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新\n优势：简单、方便 缺点：时效性差，缓存过期之前可能不一致 场景：更新频率较低，时效性要求低的业务 同步双写：在修改数据库的同时，直接修改缓存\n优势：时效性强，缓存与数据库强一致 缺点：有代码侵入，耦合度高 场景：对一致性、时效性要求较高的缓存数据 **异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据\n优势：低耦合，可以同时通知多个缓存服务 缺点：时效性一般，可能存在中间不一致状态 场景：时效性要求一般，有多个服务需要同步 而异步实现又可以基于MQ或者Canal来实现：\n1）基于MQ的异步通知：\n解读：\n商品服务完成对数据的修改后，只需要发送一条消息到MQ中。 缓存服务监听MQ消息，然后完成对缓存的更新 依然有少量的代码侵入\n2）基于Canal的通知\n解读：\n商品服务完成商品修改后，业务直接结束，没有任何代码侵入 Canal监听MySQL变化，当发现变化后，立即通知缓存服务 缓存服务接收到canal通知，更新缓存 代码零侵入\n5.2.安装Canal 5.2.1.认识Canal Canal [kə\u0026rsquo;næl]，译意为水道/管道/沟渠，canal是阿里巴巴旗下的一款开源项目，基于Java开发。基于数据库增量日志解析，提供增量数据订阅\u0026amp;消费。GitHub的地址：https://github.com/alibaba/canal\nCanal是基于mysql的主从同步来实现的，MySQL主从同步的原理如下：\nMySQL master 将数据变更写入二进制日志( binary log），其中记录的数据叫做binary log events MySQL slave 将 master 的 binary log events拷贝到它的中继日志(relay log) MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 而Canal就是把自己伪装成MySQL的一个slave节点，从而监听master的binary log变化。再把得到的变化信息通知给Canal的客户端，进而完成对其它数据库的同步。\n5.2.2.安装Canal 5.3.监听Canal Canal提供了各种语言的客户端，当Canal监听到binlog变化时，会通知Canal的客户端。\n我们可以利用Canal提供的Java客户端，监听Canal通知消息。当收到变化的消息时，完成对缓存的更新。\n不过这里我们会使用GitHub上的第三方开源的canal-starter客户端。地址：https://github.com/NormanGyllenhaal/canal-client\n与SpringBoot完美整合，自动装配，比官方客户端要简单好用很多。\n5.3.1.引入依赖： 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;top.javatool\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;canal-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.1-RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 5.3.2.编写配置： 1 2 3 canal: destination: heima # canal的集群名字，要与安装canal时设置的名称一致 server: 192.168.150.101:11111 # canal服务地址 5.3.3.修改Item实体类 通过@Id、@Column、等注解完成Item与数据库表字段的映射：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package com.heima.item.pojo; import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.annotation.TableField; import com.baomidou.mybatisplus.annotation.TableId; import com.baomidou.mybatisplus.annotation.TableName; import lombok.Data; import org.springframework.data.annotation.Id; import org.springframework.data.annotation.Transient; import javax.persistence.Column; import java.util.Date; @Data @TableName(\u0026#34;tb_item\u0026#34;) public class Item { @TableId(type = IdType.AUTO) @Id private Long id;//商品id @Column(name = \u0026#34;name\u0026#34;) private String name;//商品名称 private String title;//商品标题 private Long price;//价格（分） private String image;//商品图片 private String category;//分类名称 private String brand;//品牌名称 private String spec;//规格 private Integer status;//商品状态 1-正常，2-下架 private Date createTime;//创建时间 private Date updateTime;//更新时间 @TableField(exist = false) @Transient private Integer stock; @TableField(exist = false) @Transient private Integer sold; } 5.3.4.编写监听器 通过实现EntryHandler\u0026lt;T\u0026gt;接口编写监听器，监听Canal消息。注意两点：\n实现类通过@CanalTable(\u0026quot;tb_item\u0026quot;)指定监听的表信息 EntryHandler的泛型是与表对应的实体类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 package com.heima.item.canal; import com.github.benmanes.caffeine.cache.Cache; import com.heima.item.config.RedisHandler; import com.heima.item.pojo.Item; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import top.javatool.canal.client.annotation.CanalTable; import top.javatool.canal.client.handler.EntryHandler; @CanalTable(\u0026#34;tb_item\u0026#34;) @Component public class ItemHandler implements EntryHandler\u0026lt;Item\u0026gt; { @Autowired private RedisHandler redisHandler; @Autowired private Cache\u0026lt;Long, Item\u0026gt; itemCache; @Override public void insert(Item item) { // 写数据到JVM进程缓存 itemCache.put(item.getId(), item); // 写数据到redis redisHandler.saveItem(item); } @Override public void update(Item before, Item after) { // 写数据到JVM进程缓存 itemCache.put(after.getId(), after); // 写数据到redis redisHandler.saveItem(after); } @Override public void delete(Item item) { // 删除数据到JVM进程缓存 itemCache.invalidate(item.getId()); // 删除数据到redis redisHandler.deleteItemById(item.getId()); } } 在这里对Redis的操作都封装到了RedisHandler这个对象中，是我们之前做缓存预热时编写的一个类，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 package com.heima.item.config; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.heima.item.pojo.Item; import com.heima.item.pojo.ItemStock; import com.heima.item.service.IItemService; import com.heima.item.service.IItemStockService; import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.util.List; @Component public class RedisHandler implements InitializingBean { @Autowired private StringRedisTemplate redisTemplate; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; private static final ObjectMapper MAPPER = new ObjectMapper(); @Override public void afterPropertiesSet() throws Exception { // 初始化缓存 // 1.查询商品信息 List\u0026lt;Item\u0026gt; itemList = itemService.list(); // 2.放入缓存 for (Item item : itemList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(item); // 2.2.存入redis redisTemplate.opsForValue().set(\u0026#34;item🆔\u0026#34; + item.getId(), json); } // 3.查询商品库存信息 List\u0026lt;ItemStock\u0026gt; stockList = stockService.list(); // 4.放入缓存 for (ItemStock stock : stockList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(stock); // 2.2.存入redis redisTemplate.opsForValue().set(\u0026#34;item:stock:id:\u0026#34; + stock.getId(), json); } } public void saveItem(Item item) { try { String json = MAPPER.writeValueAsString(item); redisTemplate.opsForValue().set(\u0026#34;item🆔\u0026#34; + item.getId(), json); } catch (JsonProcessingException e) { throw new RuntimeException(e); } } public void deleteItemById(Long id) { redisTemplate.delete(\u0026#34;item🆔\u0026#34; + id); } } ","permalink":"https://cold-bin.github.io/post/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/","tags":["lua脚本","多级缓存"],"title":"Redis高级篇之多级缓存"},{"categories":["汇编"],"contents":"标志寄存器 1、标志寄存器 CPU内部的寄存器中，有一种特殊的寄存器（对于不同的处理机，个数和结构都可能不同）具有以下3种作用。\n（1）用来存储相关指令的某些执行结果；\n（2）用来为CPU执行相关指令提供行为依据；\n（3）用来控制CPU的相关工作方式。\n这种特殊的寄存器在8086CPU中，被称为标志寄存器（flag）。\n8086CPU的标志寄存器有16位，其中存储的信息通常被称为程序状态字（PSW-Program Status Word）\nflag寄存器是按位起作用的，它的每一位都有专门的含义，记录特定的信息。\n在8086CPU的指令集中，有的指令的执行是影响标志寄存器的，比如，add、sub、mul、div、inc、or、and等，它们大都是运算指令（进行逻辑或算术运算）；有的指令的执行对标志寄存器没有影响，比如，mov、push、pop等，它们大都是传送指令\n1、零标志位 (ZF) 零标志位（Zero Flag）。它记录相关指令执行后，其结果是否为0。\n如果结果为0，那么zf = 1(表示结果是0)；如果结果不为0，那么zf = 0。\n1 2 3 4 5 mov ax, 1 sub ax, 1 ;执行后，结果为0，则zf = 1 mov ax, 2 sub ax, 1 ;执行后，结果不为0，则zf = 0 2、奇偶标志位 (PF) 奇偶标志位（Parity Flag）。它记录相关指令执行后，其结果的所有bit位中1的个数是否为偶数。\n如果1的个数为偶数，pf = 1，如果为奇数，那么pf = 0。\n1 2 3 4 5 mov al, 1 add al, 10 ;执行后，结果为00001011B，其中有3（奇数）个1，则pf = 0； mov al, 1 or al, 2 ;执行后，结果为00000011B，其中有2（偶数）个1，则pf = 1； 3、符号标志位(SF) 符号标志位(Symbol Flag)。它记录相关指令执行后，其结果是否为负。\n如果结果为负，sf = 1；如果非负，sf = 0。\n计算机中通常用补码来表示有符号数据。计算机中的一个数据可以看作是有符号数，也可以看成是无符号数。\n00000001B，可以看作为无符号数1，或有符号数+1； 10000001B，可以看作为无符号数129，也可以看作有符号数-127。\n对于同一个二进制数据，计算机可以将它当作无符号数据来运算，也可以当作有符号数据来运算\nCPU在执行add等指令的时候，就包含了两种含义:可以将add指令进行的运算当作无符号数的运算，也可以将add指令进行的运算当作有符号数的运算\nSF标志，就是CPU对有符号数运算结果的一种记录，它记录数据的正负。在我们将数据当作有符号数来运算的时候，可以通过它来得知结果的正负。如果我们将数据当作无符号数来运算，SF的值则没有意义，虽然相关的指令影响了它的值\n1 2 3 4 5 mov al, 10000001B add al, 1 ;执行后，结果为10000010B，sf = 1，表示：如果指令进行的是有符号数运算，那么结果为负； mov al, 10000001B add al, 01111111B ;执行后，结果为0，sf = 0，表示：如果指令进行的是有符号数运算，那么结果为非负 3、进位标志位(CF) 进位标志位(Carry Flag)。一般情况下，在进行无符号数运算的时候，它记录了运算结果的最高有效位向更高位的进位值，或从更高位的借位值 97H - 98H 产生借位CF = 1 ==\u0026gt; (al) = 197H - 98H = FFH\n4、溢出标志位(OF) 溢出标志位(Overflow Flag)。一般情况下，OF记录了有符号数运算的结果是否发生了溢出。\n如果发生溢出，OF = 1；如果没有，OF = 0。\nCF和OF的区别：CF是对无符号数运算有意义的标志位，而OF是对有符号数运算有意义的标志位\nCPU在执行add等指令的时候，就包含了两种含义：无符号数运算和有符号数运算。\n对于无符号数运算，CPU用CF位来记录是否产生了进位； 对于有符号数运算，CPU用OF位来记录是否产生了溢出，当然，还要用SF位来记录结果的符号。 1 2 3 4 5 6 7 8 9 10 11 mov al, 98 add al, 99 ;执行后将产生溢出。因为进行的\u0026#34;有符号数\u0026#34;运算是：（al）=（al）+ 99 = 98 + 99=197 = C5H 为-59的补码 ;而结果197超出了机器所能表示的8位有符号数的范围：-128-127。 ;add 指令执行后：无符号运算没有进位CF=0，有符号运算溢出OF=1 ;当取出的数据C5H按无符号解析C5H = 197, 当按有符号解析通过SP得知数据为负,即C5H为-59补码存储， mov al，0F0H ;F0H，为有符号数-16的补码 -Not(F0 - 1) add al，088H ;88H，为有符号数-120的补码 -Not(88- 1) ;执行后，将产生溢出。因为add al, 088H进行的有符号数运算结果是：（al）= -136 ;而结果-136超出了机器所能表示的8位有符号数的范围：-128-127。 ;add 指令执行后：无符号运算有进位CF=1，有符号运算溢出OF=1 2、adc指令和sbb指令 adc是带进位加法指令，它利用了CF位上记录的进位值。\n指令格式：adc 操作对象1, 操作对象2\n功能：操作对象1 = 操作对象1 + 操作对象2 + CF\n1 2 3 4 mov ax, 2 mov bx, 1 sub bx, ax ;无符号运算借位CF=1，有符号运算OF = 0 adc ax, 1 ;执行后，（ax）= 4。adc执行时，相当于计算：(ax)+1+CF = 2+1+1 = 4。 1 2 3 4 5 6 ;计算1EF000H+201000H，结果放在ax（高16位）和bx（低16位）中。 ;将计算分两步进行，先将低16位相加，然后将高16位和进位值相加。 mov ax, 001EH mov bx, 0F000H add bx, 1000H adc ax, 0020H sbb指令\nsbb是带借位减法指令，它利用了CF位上记录的借位值。\n指令格式：sbb 操作对象1, 操作对象2\n功能：操作对象1 = 操作对象1 - 操作对象2 - CF\n1 2 3 4 5 ;计算 003E1000H - 00202000H，结果放在ax，bx中，程序如下： mov bx, 1000H mov ax, 003EH sub bx, 2000H sbb ax, 0020H 3、cmp指令 cmp是比较指令，cmp的功能相当于减法指令，只是不保存结果。cmp指令执行后，将对标志寄存器产生影响。\n其他相关指令通过识别这些被影响的标志寄存器位来得知比较结果。\ncmp指令格式：cmp 操作对象1，操作对象2\n例如： 指令cmp ax, ax，做（ax）-（ax）的运算，结果为0，但并不在ax中保存，仅影响flag的相关各位。 指令执行后：zf=1，pf=1，sf=0，cf=0，of=0。\nCPU在执行cmp指令的时候，也包含两种含义：进行无符号数运算和进行有符号数运算。\ncmp ax, bx 无符号比较时 (ax) = (bx) zf = 1 (ax) ≠ (bx) zf = 0 (ax) \u0026lt; (bx) cf = 1 (ax) ≥ (bx) cf = 0 (ax) \u0026gt; (bx) cf = 0 且 zf = 0 (ax) ≤ (bx) cf = 1 且 zf = 1 上面的表格可以正推也可以逆推\n如果用cmp来进行有符号数比较时 SF只能记录实际结果的正负，发生溢出的时候，实际结果的正负不能说明逻辑上真正结果的正负。 但是逻辑上的结果的正负，才是cmp指令所求的真正结果，所以我们在考察SF的同时考察OF，就可以得知逻辑上真正结果的正负，同时就知道比较的结果。\n1 2 3 4 5 mov ah, 08AH ; -Not(8A-1) = -118 即当成有符号数时为-118 mov bh, 070H ; 有符号数时最高位为0为正数， 70H = 112 cmp ah, bh ;（ah）-（bh）实际得到的结果是1AH ; 在逻辑上，运算所应该得到的结果是：（-118）- 112 = -230 ; sf记录实际结果的正负，所以sf=0 cmp ah, bh （1）如果sf=1，而of=0 。 of=0说明没有溢出，逻辑上真正结果的正负=实际结果的正负； sf=1，实际结果为负，所以逻辑上真正的结果为负，所以（ah）\u0026lt;（bh）\n（2）如果sf=1，而of=1： of=1，说明有溢出，逻辑上真正结果的正负≠实际结果的正负； sf=1，实际结果为负。 实际结果为负，而又有溢出，这说明是由于溢出导致了实际结果为负，，如果因为溢出导致了实际结果为负，那么逻辑上真正的结果必然为正。 这样，sf=1，of=1，说明了（ah）\u0026gt;（bh）。\n（3）如果sf=0，而of=1。of=1，说明有溢出，逻辑上真正结果的正负≠实际结果的正负；sf=0，实际结果非负。而of=1说明有溢出，则结果非0，所以，实际结果为正。 实际结果为正，而又有溢出，这说明是由于溢出导致了实际结果非负，如果因为溢出导致了实际结果为正，那么逻辑上真正的结果必然为负。这样，sf=0，of=1，说明了（ah）\u0026lt;（bh）。 （4）如果sf=0，而of=0 of=0，说明没有溢出，逻辑上真正结果的正负=实际结果的正负；sf=0，实际结果非负，所以逻辑上真正的结果非负，所以（ah）≥（bh）。\n4、检测比较结果的条件转移指令 可以根据某种条件，决定是否修改IP的指令\njcxz它可以检测cx中的数值，如果（cx）=0，就修改IP，否则什么也不做。\n所有条件转移指令的转移位移都是[-128，127]。\n多数条件转移指令都检测标志寄存器的相关标志位，根据检测的结果来决定是否修改IP\n这些条件转移指令通常都和cmp相配合使用,它们所检测的标志位，都是cmp指令进行无符号数比较的时记录比较结果的标志位\n根据无符号数的比较结果进行转移的条件转移指令（它们检测zf、cf的值）\n指令 含义 检测的相关标志位 je 等于则转移 zf = 1 jne 不等于则转移 zf = 0 jb 低于则转移 cf = 1 jnb 不低于则转移 cf = 0 ja 高于则转移 cf = 0 且 zf = 0 jna 不高于则转移 cf = 1 且 zf = 1 j：jump，e：equal，b：below，a：above，n：not\n1 2 3 4 5 6 7 8 9 10 11 12 13 ;编程，统计data段中数值为8的字节的个数，用ax保存统计结果。 mov ax, data mov ds, ax mov bx, 0 ;ds:bx指向第一个字节 mov ax, 0 ;初始化累加器mov cx，8 s: cmp byte ptr [bx], 8 ;和8进行比较 jne next ;如果不相等转到next，继续循环 inc ax ;如果相等就将计数值加1 next: inc bx loop s ;程序执行后：（ax）=3 5、DF标志和串传送指令 方向标志位。在串处理指令中，控制每次操作后si、di的增减。\ndf = 0每次操作后si、di递增； df = 1每次操作后si、di递减。 格式：movsb 功能：将ds:si指向的内存单元中的字节送入es:di中，然后根据标志寄存器df位的值，将si和di递增或递减\n格式：movsw 功能：将ds:si指向的内存字单元中的字送入es:di中，然后根据标志寄存器df位的值，将si和di递增2或递减2。\n格式：rep movsb movsb和movsw进行的是串传送操作中的一个步骤，一般来说，movsb和movsw都和rep配合使用， 功能：rep的作用是根据cx的值，重复执行后面的串传送指令\n8086CPU提供下面两条指令对df位进行设置。\ncld指令：将标志寄存器的df位置0 std指令：将标志寄存器的df位置1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ;将data段中的第一个字符串复制到它后面的空间中。 data segment db \u0026#39;Welcome to masm!\u0026#39; db 16 dup (0) data ends mov ax, data mov ds, ax mov si, 0 ;ds:si 指向data:0 mov es, ax mov di, 16 ;es:di指向data:0010 mov cx, 16 ;（cx）=16，rep循环16次 cld ;设置df=0，正向传送 rep movsb 6、pushf和popf pushf的功能是将标志寄存器的值压栈，而popf是从栈中弹出数据，送入标志寄存器中\npushf和popf，为直接访问标志寄存器提供了一种方法。\n7、总结 当使用某些运算指令运算时，如果数据发生溢出或需要借位，都会在标志寄存器的某个位有所体现。那么可以根据这个标志寄存器实现大数之间的运算 串转移指令，算是封装的汇编api ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/","tags":[],"title":"汇编之标志寄存器"},{"categories":["redis","分布式系统","缓存"],"contents":"[toc]\n基于Redis集群解决单机Redis存在的问题 单机的Redis存在四大问题：\n分布式缓存 1.Redis持久化 Redis有两种持久化方案：\nRDB持久化 AOF持久化 1.1.RDB持久化 RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为RDB文件，默认是保存在当前运行目录。\n1.1.1.执行时机 RDB持久化在四种情况下会执行：\n执行save命令 执行bgsave命令 Redis停机时 触发RDB条件时 1）save命令\n执行下面的命令，可以立即执行一次RDB：\nsave命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。\n2）bgsave命令\n下面的命令可以异步执行RDB：\n这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。\n3）停机时\nRedis停机时会执行一次save命令，实现RDB持久化。\n4）触发RDB条件\nRedis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下：\n1 2 3 4 5 # 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save \u0026#34;\u0026#34; 则表示禁用RDB save 900 1 # 依次类推 save 300 10 save 60 10000 RDB的其它配置也可以在redis.conf文件中设置：\n1 2 3 4 5 6 7 8 # 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱 rdbcompression yes # RDB文件名称 dbfilename dump.rdb # 文件保存的路径目录 dir ./ SAVE 保存是阻塞主进程，客户端无法连接redis，等SAVE完成后，主进程才开始工作，客户端可以连接\nBGSAVE 是fork一个save的子进程，在执行save过程中，不影响主进程，客户端可以正常链接redis，等子进程fork执行save完成后，通知主进程，子进程关闭。\n1.1.2.RDB原理 RDB持久化是指在指定的时间间隔内将redis内存中的数据集快照写入磁盘，实现原理是redis服务在指定的时间间隔内先fork一个子进程，由子进程将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储，生成dump.rdb文件存放在磁盘中。\nfork采用的是copy-on-write技术：\n当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作。 1.1.3.小结 RDB方式bgsave的基本流程？\nfork主进程得到一个子进程，共享内存空间 子进程读取内存数据并写入新的RDB文件 用新RDB文件替换旧的RDB文件 RDB会在什么时候执行？save 60 1000代表什么含义？\n默认是服务停止时 代表60秒内至少执行1000次修改则触发RDB RDB优点？\n一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化(bgsave)时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。(save会阻塞主进程) 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB的缺点？\nRDB执行间隔时间长，两次RDB之间写入数据有丢失的风险 当内存里数据集较大时，fork子进程、压缩、写出RDB文件都比较耗时，那么fork子进程就比较消耗性能，容易拖垮服务 1.2.AOF持久化 1.2.1.AOF原理 AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。\n1.2.2.AOF配置 AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF：\n1 2 3 4 # 是否开启AOF功能，默认是no appendonly yes # AOF文件的名称 appendfilename \u0026#34;appendonly.aof\u0026#34; AOF的命令记录的频率也可以通过redis.conf文件来配：\n1 2 3 4 5 6 # 表示每执行一次写命令，立即记录到AOF文件 appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案 appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 appendfsync no 三种策略对比：\n1.2.3.AOF文件重写 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。\n如图，AOF原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。\n所以重写命令后，AOF文件内容就是：mset name jack num 666\nRedis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置：\n1 2 3 4 # AOF文件比上次文件增长超过多少百分比，则触发重写 auto-aof-rewrite-percentage 100 # AOF文件体积最小多大以上才触发重写 auto-aof-rewrite-min-size 64mb 1.3.RDB与AOF对比 RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。\n2.Redis主从 2.1.搭建主从架构 单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。\n像MySQL一样，redis是支持主从同步的，而且也支持一主多从以及多级从结构。 主从结构，一是为了纯粹的冗余备份，二是为了提升读性能，比如很消耗性能的SORT就可以由从服务器来承担。 redis的主从同步是异步进行的，这意味着主从同步不会影响主逻辑，也不会降低redis的处理性能。 主从架构中，可以考虑关闭主服务器的数据持久化功能，只让从服务器进行持久化，这样可以提高主服务器的处理性能。\n2.2.主从数据同步原理 2.2.1.全量同步 主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程：\n这里有一个问题，master如何得知salve是第一次来连接呢？？\n有几个概念，可以作为判断依据：\nReplication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。\n因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。\nmaster判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。\nmaster会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。\n因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。\n如图：\n完整流程描述：\nslave节点请求增量同步 master节点判断replid，发现不一致，拒绝增量同步，开启全量同步 master将完整内存数据生成RDB，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave slave执行接收到的命令，保持与master之间的同步 2.2.2.增量同步 全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。\n什么是增量同步？就是只更新slave与master存在差异的部分数据。如图：\n那么master怎么知道slave与自己的数据差异在哪里呢?\n2.2.3.repl_backlog原理 master怎么知道slave与自己的数据差异在哪里呢?\n这就要说到全量同步时的repl_baklog文件了。\n这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。\nrepl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset：\nslave与master的offset之间的差异，就是salve需要增量拷贝的数据了。\n随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset：\n直到数组被填满：\n此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。\n但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset：\n如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖：\n棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。\n2.3.主从同步优化 主从同步可以保证主从数据的一致性，非常重要\n可以从以下几个方面来优化Redis主从就集群：\n在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 主从从架构图：\n2.4.小结 简述全量同步和增量同步区别？\n全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？\nslave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？\nslave节点断开又恢复，并且在repl_baklog中能找到offset时 3.Redis哨兵 Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。\n3.1.哨兵原理 3.1.1.集群结构和作用 哨兵的结构如图：\n哨兵的作用如下：\n监控：Sentinel 会不断检查您的master和slave是否按预期工作 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 3.1.2.集群监控原理 Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令：\n主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 3.1.3.集群故障恢复原理 一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的：\n首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 最后是判断slave节点的运行id大小，越小优先级越高。 当选出一个新的master后，该如何实现切换呢？\n流程如下：\nsentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master sentinel给所有其它slave发送slaveof 192.168.150.101 7002命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点 3.1.4.小结 Sentinel的三个作用是什么？\n监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？\n每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？\n首先选定一个slave作为新的master，执行slaveof no on命令 然后让所有节点都执行slaveof 新master主机 端口号 修改故障节点配置，添加slaveof 新master 端口号 3.2.搭建哨兵集群 3.3.RedisTemplate 在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。\n下面，我们通过一个测试来实现RedisTemplate集成哨兵机制。\n3.3.1.导入Demo工程 首先，我们引入课前资料提供的Demo工程：\n3.3.2.引入依赖 在项目的pom文件中引入依赖：\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 3.3.3.配置Redis地址 然后在配置文件application.yml中指定redis的sentinel相关信息：\n1 2 3 4 5 6 7 8 spring: redis: sentinel: master: mymaster nodes: - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 3.3.4.配置读写分离 在项目的启动类中，添加一个新的bean：\n1 2 3 4 @Bean public LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer(){ return clientConfigurationBuilder -\u0026gt; clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED); } 这个bean中配置的就是读写策略，包括四种：\nMASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master 4.Redis分片集群 4.1.搭建分片集群 主从和哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决：\n海量数据存储问题：我们知道，主从集群模式下，redis的单个master就将所有数据饱揽下来。那么这样始终有一天，面临内存不足的问题，某些关键数据可能会被LRU所淘汰。 高并发写的问题：虽然主从架构可以提高并发读的性能，但是对于高并发写性能提升意义不大。因为在主从集群下，master只要一个，也就是redis的写只能写在一个master节点上（虽然我们利用主从集群抽离master上的读命令执行，但依然提升有限）。问题主要出现在master只有一个，而且redis执行命令都是单线程的，只能一条一条的执行。那么解决方案显而易见：我们可以多选几个master出来执行写命令。而且这几个master的数据是不一致的，这样可以减轻主从集群模式下，单节点master数据存储的压力。 使用分片集群可以解决上述问题（分片感觉就是多个集群有机结合），如图:\n分片集群特征：\n集群中有多个master，每个master保存不同数据\n每个master都可以有多个slave节点\nmaster之间通过ping监测彼此健康状态\n客户端请求可以访问集群任意节点，最终都会被转发到正确节点\n这里面最大的问题是：请求如何知道自己的数据在哪个master？那么就涉及到查找和寻址的问题，典型的解决方案就是哈希的思想，我们将\n4.2.散列插槽 4.2.1.插槽原理 Redis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到：\n数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况：\nkey中包含\u0026quot;{}\u0026quot;，且“{}”中至少包含1个字符，“{}”中的部分是有效部分 key中不包含“{}”，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。\n如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到7003节点。\n到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点\n散列过程里存在一个巨严重的问题：\n当我们需要增删redis分片集群的master节点时，那么hash的散列表映射长度发生变化，那么就会导致后续请求的master节点命中失败。\n那么怎么解决呢？这时候就需要引入高级一点的哈希————一致性哈希\nredis的一致性哈希有三大特性：\nkey哈希结果尽可能分配到不同Redis实例 当实例增加或移除，需要保护已映射的内容不会重新被分配到新实例上 对key的哈希应尽量避免重复 分片实现： 前面谈到主从切换的哨兵模式已经提到，哨兵模式可以实现高可用以及读写分离，但是缺点在于所有Redis实例存储的数据全部一致，所以Redis支持cluster模式，可以简单将cluster理解为Redis集群管理的一个插件，通过它可以实现Redis的分布式存储。\n数据分片方式一般有三种：客户端分片、代理分片和服务器分片。\n1）客户端分片\n定义：客户端自己计算key需要映射到哪一个Redis实例。 优点：客户端分片最明显的好处在于降低了集群的复杂度，而服务器之间没有任何关联性，数据分片由客户端来负责实现。 缺点：客户端实现分片则客户端需要知道当前集群下不同Redis实例的信息，当新增Redis实例时需要支持动态分片，多数Redis需要重启才能实现该功能。 2）代理分片\n定义：客户端将请求发送到代理，代理通过计算得到需要映射的集群实例信息，然后将客户端的请求转发到对应的集群实例上，然后返回响应给客户端。 优点：降低了客户端的复杂度，客户端不用关心后端Redis实例的状态信息。 缺点：多了一个中间分发环节，所以对性能有些取的损失。 3）服务器分片\n定义：客户端可以和集群中任意Redis实例通信，当客户端访问某个实例时，服务器进行计算key应该映射到哪个具体的Redis实例中存储，如果映射的实例不是当前实例，则该实例主动引导客户端去对应实例对key进行操作。这其实是一个重定向的过程**。**这个过程不是从当前Redis实例转发到对应的Redis实例，而是客户端收到服务器通知具体映射的Redis实例重定向到映射的实例中。当前还不能完全适用于生产环境。 优点：支持高可用，任意实例都有主从，主挂了从会自动接管。 缺点：需要客户端语言实现服务器集群协议，但是目前大多数语言都有其客户端实现版本。 4）预分片 从上面可以清楚地看出，分片机制增加或移除实例是非常麻烦的一件事，所以我们可以考虑一开始就开启32个节点实例，当我们可以新增Redis服务器时，我们可以将一半的节点移动到新的Redis服务器。这样我们只需要在新服务器启动一个空节点，然后移动数据，配置新节点为源节点的从节点，然后更新被移动节点的ip信息，然后向新服务器发送slaveof命令关闭主从配置，最后关闭旧服务器不需要使用的实例并且重新启动客户端。这样我们就可以在几乎不需要停机时间时完成数据的移动。\n分片机制的缺点\n分片是由多台Redis实例共同运转，所以如果其中一个Redis实例宕机，则整个分片都将无法使用，所以分片机制无法实现高可用。 如果有不同的key映射到不同的Redis实例，这时候不能对这两个key做交集或者使用事务。=\u0026gt; 单机上的redis事务不支持，只有使用分布式事务 使用分片机制因为涉及多实例，数据处理比较复杂。 分片中对于实例的添加或删除会很复杂，不过可以使用预分片技术进行改善。 4.2.1.小结 Redis如何判断某个key应该在哪个实例？\n将16384个插槽分配到不同的实例 根据key的有效部分计算哈希值，对16384取余 余数作为插槽，寻找插槽所在实例即可 如何将同一类数据固定的保存在同一个Redis实例？\n这一类数据使用相同的有效部分，例如key都以{typeId}为前缀 redis分片机制详解\n4.3.集群伸缩 redis-cli \u0026ndash;cluster提供了很多操作集群的命令，可以通过下面方式查看：\n比如，添加节点的命令：\n4.3.1.需求分析 需求：向集群中添加一个新的master节点，并向其中存储 num = 10\n启动一个新的redis实例，端口为7004 添加7004到之前的集群，并作为一个master节点 给7004节点分配插槽，使得num这个key可以存储到7004实例 这里需要两个新的功能：\n添加一个节点到集群中 将部分插槽分配到新插槽 4.3.2.创建新的redis实例 创建一个文件夹：\n1 mkdir 7004 拷贝配置文件：\n1 cp redis.conf /7004 修改配置文件：\n1 sed /s/6379/7004/g 7004/redis.conf 启动\n1 redis-server 7004/redis.conf 4.3.3.添加新节点到redis 添加节点的语法如下：\n执行命令：\n1 redis-cli --cluster add-node 192.168.150.101:7004 192.168.150.101:7001 通过命令查看集群状态：\n1 redis-cli -p 7001 cluster nodes 如图，7004加入了集群，并且默认是一个master节点：\n但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上\n4.3.4.转移插槽 我们要将num存储到7004节点，因此需要先看看num的插槽是多少：\n如上图所示，num的插槽为2765.\n我们可以将0~3000的插槽从7001转移到7004，命令格式如下：\n具体命令如下：\n建立连接：\n得到下面的反馈：\n询问要移动多少个插槽，我们计划是3000个：\n新的问题来了：\n那个node来接收这些插槽？？\n显然是7004，那么7004节点的id是多少呢？\n复制这个id，然后拷贝到刚才的控制台后：\n这里询问，你的插槽是从哪里移动过来的？\nall：代表全部，也就是三个节点各转移一部分 具体的id：目标节点的id done：没有了 这里我们要从7001获取，因此填写7001的id：\n填完后，点击done，这样插槽转移就准备好了：\n确认要转移吗？输入yes：\n然后，通过命令查看结果：\n可以看到：\n目的达成。\n4.4.故障转移 集群初识状态是这样的：\n其中7001、7002、7003都是master，我们计划让7002宕机。\n4.4.1.自动故障转移 当集群中有一个master宕机会发生什么呢？\n直接停止一个redis实例，例如7002：\n1 redis-cli -p 7002 shutdown 1）首先是该实例与其它实例失去连接\n2）然后是疑似宕机：\n3）最后是确定下线，自动提升一个slave为新的master：\n4）当7002再次启动，就会变为一个slave节点了：\n4.4.2.手动故障转移 利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下：\n这种failover命令可以指定三种模式：\n缺省：默认的流程，如图1~6歩 force：省略了对offset的一致性校验 takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见 案例需求：在7002这个slave节点执行手动故障转移，重新夺回master地位\n步骤如下：\n1）利用redis-cli连接7002这个节点\n2）执行cluster failover命令\n如图：\n效果：\n4.5.RedisTemplate访问分片集群 RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致：\n1）引入redis的starter依赖\n2）配置分片集群地址\n3）配置读写分离\n与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下：\n1 2 3 4 5 6 7 8 9 10 spring: redis: cluster: nodes: - 192.168.150.101:7001 - 192.168.150.101:7002 - 192.168.150.101:7003 - 192.168.150.101:8001 - 192.168.150.101:8002 - 192.168.150.101:8003 ","permalink":"https://cold-bin.github.io/post/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/","tags":["rdb与aof持久化","redis主从架构","redis哨兵模式","redis分片集群"],"title":"Redis高级篇之分布式缓存"},{"categories":["汇编"],"contents":"call和ret指令 call和ret指令都是转移指令，它们都修改IP，或同时修改CS和IP。\n1、ret 和 retf ret指令用栈中的数据，修改IP的内容，从而实现近转移； retf指令用栈中的数据，修改CS和IP的内容，从而实现远转移。 CPU执行ret指令时，相当于进行： pop IP：\n（1）(IP) = ( (ss) * 16 + (sp) )\n（2）(sp) = (sp) + 2\nCPU执行retf指令时，相当于进行：pop IP, pop CS：\n（1）(IP) = ( (ss) * 16 + (sp) )\n（2）(sp) = (sp) + 2\n（3）(CS) = ( (ss) * 16 + (sp) )\n（4）(sp) = (sp) + 2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 assume cs:code stack seqment db 16 dup (0) stack ends code segment mov ax, 4c00h int 21h start:\tmov ax, stack mov ss, ax mov sp, 16 mov ax, 0 push ax ;ax入栈 mov bx, 0 ret ;ret指令执行后，(IP)=0，CS:IP指向代码段的第一条指令。可以push cs push ax retf code ends end start 2、call 指令 call指令经常跟ret指令配合使用，因此CPU执行call指令，进行两步操作：\n（1）将当前的 IP 或 CS和IP 压入栈中；\n（2）转移（jmp）。\ncall指令不能实现短转移，除此之外，call指令实现转移的方法和 jmp 指令的原理相同。\ncall 标号（近转移）\nCPU执行此种格式的call指令时，相当于进行 push IP jmp near ptr 标号\ncall far ptr 标号（段间转移）\nCPU执行此种格式的call指令时，相当于进行：push CS，push IP jmp far ptr 标号\n1 call 16位寄存器 CPU执行此种格式的call指令时，相当于进行： push IP jmp 16位寄存器\n1 call word ptr 内存单元地址 CPU执行此种格式的call指令时，相当于进行：push IP jmp word ptr 内存单元地址\n1 2 3 4 5 mov sp, 10h mov ax, 0123h mov ds:[0], ax call word ptr ds:[0] ;执行后，(IP)=0123H，(sp)=0EH call dword ptr 内存单元地址\nCPU执行此种格式的call指令时，相当于进行：push CS push IP jmp dword ptr 内存单元地址\n1 2 3 4 5 6 mov sp, 10h mov ax, 0123h mov ds:[0], ax mov word ptr ds:[2], 0 call dword ptr ds:[0] ;执行后，(CS)=0，(IP)=0123H，(sp)=0CH 3、call 和 ret 的配合使用 分析下面程序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 assume cs:code code segment start:\tmov ax,1 mov cx,3 call s ;（1）CPU指令缓冲器存放call指令，IP指向下一条指令（mov bx, ax），执行call指令，IP入栈，jmp mov bx,ax\t;（4）IP重新指向这里 bx = 8 mov ax,4c00h int 21h s: add ax,ax loop s;（2）循环3次ax = 8 ret;（3）return : pop IP code ends end start call 与 ret 指令共同支持了汇编语言编程中的模块化设计，用来编写子程序\n4、寄存器冲突 在设计子程序时，由于寄存器资源有限，难免会出现子程序和调用程序使用了同一个寄存器，而且子程序还修改了这个寄存器，那么这样就存在巨大的问题。如何解决呢？\n其实，这个就类似于高级语言的函数调用。既然都要使用某个寄存器，那么就可以这样：当进入子程序时，我们将调用程序寄存器里的数据放到栈里面存放，这样子程序就可以放心使用啦。子程序结束之后，我们再将数据恢复到寄存器里即可。\n","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/","tags":["汇编函数结构"],"title":"汇编之call和ret指令"},{"categories":["汇编"],"contents":"转移指令的原理 可以修改IP，或同时修改CS和IP的指令统称为转移指令。概括地讲，转移指令就是可以控制CPU执行内存中某处代码的指令。\n8086CPU的转移行为有以下几类。\n只修改IP时，称为段内转移，比如：jmp ax。 同时修改CS和IP时，称为段间转移，比如：jmp 1000:0。 由于转移指令对IP的修改范围不同，段内转移又分为：短转移和近转移。\n短转移IP的修改范围为-128 ~ 127。 近转移IP的修改范围为-32768 ~ 32767。 8086CPU的转移指令分为以下几类。\n无条件转移指令（如：jmp） 条件转移指令 循环指令（如：loop） 过程 中断 1、操作符offset 操作符offset在汇编语言中是由编译器处理的符号，它的功能是取得标号的偏移地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 ;将s处的一条指令复制到s0处 assume cs:codesg codesg segment s: mov ax, bx ;（mov ax,bx 的机器码占两个字节） mov si, offset s ;获得标号s的偏移地址 mov di, offset s0 ;获得标号s0的偏移地址 mov ax, cs:[si] mov cs:[di], ax s0: nop ;（nop的机器码占一个字节） nop codesg ends ends 2、jmp指令 jmp为无条件转移，转到标号处执行指令可以只修改IP，也可以同时修改CS和IP；\njmp指令要给出两种信息：\n转移的目的地址 转移的距离（段间转移、段内短转移，段内近转移） jmp short 标号 jmp near ptr 标号 jcxz 标号 loop 标号 等几种汇编指令，它们对 IP的修改\n是根据转移目的地址和转移起始地址之间的位移来进行的。在它们对应的机器码中不包含转移的目的地址，而包含的是到目的地址的位移距离。\n1、依据位移进行转移的jmp指令 jmp short 标号（段内短转移）\n指令“jmp short 标号”的功能为(IP)=(IP)+8位位移，转到标号处执行指令\n（1）8位位移 = “标号”处的地址 - jmp指令后的第一个字节的地址；\n（2）short指明此处的位移为8位位移；\n（3）8位位移的范围为-128~127，用补码表示\n（4）8位位移由编译程序在编译时算出。\n1 2 3 4 5 6 7 8 assume cs:codesg codesg segment start:mov ax,0 jmp short s ;s不是被翻译成目的地址 add ax, 1 s:inc ax ;程序执行后， ax中的值为 1 codesg ends end start CPU不需要这个目的地址就可以实现对IP的修改。这里是依据位移进行转移\njmp short s指令的读取和执行过程：\n(CS)=0BBDH，(IP)=0006，上一条指令执行结束后CS:IP指向EB 03（jmp short s的机器码）； 读取指令码EB 03进入指令缓冲器； (IP) = (IP) + 所读取指令的长度 = (IP) + 2 = 0008，CS:IP指向add ax,1； CPU指行指令缓冲器中的指令EB 03； 指令EB 03执行后，(IP)=000BH，CS:IP指向inc ax jmp near ptr 标号 （段内近转移）\n指令“jmp near ptr 标号”的功能为：(IP) = (IP) + 16位位移。\n2、转移的目的地址在指令中的jmp指令 jmp far ptr 标号（段间转移或远转移）\n指令 “jmp far ptr 标号” 功能如下：\n(CS) = 标号所在段的段地址； (IP) = 标号所在段中的偏移地址。 far ptr指明了指令用标号的段地址和偏移地址修改CS和IP。 1 2 3 4 5 6 7 8 9 10 11 assume cs:codesg codesg segment start: mov ax, 0 mov bx, 0 jmp far ptr s ;s被翻译成转移的目的地址0B01 BD0B db 256 dup (0) ;转移的段地址：0BBDH，偏移地址：010BH s: add ax,1 inc ax codesg ends end start 12345678910 3、转移地址在寄存器或内存中的jmp指令 1 jmp 16位寄存器 功能：IP =（16位寄存器）\n转移地址在内存中的jmp指令有两种格式：\njmp word ptr 内存单元地址（段内转移） 功能：从内存单元地址处开始存放着一个字，是转移的目的偏移地址。\n1 2 3 4 mov ax, 0123H mov ds:[0], ax jmp word ptr ds:[0] ;执行后，(IP)=0123H jmp dword ptr 内存单元地址（段间转移） 功能：从内存单元地址处开始存放着两个字，高地址处的字是转移的目的段地址，低地址处是转移的目的偏移地址。\n(CS)=(内存单元地址+2) (IP)=(内存单元地址) 1 2 3 4 5 6 7 8 mov ax, 0123H mov ds:[0], ax;偏移地址 mov word ptr ds:[2], 0;段地址 jmp dword ptr ds:[0] ;执行后， ;(CS)=0 ;(IP)=0123H ;CS:IP 指向 0000:0123。 4、jcxz指令和loop指令 jcxz指令\njcxz指令为有条件转移指令，所有的有条件转移指令都是短转移，\n在对应的机器码中包含转移的位移，而不是目的地址。对IP的修改范围都为-128~127。\n指令格式：jcxz 标号（如果(cx)=0，则转移到标号处执行。）\n当(cx) = 0时，(IP) = (IP) + 8位位移\n8位位移 = “标号”处的地址 - jcxz指令后的第一个字节的地址； 8位位移的范围为-128~127，用补码表示； 8位位移由编译程序在编译时算出。 当(cx)!=0时，什么也不做（程序向下执行）\nloop指令\nloop指令为循环指令，所有的循环指令都是短转移，在对应的机器码中包含转移的位移，而不是目的地址。\n对IP的修改范围都为-128~127。\n指令格式：loop 标号 ((cx) = (cx) - 1，如果(cx) ≠ 0，转移到标号处执行)。\n(cx) = (cx) - 1；如果 (cx) != 0，(IP) = (IP) + 8位位移。\n8位位移 = 标号处的地址 - loop指令后的第一个字节的地址； 8位位移的范围为-128~127，用补码表示； 8位位移由编译程序在编译时算出。 如果（cx）= 0，什么也不做（程序向下执行）。\n3、总结 转移指令时，都是以下一个指令的偏移量为依据，如果是下一个指令的目标地址，那么这段程序就不能随意放到某个地址了。使用偏移量就可以很好解决这个问题：无论这段代码放到哪里都可以通过位移来找寻指令 归根结底，指令其实就是cs:ip指向地址的内容，那么我们要转移指令，就是修改cs:ip的指向地址 实现循环：每次循环体结束都会返回到循环代码的头部，那么如何实现呢？答案就是：转移指令，每当循环执行完毕时，我们就将指令转移到循环体头部； 实现if-else：类似循环，不过使用的是条件转移指令 ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/","tags":["汇编的转移指令"],"title":"汇编之转移指令原理"},{"categories":["分布式系统"],"contents":"分布式锁 以下文章来自转载，并做了一些必要的补充和改进\n什么是锁？ 在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。 而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。 不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。 除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。 什么是分布式？ 分布式的 CAP 理论告诉我们:\n任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。\n目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。\n分布式场景 此处主要指集群模式下，多个相同服务同时开启.\n在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。\n分布式与单机情况下最大的不同在于其不是多线程而是多进程。 多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。 什么是分布式锁？ 当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑） 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。 我们需要怎样的分布式锁？ 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条） 这把锁最好是一把公平锁（根据业务需求考虑要不要这条） 有高可用的获取锁和释放锁功能 获取锁和释放锁的性能要好 基于数据库做分布式锁 基于乐观锁\n基于表主键唯一做分布式锁 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。\n上面这种简单的实现有以下几个问题：\n这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。 在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。 当然，我们也可以有其他方式解决上面的问题。\n数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。 非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。 比较好的办法是在程序中生产主键进行防重。 基于表字段版本号做分布式锁 这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。\n基于悲观锁\n基于数据库排他锁做分布式锁 在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。\n我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。\n这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。\n阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。 但是还是无法直接解决数据库单点和可重入问题。\n这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。\n还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。\n优缺点 优点：简单，易于理解\n缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱）\n基于 Redis 做分布式锁 基于 redis 的 setnx()、expire() 方法做分布式锁 setnx() setnx 的含义就是 SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0。\nexpire() expire 设置过期时间，要注意的是 setnx 命令不能设置 key 的超时时间，只能通过 expire() 来对 key 设置。\n使用步骤 1、setnx(lockkey, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功\n2、expire() 命令对 lockkey 设置超时时间，为的是避免死锁问题。\n3、然后假设多个线程来抢锁，抢锁的时候附带自己的线程标识，然后将锁和线程标识进行绑定，然后就执行自己的业务；\n4、执行成功之后，就释放锁。释放锁时，为保证释放的锁确实是自己的锁，需要先判断锁是否是自己这个线程的，如果是那么就释放锁；如果不是就不能释放锁。当然应该保证这系列操作的原子性，考虑使用redis+lua来实现\n缺点 这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。例如，可能当redis刚好执行到setnx指令就宕机了，expire指令来不及执行，这就可能导致这个锁没有超时，就可能会存在死锁的危险。\n当然，对业务数据的一致性要求不高，允许丢失一些数据，这个方案也是可以接受的\n针对缺点的解决方案 解决方案一：不信任redis的超时时间，将锁超时下放到程序层面来做，也就是说我们只在redis原子地执行一个命令setnx，也就是下面的方案，当然也引出了解决方案二；\n解决方案二：将setnx和expire这两个操作包裹成一个原子性操作 =\u0026gt; 较优的一种方案：redis+lua实现\n虽然setnx是原子的，但是setnx+expire并不是原子操作\n所以，一旦涉及redis多条指令顺序执行，最好都是redis+lua来实现业务逻辑\n基于 redis 的 setnx()、get()、getset()方法做分布式锁 这个方案的背景主要是在 setnx() 和 expire() 的方案上针对可能存在的死锁问题，做了一些优化。\ngetset() 这个命令主要有两个参数 getset(key，newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果：\ngetset(key, \u0026ldquo;value1\u0026rdquo;) 返回 null 此时 key 的值会被设置为 value1 getset(key, \u0026ldquo;value2\u0026rdquo;) 返回 value1 此时 key 的值会被设置为 value2 依次类推！ 使用步骤 setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。 get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。 计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。 判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 import cn.com.tpig.cache.redis.RedisService; import cn.com.tpig.utils.SpringUtils; //redis分布式锁 public final class RedisLockUtil { private static final int defaultExpire = 60; private RedisLockUtil() { // } /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock(String key, int expire) { RedisService redisService = SpringUtils.getBean(RedisService.class); long status = redisService.setnx(key, \u0026#34;1\u0026#34;); if(status == 1) { redisService.expire(key, expire); return true; } return false; } public static boolean lock(String key) { return lock2(key, defaultExpire); } /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock2(String key, int expire) { RedisService redisService = SpringUtils.getBean(RedisService.class); long value = System.currentTimeMillis() + expire; long status = redisService.setnx(key, String.valueOf(value)); if(status == 1) { return true; } long oldExpireTime = Long.parseLong(redisService.get(key, \u0026#34;0\u0026#34;)); if(oldExpireTime \u0026lt; System.currentTimeMillis()) { //超时 long newExpireTime = System.currentTimeMillis() + expire; long currentExpireTime = Long.parseLong(redisService.getSet(key, String.valueOf(newExpireTime))); if(currentExpireTime == oldExpireTime) { return true; } } return false; } public static void unLock1(String key) { RedisService redisService = SpringUtils.getBean(RedisService.class); redisService.del(key); } public static void unLock2(String key) { RedisService redisService = SpringUtils.getBean(RedisService.class); long oldExpireTime = Long.parseLong(redisService.get(key, \u0026#34;0\u0026#34;)); if(oldExpireTime \u0026gt; System.currentTimeMillis()) { redisService.del(key); } } } public void drawRedPacket(long userId) { String key = \u0026#34;draw.redpacket.userid:\u0026#34; + userId; boolean lock = RedisLockUtil.lock2(key, 60); if(lock) { try { //领取操作 } finally { //释放锁 RedisLockUtil.unLock(key); } } else { new RuntimeException(\u0026#34;重复领取奖励\u0026#34;); } } 基于 Redlock 做分布式锁 Redlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立的 Redis 节点（通常情况下 N 可以设置成 5）。\n算法的步骤如下：\n客户端获取当前时间，以毫秒为单位。 客户端尝试获取 N 个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N 个节点以相同的 key 和 value 获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是 10s，那么接口超时大概设置 5-50ms。这样可以在有 redis 节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。 客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过 3 个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。 客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。 如果客户端获取锁失败了，客户端会依次删除所有的锁。 使用 Redlock 算法，可以保证在挂掉最多 2 个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于 redis 的高效性能，分布式缓存锁性能并不比数据库锁差。 但是，有一位分布式的专家写了一篇文章《How to do distributed locking》，质疑 Redlock 的正确性。\nhttps://blog.csdn.net/jek123456/article/details/72954106\n优缺点 优点：\n性能高\n缺点：\n失效时间设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。\n基于 redisson 做分布式锁 redisson 是 redis 官方的分布式锁组件。GitHub 地址：https://github.com/redisson/redisson\n上面的这个问题 ——\u0026gt; 失效时间设置多长时间为好？\n这个问题在 redisson 的做法是：每获得一个锁时，只设置一个很短的超时时间，同时起一个线程在每次快要到超时时间时去刷新锁的超时时间。在释放锁的同时结束这个线程。\n基于 ZooKeeper 做分布式锁 zookeeper 锁相关基础知识 zk 一般由多个节点构成（单数），采用 zab 一致性协议。因此可以将 zk 看成一个单点结构，对其修改数据其内部自动将所有节点数据进行修改而后才提供查询服务。 zk 的数据以目录树的形式，每个目录称为 znode， znode 中可存储数据（一般不超过 1M），还可以在其中增加子节点。 子节点有三种类型。序列化节点，每在该节点下增加一个节点自动给该节点的名称上自增。临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除。最后就是普通节点。 Watch 机制，client 可以监控每个节点的变化，当产生变化会给 client 产生一个事件。 zk 基本锁 原理：利用临时节点与 watch 机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch/lock 节点，有删除操作后再去争锁。临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。 缺点：所有取锁失败的进程都监听父节点，很容易发生羊群效应，即当释放锁后所有等待进程一起来创建节点，并发量很大。 zk 锁优化 原理：上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。 步骤： 在 /lock 节点下创建一个有序临时节点 (EPHEMERAL_SEQUENTIAL)。 判断创建的节点序号是否最小，如果是最小则获取锁成功。不是则取锁失败，然后 watch 序号比本身小的前一个节点。 当取锁失败，设置 watch 后则等待 watch 事件到来后，再次判断是否序号最小。 取锁成功则执行代码，最后释放锁（删除该节点）。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 import java.io.IOException; import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.concurrent.CountDownLatch; import java.util.concurrent.TimeUnit; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.KeeperException; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooDefs; import org.apache.zookeeper.ZooKeeper; import org.apache.zookeeper.data.Stat; public class DistributedLock implements Lock, Watcher{ private ZooKeeper zk; private String root = \u0026#34;/locks\u0026#34;;//根 private String lockName;//竞争资源的标志 private String waitNode;//等待前一个锁 private String myZnode;//当前锁 private CountDownLatch latch;//计数器 private int sessionTimeout = 30000; private List\u0026lt;Exception\u0026gt; exception = new ArrayList\u0026lt;Exception\u0026gt;(); /** * 创建分布式锁,使用前请确认config配置的zookeeper服务可用 * @param config 127.0.0.1:2181 * @param lockName 竞争资源标志,lockName中不能包含单词lock */ public DistributedLock(String config, String lockName){ this.lockName = lockName; // 创建一个与服务器的连接 try { zk = new ZooKeeper(config, sessionTimeout, this); Stat stat = zk.exists(root, false); if(stat == null){ // 创建根节点 zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); } } catch (IOException e) { exception.add(e); } catch (KeeperException e) { exception.add(e); } catch (InterruptedException e) { exception.add(e); } } /** * zookeeper节点的监视器 */ public void process(WatchedEvent event) { if(this.latch != null) { this.latch.countDown(); } } public void lock() { if(exception.size() \u0026gt; 0){ throw new LockException(exception.get(0)); } try { if(this.tryLock()){ System.out.println(\u0026#34;Thread \u0026#34; + Thread.currentThread().getId() + \u0026#34; \u0026#34; +myZnode + \u0026#34; get lock true\u0026#34;); return; } else{ waitForLock(waitNode, sessionTimeout);//等待锁 } } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } } public boolean tryLock() { try { String splitStr = \u0026#34;_lock_\u0026#34;; if(lockName.contains(splitStr)) throw new LockException(\u0026#34;lockName can not contains \\\\u000B\u0026#34;); //创建临时子节点 myZnode = zk.create(root + \u0026#34;/\u0026#34; + lockName + splitStr, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(myZnode + \u0026#34; is created \u0026#34;); //取出所有子节点 List\u0026lt;String\u0026gt; subNodes = zk.getChildren(root, false); //取出所有lockName的锁 List\u0026lt;String\u0026gt; lockObjNodes = new ArrayList\u0026lt;String\u0026gt;(); for (String node : subNodes) { String _node = node.split(splitStr)[0]; if(_node.equals(lockName)){ lockObjNodes.add(node); } } Collections.sort(lockObjNodes); System.out.println(myZnode + \u0026#34;==\u0026#34; + lockObjNodes.get(0)); if(myZnode.equals(root+\u0026#34;/\u0026#34;+lockObjNodes.get(0))){ //如果是最小的节点,则表示取得锁 return true; } //如果不是最小的节点，找到比自己小1的节点 String subMyZnode = myZnode.substring(myZnode.lastIndexOf(\u0026#34;/\u0026#34;) + 1); waitNode = lockObjNodes.get(Collections.binarySearch(lockObjNodes, subMyZnode) - 1); } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } return false; } public boolean tryLock(long time, TimeUnit unit) { try { if(this.tryLock()){ return true; } return waitForLock(waitNode,time); } catch (Exception e) { e.printStackTrace(); } return false; } private boolean waitForLock(String lower, long waitTime) throws InterruptedException, KeeperException { Stat stat = zk.exists(root + \u0026#34;/\u0026#34; + lower,true); //判断比自己小一个数的节点是否存在,如果不存在则无需等待锁,同时注册监听 if(stat != null){ System.out.println(\u0026#34;Thread \u0026#34; + Thread.currentThread().getId() + \u0026#34; waiting for \u0026#34; + root + \u0026#34;/\u0026#34; + lower); this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; } return true; } public void unlock() { try { System.out.println(\u0026#34;unlock \u0026#34; + myZnode); zk.delete(myZnode,-1); myZnode = null; zk.close(); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } public void lockInterruptibly() throws InterruptedException { this.lock(); } public Condition newCondition() { return null; } public class LockException extends RuntimeException { private static final long serialVersionUID = 1L; public LockException(String e){ super(e); } public LockException(Exception e){ super(e); } } } 优缺点 优点：\n有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。\n缺点：\n性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。还需要对 ZK的原理有所了解。\n基于 Consul 做分布式锁 DD 写过类似文章，其实主要利用 Consul 的 Key / Value 存储 API 中的 acquire 和 release 操作来实现。\n文章地址：http://blog.didispace.com/spring-cloud-consul-lock-and-semphore/\n使用分布式锁的注意事项 1、注意分布式锁的开销\n2、注意加锁的粒度\n3、加锁的方式\n总结：如何选择合适的锁 业务还在单机就可以搞定的量级时，那么按照需求使用任意的单机锁方案就可以。\n如果发展到了分布式服务阶段，但业务规模不大，qps很小的情况下，使用哪种锁方案都差不多。如果公司内已有可以使用的ZooKeeper、etcd或者Redis集群，那么就尽量在不引入新的技术栈的情况下满足业务需求。\n业务发展到一定量级的话，就需要从多方面来考虑了。首先是你的锁是否在任何恶劣的条件下都不允许数据丢失，如果不允许，那么就不要使用Redis的setnx的简单锁。\n对锁数据的可靠性要求极高的话，那只能使用etcd或者ZooKeeper这种通过一致性协议保证数据可靠性的锁方案。但可靠的背面往往都是较低的吞吐量和较高的延迟。需要根据业务的量级对其进行压力测试，以确保分布式锁所使用的etcd或ZooKeeper集群可以承受得住实际的业务请求压力。需要注意的是，etcd和Zookeeper集群是没有办法通过增加节点来提高其性能的。要对其进行横向扩展，只能增加搭建多个集群来支持更多的请求。这会进一步提高对运维和监控的要求。多个集群可能需要引入proxy，没有proxy那就需要业务去根据某个业务id来做分片。如果业务已经上线的情况下做扩展，还要考虑数据的动态迁移。这些都不是容易的事情。\n在选择具体的方案时，还是需要多加思考，对风险早做预估。\n","permalink":"https://cold-bin.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","tags":["实现redlock"],"title":"分布式锁"},{"categories":["汇编"],"contents":"div指令、dd、dup、mul指令 div是除法指令\n除数：有8位和16位两种，在一个寄存器或内存单元中。 被除数：默认放在AX或DX和AX中， 如果除数为8位，被除数则为16位，默认在AX中存放； 如果除数为16位，被除数则为32位，在DX和AX中存放，DX存放高16位，AX存放低16位。 结果： 如果除数为8位，则AL存储除法操作的商，AH存储除法操作的余数； 如果除数为16位，则AX存储除法操作的商，DX存储除法操作的余数。 1 2 3 4 5 6 7 8 9 10 11 ;利用除法指令计算100001/100。 ;100001D = 186A1H mov dx, 1 mov ax, 86A1H ;(dx)*10000H+(ax)=100001 mov bx, 100 div bx ;利用除法指令计算1001/100 mov ax, 1001 mov bl, 100 div b1 伪指令dd\ndb和dw定义字节型数据和字型数据。\ndd是用来定义dword（double word，双字）型数据的伪指令\n操作符dup\ndup在汇编语言中同db、dw、dd等一样，也是由编译器识别处理的符号。 它和db、dw、dd等数据定义伪指令配合使用，用来进行数据的重复\n1 2 3 db 3 dup (0) ;定义了3个字节，它们的值都是0，相当于db 0，0，0。 db 3 dup (0, 1, 2) ;定义了9个字节，它们是0、1、2、0、1、2、0、1、2，相当于db 0，1，2，0，1，2，0，1，2。 db 3 dup (\u0026#39;abc\u0026#39;, \u0026#39;ABC\u0026#39;) ;定义了18个字节，它们是abcABCabcABCabcABCC，相当于db \u0026#39;abc\u0026#39;, \u0026#39;ABC\u0026#39; ,\u0026#39;abc\u0026#39; , \u0026#39;ABC, \u0026#39;abc\u0026#39;, \u0026#39;ABC\u0026#39;。 mul 指令\nmul是乘法指令，使用 mul 做乘法的时候：相乘的两个数：要么都是8位，要么都是16位。\n8 位： AL中和 8位寄存器或内存字节单元中； 16 位： AX中和 16 位寄存器或内存字单元中。 结果\n8位：AX中； 16位：DX（高位）和 AX（低位）中。 格式：mul 寄存器 或 mul 内存单元\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ;计算100*10 ;100和10小于255，可以做8位乘法 mov al,100 mov bl,10 mul bl ;结果： (ax)=1000（03E8H） 12345678 ;计算100*10000 ;100小于255，可10000大于255，所以必须做16位乘法，程序如下： mov ax,100 mov bx,10000 mul bx ;结果： (ax)=4240H，(dx)=000FH （F4240H=1000000） ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/","tags":["汇编运算指令"],"title":"汇编之数据处理问题"},{"categories":["linux","shell"],"contents":"Linux 概述 什么是Linux？ Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 Unix 的多用户、多任务、支持多线程和多CPU的操作系统。\n它能运行主要的 Unix工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。\nUnix和Linux有什么区别？ Linux 和 Unix 都是功能强大的操作系统，都是应用广泛的服务器操作系统，有很多相似之处，甚至有一部分人错误地认为 Unix 和 Linux 操作系统是一样的，然而，事实并非如此，以下是两者的区别。\n开源性 Linux 是一款开源操作系统，不需要付费，即可使用；Unix 是一款对源码实行知识产权保护的传统商业软件，使用需要付费授权使用。\n跨平台性 Linux 操作系统具有良好的跨平台性能，可运行在多种硬件平台上；Unix 操作系统跨平台性能较弱，大多需与硬件配套使用。\n可视化界面 Linux除了进行命令行操作，还有窗体管理系统；Unix只是命令行下的系统。\n硬件环境 Linux 操作系统对硬件的要求较低，安装方法更易掌握；Unix 对硬件要求比较苛刻，按照难度较大。\n用户群体 Linux 的用户群体很广泛，个人和企业均可使用；Unix 的用户群体比较窄，多是安全性要求高的大型企业使用，如银行、电信部门等，或者 Unix 硬件厂商使用，如 Sun 等。\n相比于 Unix 操作系统，Linux 操作系统更受广大计算机爱好者的喜爱，主要原因是 Linux操作系统具有 Unix 操作系统的全部功能，并且能够在普通 PC 计算机上实现全部的 Unix 特性，开源免费的特性，更容易普及使用！\n什么是 Linux 内核？ Linux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根据需要执行软件。\n系统内存管理 应用程序管理 硬件设备管理 文件系统管理\nLinux的基本组件是什么？ 就像任何其他典型的操作系统一样，Linux 拥有所有这些组件：\n内核，shell 和 GUI，系统实用程序和应用程序。Linux 比其他操作系统更具优势的是每个方面都附带其他功能，所有代码都可以免费下载。\nLinux 的体系结构 从大的方面讲，Linux 体系结构可以分为两块：\n用户空间(User Space)：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 内核空间(Kernel Space)：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、平台架构相关的代码(Architecture-Dependent Kernel Code) 为什么 Linux 体系结构要分为用户空间和内核空间的原因？ 1、现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。\n2、Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。\n用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空间的转移：1）系统调用；2）硬件中断。\nBASH和DOS之间的基本区别是什么？ BASH和DOS控制台之间的主要区别在于3个方面：\nBASH命令区分大小写，而DOS命令则不区分; 在BASH下，/ character是目录分隔符，\\作为转义字符。在DOS下，/用作命令参数分隔符，\\是目录分隔符 DOS遵循命名文件中的约定，即8个字符的文件名后跟一个点，扩展名为3个字符。BASH没有遵循这样的惯例。 Linux 开机启动过程？ 了解即可。\n主机加电自检，加载 BIOS 硬件信息。 读取 MBR 的引导文件(GRUB、LILO)。 引导 Linux 内核。 运行第一个进程 init (进程号永远为 1 )。 进入相应的运行级别。 运行终端，输入用户名和密码。 Linux系统缺省的运行级别？ 关机。 单机用户模式。 字符界面的多用户模式(不支持网络)。 字符界面的多用户模式。 未分配使用。 图形界面的多用户模式。 重启。 Linux 使用的进程间通信方式？ 了解即可，不需要太深入。\n管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 信号(signal) 。 消息队列。 共享内存。 信号量。 套接字(socket) 。 Linux 有哪些系统日志文件？ 比较重要的是 /var/log/messages 日志文件。\n该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。\n另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。\nLinux 系统安装多个桌面环境有帮助吗？ 通常，一个桌面环境，如 KDE 或 Gnome，足以在没有问题的情况下运行。尽管系统允许从一个环境切换到另一个环境，但这对用户来说都是优先考虑的问题。\n有些程序在一个环境中工作而在另一个环境中无法工作，因此它也可以被视为选择使用哪个环境的一个因素。\n什么是交换空间？ 交换空间是Linux使用的一定空间，用于临时保存一些并发运行的程序。当RAM没有足够的内存来容纳正在执行的所有程序时，就会发生这种情况。\n什么是root帐户? root 帐户就像一个系统管理员帐户，允许你完全控制系统。你可以在此处创建和维护用户帐户，为每个帐户分配不同的权限。每次安装 Linux 时都是默认帐户。\n什么是LILO？ LILO 是 Linux 的引导加载程序。它主要用于将 Linux 操作系统加载到主内存中，以便它可以开始运行。\n什么是BASH？ BASH 是 Bourne Again SHell 的缩写。它由 Steve Bourne 编写，作为原始 Bourne Shell（由/bin /sh表示）的替代品。它结合了原始版本的 Bourne Shell 的所有功能，以及其他功能，使其更容易使用。从那以后，它已被改编为运行 Linux 的大多数系统的默认 shell。\n什么是CLI？ 命令行界面（英语：command-line interface，缩写：CLI）是在图形用户界面得到普及之前使用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。也有人称之为字符用户界面（CUI）。\n通常认为，命令行界面（CLI）没有图形用户界面（GUI）那么方便用户操作。因为，命令行界面的软件通常需要用户记忆操作的命令，但是，由于其本身的特点，命令行界面要较图形用户界面节约计算机系统的资源。在熟记命令的前提下，使用命令行界面往往要较使用图形用户界面的操作速度要快。所以，图形用户界面的操作系统中，都保留着可选的命令行界面。\n什么是GUI？ 图形用户界面（Graphical User Interface，简称 GUI，又称图形用户接口）是指采用图形方式显示的计算机操作用户界面。\n图形用户界面是一种人与计算机通信的界面显示格式，允许用户使用鼠标等输入设备操纵屏幕上的图标或菜单选项，以选择命令、调用文件、启动程序或执行其它一些日常任务。与通过键盘输入文本或字符命令来完成例行任务的字符界面相比，图形用户界面有许多优点。\n开源的优势是什么？开源允许你将软件（包括源代码）免费分发给任何感兴趣的人。然后，人们可以添加功能，甚至可以调试和更正源代码中的错误。它们甚至可以让它运行得更好，然后再次自由地重新分配这些增强的源代码。这最终使社区中的每个人受益。\nGNU项目的重要性是什么？这种所谓的自由软件运动具有多种优势，例如可以自由地运行程序以及根据你的需要自由学习和修改程序。它还允许你将软件副本重新分发给其他人，以及自由改进软件并将其发布给公众。\n磁盘、目录、文件 简单 Linux 文件系统？ 在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。\n也就是说在 Linux 系统中有一个重要的概念一切都是文件。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。\nLinux 支持 5 种文件类型，如下图所示：\nLinux 的目录结构是怎样的？ 这个问题，一般不会问。更多是实际使用时，需要知道。\nLinux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录：\n常见目录说明：\n/bin：存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里； /etc：存放系统管理和配置文件； /home：存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； /usr：用于存放系统应用程序； /opt：额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc：虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root：超级用户（系统管理员）的主目录（特权阶级o）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev：用于存放设备文件； /mnt：系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot：存放用于系统引导时使用的各种文件； /lib：存放着和系统运行相关的库文件 ； /tmp：用于存放各种临时文件，是公用的临时文件存储点； /var：用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found：这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 什么是 inode ？ 一般来说，面试不会问 inode 。但是 inode 是一个重要概念，是理解 Unix/Linux 文件系统和硬盘储存的基础。\n理解inode，要从文件储存说起。\n文件储存在硬盘上，硬盘的最小存储单位叫做\u0026quot;扇区\u0026quot;（Sector）。每个扇区储存512字节（相当于0.5KB）。\n操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个\u0026quot;块\u0026quot;（block）。这种由多个扇区组成的\u0026quot;块\u0026quot;，是文件存取的最小单位。\u0026ldquo;块\u0026quot;的大小，最常见的是4KB，即连续八个 sector组成一个 block。\n文件数据都储存在\u0026quot;块\u0026quot;中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为\u0026quot;索引节点\u0026rdquo;。\n每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。\n简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程？ 如果看的一脸懵逼，也没关系。一般来说，面试官不太会问这个题目。\nLinux 通过 inode 节点表将文件的逻辑结构和物理结构进行转换。\ninode 节点是一个 64 字节长的表，表中包含了文件的相关信息，其中有文件的大小、文件所有者、文件的存取许可方式以及文件的类型等重要信息。在 inode 节点表中最重要的内容是磁盘地址表。在磁盘地址表中有 13 个块号，文件将以块号在磁盘地址表中出现的顺序依次读取相应的块。\nLinux 文件系统通过把 inode 节点和文件名进行连接，当需要读取该文件时，文件系统在当前目录表中查找该文件名对应的项，由此得到该文件相对应的 inode 节点号，通过该 inode 节点的磁盘地址表把分散存放的文件物理块连接成文件的逻辑结构。\n什么是硬链接和软链接？ 1）硬链接 由于 Linux 下的文件是通过索引节点(inode)来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接数就加 1 。\n不足：1）不可以在不同文件系统的文件间建立链接；2）只有超级用户才可以为目录创建硬链接。\n2）软链接 软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件进行链接。\n不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。实际场景下，基本是使用软链接。总结区别如下：\n硬链接不可以跨分区，软件链可以跨分区。硬链接指向一个 inode 节点，而软链接则是创建一个新的 inode 节点。删除硬链接文件，不会删除原文件，删除软链接文件，会把原文件删除。\nRAID 是什么? RAID 全称为独立磁盘冗余阵列(Redundant Array of Independent Disks)，基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大的硬盘。RAID 通常被用在服务器电脑上，使用完全相同的硬盘组成一个逻辑扇区，因此操作系统只会把它当做一个硬盘。\nRAID 分为不同的等级，各个不同的等级均在数据可靠性及读写性能上做了不同的权衡。在实际应用中，可以依据自己的实际需求选择不同的 RAID 方案。\n当然，因为很多公司都使用云服务，大家很难接触到 RAID 这个概念，更多的可能是普通云盘、SSD 云盘酱紫的概念。\n安全 一台 Linux 系统初始化环境后需要做一些什么安全工作？\n1、添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号。\n修改 SSH 端口不一定绝对哈。当然，如果要暴露在外网，建议改下。l\n2、服务器使用密钥登陆，禁止密码登陆。\n3、开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。\n4、装 fail2ban 这种防止 SSH 暴力破击的软件。\n5、设置只允许公司办公网出口 IP 能登陆服务器(看公司实际需要)\n也可以安装 VPN 等软件，只允许连接 VPN 到服务器上。\n6、修改历史命令记录的条数为 10 条。\n7、只允许有需要的服务器可以访问外网，其它全部禁止。\n8、做好软件层面的防护。\n8.1 设置 nginx_waf 模块防止 SQL 注入。\n8.2 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。\n什么叫 CC 攻击？什么叫 DDOS 攻击？ CC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资源消耗殆尽。\nDDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击平台，来对一个或多个目标发动 DDOS 攻击。\n攻击，即是通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。\n怎么预防 CC 攻击和 DDOS 攻击？ 防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。\n流量清洗这一块，主要是买 ISP 服务商的防攻击的服务就可以，机房一般有空余流量，我们一般是买服务，毕竟攻击不会是持续长时间。\n什么是网站数据库注入？ 由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法性进行判断。 应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想得知的数据，这就是所谓的 SQL 注入。 SQL注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。 如何过滤与预防？ 数据库网页端注入这种，可以考虑使用 nginx_waf 做过滤与预防。\nShell 本小节为选读。我也不太会写 Shell 脚本，都是写的时候，在网络上拼拼凑凑。。。\nShell 脚本是什么？ 一个 Shell 脚本是一个文本文件，包含一个或多个命令。作为系统管理员，我们经常需要使用多个命令来完成一项任务，我们可以添加这些所有命令在一个文本文件(Shell 脚本)来完成这些日常工作任务。\n什么是默认登录 Shell ？ 在 Linux 操作系统，\u0026quot;/bin/bash\u0026quot; 是默认登录 Shell，是在创建用户时分配的。\n使用 chsh 命令可以改变默认的 Shell 。示例如下所示：\n1 2 ## chsh \u0026lt;用户名\u0026gt; -s \u0026lt;新shell\u0026gt; ## chsh ThinkWon -s /bin/sh 在 Shell 脚本中，如何写入注释？ 注释可以用来描述一个脚本可以做什么和它是如何工作的。每一行注释以 # 开头。例子如下：\n1 2 3 #!/bin/bash ## This is a command echo “I am logged in as $USER” 语法级 可以在 Shell 脚本中使用哪些类型的变量？ 在 Shell 脚本，我们可以使用两种类型的变量：\n系统定义变量 系统变量是由系统系统自己创建的。这些变量通常由大写字母组成，可以通过 set 命令查看。\n用户定义变量 用户变量由系统用户来生成和定义，变量的值可以通过命令 \u0026ldquo;echo $\u0026lt;变量名\u0026gt;\u0026rdquo; 查看。\nShell脚本中 $? 标记的用途是什么？ 在写一个 Shell 脚本时，如果你想要检查前一命令是否执行成功，在 if 条件中使用 $? 可以来检查前一命令的结束状态。\n如果结束状态是 0 ，说明前一个命令执行成功。例如：如果结束状态不是0，说明命令执行失败。例如：\nBourne Shell(bash) 中有哪些特殊的变量？ 下面的表列出了 Bourne Shell 为命令行设置的特殊变量。\n1 2 3 4 5 6 7 8 内建变量 解释 $0 命令行中的脚本名字 $1 第一个命令行参数 $2 第二个命令行参数 ….. ……. $9 第九个命令行参数 $## 命令行参数的数量 $* 所有命令行参数，以空格隔开 如何取消变量或取消变量赋值？ unset 命令用于取消变量或取消变量赋值。语法如下所示：\n1 ## unset \u0026lt;变量名\u0026gt; Shell 脚本中 if 语法如何嵌套? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 if [ 条件 ] then 命令1 命令2 ….. else if [ 条件 ] then 命令1 命令2 …. else 命令1 命令2 ….. fi fi 在 Shell 脚本中如何比较两个数字？ 在 if-then 中使用测试命令（ -gt 等）来比较两个数字。例如：\n1 2 3 4 5 6 7 8 9 #!/bin/bash x=10 y=20 if [ $x -gt $y ] then echo “x is greater than y” else echo “y is greater than x” fi Shell 脚本中 case 语句的语法? 基础语法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 case 变量 in 值1) 命令1 命令2 ….. 最后命令 !! 值2) 命令1 命令2 …… 最后命令 ;; esac Shell 脚本中 for 循环语法？ 基础语法如下：\n1 2 3 4 5 6 7 for 变量 in 循环列表 do 命令1 命令2 …. 最后命令 done Shell 脚本中 while 循环语法？ 如同 for 循环，while 循环只要条件成立就重复它的命令块。不同于 for 循环，while 循环会不断迭代，直到它的条件不为真。\n基础语法：\n1 2 3 4 while [ 条件 ] do 命令… done do-while 语句的基本格式？ do-while 语句类似于 while 语句，但检查条件语句之前先执行命令（LCTT 译注：意即至少执行一次。）。下面是用 do-while 语句的语法：\n1 2 3 4 do { 命令 } while (条件) Shell 脚本中 break 命令的作用？ break 命令一个简单的用途是退出执行中的循环。我们可以在 while 和 until 循环中使用 break 命令跳出循环。\nShell 脚本中 continue 命令的作用？ continue 命令不同于 break 命令，它只跳出当前循环的迭代，而不是整个循环。continue 命令很多时候是很有用的，例如错误发生，但我们依然希望继续执行大循环的时候。\n如何使脚本可执行? 使用 chmod 命令来使脚本可执行。例子如下：chmod a+x myscript.sh 。\n#!/bin/bash 的作用？ #!/bin/bash 是 Shell 脚本的第一行，称为释伴（shebang）行。\n这里 # 符号叫做 hash ，而 ! 叫做 bang。它的意思是命令通过 /bin/bash 来执行。\n如何调试 Shell脚本？ 使用 -x 数（sh -x myscript.sh）可以调试 Shell脚本。 另一个种方法是使用 -nv 参数(sh -nv myscript.sh)。 如何将标准输出和错误输出同时重定向到同一位置? 方法一：2\u0026gt;\u0026amp;1 (如## ls /usr/share/doc \u0026gt; out.txt 2\u0026gt;\u0026amp;1 ) 。 方法二：\u0026amp;\u0026gt; (如## ls /usr/share/doc \u0026amp;\u0026gt; out.txt ) 。 在 Shell 脚本中，如何测试文件？ test 命令可以用来测试文件。基础用法如下表格：\n1 2 3 4 5 6 7 8 Test 用法 -d 文件名 如果文件存在并且是目录，返回true -e 文件名 如果文件存在，返回true -f 文件名 如果文件存在并且是普通文件，返回true -r 文件名 如果文件存在并可读，返回true -s 文件名 如果文件存在并且不为空，返回true -w 文件名 如果文件存在并可写，返回true -x 文件名 如果文件存在并可执行，返回true 在 Shell 脚本如何定义函数呢？ 函数是拥有名字的代码块。当我们定义代码块，我们就可以在我们的脚本调用函数名字，该块就会被执行。示例如下所示：\n1 2 3 4 5 6 7 $ diskusage () { df -h ; } 译注：下面是我给的shell函数语法，原文没有 [ function ] 函数名 [()] { 命令; [return int;] } 如何让 Shell 就脚本得到来自终端的输入? read 命令可以读取来自终端（使用键盘）的数据。read 命令得到用户的输入并置于你给出的变量中。例子如下：\n1 2 3 4 5 6 7 8 9 ## vi /tmp/test.sh #!/bin/bash echo ‘Please enter your name’ read name echo “My Name is $name” ## ./test.sh Please enter your name ThinkWon My Name is ThinkWon 如何执行算术运算？ 有两种方法来执行算术运算：\n使用 expr 命令：## expr 5 + 2 。 用一个美元符号和方括号（表达式）：[16 + 4] ; test=$[16 + 4] 。 编程题 1.判断一文件是不是字符设备文件，如果是将其拷贝到 /dev 目录下？\n1 2 3 4 5 #!/bin/bash read -p \u0026#34;Input file name: \u0026#34; FILENAME if [ -c \u0026#34;$FILENAME\u0026#34; ];then cp $FILENAME /dev fi 2.添加一个新组为 class1 ，然后添加属于这个组的 30 个用户，用户名的形式为 stdxx ，其中 xx 从 01 到 30 ？\n1 2 3 4 5 6 7 8 9 10 #!/bin/bash groupadd class1 for((i=1;i\u0026lt;31;i++)) do if [ $i -le 10 ];then useradd -g class1 std0$i else useradd -g class1 std$i fi done 3.编写 Shell 程序，实现自动删除 50 个账号的功能，账号名为stud1 至 stud50 ？\n1 2 3 4 5 #!/bin/bash for((i=1;i\u0026lt;51;i++)) do userdel -r stud$i done 4.写一个 sed 命令，修改 /tmp/input.txt 文件的内容？要求：\n删除所有空行。 一行中，如果包含 “11111”，则在 “11111” 前面插入 “AAA”，在 “11111” 后面插入 “BBB” 。比如：将内容为 0000111112222 的一行改为 0000AAA11111BBB2222 。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [root@~]## cat -n /tmp/input.txt 1 000011111222 2 3 000011111222222 4 11111000000222 5 6 7 111111111111122222222222 8 2211111111 9 112222222 10 1122 11 ## 删除所有空行命令 [root@~]## sed \u0026#39;/^$/d\u0026#39; /tmp/input.txt 000011111222 000011111222222 11111000000222 111111111111122222222222 2211111111 112222222 1122 ## 插入指定的字符 [root@~]## sed \u0026#39;s#\\(11111\\)#AAA\\1BBB#g\u0026#39; /tmp/input.txt 0000AAA11111BBB222 0000AAA11111BBB222222 AAA11111BBB000000222 AAA11111BBBAAA11111BBB11122222222222 22AAA11111BBB111 112222222 1122 实战 如何选择 Linux 操作系统版本? 一般来讲，桌面用户首选 Ubuntu ；服务器首选 RHEL 或 CentOS ，两者中首选 CentOS 。\n根据具体要求：\n安全性要求较高，则选择 Debian 或者 FreeBSD 。 需要使用数据库高级服务和电子邮件网络应用的用户可以选择 SUSE 。 想要新技术新功能可以选择 Feddora ，Feddora 是 RHEL 和 CentOS 的一个测试版和预发布版本。 【重点】根据现有状况，绝大多数互联网公司选择 CentOS 。现在比较常用的是 6 系列，现在市场占有大概一半左右。另外的原因是 CentOS 更侧重服务器领域，并且无版权约束。 CentOS 7 系列，也慢慢使用的会比较多了。\n如何规划一台 Linux 主机，步骤是怎样？ 确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。 不同的用途，机器的配置会有所不同。\n确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。 需要优化系统的哪些参数，需要创建哪些用户等等的。 请问当用户反馈网站访问慢，你会如何处理？ 有哪些方面的因素会导致网站网站访问慢？ 服务器出口带宽不够用 本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口带宽就小，访问速度自然就会慢。 跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是长城宽带或联通，这也可能导致带宽的缩减。 服务器负载过大，导致响应不过来 可以从两个方面入手分析：\n分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。 如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访问等。 数据库瓶颈 如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。然后，也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。\n网站开发代码没有优化好 例如 SQL 语句没有优化，导致数据库读写相当耗时。\n针对网站访问慢，怎么去排查？ 首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。 不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。\n如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。 针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。 如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。 怎么去解决？ 如果是出口带宽问题，那么久申请加大出口带宽。 如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 申请购买 CDN 服务，加载用户的访问。 如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。 Linux 性能调优都有哪几种方法？ Disabling daemons (关闭 daemons)。 Shutting down the GUI (关闭 GUI)。 Changing kernel parameters (改变内核参数)。 Kernel parameters (内核参数)。 Tuning the processor subsystem (处理器子系统调优)。 Tuning the memory subsystem (内存子系统调优)。 Tuning the file system (文件系统子系统调优)。 Tuning the network subsystem（网络子系统调优)。 环境变量的设置与修改 变量 要解释环境变量，得先明白变量是什么，准确的说应该是 Shell 变量，所谓变量就是计算机中用于记录一个值（不一定是数值，也可以是字符或字符串）的符号，而这些符号将用于不同的运算处理中。通常变量与值是一对一的关系，可以通过表达式读取它的值并赋值给其它变量，也可以直接指定数值赋值给任意变量。为了便于运算和处理，大部分的编程语言会区分变量的类型，用于分别记录数值、字符或者字符串等等数据类型。Shell 中的变量也基本如此，有不同类型（但不用专门指定类型名），可以参与运算，有作用域限定。\n变量的作用域即变量的有效范围（比如一个函数中、一个源文件中或者全局范围），在该范围内只能有一个同名变量。一旦离开则该变量无效，如同不存在这个变量一般。\n在 Shell 中如何创建一个变量，如何给变量赋值和如何读取变量的值呢？这部分内容会在 高级 bash 脚本编程指南 这门课中详细介绍，这里我简单举例说明一下：\n使用 declare 命令创建一个变量名为 tmp 的变量：\n1 declare tmp 其实也可以不用 declare 预声明一个变量，直接即用即创建，这里只是告诉你 declare 的作用，这在创建其它指定类型的变量（如数组）时会用到。\n使用 = 号赋值运算符，将变量 tmp 赋值为 shiyanlou。注意，与其他语言不同的是， Shell 中的赋值操作，= 两边不可以输入空格，否则会报错。\n1 2 3 4 5 # 正确的赋值 tmp=shiyanlou # 错误的赋值 tmp = shiyanlou 读取变量的值，使用 echo 命令和 $ 符号（$ 符号用于表示引用一个变量的值，初学者经常忘记输入）：\n1 echo $tmp 注意：并不是任何形式的变量名都是可用的，变量名只能是英文字母、数字或者下划线，且不能以数字作为开头。\n环境变量 简单理解了变量的概念，就很容易理解环境变量了。环境变量的作用域比自定义变量的要大，如 Shell 的环境变量作用于自身和它的子进程。在所有的 UNIX 和类 UNIX 系统中，每个进程都有其各自的环境变量设置，且默认情况下，当一个进程被创建时，除了创建过程中明确指定的话，它将继承其父进程的绝大部分环境设置。Shell 程序也作为一个进程运行在操作系统之上，而我们在 Shell 中运行的大部分命令都将以 Shell 的子进程的方式运行。\n通常我们会涉及到的变量类型有三种：\n当前 Shell 进程私有用户自定义变量，如上面我们创建的 tmp 变量，只在当前 Shell 中有效。 Shell 本身内建的变量。 从自定义变量导出的环境变量。 也有三个与上述三种环境变量相关的命令：set，env，export。这三个命令很相似，都是用于打印环境变量信息，区别在于涉及的变量范围不同。详见下表：\n命 令 说 明 set 显示当前 Shell 所有变量，包括其内建环境变量（与 Shell 外观等相关），用户自定义变量及导出的环境变量。 env 显示与当前用户相关的环境变量，还可以让命令在指定环境中运行。 export 显示从 Shell 中导出成环境变量的变量，也能通过它将自定义变量导出为环境变量。 你可以更直观的使用 vimdiff 工具比较一下它们之间的差别：\n1 2 3 4 5 temp=shiyanlou export temp_env=shiyanlou env|sort\u0026gt;env.txt export|sort\u0026gt;export.txt set|sort\u0026gt;set.txt 上述操作将命令输出通过管道 | 使用 sort 命令排序，再重定向到对象文本文件中。管道的概念后面我们会学到，现在你知道这是什么意思就行了。\n1 vimdiff env.txt export.txt set.txt 使用 vimdiff 工具比较导出的几个文件的内容，退出 vimdiff 需要按下 Esc 后输入 :q 即可退出。\n关于哪些变量是环境变量，可以简单地理解成在当前进程的子进程有效则为环境变量，否则不是（有些人也将所有变量统称为环境变量，只是以全局环境变量和局部环境变量进行区分，我们只要理解它们的实质区别即可）。我们这里用 export 命令来体会一下，先在 Shell 中设置一个变量 temp=shiyanlou，然后再新创建一个子 Shell 查看 temp 变量的值：\n注意：为了与普通变量区分，通常我们习惯将环境变量名设为大写。\n环境变量永久生效方案 但是问题来了，当你关机后，或者关闭当前的 shell 之后，环境变量就没了啊。怎么才能让环境变量永久生效呢？\n按变量的生存周期来划分，Linux 变量可分为两类：\n永久的：需要修改配置文件，变量永久生效； 临时的：使用 export 命令行声明即可，变量在关闭 shell 时失效。 这里介绍两个重要文件 /etc/bashrc（有的 Linux 没有这个文件） 和 /etc/profile ，它们分别存放的是 shell 变量和环境变量。还有要注意区别的是每个用户目录下的一个隐藏文件：\n1 2 3 # .profile 可以用 ls -a 查看 cd /home/shiyanlou ls -a 这个 .profile 只对当前用户永久生效。因为它保存在当前用户的 Home 目录下，当切换用户时，工作目录可能一并被切换到对应的目录中，这个文件就无法生效。而写在 /etc/profile 里面的是对所有用户永久生效，所以如果想要添加一个永久生效的环境变量，只需要打开 /etc/profile，在最后加上你想添加的环境变量就好啦。\n在前面我们应该注意到 PATH 里面的路径是以 : 作为分割符的，所以我们可以这样添加自定义路径：\n1 PATH=$PATH:/home/shiyanlou/mybin 注意这里一定要使用绝对路径。\n现在你就可以在任意目录执行那两个命令了（注意需要去掉前面的 ./）。你可能会意识到这样还并没有很好的解决问题，因为我给 PATH 环境变量追加了一个路径，它也只是在当前 Shell 有效，我一旦退出终端，再打开就会发现又失效了。有没有方法让添加的环境变量全局有效？或者每次启动 Shell 时自动执行上面添加自定义路径到 PATH 的命令？下面我们就来说说后一种方式——让它自动执行。\n在每个用户的 home 目录中有一个 Shell 每次启动时会默认执行一个配置脚本，以初始化环境，包括添加一些用户自定义环境变量等等。实验楼的环境使用的 Shell 是 zsh，它的配置文件是 .zshrc，相应的如果使用的 Shell 是 Bash，则配置文件为 .bashrc。它们在 etc 下还都有一个或多个全局的配置文件，不过我们一般只修改用户目录下的配置文件。Shell 的种类有很多，可以使用 cat /etc/shells 命令查看当前系统已安装的 Shell。\n我们可以简单地使用下面命令直接添加内容到 .zshrc 中：\n1 echo \u0026#34;PATH=$PATH:/home/shiyanlou/mybin\u0026#34; \u0026gt;\u0026gt; .zshrc 上述命令中 \u0026gt;\u0026gt; 表示将标准输出以追加的方式重定向到一个文件中，注意前面用到的 \u0026gt; 是以覆盖的方式重定向到一个文件中，使用的时候一定要注意分辨。在指定文件不存在的情况下都会创建新的文件。\n变量修改 变量的修改有以下几种方式：\n变量设置方式 说明 ${变量名#匹配字串} 从头向后开始匹配，删除符合匹配字串的最短数据 ${变量名##匹配字串} 从头向后开始匹配，删除符合匹配字串的最长数据 ${变量名%匹配字串} 从尾向前开始匹配，删除符合匹配字串的最短数据 ${变量名%%匹配字串} 从尾向前开始匹配，删除符合匹配字串的最长数据 ${变量名/旧的字串/新的字串} 将符合旧字串的第一个字串替换为新的字串 ${变量名//旧的字串/新的字串} 将符合旧字串的全部字串替换为新的字串 比如我们可以修改前面添加到 PATH 的环境变量，将添加的 mybin 目录从环境变量里删除。为了避免操作失误导致命令找不到，我们先将 PATH 赋值给一个新的自定义变量 mypath：\n1 2 3 4 5 mypath=$PATH echo $mypath mypath=${mypath%/home/shiyanlou/mybin} # 或使用通配符 * 表示任意多个任意字符 mypath=${mypath%*/mybin} 可以看到路径已经不存在了。\n变量删除 可以使用 unset 命令删除一个环境变量：\n1 unset mypath 文件管理命令 file命令 我们可以使用 file 命令查看文件的类型：\n1 file /bin/ls 说明这是一个可执行文件，运行在 64 位平台，并使用了动态链接文件（共享库）。\n与 Windows 不同的是，如果你新建了一个 shiyanlou.txt 文件，Windows 会自动把它识别为文本文件，而 file 命令会识别为一个空文件。这个前面我提到过，在 Linux 中文件的类型不是根据文件后缀来判断的。当你在文件里输入内容后才会显示文件类型。\ncat 命令 cat 命令用于连接文件并打印到标准输出设备上。\ncat 主要有三大功能：\n1.一次显示整个文件:\n1 cat filename 2.从键盘创建一个文件:\n1 cat \u0026gt; filename 只能创建新文件，不能编辑已有文件。\n3.将几个文件合并为一个文件:\n1 cat file1 file2 \u0026gt; file -b 对非空输出行号 -n 输出所有行号\n还有tac、nl命令，都是复制文件内容到终端，tac表示倒序输出在终端，nl表示正序并加上行号打印到终端上\n实例： （1）把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里\n1 cat -n log2012.log log2013.log （2）把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里\n1 cat -b log2012.log log2013.log log.log （3）使用 here doc 生成新文件\n1 2 3 4 5 6 7 8 9 10 cat \u0026gt;log.txt \u0026lt;\u0026lt;EOF \u0026gt;Hello \u0026gt;World \u0026gt;PWD=$(pwd) \u0026gt;EOF ls -l log.txt cat log.txt Hello World PWD=/opt/soft/test （4）反向列示\n1 2 3 4 tac log.txt PWD=/opt/soft/test World Hello chmod 命令 Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。\n用于改变 linux 系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。\n每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。\n以文件 log2012.log 为例：\n1 -rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。\n常用参数：\n1 2 -c 当发生改变时，报告处理信息 -R 处理指定目录以及其子目录下所有文件 权限范围：\n1 2 3 4 u ：目录或者文件的当前的用户 g ：目录或者文件的当前的群组 o ：除了目录或者文件的当前用户或群组之外的用户或者群组 a ：所有的用户及群组 权限代号：\n1 2 3 4 5 r ：读权限，用数字4表示 w ：写权限，用数字2表示 x ：执行权限，用数字1表示 - ：删除权限，用数字0表示 s ：特殊权限 实例： （1）增加文件 t.log 所有用户可执行权限\n1 chmod a+x t.log （2）减少文件 t.log 所有用户读写权限\n1 chmod a-rw t.log （3）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息\n1 chmod u=r t.log -c （4）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限\n1 chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c) （5）将 test 目录及其子目录所有文件添加可读权限\n1 chmod u+r,g+r,o+r -R text/ -c chown 命令 chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。\n1 2 -c 显示更改的部分的信息 -R 处理指定目录及子目录 实例： （1）改变拥有者和群组 并显示改变信息\n1 chown -c mail:mail log2012.log （2）改变文件群组\n1 chown -c :mail t.log （3）改变文件夹及子文件目录属主及属组为 mail\n1 chown -cR mail: test/ cp 命令 将源文件复制至目标文件，或将多个源文件复制至目标目录。\n注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！\n1 2 3 -i 提示 -r 复制目录及目录内所有项目 -a 复制的文件与原文件时间一样 实例： （1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。\n1 cp -ai a.txt test （2）为 a.txt 建议一个链接（快捷方式）\n1 cp -s a.txt link_a.txt find 命令 用于在文件树中查找文件，并作出相应的处理。\n命令格式：\n1 find pathname -options [-print -exec -ok ...] 命令参数：\n1 2 3 4 pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print：find命令将匹配的文件输出到标准输出。 -exec：find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为\u0026#39;command\u0026#39; { } \\;，注意{ }和\\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项：\n1 2 3 4 5 6 7 8 9 10 11 -name 按照文件名查找文件 -perm 按文件权限查找文件 -user 按文件属主查找文件 -group 按照文件所属的组来查找文件。 -type 查找某一类型的文件，诸如： b - 块设备文件 d - 目录 c - 字符设备文件 l - 符号链接文件 p - 管道文件 f - 普通文件 实例： （1）查找 48 小时内修改过的文件\n1 find -atime -2 （2）在当前目录查找 以 .log 结尾的文件。. 代表当前目录\n1 find ./ -name \u0026#39;*.log\u0026#39; （3）查找 /opt 目录下 权限为 777 的文件\n1 find /opt -perm 777 （4）查找大于 1K 的文件\n1 find -size +1000c (5)查找等于 1000 字符的文件\n1 find -size 1000c -exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。\nhead 命令 head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。\n常用参数：\n1 -n\u0026lt;行数\u0026gt; 显示的行数（行数为复数表示从最后向前数） 实例： （1）显示 1.log 文件中前 20 行\n1 head 1.log -n 20 （2）显示 1.log 文件前 20 字节\n1 head -c 20 log2014.log （3）显示 t.log最后 10 行\n1 head -n -10 t.log less 命令 less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。\n常用命令参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 -i 忽略搜索时的大小写 -N 显示每行的行号 -o \u0026lt;文件名\u0026gt; 将less 输出的内容在指定文件中保存起来 -s 显示连续空行为一行 /字符串：向下搜索“字符串”的功能 ?字符串：向上搜索“字符串”的功能 n：重复前一个搜索（与 / 或 ? 有关） N：反向重复前一个搜索（与 / 或 ? 有关） -x \u0026lt;数字\u0026gt; 将“tab”键显示为规定的数字空格 b 向后翻一页 d 向后翻半页 h 显示帮助界面 Q 退出less 命令 u 向前滚动半页 y 向前滚动一行 空格键 滚动一行 回车键 滚动一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 实例： （1）ps 查看进程信息并通过 less 分页显示\n1 ps -aux | less -N （2）查看多个文件\n1 less 1.log 2.log 可以使用 n 查看下一个，使用 p 查看前一个。\nln 命令 功能是为文件在另外一个位置建立一个同步的链接，当在不同目录需要该问题时，就不需要为每一个目录创建同样的文件，通过 ln 创建的链接（link）减少磁盘占用量。\n链接分类：软件链接及硬链接\n软链接：\n软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 软链接可以 跨文件系统 ，硬链接不可以 软链接可以对一个不存在的文件名进行链接 软链接可以对目录进行链接 硬链接:\n硬链接，以文件副本的形式存在。但不占用实际空间。 不允许给目录创建硬链接 硬链接只有在同一个文件系统中才能创建 需要注意： 第一：ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二：ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 第三：ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 常用参数：\n1 2 3 -b 删除，覆盖以前建立的链接 -s 软链接（符号链接） -v 显示详细处理过程 实例： （1）给文件创建软链接，并显示操作信息\n1 ln -sv source.log link.log （2）给文件创建硬链接，并显示操作信息\n1 ln -v source.log link1.log （3）给目录创建软链接\n1 ln -sv /opt/soft/test/test3 /opt/soft/test/test5 locate 命令 locate 通过搜寻系统内建文档数据库达到快速找到档案，数据库由 updatedb 程序来更新，updatedb 是由 cron daemon 周期性调用的。默认情况下 locate 命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是 locate 所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb 每天会跑一次，可以由修改 crontab 来更新设定值 (etc/crontab)。\nlocate 与 find 命令相似，可以使用如 *、? 等进行正则匹配查找\n常用参数：\n1 2 3 -l num（要显示的行数） -f 将特定的档案系统排除在外，如将proc排除在外 -r 使用正则运算式做为寻找条件\\ 实例： （1）查找和 pwd 相关的所有文件(文件名中包含 pwd）\n1 locate pwd （2）搜索 etc 目录下所有以 sh 开头的文件\n1 locate /etc/sh （3）查找 /var 目录下，以 reason 结尾的文件\n1 locate -r \u0026#39;^/var.*reason$\u0026#39;（其中.表示一个字符，*表示任务多个；.*表示任意多个字符） more 命令 功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。\n1 2 3 4 5 6 7 8 9 10 11 命令参数： +n 从笫 n 行开始显示 -n 定义屏幕大小为n行 +/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示 -d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能 -l 忽略Ctrl+l（换页）字符 -p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似 -s 把连续的多个空行显示为一行 -u 把文件内容中的下画线去掉 常用操作命令：\n1 2 3 4 5 6 7 8 9 Enter 向下 n 行，需要定义。默认为 1 行 Ctrl+F 向下滚动一屏 空格键 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 V 调用vi编辑器 !命令 调用Shell，并执行命令 q 退出more 实例： （1）显示文件中从第3行起的内容\n1 more +3 text.txt （2）在所列出文件目录详细信息，借助管道使每次显示 5 行\n1 ls -l | more -5 按空格显示下 5 行。\n与more****相似的命令less**来分页查看文件内容。\nmv 命令 移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。\n当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。\n实例： （1）将文件 test.log 重命名为 test1.txt\n1 mv test.log test1.txt （2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中\n1 mv llog1.txt log2.txt log3.txt /test3 （3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖\n1 mv -i log1.txt log2.txt （4）移动当前文件夹下的所有文件到上一级目录\n1 mv * ../ rename命令 要实现批量重命名，mv 命令就有点力不从心了，我们可以使用一个看起来更专业的命令 rename 来实现。不过它要用 perl 正则表达式来作为参数，关于正则表达式我们要在后面才会介绍到，这里只做演示，你只要记得这个 rename 命令可以批量重命名就好了，以后再重新学习也不会有任何问题，毕竟你已经掌握了一个更常用的 mv 命令。\nrename 命令并不是内置命令，若提示无该命令可以使用 sudo apt-get install rename 命令自行安装。\n实例： 1 2 3 4 5 6 7 8 9 10 cd /home/shiyanlou/ # 使用通配符批量创建 5 个文件: touch file{1..5}.txt # 批量将这 5 个后缀为 .txt 的文本文件重命名为以 .c 为后缀的文件: rename \u0026#39;s/\\.txt/\\.c/\u0026#39; *.txt # 批量将这 5 个文件，文件名和后缀改为大写: rename \u0026#39;y/a-z/A-Z/\u0026#39; *.c 简单解释一下上面的命令，rename 是先使用第二个参数的通配符匹配所有后缀为 .txt 的文件，然后使用第一个参数提供的正则表达式将匹配的这些文件的 .txt 后缀替换为 .c，这一点在我们后面学习了 sed 命令后，相信你会更好地理解。\nrm 命令 删除一个目录中的一个或多个文件或目录，如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。\n1 rm [选项] 文件… 实例： （1）删除任何 .log 文件，删除前逐一询问确认：\n1 rm -i *.log （2）删除 test 子目录及子目录中所有档案删除，并且不用一一确认：\n1 rm -rf test （3）删除以 -f 开头的文件\n1 rm -- -f* tail 命令 用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。\n常用参数：\n1 2 -f 循环读取（常用于查看递增的日志文件） -n\u0026lt;行数\u0026gt; 显示行数（从后向前） （1）循环读取逐渐增加的文件内容\n1 ping 127.0.0.1 \u0026gt; ping.log \u0026amp; 后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。\n1 tail -f ping.log （查看日志）\ntouch 命令 Linux touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。\nls -l 可以显示档案的时间记录。\n语法 1 touch [-acfm][-d\u0026lt;日期时间\u0026gt;][-r\u0026lt;参考文件或目录\u0026gt;] [-t\u0026lt;日期时间\u0026gt;][--help][--version][文件或目录…] 参数说明：\na 改变档案的读取时间记录。 m 改变档案的修改时间记录。 c 假如目的档案不存在，不会建立新的档案。与 \u0026ndash;no-create 的效果一样。 f 不使用，是为了与其他 unix 系统的相容性而保留。 r 使用参考档的时间记录，与 \u0026ndash;file 的效果一样。 d 设定时间与日期，可以使用各种不同的格式。 t 设定档案的时间记录，格式与 date 指令相同。 –no-create 不会建立新档案。 –help 列出指令格式。 –version 列出版本讯息。 实例 使用指令\u0026quot;touch\u0026quot;修改文件\u0026quot;testfile\u0026quot;的时间属性为当前系统时间，输入如下命令：\n1 $ touch testfile #修改文件的时间属性 首先，使用ls命令查看testfile文件的属性，如下所示：\n1 2 3 $ ls -l testfile #查看文件的时间属性 #原来文件的修改时间为16:09 -rw-r--r-- 1 hdd hdd 55 2011-08-22 16:09 testfile 执行指令\u0026quot;touch\u0026quot;修改文件属性以后，并再次查看该文件的时间属性，如下所示：\n1 2 3 4 $ touch testfile #修改文件时间属性为当前系统时间 $ ls -l testfile #查看文件的时间属性 #修改后文件的时间属性为当前系统时间 -rw-r--r-- 1 hdd hdd 55 2011-08-22 19:53 testfile 使用指令\u0026quot;touch\u0026quot;时，如果指定的文件不存在，则将创建一个新的空白文件。例如，在当前目录下，使用该指令创建一个空白文件\u0026quot;file\u0026quot;，输入如下命令：\n1 $ touch file #创建一个名为“file”的新的空白文件 vim 命令 Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。\n打开文件并跳到第 10 行：vim +10 filename.txt 。 打开文件跳到第一个匹配的行：vim +/search-term filename.txt 。 以只读模式打开文件：vim -R /etc/passwd 。 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。\n简单的说，我们可以将这三个模式想成底下的图标来表示：\nwhereis 命令 whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis 及 locate 都是基于系统内建的数据库进行搜索，因此效率很高，而find则是遍历硬盘查找文件。\n常用参数：\n1 2 3 4 -b 定位可执行文件。 -m 定位帮助文件。 -s 定位源代码文件。 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 实例： （1）查找 locate 程序相关文件\n1 whereis locate （2）查找 locate 的源码文件\n1 whereis -s locate （3）查找 lcoate 的帮助文件\n1 whereis -m locate which 命令 在 linux 要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索：\n1 2 3 4 which 查看可执行文件的位置。 whereis 查看文件的位置。 locate 配合数据库查看文件位置。 find 实际搜寻硬盘查询文件名称。 which 是在 PATH 就是指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果。使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n常用参数：\n1 -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 实例： （1）查看 ls 命令是否存在，执行哪个\n1 which ls （2）查看 which\n1 which which （3）查看 cd\n1 which cd（显示不存在，因为 cd 是内建命令，而 which 查找显示是 PATH 中的命令） 查看当前 PATH 配置：\n1 echo $PATH 或使用 env 查看所有环境变量及对应值\n文档编辑命令 grep 命令 强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。\ngrep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。\n命令格式：\n1 grep [option] pattern file|dir 常用参数：\n1 2 3 4 5 6 7 8 9 -A n --after-context显示匹配字符后n行 -B n --before-context显示匹配字符前n行 -C n --context 显示匹配字符前后n行 -c --count 计算符合样式的列数 -i 忽略大小写 -l 只列出文件内容符合指定的样式的文件名称 -f 从文件中读取关键词 -n 显示匹配内容的所在文件中行数 -R 递归查找文件夹 grep 的规则表达式:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ^ #锚定行的开始 如：\u0026#39;^grep\u0026#39;匹配所有以grep开头的行。 $ #锚定行的结束 如：\u0026#39;grep$\u0026#39;匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：\u0026#39;gr.p\u0026#39;匹配gr后接一个任意字符，然后是p。 * #匹配零个或多个先前字符 如：\u0026#39;*grep\u0026#39;匹配所有一个或多个空格后紧跟grep的行。 .* #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如\u0026#39;[Gg]rep\u0026#39;匹配Grep和grep。 [^] #匹配一个不在指定范围内的字符，如：\u0026#39;[^A-FH-Z]rep\u0026#39;匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 \\(..\\) #标记匹配字符，如\u0026#39;\\(love\\)\u0026#39;，love被标记为1。 \\\u0026lt; #锚定单词的开始，如:\u0026#39;\\\u0026lt;grep\u0026#39;匹配包含以grep开头的单词的行。 \\\u0026gt; #锚定单词的结束，如\u0026#39;grep\\\u0026gt;\u0026#39;匹配包含以grep结尾的单词的行。 x\\{m\\} #重复字符x，m次，如：\u0026#39;0\\{5\\}\u0026#39;匹配包含5个o的行。 x\\{m,\\} #重复字符x,至少m次，如：\u0026#39;o\\{5,\\}\u0026#39;匹配至少有5个o的行。 x\\{m,n\\} #重复字符x，至少m次，不多于n次，如：\u0026#39;o\\{5,10\\}\u0026#39;匹配5--10个o的行。 \\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：\u0026#39;G\\w*p\u0026#39;匹配以G后跟零个或多个文字或数字字符，然后是p。 \\W #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \\b #单词锁定符，如: \u0026#39;\\bgrep\\b\u0026#39;只匹配grep。 实例： （1）查找指定进程\n1 ps -ef | grep svn （2）查找指定进程个数\n1 ps -ef | grep svn -c （3）从文件中读取关键词\n1 cat test1.txt | grep -f key.log （4）从文件夹中递归查找以grep开头的行，并只列出文件\n1 grep -lR \u0026#39;^grep\u0026#39; /tmp （5）查找非x开关的行内容\n1 grep \u0026#39;^[^x]\u0026#39; test.txt （6）显示包含 ed 或者 at 字符的内容行\n1 grep -E \u0026#39;ed|at\u0026#39; test.txt wc 命令 wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出\n命令格式：\n1 wc [option] file.. 命令参数：\n1 2 3 4 -c 统计字节数 -l 统计行数 -m 统计字符数 -w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串 实例： （1）查找文件的 行数 单词数 字节数 文件名\n1 wc text.txt 结果：\n1 7 8 70 test.txt （2）统计输出结果的行数\n1 cat test.txt | wc -l 磁盘管理命令 cd 命令 cd(changeDirectory) 命令语法：\n1 cd [目录名] 说明：切换当前目录至 dirName。\n实例： （1）进入要目录\n1 cd / （2）进入 “home” 目录\n1 cd ~ （3）进入上一次工作路径\n1 cd - （4）把上个命令的参数作为cd参数使用。\n1 cd !$ df 命令 显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。\n默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示：\n1 2 3 4 5 6 -a 全部文件系统列表 -h 以方便阅读的方式显示信息 -i 显示inode信息 -k 区块为1024字节 -l 只显示本地磁盘 -T 列出文件系统类型 实例： （1）显示磁盘使用情况\n1 df -l （2）以易读方式列出所有文件系统及其类型\n1 df -haT du 命令 du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看：\n命令格式：\n1 du [选项] [文件] 常用参数：\n1 2 3 4 5 6 7 -a 显示目录中所有文件大小 -k 以KB为单位显示文件大小 -m 以MB为单位显示文件大小 -g 以GB为单位显示文件大小 -h 以易读方式显示文件大小 -s 仅显示总计 -c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和 实例： （1）以易读方式显示文件夹内及子文件夹大小\n1 du -h scf/ （2）以易读方式显示文件夹内所有文件大小\n1 du -ah scf/ （3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和\n1 du -hc test/ scf/ （4）输出当前目录下各个子目录所使用的空间\n1 du -hc --max-depth=1 scf/ ls命令 就是 list 的缩写，通过 ls 命令不仅可以查看 linux 文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)查看目录信息等等。\n常用参数搭配：\n1 2 3 4 5 6 7 ls -a 列出目录所有文件，包含以.开始的隐藏文件 ls -A 列出除.及..的其它文件 ls -r 反序排列 ls -t 以文件修改时间排序 ls -S 以文件大小排序 ls -h 以易读大小显示 ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来 实例： (1) 按易读方式按时间反序排序，并显示文件详细信息\n1 ls -lhrt (2) 按大小反序显示文件详细信息\n1 ls -lrS (3)列出当前目录中所有以\u0026quot;t\u0026quot;开头的目录的详细内容\n1 ls -l t* (4) 列出文件绝对路径（不包含隐藏文件）\n1 ls | sed \u0026#34;s:^:`pwd`/:\u0026#34; (5) 列出文件绝对路径（包含隐藏文件）\n1 find $pwd -maxdepth 1 | xargs ls -ld mkdir 命令 mkdir 命令用于创建文件夹。\n可用选项：\n-m: 对新建目录设置存取权限，也可以用 chmod 命令设置; -p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。 实例： （1）当前工作目录下创建名为 t的文件夹\n1 mkdir t （2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建：\n1 mkdir -p /tmp/test/t1/t pwd 命令 pwd 命令用于查看当前工作目录路径。\n实例： （1）查看当前路径\n1 pwd （2）查看软链接的实际路径\n1 pwd -P rmdir 命令 从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。\n注意：不能删除非空目录\n实例： （1）当 parent 子目录被删除后使它也成为空目录的话，则顺便一并删除：\n1 rmdir -p parent/child/child11 网络通讯命令 ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 iptables 命令 iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如：\n把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp \u0026ndash;dport 80 -j REJECT 。 开启 80 端口，因为web对外都是这个端口 1 iptables -A INPUT -p tcp --dport 80 -j ACCEP 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 netstat 命令 Linux netstat命令用于显示网络状态。\n利用netstat指令可让你得知整个Linux系统的网络情况。\n语法\n1 netstat [-acCeFghilMnNoprstuvVwx][-A\u0026lt;网络类型\u0026gt;][--ip] 参数说明：\n-a或–all 显示所有连线中的Socket。 -A\u0026lt;网络类型\u0026gt;或–\u0026lt;网络类型\u0026gt; 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定\u0026quot;-A unix\u0026quot;参数相同。 –ip或–inet 此参数的效果和指定\u0026quot;-A inet\u0026quot;参数相同。 实例 如何查看系统都开启了哪些端口？\n1 2 3 4 5 6 7 8 9 10 [root@centos6 ~ 13:20 #55]# netstat -lnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1035/sshd tcp 0 0 :::22 :::* LISTEN 1035/sshd udp 0 0 0.0.0.0:68 0.0.0.0:* 931/dhclient Active UNIX domain sockets (only servers) Proto RefCnt Flags Type State I-Node PID/Program name Path unix 2 [ ACC ] STREAM LISTENING 6825 1/init @/com/ubuntu/upstart unix 2 [ ACC ] STREAM LISTENING 8429 1003/dbus-daemon /var/run/dbus/system_bus_socket 如何查看网络连接状况？\n1 2 3 4 5 6 7 [root@centos6 ~ 13:22 #58]# netstat -an Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 192.168.147.130:22 192.168.147.1:23893 ESTABLISHED tcp 0 0 :::22 :::* LISTEN udp 0 0 0.0.0.0:68 0.0.0.0:* 如何统计系统当前进程连接数？\n输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？\n严格来说，这个题目考验的是对 awk 的使用。\n首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下：\n1 2 3 4 5 6 7 tcp 0 0 120.27.146.122:80 113.65.18.33:62721 ESTABLISHED tcp 0 0 120.27.146.122:80 27.43.83.115:47148 ESTABLISHED tcp 0 0 120.27.146.122:58838 106.39.162.96:443 ESTABLISHED tcp 0 0 120.27.146.122:52304 203.208.40.121:443 ESTABLISHED tcp 0 0 120.27.146.122:33194 203.208.40.122:443 ESTABLISHED tcp 0 0 120.27.146.122:53758 101.37.183.144:443 ESTABLISHED tcp 0 0 120.27.146.122:27017 23.105.193.30:50556 ESTABLISHED ping 命令 Linux ping命令用于检测主机。\n执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。\n指定接收包的次数\n1 ping -c 2 www.baidu.com telnet 命令 Linux telnet命令用于远端登入。\n执行telnet指令开启终端机阶段作业，并登入远端主机。\n语法\n1 telnet [-8acdEfFKLrx][-b\u0026lt;主机别名\u0026gt;][-e\u0026lt;脱离字符\u0026gt;][-k\u0026lt;域名\u0026gt;][-l\u0026lt;用户名称\u0026gt;][-n\u0026lt;记录文件\u0026gt;][-S\u0026lt;服务类型\u0026gt;][-X\u0026lt;认证形态\u0026gt;][主机名称或IP地址\u0026lt;通信端口\u0026gt;] 参数说明：\n-8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b\u0026lt;主机别名\u0026gt; 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e\u0026lt;脱离字符\u0026gt; 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定\u0026quot;-F\u0026quot;参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k\u0026lt;域名\u0026gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l\u0026lt;用户名称\u0026gt; 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n\u0026lt;记录文件\u0026gt; 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S\u0026lt;服务类型\u0026gt; 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X\u0026lt;认证形态\u0026gt; 关闭指定的认证形态。 实例 登录远程主机\n1 2 # 登录IP为 192.168.0.5 的远程主机 telnet 192.168.0.5 系统管理命令 date 命令 显示或设定系统的日期与时间。\n命令参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -d\u0026lt;字符串\u0026gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。 -s\u0026lt;字符串\u0026gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。 -u 显示GMT。 %H 小时(00-23) %I 小时(00-12) %M 分钟(以00-59来表示) %s 总秒数。起算时间为1970-01-01 00:00:00 UTC。 %S 秒(以本地的惯用法来表示) %a 星期的缩写。 %A 星期的完整名称。 %d 日期(以01-31来表示)。 %D 日期(含年月日)。 %m 月份(以01-12来表示)。 %y 年份(以00-99来表示)。 %Y 年份(以四位数来表示)。 实例： （1）显示下一天\n1 date +%Y%m%d --date=\u0026#34;+1 day\u0026#34; //显示下一天的日期 （2）-d参数使用\n1 2 3 4 5 6 7 date -d \u0026#34;nov 22\u0026#34; 今年的 11 月 22 日是星期三 date -d \u0026#39;2 weeks\u0026#39; 2周后的日期 date -d \u0026#39;next monday\u0026#39; (下周一的日期) date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%d date -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%d date -d last-month +%Y%m(上个月是几月) date -d next-month +%Y%m(下个月是几月) free 命令 显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。\n命令参数：\n1 2 3 4 5 6 -b 以Byte显示内存使用情况 -k 以kb为单位显示内存使用情况 -m 以mb为单位显示内存使用情况 -g 以gb为单位显示内存使用情况 -s\u0026lt;间隔秒数\u0026gt; 持续显示内存 -t 显示内存使用总合 实例： （1）显示内存使用情况\n1 2 3 free free -k free -m （2）以总和的形式显示内存的使用信息\n1 free -t （3）周期性查询内存使用情况\n1 free -s 10 kill 命令 发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用\u0026quot;-KILL\u0026quot; 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。\n常用参数：\n1 2 3 4 5 -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 实例： （1）先使用ps查找进程pro1，然后用kill杀掉\n1 kill -9 $(ps -ef | grep pro1) ps 命令 ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top\nlinux上进程有5种状态:\n运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps 工具标识进程的5种状态码:\n1 2 3 4 5 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数：\n1 2 3 4 5 6 7 8 -A 显示所有进程 a 显示所有进程 -a 显示同一终端下所有进程 c 显示进程真实名称 e 显示环境变量 f 显示进程间的关系 r 显示当前终端运行的进程 -aux 显示所有包含其它使用的进程 实例： （1）显示当前所有进程环境变量及进程间关系\n1 ps -ef （2）显示当前所有进程\n1 ps -A （3）与grep联用查找某进程\n1 ps -aux | grep apache （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码\n1 ps aux | grep \u0026#39;(cron|syslog)\u0026#39; rpm 命令 Linux rpm 命令用于管理套件。\nrpm(redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。\n1 2 3 4 5 6 # 查看系统自带jdk rpm -qa | grep jdk # 删除系统自带jdk rpm -e --nodeps 查看jdk显示的数据 # 安装jdk rpm -ivh jdk-7u80-linux-x64.rpm top 命令 显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等\n常用参数：\n1 2 3 4 -c 显示完整的进程命令 -s 保密模式 -p \u0026lt;进程号\u0026gt; 指定进程显示 -n \u0026lt;次数\u0026gt;循环显示次数 实例： 1 2 3 4 5 6 7 top - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35 Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombie Cpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%st Mem: 32949016k total, 14411180k used, 18537836k free, 169884k buffers Swap: 32764556k total, 0k used, 32764556k free, 3612636k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 前五行是当前系统情况整体的统计信息区。\n第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：\n14:06:23 — 当前系统时间\nup 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！）\n2 users — 当前有2个用户登录系统\nload average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。\nload average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。\n第二行，Tasks — 任务（进程），具体信息说明如下：\n系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。\n第三行，cpu状态信息，具体属性说明如下：\n1 2 3 4 5 6 7 5.9%us — 用户空间占用CPU的百分比。 3.4% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 90.4% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.2% si — 软中断（Software Interrupts）占用CPU的百分比 备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！\n第四行，内存状态，具体信息如下：\n1 2 3 4 32949016k total — 物理内存总量（32GB） 14411180k used — 使用中的内存总量（14GB） 18537836k free — 空闲内存总量（18GB） 169884k buffers — 缓存的内存量 （169M） 第五行，swap交换分区信息，具体信息说明如下：\n1 2 3 4 32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 第六行，空行。\n第七行以下：各进程（任务）的状态监控，项目列信息说明如下：\n1 2 3 4 5 6 7 8 9 10 11 12 PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） top 交互命令 1 2 3 4 5 6 7 h 显示top交互命令帮助信息 c 切换显示命令名称和完整命令行 m 以内存使用率排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 o或者O 改变显示项目的顺序 yum 命令 yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。\n基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。\nyum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。\n列出所有可更新的软件清单命令：yum check-update 更新所有软件命令：yum update 仅安装指定的软件命令：yum install \u0026lt;package_name\u0026gt; 仅更新指定的软件命令：yum update \u0026lt;package_name\u0026gt; 列出所有可安裝的软件清单命令：yum list 删除软件包命令：yum remove \u0026lt;package_name\u0026gt; 查找软件包 命令：yum search 清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 实例 安装 pam-devel\n1 [root@www ~]# yum install pam-devel 备份压缩命令 bzip2 命令 创建 *.bz2 压缩文件：bzip2 test.txt 。 解压 *.bz2 文件：bzip2 -d test.txt.bz2 。 gzip 命令 创建一个 *.gz 的压缩文件：gzip test.txt 。 解压 *.gz 文件：gzip -d test.txt.gz 。 显示压缩的比率：gzip -l *.gz 。 tar 命令 用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。\n弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件\n常用参数：\n1 2 3 4 5 6 7 8 9 10 -c 建立新的压缩文件 -f 指定压缩文件 -r 添加文件到已经压缩文件包中 -u 添加改了和现有的文件到压缩包中 -x 从压缩包中抽取文件 -t 显示压缩文件中的内容 -z 支持gzip压缩 -j 支持bzip2压缩 -Z 支持compress解压文件 -v 显示操作过程 有关 gzip 及 bzip2 压缩:\n1 2 3 4 5 gzip 实例：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz 对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz bz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2 对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2 实例： （1）将文件全部打包成 tar 包\n1 tar -cvf log.tar 1.log,2.log 或tar -cvf log.* （2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩\n1 tar -zcvf /tmp/etc.tar.gz /etc （3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的）\n1 tar -ztvf /tmp/etc.tar.gz （4）要压缩打包 /home, /etc ，但不要 /home/dmtsai\n1 tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc unzip 命令 解压 *.zip 文件：unzip test.zip 。 查看 *.zip 文件的内容：unzip -l jasper.zip 。 ","permalink":"https://cold-bin.github.io/post/shell%E7%BC%96%E7%A8%8B%E5%88%9D%E8%AF%86%E5%92%8Clinux%E5%8F%8A%E5%85%B6%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":["linux常用命令"],"title":"Shell编程初识和Linux及其常用命令"},{"categories":["linux"],"contents":"linux常用命令 零、linux初窥 一、命令行操作体验 在 linux 中，最最重要的就是命令，这就包含了 2 个过程，输入和输出\n输入：输入当然就是打开终端，然后按键盘输入，然后按回车，输入格式一般就是这类的 1 2 3 4 5 6 7 8 #创建一个名为 file 的文件，touch是一个命令 touch file #进入一个目录，cd是一个命令 cd /etc/ #查看当前所在目录 pwd 输出：输出会返回你想要的结果，比如你要看什么文件，就会返回文件的内容。如果只是执行，执行失败会告诉你哪里错了，如果执行成功那么会没有输出，因为 linux 的哲学就是：没有结果就是最好的结果 1. 开始 如图，双击桌面上的 Xfce 终端 图标打开终端后系统会自动运行 Shell 程序，然后我们就可以输入命令让系统来执行了：\n1) 重要快捷键 真正学习命令行之前，你先要掌握几个十分有用、必需掌握的小技巧：\n[Tab] 使用Tab键来进行命令补全，Tab键一般是在字母Q旁边，这个技巧给你带来的最大的好处就是当你忘记某个命令的全称时可以只输入它的开头的一部分，然后按下Tab键就可以得到提示或者帮助完成：\n当然不止补全命令，补全目录、补全命令参数都是没问题的：\n[Ctrl+c] 想想你有没有遇到过这种情况，当你在 Linux 命令行中无意输入了一个不知道的命令，或者错误地使用了一个命令，导致在终端里出现了你无法预料的情况，比如，屏幕上只有光标在闪烁却无法继续输入命令，或者不停地输出一大堆你不想要的结果。你想要立即停止并恢复到你可控的状态，那该怎么办呢？这时候你就可以使用Ctrl+c键来强行终止当前程序（你可以放心它并不会使终端退出）。\n尝试输入以下命令：\n1 tail 然后你会发现你接下来的输入都没有任何反应了，只是将你输入的东西显示出来，现在你可以使用Ctrl+c，来中断这个你目前可能还不知道是什么的程序（在后续课程中我们会具体解释这个tail命令是什么）。\n又或者输入：\n1 find / 显然这不是你想的结果，可以使用Ctrl+c结束。\n虽然这个按着很方便，但不要随便按，因为有时候，当你看到终端没有任何反应或提示，也不能接受你的输入时，可能只是运行的程序需要你耐心等一下，就不要急着按Ctrl+c了。\n其他一些常用快捷键 按键 作用 Ctrl+d 键盘输入结束或退出终端 Ctrl+s 暂停当前程序，暂停后按下任意键恢复运行 Ctrl+z 将当前程序放到后台运行，恢复到前台为命令fg Ctrl+a 将光标移至输入行头，相当于Home键 Ctrl+e 将光标移至输入行末，相当于End键 Ctrl+k 删除从光标所在位置到行末 Alt+Backspace 向前删除一个单词 Shift+PgUp 将终端显示向上滚动 Shift+PgDn 将终端显示向下滚动 2) 利用历史输入命令 很简单，你可以使用键盘上的方向上键↑，恢复你之前输入过的命令，你一试便知。\n3) 通配符 通配符是一种特殊语句，主要有星号（*）和问号（?），用来对字符串进行模糊匹配（比如文件名、参数名）。当查找文件夹时，可以使用它来代替一个或多个真正字符；当不知道真正字符或者懒得输入完整名字时，常常使用通配符代替一个或多个真正字符。\n终端里面输入的通配符是由 Shell 处理的，不是由所涉及的命令语句处理的，它只会出现在命令的“参数值”里（它不能出现在命令名称里， 命令不记得，那就用Tab补全）。当 Shell 在“参数值”中遇到了通配符时，Shell 会将其当作路径或文件名在磁盘上搜寻可能的匹配：若符合要求的匹配存在，则进行代换（路径扩展）；否则就将该通配符作为一个普通字符传递给“命令”，然后再由命令进行处理。总之，通配符实际上就是一种 Shell 实现的路径扩展功能。在通配符被处理后， Shell 会先完成该命令的重组，然后继续处理重组后的命令，直至执行该命令。\n首先回到用户家目录：\n1 cd /home/shiyanlou 然后使用 touch 命令创建 2 个文件，后缀都为 txt：\n1 touch asd.txt fgh.txt 可以给文件随意命名，假如过了很长时间，你已经忘了这两个文件的文件名，现在你想在一大堆文件中找到这两个文件，就可以使用通配符：\n1 ls *.txt 在创建文件的时候，如果需要一次性创建多个文件，比如：“love_1_linux.txt，love_2_linux.txt，\u0026hellip; love_10_linux.txt”。在 Linux 中十分方便：\n1 touch love_{1..10}_shiyanlou.txt Shell 常用通配符：\n字符 含义 * 匹配 0 或多个字符 ? 匹配任意一个字符 [list] 匹配 list 中的任意单一字符 [^list] 匹配 除 list 中的任意单一字符以外的字符 [c1-c2] 匹配 c1-c2 中的任意单一字符 如：[0-9][a-z] {string1,string2,...} 匹配 string1 或 string2 (或更多)其一字符串 {c1..c2} 匹配 c1-c2 中全部字符 如{1..10} 4) 在命令行中获取帮助 在 Linux 环境中，如果你遇到困难，可以使用man命令，它是Manual pages的缩写。\nManual pages 是 UNIX 或类 UNIX 操作系统中在线软件文档的一种普遍的形式， 内容包括计算机程序（包括库和系统调用）、正式的标准和惯例，甚至是抽象的概念。用户可以通过执行man命令调用手册页。\n你可以使用如下方式来获得某个命令的说明和使用方式的详细介绍：\n1 man \u0026lt;command_name\u0026gt; 比如你想查看 man 命令本身的使用方式，你可以输入：\n1 man man 通常情况下，man 手册里面的内容都是英文的，这就要求你有一定的英文基础。man 手册的内容很多，涉及了 Linux 使用过程中的方方面面。为了便于查找，man 手册被进行了分册（分区段）处理，在 Research UNIX、BSD、OS X 和 Linux 中，手册通常被分为 8 个区段，安排如下：\n区段 说明 1 一般命令 2 系统调用 3 库函数，涵盖了 C 标准函数库 4 特殊文件（通常是/dev 中的设备）和驱动程序 5 文件格式和约定 6 游戏和屏保 7 杂项 8 系统管理命令和守护进程 要查看相应区段的内容，就在 man 后面加上相应区段的数字即可，如：\n1 man 1 ls 会显示第一区段中的ls命令 man 页面。\n所有的手册页遵循一个常见的布局，为了通过简单的 ASCII 文本展示而被优化，而这种情况下可能没有任何形式的高亮或字体控制。一般包括以下部分内容：\nNAME（名称）\n该命令或函数的名称，接着是一行简介。\nSYNOPSIS（概要）\n对于命令，正式的描述它如何运行，以及需要什么样的命令行参数。对于函数，介绍函数所需的参数，以及哪个头文件包含该函数的定义。\nDESCRIPTION（说明）\n命令或函数功能的文本描述。\nEXAMPLES（示例）\n常用的一些示例。\nSEE ALSO（参见）\n相关命令或函数的列表。\n也可能存在其它部分内容，但这些部分没有得到跨手册页的标准化。常见的例子包括：OPTIONS（选项），EXIT STATUS（退出状态），ENVIRONMENT（环境），BUGS（程序漏洞），FILES（文件），AUTHOR（作者），REPORTING BUGS（已知漏洞），HISTORY（历史）和 COPYRIGHT（版权）。\n通常 man 手册中的内容很多，你可能不太容易找到你想要的结果，不过幸运的是你可以在 man 中使用搜索/\u0026lt;你要搜索的关键字\u0026gt;，查找完毕后你可以使用n键切换到下一个关键字所在处，shift+n为上一个关键字所在处。使用Space（空格键）翻页，Enter（回车键）向下滚动一行，或者使用k，j（vim 编辑器的移动键）进行向前向后滚动一行。按下h键为显示使用帮助（因为 man 使用 less 作为阅读器，实为less工具的帮助），按下q退出。\n想要获得更详细的帮助，你还可以使用info命令，不过通常使用man就足够了。如果你知道某个命令的作用，只是想快速查看一些它的某个具体参数的作用，那么你可以使用--help参数，大部分命令都会带有这个参数，如：\n1 ls --help 二、用户及文件权限管理 Linux 是一个可以实现多用户登录的操作系统，比如“李雷”和“韩梅梅”都可以同时登录同一台主机，他们共享一些主机的资源，但他们也分别有自己的用户空间，用于存放各自的文件。但实际上他们的文件都是放在同一个物理磁盘上的甚至同一个逻辑分区或者目录里，但是由于 Linux 的 用户管理 和 权限机制，不同用户不可以轻易地查看、修改彼此的文件。\n1. 查看用户 请打开终端，输入命令：\n1 2 3 4 5 who am i # 或者 who mom likes 输出的第一列表示打开当前伪终端的用户的用户名（要查看当前登录用户的用户名，去掉空格直接使用 whoami 即可），第二列的 pts/0 中 pts 表示伪终端，所谓伪是相对于 /dev/tty 设备而言的，还记得上一节讲终端时的那七个使用 [Ctrl]+[Alt]+[F1]～[F7] 进行切换的 /dev/tty 设备么，这是“真终端”，伪终端就是当你在图形用户界面使用 /dev/tty7 时每打开一个终端就会产生一个伪终端，pts/0 后面那个数字就表示打开的伪终端序号，你可以尝试再打开一个终端，然后在里面输入 who am i，看第二列是不是就变成 pts/1 了，第三列则表示当前伪终端的启动时间。\n还有一点需要注意的是，在某些环境中 who am i 和 who mom likes 命令不会输出任何内容，这是因为当前使用的 SHELL 不是登录时的 SHELL，没有用户与 who 的 stdin 相关联，因此不会输出任何内容。例如我在本地的 Ubuntu 系统上输入这个命令就不会有提示。\n此时我们只需要打开一个登录 SHELL 的终端例如 Tmux，或者通过 ssh 登录到本机，再在新的终端里执行命令即可。\n1 tmux who 命令其它常用参数\n参数 说明 -a 打印能打印的全部 -d 打印死掉的进程 -m 同am i，mom likes -q 打印当前登录用户数及用户名 -u 打印当前登录用户登录信息 -r 打印运行等级 2. 创建及切换用户 在 Linux 系统里， root 账户拥有整个系统至高无上的权限，比如新建和添加用户。\nroot 权限，系统权限的一种，与 SYSTEM 权限可以理解成一个概念，但高于 Administrator 权限，root 是 Linux 和 UNIX 系统中的超级管理员用户帐户，该帐户拥有整个系统至高无上的权力，所有对象他都可以操作，所以很多黑客在入侵系统的时候，都要把权限提升到 root 权限，这个操作等同于在 Windows 下就是将新建的非法帐户添加到 Administrators 用户组。更比如安卓操作系统中（基于 Linux 内核）获得 root 权限之后就意味着已经获得了手机的最高权限，这时候你可以对手机中的任何文件（包括系统文件）执行所有增、删、改、查的操作。\n大部分 Linux 系统在安装时都会建议用户新建一个用户而不是直接使用 root 用户进行登录，当然也有直接使用 root 登录的例如 Kali（基于 Debian 的 Linux 发行版，集成大量工具软件，主要用于数字取证的操作系统）。一般我们登录系统时都是以普通账户的身份登录的，要创建用户需要 root 权限，这里就要用到 sudo 这个命令了。不过使用这个命令有两个大前提，一是你要知道当前登录用户的密码，二是当前用户必须在 sudo 用户组。shiyanlou 用户也属于 sudo 用户组（稍后会介绍如何查看和添加用户组）。\nsu，su- 与 sudo 需要注意 Linux 环境下输入密码是不会显示的。\nsu \u0026lt;user\u0026gt; 可以切换到用户 user，执行时需要输入目标用户的密码，sudo \u0026lt;cmd\u0026gt; 可以以特权级别运行 cmd 命令，需要当前用户属于 sudo 组，且需要输入当前用户的密码。su - \u0026lt;user\u0026gt; 命令也是切换用户，但是同时用户的环境变量和工作目录也会跟着改变成目标用户所对应的。\n现在我们新建一个叫 lilei 的用户：\n1 sudo adduser lilei 实验楼的环境目前设置为 shiyanlou 用户执行 sudo 不需要输入密码，通常此处需要按照提示输入 shiyanlou 密码（Linux 下密码输入是不显示任何内容的，shiyanlou 用户密码可以在右侧环境信息里查看，请勿自行设置密码）。然后是给 lilei 用户设置密码，后面的选项的一些内容你可以选择直接回车使用默认值。\n这个命令不但可以添加用户到系统，同时也会默认为新用户在 /home 目录下创建一个工作目录：\n1 ls /home 现在你已经创建好一个用户，并且你可以使用你创建的用户登录了，使用如下命令切换登录用户：\n1 su -l lilei 输入刚刚设置的 lilei 的密码，然后输入如下命令并查看输出：\n1 2 3 who am i whoami pwd 你发现了区别了吗？这就是上一小节我们讲到的 who am i 和 whoami 命令的区别。\n退出当前用户跟退出终端一样，可以使用 exit 命令或者使用快捷键 Ctrl+D。\n3. 用户组 在 Linux 里面每个用户都有一个归属（用户组），用户组简单地理解就是一组用户的集合，它们共享一些资源和权限，同时拥有私有资源，就跟家的形式差不多，你的兄弟姐妹（不同的用户）属于同一个家（用户组），你们可以共同拥有这个家（共享资源），爸妈对待你们都一样（共享权限），你偶尔写写日记，其他人未经允许不能查看（私有资源和权限）。当然一个用户是可以属于多个用户组的，正如你既属于家庭，又属于学校或公司。\n在 Linux 里面如何知道自己属于哪些用户组呢？\n方法一：使用 groups 命令 1 groups shiyanlou 其中冒号之前表示用户，后面表示该用户所属的用户组。这里可以看到 shiyanlou 用户属于 shiyanlou 用户组，每次新建用户如果不指定用户组的话，默认会自动创建一个与用户名相同的用户组（差不多就相当于家长的意思）。\n默认情况下在 sudo 用户组里的可以使用 sudo 命令获得 root 权限。shiyanlou 用户也可以使用 sudo 命令，为什么这里没有显示在 sudo 用户组里呢？可以查看下 /etc/sudoers.d/shiyanlou 文件，我们在 /etc/sudoers.d 目录下创建了这个文件，从而给 shiyanlou 用户赋予了 sudo 权限：\n方法二：查看 /etc/group 文件 1 cat /etc/group | sort 这里 cat 命令用于读取指定文件的内容并打印到终端输出，后面会详细讲它的使用。 | sort 表示将读取的文本进行一个字典排序再输出，然后你将看到如下一堆输出，你可以在最下面看到 shiyanlou 的用户组信息：\n没找到？没关系，你可以使用 grep 命令过滤掉一些你不想看到的结果：\n1 cat /etc/group | grep -E \u0026#34;shiyanlou\u0026#34; /etc/group 文件格式说明\n/etc/group 的内容包括用户组（Group）、用户组口令、GID（组 ID） 及该用户组所包含的用户（User），每个用户组一条记录。格式如下：\ngroup_name:password:GID:user_list\n你看到上面的 password 字段为一个 x，并不是说密码就是它，只是表示密码不可见而已。\n这里需要注意，如果用户的 GID 等于用户组的 GID，那么最后一个字段 user_list 就是空的，这里的 GID 是指用户默认所在组的 GID，可以使用 id 命令查看。比如 shiyanlou 用户，在 /etc/group 中的 shiyanlou 用户组后面是不会显示的。lilei 用户，在 /etc/group 中的 lilei 用户组后面是不会显示的。\n将其它用户加入 sudo 用户组usermod 命令 默认情况下新创建的用户是不具有 root 权限的，也不在 sudo 用户组，可以让其加入 sudo 用户组从而获取 root 权限：\n1 2 3 # 注意 Linux 上输入密码是不会显示的 su -l lilei sudo ls 会提示 lilei 不在 sudoers 文件中，意思就是 lilei 不在 sudo 用户组中，至于 sudoers 文件（/etc/sudoers）你现在最好不要动它，操作不慎会导致比较麻烦的后果。\n使用 usermod 命令可以为用户添加用户组，同样使用该命令你必需有 root 权限，你可以直接使用 root 用户为其它用户添加用户组，或者用其它已经在 sudo 用户组的用户使用 sudo 命令获取权限来执行该命令。\n这里我用 shiyanlou 用户执行 sudo 命令将 lilei 添加到 sudo 用户组，让它也可以使用 sudo 命令获得 root 权限，首先我们切换回 shiyanlou 用户。\n1 su - shiyanlou 此处需要输入 shiyanlou 用户密码，shiyanlou 的密码可以在右侧工具栏的环境信息里看到。\n当然也可以通过 sudo passwd shiyanlou 进行设置，或者你直接关闭当前终端打开一个新的终端。\n1 2 3 groups lilei sudo usermod -G sudo lilei groups lilei 然后你再切换回 lilei 用户，现在就可以使用 sudo 获取 root 权限了。\n4. 删除用户和用户组 1) deluser命令 删除用户是很简单的事：\n1 sudo deluser lilei --remove-home 使用 --remove-home 参数在删除用户时候会一并将该用户的工作目录一并删除。如果不使用那么系统会自动在 /home 目录为该用户保留工作目录。\n2）groupdel 命令 删除用户组可以使用 groupdel 命令，倘若该群组中仍包括某些用户，则必须先删除这些用户后，才能删除群组。\n三、linux文件权限 1. 查看文件权限 1) ll或ls -l命令 ll或ls -l查看当前或指定目录下的非隐藏文件的所有属性，包括文件权限、修改时间、文件拥有及创建用户、大小、文件名等信息\n另外，ls -a的-a参数可以显示隐藏文件\n2）权限字母表示 2. 变更文件所有者 chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。\n1 2 -c 显示更改的部分的信息 -R 处理指定目录及子目录 chown命令 （1）改变拥有者和群组 并显示改变信息\n1 chown -c mail:mail log2012.log （2）改变文件群组\n1 chown -c :mail t.log （3）改变文件夹及子文件目录属主及属组为 mail\n1 chown -cR mail: test/ 3. 修改文件权限 chmod命令 一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。\n每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。\n以文件 log2012.log 为例：\n1 -rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。\n常用参数：\n1 2 -c 当发生改变时，报告处理信息 -R 处理指定目录以及其子目录下所有文件 权限范围：\n1 2 3 4 u ：目录或者文件的当前的用户 g ：目录或者文件的当前的群组 o ：除了目录或者文件的当前用户或群组之外的用户或者群组 a ：所有的用户及群组 权限代号：\n1 2 3 4 5 r ：读权限，用数字4表示 w ：写权限，用数字2表示 x ：执行权限，用数字1表示 - ：删除权限，用数字0表示 s ：特殊权限 （1）增加文件 t.log 所有用户可执行权限\n1 chmod a+x t.log （2）减少文件 t.log 所有用户读写权限\n1 chmod a-rw t.log （3）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息\n1 chmod u=r t.log -c （4）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限\n1 chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c) （5）将 test 目录及其子目录所有文件添加可读权限\n1 chmod u+r,g+r,o+r -R text/ -c 4. 更多 adduser 和 useradd 答：useradd 只创建用户，不会创建用户密码和工作目录，创建完了需要使用 passwd \u0026lt;username\u0026gt; 去设置新用户的密码。adduser 在创建用户的同时，会创建工作目录和密码（提示你设置），做这一系列的操作。其实 useradd、userdel 这类操作更像是一种命令，执行完了就返回。而 adduser 更像是一种程序，需要你输入、确定等一系列操作。\n四、Linux 目录结构及文件基本操作 1. linux目录结构（FHS准则） 2. pwd命令 打印当前绝对路径\n3. cd命令 用于切换目录，不能切换至某个具体文件下。在 Linux 里面使用 . 表示当前目录，.. 表示上一级目录（注意，我们上一节介绍过的，以 . 开头的文件都是隐藏文件，所以这两个目录必然也是隐藏的，你可以使用 ls -a 命令查看隐藏文件），- 表示上一次所在目录，～ 通常表示当前用户的 home 目录。在 Linux 里面使用 . 表示当前目录，.. 表示上一级目录（注意，我们上一节介绍过的，以 . 开头的文件都是隐藏文件，所以这两个目录必然也是隐藏的，你可以使用 ls -a 命令查看隐藏文件），- 表示上一次所在目录，～ 通常表示当前用户的 home 目录。\n1）绝对路径 关于绝对路径，简单地说就是以根\u0026quot; / \u0026ldquo;目录为起点的完整路径，以你所要到的目录为终点，表现形式如： /usr/local/bin，表示根目录下的 usr 目录中的 local 目录中的 bin 目录。\n2）相对路径 相对路径，也就是相对于你当前的目录的路径，相对路径是以当前目录 . 为起点，以你所要到的目录为终点，表现形式如： usr/local/bin （这里假设你当前目录为根目录）。你可能注意到，我们表示相对路径实际并没有加上表示当前目录的那个 . ，而是直接以目录名开头，因为这个 usr 目录为 / 目录下的子目录，是可以省略这个 . 的（以后会讲到一个类似不能省略的情况）；如果是当前目录的上一级目录，则需要使用 .. ，比如你当前目录为 /home/shiyanlou 目录下，根目录就应该表示为 ../../ ，表示上一级目录（ home 目录）的上一级目录（ / 目录）。\n4. 新建 1）新建空白文件touch命令 1 2 3 touch file{1..10}.txt#创建十个文件 #用法：touch 文件名 #没有的文件会创建一个新文件，若当前目录存在同名文件，则 touch 命令，则会更改该文件夹的时间戳而不是新建文件。 2）新建目录与多级目录mkdir 可用选项：\n-m: 对新建目录设置存取权限，也可以用 chmod 命令设置; -p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。 （1）当前工作目录下创建名为 t的文件夹\n1 mkdir t （2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建：\n1 mkdir -p /tmp/test/t1/t 5. 复制 1）复制文件cp命令 注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！\n1 2 3 -i 提示 -r 复制目录及目录内所有项目 -a 复制的文件与原文件时间一样 （1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。\n1 cp -ai a.txt test下 （2）为 a.txt 建议一个链接（快捷方式）\n1 cp -s a.txt link_a.txt 2）复制目录cp命令 添加参数-r即可\n6. 删除 1）删除文件rm命令 使用 rm（remove files or directories）命令删除一个文件：\n1 rm test 有时候你会遇到想要删除一些为只读权限的文件，直接使用 rm 删除会显示一个提示，如下：\n你如果想忽略这提示，直接删除文件，可以使用 -f 参数强制删除：\n1 rm -f test 2）删除目录rm命令 跟复制目录一样，要删除一个目录，也需要加上 -r 或 -R 参数：\n1 rm -r family 遇到权限不足删除不了的目录也可以和删除文件一样加上 -f 参数：\n1 rm -rf family 7. 移动文件与文件重命名 移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。\n当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。\n（1）将文件 test.log 重命名为 test1.txt\n1 mv test.log test1.txt （2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中\n1 mv llog1.txt log2.txt log3.txt /test3 （3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖\n1 mv -i log1.txt log2.txt （4）移动当前文件夹下的所有文件到上一级目录\n1 mv * ../ 8. 查看 1) cat，tac 和 nl 命令查看文件内容 前两个命令都是用来打印文件内容到标准输出（终端），其中 cat 为正序显示，tac 为倒序显示。\n标准输入输出：当我们执行一个 shell 命令行时通常会自动打开三个标准文件，即标准输入文件（stdin），默认对应终端的键盘、标准输出文件（stdout）和标准错误输出文件（stderr），后两个文件都对应被重定向到终端的屏幕，以便我们能直接看到输出内容。进程将从标准输入文件中得到输入数据，将正常输出数据输出到标准输出文件，而将错误信息送到标准错误文件中。\n比如我们要查看之前从 /etc 目录下拷贝来的 passwd 文件：\n1 2 3 cd /home/shiyanlou cp /etc/passwd passwd cat passwd 可以加上 -n 参数显示行号：\n1 cat -n passwd nl 命令，添加行号并打印，这是个比 cat -n 更专业的行号打印命令。\n这里简单列举它的常用的几个参数：\n1 2 3 4 5 6 7 8 -b : 指定添加行号的方式，主要有两种： -b a:表示无论是否为空行，同样列出行号(\u0026#34;cat -n\u0026#34;就是这种方式) -b t:只列出非空行的编号并列出（默认为这种方式） -n : 设置行号的样式，主要有三种： -n ln:在行号字段最左端显示 -n rn:在行号字段最右边显示，且不加 0 -n rz:在行号字段最右边显示，且加 0 -w : 行号字段占用的位数(默认为 6 位) 你会发现使用这几个命令，默认的终端窗口大小，一屏显示不完文本的内容，得用鼠标拖动滚动条或者滑动滚轮才能继续往下翻页，要是可以直接使用键盘操作翻页就好了，那么你就可以使用下面要介绍的命令。\n2) more 和 less 命令分页查看文件内容 如果说上面的 cat 是用来快速查看一个文件的内容的，那么这个 more 和 less 就是天生用来\u0026quot;阅读\u0026quot;一个文件的内容的，比如说 man 手册内部就是使用的 less 来显示内容。其中 more 命令比较简单，只能向一个方向滚动，而 less 为基于 more 和 vi （一个强大的编辑器，我们有单独的课程来让你学习）开发，功能更强大。less 的使用基本和 more 一致，具体使用请查看 man 手册，这里只介绍 more 命令的使用。\n使用 more 命令打开 passwd 文件：\n1 more passwd 打开后默认只显示一屏内容，终端底部显示当前阅读的进度。可以使用 Enter 键向下滚动一行，使用 Space 键向下滚动一屏，按下 h 显示帮助，q 退出。\n3) head 和 tail 命令查看文件内容 这两个命令，那些性子比较急的人应该会喜欢，因为它们一个是只查看文件的头几行（默认为 10 行，不足 10 行则显示全部）和尾几行。还是拿 passwd 文件举例，比如当我们想要查看最近新增加的用户，那么我们可以查看这个 /etc/passwd 文件，不过我们前面也看到了，这个文件里面一大堆乱糟糟的东西，看起来实在费神啊。因为系统新增加一个用户，会将用户的信息添加到 passwd 文件的最后，那么这时候我们就可以使用 tail 命令了：\n1 tail /etc/passwd 甚至更直接的只看一行， 加上 -n 参数，后面紧跟行数：\n1 tail -n 1 /etc/passwd 关于 tail 命令，不得不提的还有它一个很牛的参数 -f，这个参数可以实现不停地读取某个文件的内容并显示。这可以让我们动态查看日志，达到实时监视的目的。\n用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。\n常用参数：\n1 2 -f 循环读取（常用于查看递增的日志文件） -n\u0026lt;行数\u0026gt; 显示行数（从后向前） （1）循环读取逐渐增加的文件内容\n1 ping 127.0.0.1 \u0026gt; ping.log \u0026amp; 后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。\n1 tail -f ping.log （查看日志）\n4) file命令查看文件类型 1 file /bin/ls 说明这是一个可执行文件，运行在 64 位平台，并使用了动态链接文件（共享库）。\n与 Windows 不同的是，如果你新建了一个 shiyanlou.txt 文件，Windows 会自动把它识别为文本文件，而 file 命令会识别为一个空文件。这个前面我提到过，在 Linux 中文件的类型不是根据文件后缀来判断的。当你在文件里输入内容后才会显示文件类型。\n9. 编辑文件 在 Linux 下面编辑文件通常我们会直接使用专门的命令行编辑器比如（emacs，vim，nano），由于涉及 Linux 上的编辑器的内容比较多，且非常重要。\n五、环境变量与文件查找 1. 环境变量 /etc/bashrc（有的 Linux 没有这个文件） 和 /etc/profile ，它们分别存放的是 shell 变量和环境变量。还有要注意区别的是每个用户目录下的一个隐藏文件：\n1 2 3 # .profile 可以用 ls -a 查看 cd /home/shiyanlou ls -a 这个 .profile 只对当前用户永久生效。因为它保存在当前用户的 Home 目录下，当切换用户时，工作目录可能一并被切换到对应的目录中，这个文件就无法生效。而写在 /etc/profile 里面的是对所有用户永久生效，所以如果想要添加一个全局、永久生效的环境变量（对所有用户都起作用的环境变量），只需要打开 /etc/profile，添加环境变量即可。\n1）临时生效 使用 export 命令行声明即可，变量在关闭当前 shell 时失效，生效范围是当前的shell，其他shell窗口不生效。\n2）永久生效 需要修改配置文件，变量永久生效，生效范围是全局生效。前面我们在 Shell 中修改了一个配置脚本文件之后（比如 zsh 的配置文件 home 目录下的 .zshrc），每次都要退出终端重新打开甚至重启主机之后其才能生效，很是麻烦，我们可以使用 source 命令来让其立即生效，如：\n1 2 cd /home/shiyanlou source .zshrc 2. linux上的可执行文件 windows上的可执行文件以.exe结尾，linux上则是以文件权限的方式展现是否可以执行。使用touch创建的空白文件夹默认权限是-rw-rw-r--不论是用户还是用户组还是其他用户及用户组都不具备可执行权，先编辑指定内容，再编译响应程序获得，再使用chmod修改编译文件的文件权限为可执行，然后直接调用可执行文件的文件名就可以运行该程序。此过程相当于windows上的编码、编译、生成可执行文件、点击运行即可\n你可能很早之前就有疑问，我们在 Shell 中输入一个命令，Shell 是怎么知道去哪找到这个命令然后执行的呢？这是通过环境变量 PATH 来进行搜索的，熟悉 Windows 的用户可能知道 Windows 中的也是有这么一个 PATH 环境变量。这个 PATH 里面就保存了 Shell 中执行的命令的搜索路径。\n查看 PATH 环境变量的内容：\n1 echo $PATH 默认情况下你会看到如下输出：\n1 /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games 如果你还记得 Linux 目录结构那一节的内容，你就应该知道上面这些目录下放的是哪一类文件了。通常这一类目录下放的都是可执行文件，当我们在 Shell 中执行一个命令时，系统就会按照 PATH 中设定的路径按照顺序依次到目录中去查找，如果存在同名的命令，则执行先找到的那个。\n创建一个 Shell 脚本文件，你可以使用 gedit，vim，sublime 等工具编辑。如果你是直接复制的话，建议使用 gedit 或者 sublime，否则可能导致代码缩进混乱。\n1 2 3 cd /home/shiyanlou touch hello_shell.sh gedit hello_shell.sh 在脚本中添加如下内容，保存并退出。\n注意不要省掉第一行，这不是注释，有用户反映有语法错误，就是因为没有了第一行。\n1 2 3 4 5 6 7 #!/bin/bash for ((i=0; i\u0026lt;10; i++));do echo \u0026#34;hello shell\u0026#34; done exit 0 为文件添加可执行权限，否则执行会报错没有权限：\n1 chmod 755 hello_shell.sh 执行脚本：\n1 2 cd /home/shiyanlou ./hello_shell.sh 创建一个 C 语言 hello world 程序：\n1 2 cd /home/shiyanlou gedit hello_world.c 输入如下内容，同样不能省略第一行。\n1 2 3 4 5 6 7 #include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;hello world!\\n\u0026#34;); return 0; } 保存后使用 gcc 生成可执行文件：\n1 gcc -o hello_world hello_world.c gcc 生成二进制文件默认具有可执行权限，不需要修改。\n在 /home/shiyanlou 家目录创建一个 mybin 目录，并将上述 hello_shell.sh 和 hello_world 文件移动到其中：\n1 2 3 cd /home/shiyanlou mkdir mybin mv hello_shell.sh hello_world mybin/ 现在你可以在 mybin 目录中分别运行你刚刚创建的两个程序：\n1 2 3 cd mybin ./hello_shell.sh ./hello_world 回到上一级目录，也就是 shiyanlou 家目录，当再想运行那两个程序时，会发现提示命令找不到，除非加上命令的完整路径，但那样很不方便，如何做到像使用系统命令一样执行自己创建的脚本文件或者程序呢？那就要将命令所在路径添加到 PATH 环境变量了。\n3. 搜索文件 1）find命令足矣 用于在文件树中查找文件，并作出相应的处理。\n命令格式：\n1 find pathname -options [-print -exec -ok ...] 命令参数：\n1 2 3 4 pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print：find命令将匹配的文件输出到标准输出。 -exec：find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为\u0026#39;command\u0026#39; { } \\;，注意{ }和\\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项：\n1 2 3 4 5 6 7 8 9 10 11 -name 按照文件名查找文件 -perm 按文件权限查找文件 -user 按文件属主查找文件 -group 按照文件所属的组来查找文件。 -type 查找某一类型的文件，诸如： b - 块设备文件 d - 目录 c - 字符设备文件 l - 符号链接文件 p - 管道文件 f - 普通文件 （1）查找 48 小时内修改过的文件\n1 find -atime -2 （2）在当前目录查找 以 .log 结尾的文件。. 代表当前目录\n1 find ./ -name \u0026#39;*.log\u0026#39; （3）查找 /opt 目录下 权限为 777 的文件\n1 find /opt -perm 777 （4）查找大于 1K 的文件\n1 find -size +1000c\t(5)查找等于 1000 字符的文件\n1 find -size 1000c -exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。\n2）忽略which、whereis以及locate 六、文件打包与解压缩 1. 概念储备 在 Windows 上最常见的不外乎这两种 *.zip，*.7z 后缀的压缩文件。而在 Linux 上面常见的格式除了以上两种外，还有 .rar，*.gz，*.xz，*.bz2，*.tar，*.tar.gz，*.tar.xz，*.tar.bz2，简单介绍如下：\n文件后缀名 说明 *.zip zip 程序打包压缩的文件 *.rar rar 程序压缩的文件 *.7z 7zip 程序压缩的文件 *.tar tar 程序打包，未压缩的文件 *.gz gzip 程序（GNU zip）压缩的文件 *.xz xz 程序压缩的文件 *.bz2 bzip2 程序压缩的文件 *.tar.gz tar 打包，gzip 程序压缩的文件 *.tar.xz tar 打包，xz 程序压缩的文件 *tar.bz2 tar 打包，bzip2 程序压缩的文件 *.tar.7z tar 打包，7z 程序压缩的文件 这么多个命令，不过我们一般只需要掌握几个命令即可，包括 zip，tar。下面会依次介绍这几个命令及对应的解压命令。\n2. zip 压缩打包程序 使用 zip 打包文件夹，注意输入完整的参数和路径： 1 2 3 4 cd /home/shiyanlou zip -r -q -o shiyanlou.zip /home/shiyanlou/Desktop du -h shiyanlou.zip file shiyanlou.zip 上面命令将目录 /home/shiyanlou/Desktop 打包成一个文件，并查看了打包后文件的大小和类型。第一行命令中，-r 参数表示递归打包包含子目录的全部内容，-q 参数表示为安静模式，即不向屏幕输出信息，-o，表示输出文件，需在其后紧跟打包输出文件名。后面使用 du 命令查看打包后文件的大小（后面会具体说明该命令）。\n设置压缩级别为 9 和 1（9 最大，1 最小），重新打包： 1 2 zip -r -9 -q -o shiyanlou_9.zip /home/shiyanlou/Desktop -x ~/*.zip zip -r -1 -q -o shiyanlou_1.zip /home/shiyanlou/Desktop -x ~/*.zip 这里添加了一个参数用于设置压缩级别 -[1-9]，1 表示最快压缩但体积大，9 表示体积最小但耗时最久。最后那个 -x 是为了排除我们上一次创建的 zip 文件，否则又会被打包进这一次的压缩文件中，注意：这里只能使用绝对路径，否则不起作用。\n我们再用 du 命令分别查看默认压缩级别、最低、最高压缩级别及未压缩的文件的大小：\n1 du -h -d 0 *.zip ~ | sort 通过 man 手册可知：\n-h， \u0026ndash;human-readable（顾名思义，你可以试试不加的情况） -d， \u0026ndash;max-depth（所查看文件的深度） 这样一目了然，理论上来说默认压缩级别应该是最高的，但是由于文件不大，这里的差异不明显（几乎看不出差别），不过你在环境中操作之后看到的压缩文件大小可能跟图上的有些不同，因为系统在使用过程中，会随时生成一些缓存文件在当前用户的家目录中，这对于我们学习命令使用来说，是无关紧要的，可以忽略这些不同。\n创建加密 zip 包 使用 -e 参数可以创建加密压缩包：\n1 zip -r -e -o shiyanlou_encryption.zip /home/shiyanlou/Desktop 注意： 关于 zip 命令，因为 Windows 系统与 Linux/Unix 在文本文件格式上的一些兼容问题，比如换行符（为不可见字符），在 Windows 为 CR+LF（Carriage-Return+Line-Feed：回车加换行），而在 Linux/Unix 上为 LF（换行），所以如果在不加处理的情况下，在 Linux 上编辑的文本，在 Windows 系统上打开可能看起来是没有换行的。如果你想让你在 Linux 创建的 zip 压缩文件在 Windows 上解压后没有任何问题，那么你还需要对命令做一些修改：\n1 zip -r -l -o shiyanlou.zip /home/shiyanlou/Desktop 需要加上 -l 参数将 LF 转换为 CR+LF 来达到以上目的。\n3. 使用 unzip 命令解压缩 zip 文件 将 shiyanlou.zip 解压到当前目录：\n1 unzip shiyanlou.zip 使用安静模式，将文件解压到指定目录：\n1 unzip -q shiyanlou.zip -d ziptest 上述指定目录不存在，将会自动创建。如果你不想解压只想查看压缩包的内容你可以使用 -l 参数：\n1 unzip -l shiyanlou.zip 注意： 使用 unzip 解压文件时我们同样应该注意兼容问题，不过这里我们关心的不再是上面的问题，而是中文编码的问题，通常 Windows 系统上面创建的压缩文件，如果有有包含中文的文档或以中文作为文件名的文件时默认会采用 GBK 或其它编码，而 Linux 上面默认使用的是 UTF-8 编码，如果不加任何处理，直接解压的话可能会出现中文乱码的问题（有时候它会自动帮你处理），为了解决这个问题，我们可以在解压时指定编码类型。\n使用 -O（英文字母，大写 o）参数指定编码类型：\n1 unzip -O GBK 中文压缩文件.zip 4. tar 打包、其压缩与解压工具 tar可以实现*.tar.gz、*.tar.xz和*tar.bz2的压缩（xz、gzip 及 bzip2）,zip还是用zip压缩工具\n用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。\n弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件\n注意：\n-f参数后面需要紧跟文件名，否则无效 使用对应压缩方式打包时，应当加上对应方式压缩解包或者查看 常用参数：\n1 2 3 4 5 6 7 8 9 10 -c 建立新的压缩文件 -f 指定压缩文件名,该参数后面需要紧跟文件名，否则不会生效 -r 添加文件到已经压缩文件包中 -u 添加改了和现有的文件到压缩包中 -x 从压缩包中抽取文件，及解压 -t 显示压缩文件中的内容 -z 支持gzip压缩 -j 支持bzip2压缩 -Z 支持compress解压文件 -v 显示操作过程 压缩文件格式 参数 *.tar.gz -z *.tar.xz -J *tar.bz2 -j 有关 gzip 及 bzip2 压缩:\n1 2 3 4 5 gzip 实例：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz 对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz bz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2 对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2 （1）将文件全部打包成 tar 包（未压缩）\n1 tar -cvf log.tar 1.log,2.log 或tar -cvf log.* （2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩\n1 tar -zcvf /tmp/etc.tar.gz /etc （3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的）\n1 tar -ztvf /tmp/etc.tar.gz （4）要压缩打包 /home, /etc ，但不要 /home/dmtsai\n1 tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc 七、文件系统操作与磁盘管理 1. df命令 显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。\n默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示：\n1 2 3 4 5 6 -a 全部文件系统列表 -h 以方便阅读的方式显示信息 -i 显示inode信息 -k 区块为1024字节 -l 只显示本地磁盘 -T 列出文件系统类型 （1）显示磁盘使用情况\n1 df -l （2）以易读方式列出所有文件系统及其类型\n1 df -haT 2. du 命令 du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看：\n命令格式：\n1 du [选项] [文件] 常用参数：\n1 2 3 4 5 6 7 -a 显示目录中所有文件大小 -k 以KB为单位显示文件大小 -m 以MB为单位显示文件大小 -g 以GB为单位显示文件大小 -h 以易读方式显示文件大小 -s 仅显示总计 -c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和 （1）以易读方式显示文件夹内及子文件夹大小\n1 du -h scf/ （2）以易读方式显示文件夹内所有文件大小\n1 du -ah scf/ （3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和\n1 du -hc test/ scf/ （4）输出当前目录下各个子目录所使用的空间\n1 du -hc --max-depth=1 scf/ 八、linux下的帮助命令 1. 内建命令与外部命令 一些查看帮助的工具在内建命令与外建命令上是有区别对待的。\n内建命令实际上是 shell 程序的一部分，其中包含的是一些比较简单的 Linux 系统命令，这些命令是写在 bash 源码的 builtins 里面的，由 shell 程序识别并在 shell 程序内部完成运行，通常在 Linux 系统加载运行时 shell 就被加载并驻留在系统内存中。而且解析内部命令 shell 不需要创建子进程，因此其执行速度比外部命令快。比如：history、cd、exit 等等。\n外部命令是 Linux 系统中的实用程序部分，因为实用程序的功能通常都比较强大，所以其包含的程序量也会很大，在系统加载时并不随系统一起被加载到内存中，而是在需要时才将其调入内存。虽然其不包含在 shell 中，但是其命令执行过程是由 shell 程序控制的。外部命令是在 Bash 之外额外安装的，通常放在/bin，/usr/bin，/sbin，/usr/sbin 等等。比如：ls、vi 等。\n简单来说就是：一个是天生自带的天赋技能，一个是后天得来的附加技能。我们可以使用　type 命令来区分命令是内建的还是外部的。例如这两个得出的结果是不同的\n1 2 3 type exit type vim 得到的是两种结果，若是对 ls 你还能得到第三种结果\n1 2 3 4 5 6 # 得到这样的结果说明是内建命令，正如上文所说内建命令都是在 bash 源码中的 builtins 的.def中 xxx is a shell builtin # 得到这样的结果说明是外部命令，正如上文所说，外部命令在/usr/bin or /usr/sbin等等中 xxx is /usr/bin/xxx # 若是得到alias的结果，说明该指令为命令别名所设定的名称； xxx is an alias for xx --xxx 2. 帮助命令的使用 1）help命令 只用于查看内建命令的使用方法，无法运用于外部命令的查看，外部命令使用参数往往使用--help查看帮助\n2）man命令 得到的内容比用help更多更详细，而且man没有内建与外部命令的区分，因为man工具是显示系统手册页中的内容\nman手册章节含义：\n章节数 说明 1 Standard commands （标准命令） 2 System calls （系统调用） 3 Library functions （库函数） 4 Special devices （设备说明） 5 File formats （文件格式） 6 Games and toys （游戏和娱乐） 7 Miscellaneous （杂项） 8 Administrative Commands （管理员命令） 9 其他（Linux 特定的）， 用来存放内核例行程序的文档。 3）info命令 info 来自自由软件基金会的 GNU 项目，是 GNU 的超文本帮助系统，能够更完整的显示出 GNU 信息。所以得到的信息当然更多\n如果没有安装info，可以执行一下程序安装\n1 2 3 4 5 # 安装 info sudo apt-get update sudo apt-get install info # 查看 ls 命令的 info info ls 九、Linux 任务计划 crontab 1. crontab的简介 crontab 命令常见于 Unix 和类 Unix 的操作系统之中（Linux 就属于类 Unix 操作系统），用于设置周期性被执行的指令。\ncrontab 命令从输入设备读取指令，并将其存放于 crontab 文件中，以供之后读取和执行。通常，crontab 储存的指令被守护进程激活，crond 为其守护进程，crond 常常在后台运行，每一分钟会检查一次是否有预定的作业需要执行。\n通过 crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell 脚本。时间间隔的单位可以是分钟、小时、日、月、周的任意组合。\n这里我们看一看 crontab 的格式：\n1 2 3 4 5 6 7 8 # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed 2. crontab的准备 日志监控 crontab 在本实验环境中需要做一些特殊的准备，首先我们会启动 rsyslog，以便我们可以通过日志中的信息来了解我们的任务是否真正的被执行了（在本实验环境中需要手动启动，而在自己本地中 Ubuntu 会默认自行启动不需要手动启动）。\n1 2 sudo apt-get install -y rsyslog sudo service rsyslog start 启动crontab 在本实验环境中 crontab 也是不被默认启动的，同时不能在后台由 upstart 来管理，所以需要我们来启动它:\n1 sudo cron －f \u0026amp; 3. crontab的使用 1）crontab -e命令 我们通过下面一个命令来添加一个计划任务：\n1 crontab -e 第一次启动会出现这样一个画面，这是让我们选择编辑的工具，选择第二个基本的 vim 就可以了。\n编写进程指定时间以及命令 crontab文件的含义：\n用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：\nminute hour day month week command\n其中：\n1 2 3 4 5 6 7 8 9 10 11 minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符：\n1 2 3 4 5 6 7 8 9 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 在了解命令格式之后，我们通过这样的一个例子来完成一个任务的添加，在文档的最后一排加上这样一排命令，该任务是每分钟我们会在/home/shiyanlou 目录下创建一个以当前的年月日时分秒为名字的空白文件 1 */1 * * * * touch /home/shiyanlou/$(date +\\%Y\\%m\\%d\\%H\\%M\\%S) 注意：\n“ % ” 在 crontab 文件中，有结束命令行、换行、重定向的作用，前面加 ” \\ ” 符号转义，否则，“ % ” 符号将执行其结束命令行或者换行的作用，并且其后的内容会被做为标准输入发送给前面的命令。\n添加成功后我们会得到最后一排 installing new crontab 的一个提示：\n2）crontab -l命令 当然我们也可以通过这样的一个指令来查看我们添加了哪些任务：\n1 crontab -l 通过图中的显示，我们也可以看出，我们正确的保存并且添加成功了该任务的：\n3）检查cron的守护进程是否启动ps aux | grep cron 虽然我们添加了任务，但是如果 cron 的守护进程并没有启动，它根本都不会监测到有任务，当然也就不会帮我们执行，我们可以通过以下 2 种方式来确定我们的 cron 是否成功的在后台启动，默默的帮我们做事，若是没有就得执行上文准备中的第二步了。\n1 2 3 4 5 ps aux | grep cron # or pgrep cron 通过下图可以看到任务在创建之后，执行了几次，生成了一些文件，且每分钟生成一个：\n4）动态查看日志tail -f 我们通过这样一个命令可以查看到执行任务命令之后在日志中的信息反馈：\n1 sudo tail -f /var/log/syslog 从图中我们可以看到分别在 13 点 28、29、30 分的 01 秒为我们在 shiyanlou 用户的家目录下创建了文件。\n5）crontab -r命令 当我们并不需要这个任务的时候我们可以使用这么一个命令去删除任务：\n1 crontab -r 通过图中我们可以看出我们删除之后再查看任务列表，系统已经显示该用户并没有任务哦。\n4. crontab的深入 每个用户使用 crontab -e 添加计划任务，都会在 /var/spool/cron/crontabs 中添加一个该用户自己的任务文档，这样目的是为了隔离。\n如果是系统级别的定时任务，需要 root 权限执行的任务应该怎么处理？\n只需要使用 sudo 编辑 /etc/crontab 文件就可以。\ncron 服务监测时间最小单位是分钟，所以 cron 会每分钟去读取一次 /etc/crontab 与 /var/spool/cron/crontabs 里面的內容。\n在 /etc 目录下，cron 相关的目录有下面几个：\n每个目录的作用：\n/etc/cron.daily，目录下的脚本会每天执行一次，在每天的 6 点 25 分时运行； /etc/cron.hourly，目录下的脚本会每个小时执行一次，在每小时的 17 分钟时运行； /etc/cron.monthly，目录下的脚本会每月执行一次，在每月 1 号的 6 点 52 分时运行； /etc/cron.weekly，目录下的脚本会每周执行一次，在每周第七天的 6 点 47 分时运行； 系统默认执行时间可以根据需求进行修改。\n十、命令执行顺序控制与管道 1. 顺序执行多条命令:; 多条命令使用分号隔开，达到顺序执行的目的。\n1 sudo apt-get update;sudo apt-get install some-tool;some-tool # 让它自己运行 2. 有选择执行多条命令\u0026amp;\u0026amp;,||命令 \u0026amp;\u0026amp;表示如果前面的命令执行结果（不是表示终端输出的内容，而是表示命令执行状态的结果）返回 0 则执行后面的，否则不执行，你可以从 $? 环境变量获取上一次命令的返回结果：\n学习过 C 语言的用户应该知道在 C 语言里面 \u0026amp;\u0026amp; 表示逻辑与，而且还有一个 || 表示逻辑或，同样 Shell 也有一个 ||，它们的区别就在于，shell 中的这两个符号除了也可用于表示逻辑与和或之外，就是可以实现这里的命令执行顺序的简单控制。|| 在这里就是与 \u0026amp;\u0026amp; 相反的控制效果，当上一条命令执行结果为 ≠0(\\$?≠0) 时则执行它后面的命令：\n1 which cowsay\u0026gt;/dev/null || echo \u0026#34;cowsay has not been install, please run \u0026#39;sudo apt-get install cowsay\u0026#39; to install\u0026#34; 除了上述基本的使用之外，我们还可以结合着 \u0026amp;\u0026amp; 和 || 来实现一些操作，比如：\n1 2 which cowsay\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#34;exist\u0026#34; || echo \u0026#34;not exist\u0026#34; #注意顺序：\u0026amp;\u0026amp;在前，表示前面执行结果不为0表示执行后面的操作，如果为0，不执行\u0026amp;\u0026amp;后面的而执行||后面的命令；||在前，表示如果命令返回0时，表示直接执行||后的语句 我画个流程图来解释一下上面的流程：\n3. 管道 管道是一种通信机制，通常用于进程间的通信（也可通过 socket 进行网络通信），它表现出来的形式就是将前面每一个进程的输出（stdout）直接作为下一个进程的输入（stdin）。\n管道又分为匿名管道和具名管道（这里将不会讨论在源程序中使用系统调用创建并使用管道的情况，它与命令行的管道在内核中实际都是采用相同的机制）。我们在使用一些过滤程序时经常会用到的就是匿名管道，在命令行中由 | 分隔符表示，| 在前面的内容中我们已经多次使用到了。具名管道简单的说就是有名字的管道，通常只会在源程序中用到具名管道。\n1）管道的含义适用| 先试用一下管道，比如查看 /etc 目录下有哪些文件和目录，使用 ls 命令来查看：\n1 ls -al /etc 有太多内容，屏幕不能完全显示，这时候可以使用滚动条或快捷键滚动窗口来查看。不过这时候可以使用管道：\n1 ls -al /etc | less 通过管道将前一个命令(ls)的输出作为下一个命令(less)的输入，然后就可以一行一行地看。\n2）cut 命令，打印每一行的某一字段 打印 /etc/passwd 文件中以 : 为分隔符的第 1 个字段和第 6 个字段分别表示用户名和其家目录：\n1 cut /etc/passwd -d \u0026#39;:\u0026#39; -f 1,6 打印 /etc/passwd 文件中每一行的前 N 个字符：\n1 2 3 4 5 6 7 8 # 前五个（包含第五个） cut /etc/passwd -c -5 # 前五个之后的（包含第五个） cut /etc/passwd -c 5- # 第五个 cut /etc/passwd -c 5 # 2 到 5 之间的（包含第五个） cut /etc/passwd -c 2-5 3）grep 命令，在文本中或 stdin 中查找匹配字符串 grep 命令是很强大的，也是相当常用的一个命令，它结合正则表达式可以实现很复杂却很高效的匹配和查找，不过在学习正则表达式之前，这里介绍它简单的使用，而关于正则表达式后面将会有单独一小节介绍到时会再继续学习 grep 命令和其他一些命令。\ngrep 命令的一般形式为：\n1 grep [命令选项]... 用于匹配的表达式 [文件]... 还是先体验一下，我们搜索/home/shiyanlou目录下所有包含\u0026quot;shiyanlou\u0026quot;的文本文件，并显示出现在文本中的行号：\n1 grep -rnI \u0026#34;shiyanlou\u0026#34; ~ -r 参数表示递归搜索子目录中的文件，-n 表示打印匹配项行号，-I 表示忽略二进制文件。这个操作实际没有多大意义，但可以感受到 grep 命令的强大与实用。\n当然也可以在匹配字段中使用正则表达式，下面简单的演示：\n1 2 # 查看环境变量中以 \u0026#34;yanlou\u0026#34; 结尾的字符串 export | grep \u0026#34;.*yanlou$\u0026#34; 其中$就表示一行的末尾。\n4）wc 命令，简单小巧的计数工具 wc 命令用于统计并输出一个文件中行、单词和字节的数目，比如输出 /etc/passwd 文件的统计信息：\n1 wc /etc/passwd 分别只输出行数、单词数、字节数、字符数和输入文本中最长一行的字节数：\n1 2 3 4 5 6 7 8 9 10 # 行数 wc -l /etc/passwd # 单词数 wc -w /etc/passwd # 字节数 wc -c /etc/passwd # 字符数 wc -m /etc/passwd # 最长行字节数 wc -L /etc/passwd 注意：对于西文字符来说，一个字符就是一个字节，但对于中文字符一个汉字是大于 或等于2 个字节的，具体数目是由字符编码决定的。\n再来结合管道来操作一下，下面统计 /etc 下面所有目录数：\n1 ls -dl /etc/*/ | wc -l 5）sort 排序命令 这个命令前面我们也是用过多次，功能很简单就是将输入按照一定方式排序，然后再输出，它支持的排序有按字典排序，数字排序，按月份排序，随机排序，反转排序，指定特定字段进行排序等等。\n默认为字典排序：\n1 cat /etc/passwd | sort 反转排序：\n1 cat /etc/passwd | sort -r 按特定字段排序：\n1 cat /etc/passwd | sort -t\u0026#39;:\u0026#39; -k 3 上面的-t参数用于指定字段的分隔符，这里是以\u0026rdquo;:\u0026ldquo;作为分隔符；-k 字段号用于指定对哪一个字段进行排序。这里/etc/passwd文件的第三个字段为数字，默认情况下是以字典序排序的，如果要按照数字排序就要加上-n参数：\n1 cat /etc/passwd | sort -t\u0026#39;:\u0026#39; -k 3 -n 注意观察第二个冒号后的数字： 6）uniq 去重命令 uniq 命令可以用于过滤或者输出重复行。\n过滤重复行 我们可以使用 history 命令查看最近执行过的命令（实际为读取 ${SHELL}_history 文件，如我们环境中的 .zsh_history 文件），不过你可能只想查看使用了哪个命令而不需要知道具体干了什么，那么你可能就会要想去掉命令后面的参数然后去掉重复的命令：\n1 history | cut -c 8- | cut -d \u0026#39; \u0026#39; -f 1 | uniq 然后经过层层过滤，你会发现确是只输出了执行的命令那一列，不过去重效果好像不明显，仔细看你会发现它确实去重了，只是不那么明显，之所以不明显是因为 uniq 命令只能去连续重复的行，不是全文去重，所以要达到预期效果，我们先排序：\n1 2 3 history | cut -c 8- | cut -d \u0026#39; \u0026#39; -f 1 | sort | uniq # 或者 history | cut -c 8- | cut -d \u0026#39; \u0026#39; -f 1 | sort -u 这就是 Linux/UNIX 哲学吸引人的地方，大繁至简，一个命令只干一件事却能干到最好。\n输出重复行 1 2 3 4 # 输出重复过的行（重复的只输出一个）及重复次数 history | cut -c 8- | cut -d \u0026#39; \u0026#39; -f 1 | sort | uniq -dc # 输出所有重复的行 history | cut -c 8- | cut -d \u0026#39; \u0026#39; -f 1 | sort | uniq -D 文本处理命令还有很多，下一节将继续介绍一些常用的文本处理的命令。\n十一、简单的文本处理 1. tr命令 tr 命令可以用来删除一段文本信息中的某些文字。或者将其进行转换。\n使用方式\n1 tr [option]...SET1 [SET2] 常用的选项有\n选项 说明 -d 删除和 set1 匹配的字符，注意不是全词匹配也不是按字符顺序匹配 -s 去除 set1 指定的在输入文本中连续并重复的字符 操作举例\n1 2 3 4 5 6 7 # 删除 \u0026#34;hello shiyanlou\u0026#34; 中所有的\u0026#39;o\u0026#39;，\u0026#39;l\u0026#39;，\u0026#39;h\u0026#39; $ echo \u0026#39;hello shiyanlou\u0026#39; | tr -d \u0026#39;olh\u0026#39; # 将\u0026#34;hello\u0026#34; 中的ll，去重为一个l $ echo \u0026#39;hello\u0026#39; | tr -s \u0026#39;l\u0026#39; # 将输入文本，全部转换为大写或小写输出（正则表达式） $ echo \u0026#39;input some text here\u0026#39; | tr \u0026#39;[:lower:]\u0026#39; \u0026#39;[:upper:]\u0026#39; # 上面的\u0026#39;[:lower:]\u0026#39; \u0026#39;[:upper:]\u0026#39;你也可以简单的写作\u0026#39;[a-z]\u0026#39; \u0026#39;[A-Z]\u0026#39;，当然反过来将大写变小写也是可以的 更多 tr 的使用，你可以使用--help或者man tr获得。\n2. col命令 col 命令可以将Tab换成对等数量的空格键，或反转这个操作。\n使用方式\n1 col [option] 常用的选项有\n选项 说明 -x 将Tab转换为空格 -h 将空格转换为Tab（默认选项） 操作举例\n1 2 3 4 # 查看 /etc/protocols 中的不可见字符，可以看到很多 ^I ，这其实就是 Tab 转义成可见字符的符号 cat -A /etc/protocols # 使用 col -x 将 /etc/protocols 中的 Tab 转换为空格，然后再使用 cat 查看，你发现 ^I 不见了 cat /etc/protocols | col -x | cat -A 3. join命令 这个命令就是用于将两个文件中包含相同内容的那一行合并在一起。\n使用方式\n1 join [option]... file1 file2 常用的选项有\n选项 说明 -t 指定分隔符，默认为空格 -i 忽略大小写的差异 -1 指明第一个文件要用哪个字段来对比，默认对比第一个字段 -2 指明第二个文件要用哪个字段来对比，默认对比第一个字段 操作举例\n1 2 3 4 5 6 7 8 9 cd /home/shiyanlou # 创建两个文件 echo \u0026#39;1 hello\u0026#39; \u0026gt; file1 echo \u0026#39;1 shiyanlou\u0026#39; \u0026gt; file2 join file1 file2 # 将 /etc/passwd 与 /etc/shadow 两个文件合并，指定以\u0026#39;:\u0026#39;作为分隔符 sudo join -t\u0026#39;:\u0026#39; /etc/passwd /etc/shadow # 将 /etc/passwd 与 /etc/group 两个文件合并，指定以\u0026#39;:\u0026#39;作为分隔符，分别比对第4和第3个字段 sudo join -t\u0026#39;:\u0026#39; -1 4 /etc/passwd -2 3 /etc/group 4. paste命令 paste这个命令与join 命令类似，它是在不对比数据的情况下，简单地将多个文件合并一起，以Tab隔开。\n使用方式\n1 paste [option] file... 常用的选项有\n选项 说明 -d 指定合并的分隔符，默认为 Tab -s 不合并到一行，每个文件为一行 操作举例\n1 2 3 4 5 echo hello \u0026gt; file1 echo shiyanlou \u0026gt; file2 echo www.shiyanlou.com \u0026gt; file3 paste -d \u0026#39;:\u0026#39; file1 file2 file3 paste -s file1 file2 file3 十二、数据流重定向 重定向操作：\n1 2 3 echo \u0026#39;hello shiyanlou\u0026#39; \u0026gt; redirect echo \u0026#39;www.shiyanlou.com\u0026#39; \u0026gt;\u0026gt; redirect cat redirect 当然前面没有用到的 \u0026lt; 和 \u0026lt;\u0026lt; 操作也是没有问题的，如你理解的一样，它们的区别在于重定向的方向不一致而已，\u0026gt; 表示是从左到右，\u0026lt; 右到左。\nLinux 默认提供了三个特殊设备，用于终端的显示和输出，分别为 stdin（标准输入，对应于你在终端的输入），stdout（标准输出，对应于终端的输出），stderr（标准错误输出，对应于终端的输出）。\n文件描述符 设备文件 说明 0 /dev/stdin 标准输入 1 /dev/stdout 标准输出 2 /dev/stderr 标准错误 文件描述符：文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。\n1. 重定向与管道区别 我们可以这样使用这些文件描述符。例如默认使用终端的标准输入作为命令的输入和标准输出作为命令的输出：\n1 cat # 按 Ctrl+C 退出 将 cat 的连续输出（heredoc 方式）重定向到一个文件：\n1 2 3 4 5 6 7 8 9 10 11 mkdir Documents cat \u0026gt; Documents/test.c \u0026lt;\u0026lt;EOF #include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world\\n\u0026#34;); return 0; } EOF 将一个文件作为命令的输入，标准输出作为命令的输出：\n1 cat Documents/test.c 将 echo 命令通过管道传过来的数据作为 cat 命令的输入，将标准输出作为命令的输出：\n1 echo \u0026#39;hi\u0026#39; | cat 将 echo 命令的输出从默认的标准输出重定向到一个普通文件：\n1 2 echo \u0026#39;hello shiyanlou\u0026#39; \u0026gt; redirect cat redirect 初学者这里要注意不要将管道和重定向混淆，管道默认是连接前一个命令的输出到下一个命令的输入，而重定向通常是需要一个文件来建立两个命令的连接，你可以仔细体会一下上述第三个操作和最后两个操作的异同点。\n2. 标准错误重定向 重定向标准输出到文件，这是一个很实用的操作，另一个很实用的操作是将标准错误重定向，标准输出和标准错误都被指向伪终端的屏幕显示，所以我们经常看到的一个命令的输出通常是同时包含了标准输出和标准错误的结果的。比如下面的操作：\n1 2 3 4 5 # 使用cat 命令同时读取两个文件，其中一个存在，另一个不存在 cat Documents/test.c hello.c # 你可以看到除了正确输出了前一个文件的内容，还在末尾出现了一条错误信息 # 下面我们将输出重定向到一个文件 cat Documents/test.c hello.c \u0026gt; somefile 遗憾的是，这里依然出现了那条错误信息，这正是因为如我上面说的那样，标准输出和标准错误虽然都指向终端屏幕，实际它们并不一样。那有的时候我们就是要隐藏某些错误或者警告，那又该怎么做呢。这就需要用到我们前面讲的文件描述符了：\n1 2 3 4 # 将标准错误重定向到标准输出，再将标准输出重定向到文件，注意要将重定向到文件写到前面 cat Documents/test.c hello.c \u0026gt;somefile 2\u0026gt;\u0026amp;1 # 或者只用bash提供的特殊的重定向符号\u0026#34;\u0026amp;\u0026#34;将标准错误和标准输出同时重定向到文件 cat Documents/test.c hello.c \u0026amp;\u0026gt;somefilehell 注意你应该在输出重定向文件描述符前加上\u0026amp;，否则 shell 会当做重定向到一个文件名为 1 的文件中\n3. 使用 tee 命令同时重定向到多个文件 你可能还有这样的需求，除了需要将输出重定向到文件，也需要将信息打印在终端。那么你可以使用 tee 命令来实现：\n1 echo \u0026#39;hello shiyanlou\u0026#39; | tee hello 4. 永久重定向 当然不需要，我们可以使用 exec 命令实现永久重定向。exec 命令的作用是使用指定的命令替换当前的 Shell，即使用一个进程替换当前进程，或者指定新的重定向：\n1 2 3 4 5 6 7 8 # 先开启一个子 Shell zsh # 使用exec替换当前进程的重定向，将标准输出重定向到一个文件 exec 1\u0026gt;somefile # 后面你执行的命令的输出都将被重定向到文件中，直到你退出当前子shell，或取消exec的重定向（后面将告诉你怎么做） ls exit cat somefile 十三、软件包管理（centos、乌班图） 包的组成：\r二进制文件、库文件、配置文件、帮助文件\r程序包管理器：\rdebian： deb文件, dpkg包管理器\rredhat： rpm文件, rpm包管理器\rrpm：Redhat Package Manager\rRPM Package Manager\r包之间：可能存在依赖关系，甚至循环依赖\r解决依赖包管理工具：\ryum：rpm包管理器的前端工具\rapt-get：deb包管理器前端工具\rzypper: suse上的rpm前端管理工具\rdnf: Fedora 18+ rpm包管理器前端管理工具\r查看二进制程序所依赖的库文件：\rldd /PATH/TO/BINARY_FILE\r管理及查看本机装载的库文件：\rldconfig :加载配置文件中指定的库文件\r/sbin/ldconfig -p :显示本机已经缓存的所有可用库文件名及文件路径映射关系\r配置文件: /etc/ld.so.conf /etc/ld.so.conf.d/*.conf\r缓存文件：/etc/ld.so.cache 程序包管理器：\r功能：将编译好的应用程序的各组成文件打包一个或几个程序包文件，从而方便快捷地实现程序包的安装、卸载、查询、升级和校验等管理操作\r数据库(公共)：/var/lib/rpm\r程序包名称及版本\r依赖关系\r功能说明\r包安装后生成的各文件路径及校验码信息\r获取程序包的途径：\r(1) 系统发版的光盘或官方的服务器\rCentOS镜像：\rhttps://www.centos.org/download/\rhttp://mirrors.aliyun.com\rhttp://mirrors.sohu.com\rhttp://mirrors.163.com\r(2) 项目官方站点\r(3) 第三方组织：\rFedora-EPEL：\rExtra Packages for Enterprise Linux\rRpmforge:RHEL推荐，包很全\r搜索引擎：\rhttp://pkgs.org\rhttp://rpmfind.net\rhttp://rpm.pbone.net\rhttps://sourceforge.net/\r(4) 自己制作\r注意：第三方包建议要检查其合法性来源合法性,程序包的完整性\rCentOS系统上使用rpm命令管理程序包：\r安装、卸载、升级、查询、校验、数据库维护\r安装：\rrpm {-i|--install} [install-options] PACKAGE_FILE… -v: verbose\r-vv: -h: 以#显示程序包管理执行进度\rrpm -ivh PACKAGE_FILE ...\r[install-options]\r--test: 测试安装，但不真正执行安装，即dry run模式\r--nodeps：忽略依赖关系\r--replacepkgs | replacefiles\r--nosignature: 不检查来源合法性\r--nodigest：不检查包完整性\r--noscripts：不执行程序包脚本\r%pre: 安装前脚本 --nopre\r%post: 安装后脚本 --nopost\r%preun: 卸载前脚本 --nopreun\r%postun: 卸载后脚本 --nopostun\r升级：\rrpm {-U|--upgrade} [install-options] PACKAGE_FILE...\rrpm {-F|--freshen} [install-options] PACKAGE_FILE...\rupgrade：安装有旧版程序包，则“升级”\r如果不存在旧版程序包，则“安装”\rfreshen：安装有旧版程序包，则“升级”\r如果不存在旧版程序包，则不执行\rrpm -Uvh PACKAGE_FILE ...\rrpm -Fvh PACKAGE_FILE ...\r--oldpackage：降级\r--force: 强制安装\r注意：\r(1) 不要对内核做升级操作；Linux支持多内核版本并存，因此，对直接安装新版本内核\r(2) 如果原程序包的配置文件安装后曾被修改，升级时，新版本的提供的同一个配置文件并不会直接覆盖老版本的配置文件，而把新版本的文件重命名(FILENAME.rpmnew)后保留\r包查询：\rrpm {-q|--query} [select-options] [query-options]\r[select-options]\r-a: 所有包\r-f: 查看指定的文件由哪个程序包安装生成\r-p rpmfile：针对尚未安装的程序包文件做查询操作\r--whatprovides CAPABILITY：查询指定的CAPABILITY由哪个包所提供\r--whatrequires CAPABILITY：查询指定的CAPABILITY被哪个包所依赖\r常用查询用法：\r-qi PACKAGE, -qf FILE, -qc PACKAGE, -ql PACKAGE, -qd PACKAGE\r-qpi PACKAGE_FILE, -qpl PACKAGE_FILE, ...\r-qa\rrpm2cpio 包文件|cpio –itv 预览包内文件\rrpm2cpio 包文件|cpio –id “*.conf” 释放包内文件\r包校验：\rrpm {-V|--verify} [select-options] [verify-options]\rS file Size differs\rM Mode differs (includes permissions and file type)\r5 digest (formerly MD5 sum) differs\rD Device major/minor number mismatch\rL readLink(2) path mismatch\rU User ownership differs\rG Group ownership differs\rT mTime differs\rP capabilities differ\r包来源合法性验正及完整性验证\r完整性验证：SHA256\r来源合法性验证：RSA\r公钥加密\r对称加密：加密、解密使用同一密钥\r非对称加密：密钥是成对儿的\rpublic key: 公钥，公开所有人\rsecret key: 私钥, 不能公开\r导入所需要公钥\rrpm -K|checksig rpmfile 检查包的完整性和签名\rrpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\rCentOS 7发行版光盘提供：RPM-GPG-KEY-CentOS-7\rrpm -qa “gpg-pubkey*”\rrpm数据库：\r数据库重建：\r/var/lib/rpm\rrpm {--initdb|--rebuilddb}\rinitdb: 初始化\r如果事先不存在数据库，则新建之\r否则，不执行任何操作\rrebuilddb：重建已安装的包头的数据库索引目录\rCentOS系统上使用yum命令：\rYUM: Yellowdog Update Modifier，rpm的前端程序，可解决软件包相关依赖性，可在多个库之间定位软件包，up2date的替代工具\ryum repository: yum repo，存储了众多rpm包，以及包的相关的元数据文件（放置于特定目录repodata下）\r文件服务器：\rhttp://\rhttps://\rftp://\rfile://\ryum配置文件：\ryum客户端配置文件：\r/etc/yum.conf：为所有仓库提供公共配置\r/etc/yum.repos.d/*.repo：为仓库的指向提供配置\r仓库指向的定义：\r[repositoryID]\rname=Some name for this repository\rbaseurl=url://path/to/repository/\renabled={1|0}\rgpgcheck={1|0}\rgpgkey=URL\renablegroups={1|0}\rfailovermethod={roundrobin|priority}\rroundrobin：意为随机挑选，默认值\rpriority:按顺序访问\rcost= 默认为1000\ryum的repo配置文件中可用的变量：\r$releasever: 当前OS的发行版的主版本号\r$arch: 平台，i386,i486,i586,x86_64等\r$basearch：基础平台；i386, x86_64\r$YUM0-$YUM9:自定义变量\ryum源：\r阿里云repo文件\rhttp://mirrors.aliyun.com/repo/\rCentOS系统的yum源\r阿里云：https://mirrors.aliyun.com/centos/$releasever/os/x86_64/\r清华大学：https://mirrors.tuna.tsinghua.edu.cn/centos/$releaseve\rEPEL的yum源\r阿里云：https://mirrors.aliyun.com/epel/$releasever/x86_64\r阿里巴巴开源软件https://opsx.alibaba.com/\ryum命令：\ryum的命令行选项：\r--nogpgcheck：禁止进行gpg check\r-y: 自动回答为“yes”\r-q：静默模式\r--disablerepo=repoidglob：临时禁用此处指定的repo\r--enablerepo=repoidglob：临时启用此处指定的repo\r--noplugins：禁用所有插件\ryum命令的用法：\ryum [options] [command] [package ...]\r显示仓库列表：\ryum repolist [all|enabled|disable\r显示程序包：\ryum list\ryum list [all | glob_exp1] [glob_exp2] [...]\ryum list {available|installed|updates} [glob_exp1] [...]\r安装程序包：\ryum install package1 [package2] [...]\ryum reinstall package1 [package2] [...] (重新安装)\r升级程序包：\ryum update [package1] [package2] [...]\ryum downgrade package1 [package2] [...] (降级)\r卸载程序包：\ryum remove | erase package1 [package2] [...]\r查看程序包information：\ryum info [...]\r查看指定的特性(可以是某文件)是由哪个程序包所提供：\ryum provides | whatprovides feature1 [feature2] [...]\r清理本地缓存：\r清除/var/cache/yum/$basearch/$releasever缓存\ryum clean [ packages | metadata | expire-cache | rpmdb | plugins | all ]\r构建缓存：\ryum makecache\r查看yum事务历史：\ryum history [info|list|packages-list|packages-info|\rsummary|addon-info|redo|undo|\rrollback|new|sync|stats]\ryum history\ryum history info 6\ryum history undo 6 日志 ：/var/log/yum.log\r安装及升级本地程序包：\ryum localinstall rpmfile1 [rpmfile2] [...]\r(用install替代)\ryum localupdate rpmfile1 [rpmfile2] [...]\r(用update替代)\r包组管理的相关命令：\ryum groupinstall group1 [group2] [...]\ryum groupupdate group1 [group2] [...]\ryum grouplist [hidden] [groupwildcard] [...]\ryum groupremove group1 [group2] [...]\ryum groupinfo group1 [...]\rUbuntu软件管理\rdpkg常见用法： man dpkg\rdpkg -i package.deb 安装包\rdpkg -r package 删除包，不建议，不自动卸载依赖于它的包\rdpkg -P package 删除包（包括配置文件）\rdpkg -l 列出当前已安装的包，类似rpm -qa\rdpkg -l package 显示该包的简要说明，类似rpm –qi\rdpkg -L package 列出该包中所包含的文件，类似rpm –ql\rdpkg -S \u0026lt;pattern\u0026gt; 搜索包含pattern的包，类似rpm –qf\rdpkg -s package 列出该包的状态，包括详细信息，类似rpm –qi\rdpkg --configure package 配置包，-a 使用，配置所有没有配置的软件包\rdpkg -c package.deb 列出 deb 包的内容\rdpkg示例：\r列出系统上安装的所有软件包\rdpkg -log\r列出软件包安装的文件\rdpkg -L bash\r查看/bin/bash来自于哪个软件包\rdpkg -S /bin/bash\r安装本地的 .deb 文件\rdpkg -i /mnt/cdrom/pool/main/z/zip/zip_3.0-11build1_amd64.deb\r卸载软件包\rdpkg -r zip\rapt命令：\rapt与apt-get命令对比：\rapt 命令 被取代的命令 命令的功能\rapt install apt-get install 安装软件包\rapt remove apt-get remove 移除软件包\rapt purge apt-get purge 移除软件包及配置文件\rapt update apt-get update 刷新存储库索引\rapt upgrade apt-get upgrade 升级所有可升级的软件包\rapt autoremove apt-get autoremove 自动删除不需要的包\rapt full-upgrade apt-get dist-upgrade 在升级软件包时自动处理依赖关系\rapt search apt-cache search 搜索应用程序\rapt show apt-cache show 显示安装细节\rapt 特有的命令：\rapt list 列出包含条件的包（已安装，可升级等）\rapt edit-sources 编辑源列表\rapt命令操作（如安装和删除软件包）记录在/var/log/dpkg.log日志文\rAPT包索引来自/etc/apt/sources.list文件和/etc/apt/sources.list.d目录中定义的存储库的可用包的数据库。要使用存储库中所做的最新更改来更新本地程序包索引\r安装包：\rapt install tree zip\r删除包：\rapt remove tree zip\r说明：apt remove中添加--purge选项会删除包配置文件，谨慎使用\r更新包索引：\rapt update\ryum 命令 yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。\n基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。\nyum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。\n列出所有可更新的软件清单命令：yum check-update 更新所有软件命令：yum update 仅安装指定的软件命令：yum install \u0026lt;package_name\u0026gt; 仅更新指定的软件命令：yum update \u0026lt;package_name\u0026gt; 列出所有可安裝的软件清单命令：yum list 删除软件包命令：yum remove \u0026lt;package_name\u0026gt; 查找软件包 命令：yum search 清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 安装 pam-devel\n1 yum install pam-devel 十四、进程概念 1. 概念的理解 首先程序与进程是什么？程序与进程又有什么区别？\n1）程序 程序（procedure）：不太精确地说，程序就是执行一系列有逻辑、有顺序结构的指令，帮我们达成某个结果。就如我们去餐馆，给服务员说我要牛肉盖浇饭，她执行了做牛肉盖浇饭这么一个程序，最后我们得到了这么一盘牛肉盖浇饭。它需要去执行，不然它就像一本武功秘籍，放在那里等人翻看。\n2）进程 进程（process）：进程是程序在一个数据集合上的一次执行过程，在早期的 UNIX、Linux 2.4 及更早的版本中，它是系统进行资源分配和调度的独立基本单位。同上一个例子，就如我们去了餐馆，给服务员说我要牛肉盖浇饭，她执行了做牛肉盖浇饭这么一个程序，而里面做饭的是一个进程，做牛肉汤汁的是一个进程，把牛肉汤汁与饭混合在一起的是一个进程，把饭端上桌的是一个进程。它就像是我们在看武功秘籍这么一个过程，然后一个篇章一个篇章地去练。\n简单来说，程序是为了完成某种任务而设计的软件，比如 vim 是程序。什么是进程呢？进程就是运行中的程序。\n程序只是一些列指令的集合，是一个静止的实体，而进程不同，进程有以下的特性：\n动态性：进程的实质是一次程序执行的过程，有创建、撤销等状态的变化。而程序是一个静态的实体。 并发性：进程可以做到在一个时间段内，有多个程序在运行中。程序只是静态的实体，所以不存在并发性。 独立性：进程可以独立分配资源，独立接受调度，独立地运行。 异步性：进程以不可预知的速度向前推进。 结构性：进程拥有代码段、数据段、PCB（进程控制块，进程存在的唯一标志）。也正是因为有结构性，进程才可以做到独立地运行。 **并发：**在一个时间段内，宏观来看有多个程序都在活动，有条不紊的执行（每一瞬间只有一个在执行，只是在一段时间有多个程序都执行过）\n**并行：**在每一个瞬间，都有多个程序都在同时执行，这个必须有多个 CPU 才行\n引入进程是因为传统意义上的程序已经不足以描述 OS 中各种活动之间的动态性、并发性、独立性还有相互制约性。程序就像一个公司，只是一些证书，文件的堆积（静态实体）。而当公司运作起来就有各个部门的区分，财务部，技术部，销售部等等，就像各个进程，各个部门之间可以独立运作，也可以有交互（独立性、并发性）。\n而随着程序的发展越做越大，又会继续细分，从而引入了线程的概念，当代多数操作系统、Linux 2.6 及更新的版本中，进程本身不是基本运行单位，而是线程的容器。就像上述所说的，每个部门又会细分为各个工作小组（线程），而工作小组需要的资源需要向上级（进程）申请。\n线程（thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。因为线程中几乎不包含系统资源，所以执行更快、更有效率。\n简而言之，一个程序至少有一个进程，一个进程至少有一个线程。线程的划分尺度小于进程，使得多线程程序的并发性高。另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。就如下图所示：\n2. 进程的分类 大概明白进程是个什么样的存在后，我们需要进一步了解的就是进程分类。可以从两个角度来分：\n以进程的功能与服务的对象来分； 以应用程序的服务类型来分； 第一个角度来看，我们可以分为用户进程与系统进程：\n用户进程：通过执行用户程序、应用程序或称之为内核之外的系统程序而产生的进程，此类进程可以在用户的控制下运行或关闭。 系统进程：通过执行系统内核程序而产生的进程，比如可以执行内存资源分配和进程切换等相对底层的工作；而且该进程的运行不受用户的干预，即使是 root 用户也不能干预系统进程的运行。 第二角度来看，我们可以将进程分为交互进程、批处理进程、守护进程：\n交互进程：由一个 shell 终端启动的进程，在执行过程中，需要与用户进行交互操作，可以运行于前台，也可以运行在后台。 批处理进程：该进程是一个进程集合，负责按顺序启动其他的进程。 守护进程：守护进程是一直运行的一种进程，在 Linux 系统启动时启动，在系统关闭时终止。它们独立于控制终端并且周期性的执行某种任务或等待处理某些发生的事件。例如 httpd 进程，一直处于运行状态，等待用户的访问。还有经常用的 cron（在 centOS 系列为 crond）进程，这个进程为 crontab 的守护进程，可以周期性的执行用户设定的某些任务。 3. 进程组与 Sessions 每一个进程都会是一个进程组的成员，而且这个进程组是唯一存在的，他们是依靠 PGID（process group ID）来区别的，而每当一个进程被创建的时候，它便会成为其父进程所在组中的一员。\n一般情况，进程组的 PGID 等同于进程组的第一个成员的 PID，并且这样的进程称为该进程组的领导者，也就是领导进程，进程一般通过使用 getpgrp() 系统调用来寻找其所在组的 PGID，领导进程可以先终结，此时进程组依然存在，并持有相同的 PGID，直到进程组中最后一个进程终结。\n与进程组类似，每当一个进程被创建的时候，它便会成为其父进程所在 Session 中的一员，每一个进程组都会在一个 Session 中，并且这个 Session 是唯一存在的，\nSession 主要是针对一个 tty 建立，Session 中的每个进程都称为一个工作(job)。每个会话可以连接一个终端(control terminal)。当控制终端有输入输出时，都传递给该会话的前台进程组。Session 意义在于将多个 jobs 囊括在一个终端，并取其中的一个 job 作为前台，来直接接收该终端的输入输出以及终端信号。 其他 jobs 在后台运行。\n前台（foreground）就是在终端中运行，能与你有交互的\n后台（background）就是在终端中运行，但是你并不能与其任何的交互，也不会显示其执行的过程\n4. 工作管理 bash(Bourne-Again shell)支持工作控制（job control），而 sh（Bourne shell）并不支持。\n并且每个终端或者说 bash 只能管理当前终端中的 job，不能管理其他终端中的 job。比如我当前存在两个 bash 分别为 bash1、bash2，bash1 只能管理其自己里面的 job 并不能管理 bash2 里面的 job\n我们都知道当一个进程在前台运作时我们可以用 ctrl + c 来终止它，但是若是在后台的话就不行了。\n1）\u0026amp;命令 我们可以通过 \u0026amp; 这个符号，让我们的命令在后台中运行：\n1 ls \u0026amp; 图中所显示的 [1] 236分别是该 job 的 job number 与该进程的 PID，而最后一行的 Done 表示该命令已经在后台执行完毕。\n2）ctrl + z 命令 我们还可以通过 ctrl + z 使我们的当前工作停止并丢到后台中去\n被停止并放置在后台的工作我们可以使用这个命令来查看：\n1 jobs 其中第一列显示的为被放置后台 job 的编号，而第二列的 ＋ 表示最近(刚刚、最后)被放置后台的 job，同时也表示预设的工作，也就是若是有什么针对后台 job 的操作，首先对预设的 job，- 表示倒数第二（也就是在预设之前的一个）被放置后台的工作，倒数第三个（再之前的）以后都不会有这样的符号修饰，第三列表示它们的状态，而最后一列表示该进程执行的命令。\n我们可以通过这样的一个命令将后台的工作拿到前台来：\n1 2 3 # 后面不加参数提取预设工作，加参数提取指定工作的编号 # ubuntu 在 zsh 中需要 %，在 bash 中不需要 % fg [%jobnumber] 之前我们通过 ctrl + z 使得工作停止放置在后台，若是我们想让其在后台运作我们就使用这样一个命令：\n1 2 #与fg类似，加参则指定，不加参则取预设 bg [%jobnumber] 3）kill命令 发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果仍无法终止该程序可用\u0026rdquo;-KILL\u0026quot; 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。\n常用参数：\n1 2 3 4 5 -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 （1）先使用ps查找进程pro1，然后用kill杀掉\n1 kill -9 $(ps -ef | grep pro1) 4）ps 命令 ps 也是我们最常用的查看进程的工具之一，我们通过这样的一个命令来了解一下，它能给我们带来哪些信息：\n1 ps -aux 1 ps axjf 我们来总体了解下会出现哪些信息给我们，这些信息又代表着什么（更多的 keywords 大家可以通过 man ps 了解）。\n内容 解释 F 进程的标志（process flags），当 flags 值为 1 则表示此子程序只是 fork 但没有执行 exec，为 4 表示此程序使用超级管理员 root 权限 USER 进程的拥有用户 PID 进程的 ID PPID 其父进程的 PID SID session 的 ID TPGID 前台进程组的 ID %CPU 进程占用的 CPU 百分比 %MEM 占用内存的百分比 NI 进程的 NICE 值 VSZ 进程使用虚拟内存大小 RSS 驻留内存中页的大小 TTY 终端 ID S or STAT 进程状态 WCHAN 正在等待的进程资源 START 启动进程的时间 TIME 进程消耗 CPU 的时间 COMMAND 命令的名称和参数 TPGID栏写着-1 的都是没有控制终端的进程，也就是守护进程\nSTAT表示进程的状态，而进程的状态有很多，如下表所示\n状态 解释 R Running.运行中 S Interruptible Sleep.等待调用 D Uninterruptible Sleep.不可中断睡眠 T Stoped.暂停或者跟踪状态 X Dead.即将被撤销 Z Zombie.僵尸进程 W Paging.内存交换 N 优先级低的进程 \u0026lt; 优先级高的进程 s 进程的领导者 L 锁定状态 l 多线程状态 + 前台进程 其中的 D 是不能被中断睡眠的状态，处在这种状态的进程不接受外来的任何 signal，所以无法使用 kill 命令杀掉处于 D 状态的进程，无论是 kill，kill -9 还是 kill -15，一般处于这种状态可能是进程 I/O 的时候出问题了。\nps 工具有许多的参数，下面给大家解释部分常用的参数。\n使用 -l 参数可以显示自己这次登录的 bash 相关的进程信息罗列出来：\n1 ps -l 相对来说我们更加常用下面这个命令，他将会罗列出所有的进程信息：\n1 ps -aux 若是查找其中的某个进程的话，我们还可以配合着 grep 和正则表达式一起使用：\n1 ps -aux | grep zsh 此外我们还可以查看时，将连同部分的进程呈树状显示出来：\n1 ps axjf 当然如果你觉得使用这样的此时没有把你想要的信息放在一起，我们也可以是用这样的命令，来自定义我们所需要的参数显示：\n1 ps -afxo user,ppid,pid,pgid,command 这是一个简单而又实用的工具，想要更灵活的使用，想要知道更多的参数我们可以使用 man 来获取更多相关的信息。\nps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top linux上进程有5种状态:\n运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps 工具标识进程的5种状态码:\n1 2 3 4 5 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数：\n1 2 3 4 5 6 7 8 -A 显示所有进程 a 显示所有进程 -a 显示同一终端下所有进程 c 显示进程真实名称 e 显示环境变量 f 显示进程间的关系 r 显示当前终端运行的进程 -aux 显示所有包含其它使用的进程 （1）显示当前所有进程环境变量及进程间关系\n1 ps -ef （2）显示当前所有进程\n1 ps -A （3）与grep联用查找某进程\n1 ps -aux | grep apache （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码\n1 ps aux | grep \u0026#39;(cron|syslog)\u0026#39; 十五、网络通讯命令 1. ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 2. iptables 命令 iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如：\n把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp \u0026ndash;dport 80 -j REJECT 。 开启 80 端口，因为web对外都是这个端口 1 iptables -A INPUT -p tcp --dport 80 -j ACCEP 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 3. netstat 命令 Linux netstat命令用于显示网络状态。\n利用netstat指令可让你得知整个Linux系统的网络情况。\n语法\n1 netstat [-acCeFghilMnNoprstuvVwx][-A\u0026lt;网络类型\u0026gt;][--ip] 参数说明：\n-a或–all 显示所有连线中的Socket。 -A\u0026lt;网络类型\u0026gt;或–\u0026lt;网络类型\u0026gt; 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定\u0026quot;-A unix\u0026quot;参数相同。 –ip或–inet 此参数的效果和指定\u0026quot;-A inet\u0026quot;参数相同。 如何查看系统都开启了哪些端口？\n1 2 3 4 5 6 7 8 9 10 [root@centos6 ~ 13:20 #55]# netstat -lnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1035/sshd tcp 0 0 :::22 :::* LISTEN 1035/sshd udp 0 0 0.0.0.0:68 0.0.0.0:* 931/dhclient Active UNIX domain sockets (only servers) Proto RefCnt Flags Type State I-Node PID/Program name Path unix 2 [ ACC ] STREAM LISTENING 6825 1/init @/com/ubuntu/upstart unix 2 [ ACC ] STREAM LISTENING 8429 1003/dbus-daemon /var/run/dbus/system_bus_socket 如何查看网络连接状况？\n1 2 3 4 5 6 7 [root@centos6 ~ 13:22 #58]# netstat -an Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 192.168.147.130:22 192.168.147.1:23893 ESTABLISHED tcp 0 0 :::22 :::* LISTEN udp 0 0 0.0.0.0:68 0.0.0.0:* 如何统计系统当前进程连接数？\n输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？\n严格来说，这个题目考验的是对 awk 的使用。\n首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下：\n1 2 3 4 5 6 7 tcp 0 0 120.27.146.122:80 113.65.18.33:62721 ESTABLISHED tcp 0 0 120.27.146.122:80 27.43.83.115:47148 ESTABLISHED tcp 0 0 120.27.146.122:58838 106.39.162.96:443 ESTABLISHED tcp 0 0 120.27.146.122:52304 203.208.40.121:443 ESTABLISHED tcp 0 0 120.27.146.122:33194 203.208.40.122:443 ESTABLISHED tcp 0 0 120.27.146.122:53758 101.37.183.144:443 ESTABLISHED tcp 0 0 120.27.146.122:27017 23.105.193.30:50556 ESTABLISHED 4. ping 命令 Linux ping命令用于检测主机。\n执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。\n指定接收包的次数\n1 ping -c 2 www.baidu.com 5. telnet 命令 Linux telnet命令用于远端登入。\n执行telnet指令开启终端机阶段作业，并登入远端主机。\n语法\n1 telnet [-8acdEfFKLrx][-b\u0026lt;主机别名\u0026gt;][-e\u0026lt;脱离字符\u0026gt;][-k\u0026lt;域名\u0026gt;][-l\u0026lt;用户名称\u0026gt;][-n\u0026lt;记录文件\u0026gt;][-S\u0026lt;服务类型\u0026gt;][-X\u0026lt;认证形态\u0026gt;][主机名称或IP地址\u0026lt;通信端口\u0026gt;] 参数说明：\n-8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b\u0026lt;主机别名\u0026gt; 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e\u0026lt;脱离字符\u0026gt; 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定\u0026quot;-F\u0026quot;参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k\u0026lt;域名\u0026gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l\u0026lt;用户名称\u0026gt; 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n\u0026lt;记录文件\u0026gt; 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S\u0026lt;服务类型\u0026gt; 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X\u0026lt;认证形态\u0026gt; 关闭指定的认证形态。 登录远程主机\n1 2 # 登录IP为 192.168.0.5 的远程主机 telnet 192.168.0.5 ","permalink":"https://cold-bin.github.io/post/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":["linux常用命令"],"title":"Linux常用命令"},{"categories":["docker"],"contents":"Docker基础篇之快速上手 第一章 Docker简介 是什么？ 问题：为什么会有 docker 的出现 一款产 品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发+运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验 Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装?也就是说，安装的时候，把原始环境-模-样地复制过来。开发人员利用Docker可以消除协作编码时“在我的机器上可正常工作”的问题。\n之前在服务器配置一个应用的运行环境，要安装各种软件，就拿尚硅谷电商项目的环境来说吧，Java/TomcatMySQL/JDBC驱动包等。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在Windows上安装的这些环境，到了Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。\n传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等java为例)。而为了让这程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker镜 像的设计，使得Docker得以打过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运.作。\ndocker理念 Docker是基于Go语言实现的云开源项目。 Docker的主要目标是“Build, Ship[ and Run Any App,Anywhere\u0026quot;，也就是通过对应用组件的封装、分发、部署、运行等生命期的管理，使用户的APP (可以是一个WEB应用或数据库应用等等)及其运行环境能够做到“一次封装，到处运行”。\nLinux容器技术的出现就解决了这样一 一个问题，而Docker就是在它的基础上发展过来的。将应用运行在Docker容器上面，而Docker容器在任何操作系统上都是一-致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作\n一句话 解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术\n能干嘛 之前的虚拟机技术 虚拟机**(virtual machine)**就是带环境安装的一种解决方案。\n它可以在一种操作系统里面运行另一种作系统，比如在Windows系统里面运行Linux系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统- -模-样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。\n虚拟机的缺点:\n1、资源占用多\n2、冗余步骤多\n3、启动慢\n容器虚拟化技术 由于前面虛拟机存在这些缺点，Linux 发展出了另一种虚拟化技术: Linux 容器(Linux Containers,缩为LXC)。\nLinux容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。.\n比较了Docker和传统虚拟化方式的不同之处:\n1、传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程;\n2、而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机为轻便。\n3、每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。\n开发/运维(DevOps) 一次构建、随处运行，\n更快速的应用交付和部署 ​\t传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化 之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测 试验证时间。\n更便捷的升级和扩缩容 ​\t随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成-块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。\n更简单的系统运维 ​\t应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度\u0026ndash;致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。\n更高效的计算资源利用 ​\tDocker是内核级虚拟化，其不像传统的虚拟化技术一样 需要额外的Hypervisor支持，所以在-台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。\n企业级 新浪\n美团\n蘑菇街\n去哪下 1、官网 docker官网： https://www.docker.com/\ndocker中文网站: https://www.docker-cn.com/\n2、仓库 Docker Hub官网：https://hub.docker.com/\n第二章 Docker安装 前提说明 CentOS Docker安装 Docker支持以下的CentOS版本: CentOS 7 (64-bit) CentOS 6.5 (64-bit)或更高的版本\n前提条件 目前，CentOS 仅发行版本中的内核支持Docker。 Docker运行在CentOS 7.上，要求系统为64位、系统内核版本为3.10以上。 Docker运行在CentOS-6.5或更高的版本的CentOS上，要求系统为64位、系统内核版本为2.6.32-431或者更高版本。\nDocker 的基本组成 docker架构图 镜像( image ) Docker镜像(lmage)就是-一个只读的模板。镜像可以用来创建Docker容器，个镜像可以创建很多容器\n容器( container) Docker利用容器(Container) 独立运行的一个或一组应用。容器是用镜像创建的运行实例。 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简 易版的Linux环境(包括root用户权限、进程空间、用户空间和网络空间等)和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角， 唯- -区别在于容器的最上面那-层是可读可写的。\n仓库( repository) 仓库(Repository) 是集中存放镜像文件的场所。 仓库(Repository)和仓库注册服务器(Registry) 是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多镜像， 每个镜像有不同的标签(tag) 。\n仓库分为公开仓库(Public) 和私有仓库(Private) 两种形式。 最大的公开仓库是Docker Hub(ttps://hub. docker.com/) 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等\n小总结 () 需要正确的理解仓储/镜像/容器这几个概念:\nDocker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一-个可交付的运行环境，这个打好的运行环境就似乎image镜像文件。只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模板。Docker根据image文件生成容器的实例。同一个image文件，可以生成多个同时运行的容器实例。\nimage文件生成的容器实例，本身也是一一个文件，称为镜像文件。\n一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一-个对应的运行实例，也就是我们的容器至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。|\n安装步骤 Centos6.8安装Docker 1、yum install -y epel-release\n2、yum install -y docker-io\n3、安装后的配置文件： etc/sysconfig/docker\n4、启动 Docker后台服务: service docker start\n5、docker version 验证\nCentos7.0安装Docker https://docs.docker.com/engine/install/centos/\n永远的helloworld 阿里云镜像加速 是什么\n​\thttps://promotion.aliyun.com/ntms/act/kubernetes.html\n注册一个属于自己的阿里云账户( 可复用淘宝账号)\n获得加速器地址连接\n​\t登录阿里云开发者平台\n​\t获取加速器地址\n配置本机Docker运行镜像加速器\n​\t鉴于国内网络问题，后续拉取Docker镜像十分缓慢，我们可以需要配置加速器来解决， 我使用的是阿里云的本人自己账号的镜像地址(需要自己注册有一个属于你自己的): ht:po/. mirror aliyuncns .com\nvim /etc/sysconfig/docker 将获得的自己账户下的阿里云加速地址配置进 other_ args-=\u0026quot;\u0026ndash;registry-mirror=https://你自 己的账号加速信息.mirror .aliyuncs.com 重新启动 Docker 后台服务：service docker restart\nLinux系统下配置完加速器需要检查是否生效\n网易云加速 基本上同上述阿里云\n启动Docker后台容器(测试运行 hello-world )\n​\tdocker run hello world\nrun干了什么\n底层原理 Docker是怎样工作的 Docker是一个Client-Server结构的系统，Docker守 护进程运行在主机上，然后通过Socket连 接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器，是一个运行时环境，就是我们前面说到的集装箱。\n为什么Docker比较比vm快 1、docker有着比虚拟机更少的抽象层。由亍docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 2、docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个 容器时,docker不需要和虚拟机一样 重新加载- - 个操作系统内核仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建\u0026ndash;个虚拟机时,虚拟机软件需要加载GuestOS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一-个docker容器只需要几秒钟。\n第三章 Docker常用命令 帮助命令 1 2 3 4 5 6 docker Version docker info docker --help 自己查看官网解释，高手都是自己练出来的，百度上只不过是翻译了下，加了点例子 镜像命令 docker images 列出本机上的镜像 OPTIONS 说明： 1 2 3 4 -a 列出本地所有的镜像(含中间映射层) -q 只显示镜像ID --digests 显示镜像的摘要信息 --no-trunc 显示完整的镜像信息 docker search 某个XXX镜像的名字 ​\t网站 www.dockerhub.com\n​\tdocker search [OPTIONS] 镜像名字\n​\tOPTIONS 说明\n1 2 3 --no-trun 显示完整的镜像描述 -s 列出收藏数不小于指定值的镜像 --automated 只列出 automated build类型的镜像 docker pull 某个镜像的名字 ​\t下载镜像\n​\tdocker pull 镜像名字[:TAG]\ndocker rmi 某个XXX镜像的名字ID ​\t删除镜像\n​\t删除单个 docker rm -f 镜像ID\n​\t删除多个 docker rm -f 镜像名1:TAG 镜像名2:TAG\n​\t删除多个 docker rmi -f ${docker images -qa}\n容器命令 有镜像才能创建容器，这是根本前提(下载一个Centos镜像演示)\n​\tdocker pull centos\n新建并启动容器 ​\tdocker run [OPTIONS] IMAGE [COMMAND][ARG]\n​\tOPTIONS 说明\n1 2 3 4 5 6 7 8 9 10 11 OPTIONS说明(常用) :有些是一个减号，有些是两个减号 --name=\u0026#34;容器新名字\u0026#34;:为容器指定一个名称; -d:后台运行容器，并返回容器ID， 也即启动守护式容器; -i:以交互模式运行容器，通常与-t同时使用; -t:为容器重新分配一个伪输入终端，通常与-i同时使用; -P:随机端口映射; -p:指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 列出当前所有正在运行的容器 ​\tdockers ps [OPTIONS]\n1 2 3 4 5 6 OPTIONS说明(常用) : -a :列出当前所有正在运行的容器+历史上运行过的 -|:显示最近创建的容器。 -n:显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 --no-trunc :不截断输出。 退出容器 两种退出方式\n​\texit 容器停止退出\n​\tctrl+P+Q 容器不停止退出\n启动容器 docker start 容器ID或容器签名\n重启容器 docker restart 容器ID或容器签名\n停止容器 docker stop 容器ID或容器签名\n强制停止容器 docker kill 容器ID或容器签名\n删除已停止的容器 docker rm 容器ID -f\n​\t一次性删除多个容器\n​\tdocker rm -f $(docker ps -a -q)\n​\tdocker ps -a -q | xargs docker rm\n重要 启动守护式容器 #使用镜像centos:latest以后台模式启动一个容器 docker run -d centos\n问题:然后docker ps -a进行查看,会发现容器已经退出 很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程. 容器运行的命令如果不是那些一直挂起的命令 (比如运行top，tail) ，就是会自动退出的。 这个是docker的机制问题,比如你的web容器，我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如 service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用,这样的容器后台启动后，会立即自杀因为他觉得他没事可做了.所以，最佳的解决方案是将你要运行的程序以前台进程的形式运行\n查看容器日志 docker logs -f -t \u0026ndash;tail 容器ID\n​\t-t 是加入时间戳\n​\t-f 跟随最新的日志打印\n​\t\u0026ndash;tail 数字显示最后多少条\n查看容器内的进程 docker top 容器ID\n查看容器内部细节 docker inspect 容器ID\n进入正在运行的容器并以命令行交互 docker exec -it 容器ID bashShell\n重新进入docker attach 容器ID\n上述两个区别\nattach 直接进入容器启动命令的终端，不会启动新的进程\nexec 实在容器中打开新的终端，并且可以穷的那个新的进程\n从容器内拷贝文件到主机上 docker cp 容器ID:容器内路径 目的主机路径\n小总结 第 四 章 Docker 镜像 是什么 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的有内容，包括代码、运行时、库、环境变量和配置文件。\nUnionFS(联合文件系统) UnionFS (状节又件示统) UnionFS (联合文件系统) : Union文件系统(UnionFS)是一一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修作为一 次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a singlevirtualfilesystem)。Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像(没有父镜像)可以制作各种具.体的应用镜像。\n特性:一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文 件系统会包含所有底层的文件和目录\nDocker镜像加载原理 Docker镜像加载原理: docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。\nbotfs(boot file system)主要包含bootloader和kernel, bootloader主 要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一-层与我们典型的Linux/Unix系统是- - -样的，包含boot加载器和内核。当boot加载完成之 后整个内核就都在内存中了，此时内存的使用权己由bootfs转交给内核，此时系统也会卸载bootfs。\nrootfs (root file system)，在bootfs之 上。 包含的就是典型Linux系统中的**/dev, /proc, /bin, /etc等标准目录和文件。rootfs就 是各种不同的操作系统发行版，比如Ubuntu**，Centos等等。\n平时我们安装的虚拟机的Centos都是好几个G ，为什么docker这里才要200m\n对于一个精简的OS, rootfs可 以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel,自只需要提供rootfs就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别，因此不同的发行版可以公用bootfs。\n分层的镜像 分层的镜像 为什么 Docker纪念馆想要采用这种分层结构 最大的一个好处就是-共享资源 比如:有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像, 同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n特点 Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称为容器层，容器层之下都叫镜像层\nDocker镜像Commit操作 docker commit 提交容器副本使之称为一个新的镜像\ndocker commit -m=\u0026ldquo;提交的描述信息\u0026rdquo; -a=\u0026ldquo;作者\u0026rdquo; 容器ID 要创建的目标镜像名:[标签名]\n案例演示： 1、从Hub上下载tomcat镜像到本地并成功运行\ndocker run -d -p 8080:8080 tomcat\n1 2 3 4 -p主机端口：docker容器端口 -P:随机分配端口 i:交互 t:终端 2、故意删除上一步镜像生产tomcat容器的文档\n3、也即当前的tomcat运行实例是一个没有文档内容的容器，以他为模板commit一个没有doc的tomcat新镜像 atguigu/tomcat02\n4、启动我们的新镜像并和原来的对比\n​\t启动atuigu/tomcat02 没有doc\n​\t启动原来tomcat他有doc\n第 五 章 Docker容器数据卷 是什么 先来看看Docker的理念: *将运用与运行的环境打包形成容器运行，运行可以伴随着容器，但是我们对数据的要求希望是持久化的 *容器之间希望有可能共享数据 Docker容器产生的数据，如果不通过docker commit生成新的镜像，使得数据做为镜像的一部分保存下来， 那么当容器删除后，数据自然也就没有了。 为了能保存数据在docker中我们使用卷。|\n一句话：有点类似我们Redis里面的rdb和aof文件\n能干嘛 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union FileSystem提供一些用 于持续存储或共享数据的特性: 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不 会在容器删除时删除其挂载的数据卷\n特点: 1:数据卷可在容器之间共享或重用数据 2:卷中的更改可以直接生效 3:数据卷中的更改不会包含在镜像的更新中 4:数据卷的生命周期一直持续到没有容器使用它为止\n容器的持久化\n容器间继承+共享数据\n数据卷 容器内添加 直接命令添加 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名\n使用该命令添加挂载需要注意：\n如果挂载一个空的数据卷到容器中的一个非空目录中，那么这个目录下的文件会被复制到数据卷中\n如果挂载一个非空的数据卷到容器中的一个目录中，那么容器中的目录中会显示数据卷中的数据。如果原来容器中的目录中有数据，那么这些原始数据会被隐藏掉\n假如，将容器里的应用程序的配置文件挂载到一个不是空的主机目录上，这个主机目录的文件会覆盖掉配置文件，配置文件隐藏，导致容器应用启动的时候，启动不成功，原因是配置文件改了。\n查看数据卷是否挂载成功\n容器和宿主机之间数据共享\n容器停止退出后，主机修改后的数据是否同步\n命令(带权限)\n​\tdocker run -it -v /宿主机绝对路径目录:/容器内目录**:ro** 镜像名 -\u0026gt;表示容器只有读权限，没有写权限。因此，但是宿主机有写权限。\nDockerFile添加 根目录下新建mydocker文件夹并进入\n可在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷\nFile构建\nbuild后生成镜像\n获得一个新镜像zzyy/centos\nrun容器\n通过上述步骤，容器内的卷目录地址已经知道，对应的主机目录在哪\n主机对应默认地址\n备注\nDocker挂载主机目录Docker访问出现cannot open directory . Permission denied 解决办法:在挂载目录后多加一个\u0026ndash;privileged=true参数即可\n数据卷容器 是什么 命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器.\n总体介绍 以上一步新建的zzyy/centos为模板并运行容器 doc1/doc2/doc3\n他们已经具有容器卷\n​\t/dataVolumeContainer1\n​\t/dataVolumeContainer2\n容器间传递共享(\u0026ndash;volumes -from) 先启动一个父容器doc1 启动后在 dataVolumeContainer1中新增内容\ndoc2/doc3 继承doc1 ​\t\u0026ndash;volumes -from\ndoc2/doc3 分别在dataVolumeContainer2各自新增内容\n回到doc1可以看到02/03各自添加的都能共享了 删除doc1 doc2修改后doc3是否可以访问 删除doc02后doc3是否访问 在进一步\n新建doc04继承doc03 然后删除doc03 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止\n第 六 章 DockerFile解析 是什么 Dockerfile是用来构建Docker镜像的构建文件，由一系列命令和参数构成的脚本 构建三步骤 ​\t编写Dockerfile文件\n​\tdocker build\n​\tdocker run\n文件什么样？？？ ​\t熟悉的Centos为例\nhttp://hub.docker.com/_/centos\nDockerFile构建过程解析 Dockerfile内容基础知识\n1、每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2、指令按照从上到下，顺序执行 3、#表示注释 4、每条指令都会创建一个新的镜像层，并对镜像进行提交\nDocker执行Dockerfile的大致流程 1、docker从基础镜像运行一个容器 2、执行一条指令并对容器作出修改 3、执行类似docker commit的操作提交一个新的镜像层 4、docker再基 于刚提交的镜像运行一一个新容器 5、执行dockerfile中的 下一条指令直到所有指令都执行完成\n小总结 从应用软件的角度来看，Dockerfile、 Docker镜像与Docker容器分别代表软件的三个不同阶段， Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件的运行态。 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。\n1、Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉 及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; 2、Docker镜像，在用Dockerfile定义一文件之后，docker build时会产生一个Docker镜像，当运行Docker镜像时，会真正开始提供服务; 3、Docker容器，容器是直接提供服务的。\nDockerFile体系结构(保留字指令) 小总结\n其中单一run指令后面需要写多行时，后面不能使用注释符号\n案例 Base 镜像(scratch) Docker Hub中 99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的\n自定义镜像mycentos 1、编写 ​\tHub默认Centos镜像是什么情况\n准备Dockerfile文件\nmyCentOS内容Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM centos MAINTAINER ZZYY\u0026lt;zzyy167@126.com\u0026gt; ENV MYPATH /usr/local WORKDIR $MYPATH #来自centos的原始镜像，没有好的yum源，先把yum源提上去，再下vim RUN cd /etc/yum.repos.d/ RUN sed -i \u0026#39;s/mirrorlist/#mirrorlist/g\u0026#39; /etc/yum.repos.d/CentOS-* RUN sed -i \u0026#39;s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\u0026#39; /etc/yum.repos.d/CentOS-* RUN yum -y install vim RUN yum -y install net-tools EXPOSE 80 CMD echo $MYPATH CMD echo \u0026#34;success--------------ok\u0026#34; CMD /bin/bash 2、构建 docker build -t 新镜像名字:TAG .\n3、运行 docker run -it 新镜像名字:TAG\n4、列出镜像的变更历史 docker history 镜像名\nCMD/ENTRYPOINT 镜像案例\n都是指定一个容器启动时要运行的命令\nCMD ​\tDockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被dockerrun之后的参数替换\n​\tCase\n​\ttomcat的讲解演示 docker run -it -p 8080:8080 tomcat ls -l\nENTRYPOINT ​\tdocker run 之后的参数会被当做参数传递给 ENTRYPOINT 之后形成新的命令组合\n​\tCase\n制作CMD版可以查询IP信息的容器\ncurl的命令解释 curl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。\n如果系统没有curl可以使用yum install curl安装，也可以下载安装。 curl是将下载文件输出到stdout 使用命令: curl http://www .baidu.com 执行后，www.baidu.com的html就会显示在屏幕上了\n这是最简单的使用方法。用这个命令获得了htp://curl.haxx.se指向的页面，同样，如果这里的URL指向的是\u0026ndash;个文件或者一幅图都可以直接下载到本地。如果下载的是HTML文档，那么缺省的将只显示文件头部，即HTML文档的header。要全部显示，请加参数-i\nWHY\n我们可以看到可执行文件找不到的报错，executable file not found。 之前我们说过，跟在镜像名后面的是command,运行时会替换CMD的默认值。 因此这里的-i替换了原来的CMD，而不是添加在原来的curl -s htp://ip.cn后面。而-i 根本不是命令，所以自然找不到。 那么如果我们希望加入-i这参数，我们就必须重新完整的输入这个命令: $ docker run myip curl -s http://ip.cn -i\n自定义镜像Tomcat 1、mkdir -p /zzyy/mydockerfile/tomcat9 2、在上述目录下 touch c.txt 3、将jdk和tomcat安装的压缩包拷贝进上一步目录 4、在zzyyuse/mydockerfile/tomcat9目录下新建Dockerfile文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 FROM centos MAINTAINER zzyy\u0026lt;zzyybs@ 126.com\u0026gt; #把宿主机当前上下文的c .txt拷贝到容器/usr/local/路径下 COPY c.txt /usr/local/cincontainer.txt #把java与tomcat添加到容器中 ADD jdk-8u171-linux x64.tar .gz /usr/local/ ADD apache-tomcat-9.0.8.tar.gz /usr/ocal/ #安装vim编辑器 RUN yum -y install vim #设置工 作访问时候的WORKDIR路径， 登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配:置java与tomcat环境变量 ENV JAVA_ HOME /usr/localjdk1 .8.0_ 171 ENV CLASSPATH $JAVA_ HOME/lib/dt.jar:$JAVA_ HOME/lib/tools.jar ENV CATALINA_ HOME /usr/local/apache-tomcat-9.0.8 ENV CATALINA_ BASE /usr/ocal/apache-tomcat-9.0.8 ENV PATH $PATH:$JAVA_ HOME/bin:$CATALINA_ HOME/ib:$CATALINA_ HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [\u0026#34;/usrl/local/apache-tomcat-9.0.8/bin/startup.sh\u0026#34; ] # CMD [\u0026#34;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh\u0026#34;,\u0026#34;run\u0026#34;] CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh \u0026amp;\u0026amp; tail -F /usr/local/apache-tomcat-9.0.8/in/logs/catalina.out 目录内容\n5、构建 构建完成\n6、run 1 2 3 docker run -d -p 9080:8080 -name myt9 -v /zyuse/mydockerfiletomcat9/test:/usrlocal/apache-tomcat9.0.8/webapps/test -v /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usrlocal/apache-tomcat-9.0.8/logs -privileged=true zzyytomcat9 备注\nDocker挂载主机目录Docker访问出现cannot open directory : Permission denied解决办法:在挂载目录后多加一个\u0026ndash;privileged=true参数即可\n7、验证 8、综合前述容器卷测试的web服务test发布 web.xml\n1 2 3 4 5 6 7 8 \u0026lt;?xml version=\u0026#34;1 .0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmIns:xsi=\u0026#34;http://www.w3.org/2001/XML Schema-instance\u0026#34; xmIns=\u0026#34;http://java sun.com/xm/ns/javaee\u0026#34; xsi:schemaL ocation=\u0026#34;http://java. sun.com/xml/ns/javaee htp:/:/java. sun.com/xml/ns/javaee/web-app_ 2_ _5.xsd\u0026#34; id=\u0026#34;WebApp_ ID\u0026#34; version=\u0026#34;2.5\u0026#34;\u0026gt; \u0026lt;display-name\u0026gt;test\u0026lt;/display-name\u0026gt; \u0026lt;/web-app\u0026gt; a.jsp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34; pageEncoding=\u0026#34;UTF-8\u0026#34;%\u0026gt; \u0026lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here \u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; welcome- \u0026lt;%=\u0026#34;i am in docker tomcat self \u0026#34;%\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;% System.out,.printIn(\u0026#34;==========docker tomcat self\u0026#34;);%\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/htmI\u0026gt; 测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34; pageEncoding=\u0026#34;UTF-8\u0026#34;%\u0026gt; \u0026lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here \u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; welcome- \u0026lt;%=\u0026#34;i am in docker tomcat self \u0026#34;%\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;% System.out,.printIn(\u0026#34;==========docker tomcat self\u0026#34;);%\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/htmI\u0026gt; 小总结 第 七 章 Docker常用安装 总体步骤 搜索镜像\n拉取镜像\n查看镜像\n启动镜像\n停止容器\n移除容器\n安装Mysql docker hub 上查找mysql镜像 从 docker hub(阿里云加速器)拉取mysql镜像到本地标签为5.6 使用mysql5.6镜像创建容器(也叫运行镜像) 使用mysql镜像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 docker run -p 12345:3306 --name mysql -v /ggcc/mysql/conf:/etc/mysql/conf.d -v /ggcc/mysql/logs:/logs -v /ggcc/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 ---------------------------------------------- 命令说明: -p 12345:3306:将主机的12345端口映射到docker容器的3306端口。 -name mysq:运行服务名字 -V /ggcc/mysql/conf:/etc/mysql/conf.d :将主机/zzyyuse/mysq|录下的conf/my.cnf挂载到容器的/etc/mysql/conf.d -v /ggcc/mysqlogs/logs: 将主机/zzyyuse/mysq|目 录下的logs 目录挂载到容器的/logs。 -V /ggcc/mysqldata:/var/lib/mysql :将主机lzzyyuse/mysql目录下的data目录挂载到容器的/var/lib/mysql . -e MYSQL_ROOT_PASSWORD=123456: 初始化root用户的密码。. -d mysql:5.6:后台程序运行mysql5.6 ---------------------------------------------- docker exec -it Mysql运行成功后的容器ID /bin/bash ---------------------------------------------- 数据备份小测试 docker exec mysql服务容器ID sh -c \u0026#39;exec mysqldump --all-databases -uroot -p\u0026#34;123456\u0026#34;\u0026#39; \u0026gt;/ggcc/all-database.sql 数据备份测试 安装Redis 从docker hu上(阿里云加速器)拉取redis镜像到本地标签为：3.2 使用redis3.2镜像创建容器(也叫运行镜像) ​\t使用镜像\n1 docker run -p 6379:6379 -v /ggcc/myredis/data:/data -v /ggcc/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes 在主机/ggcc/myredis/conf/redis.conf目录上新建redis.conf文件 vim /ggcc/myredis/conf/redis.conf/redis.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 # Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k =\u0026gt; 1000 bytes # 1kb =\u0026gt; 1024 bytes # 1m =\u0026gt; 1000000 bytes # 1mb =\u0026gt; 1024*1024 bytes # 1g =\u0026gt; 1000000000 bytes # 1gb =\u0026gt; 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \u0026#34;include\u0026#34; won\u0026#39;t be rewritten by command \u0026#34;CONFIG REWRITE\u0026#34; # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you\u0026#39;d better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so ################################## NETWORK ##################################### # By default, if no \u0026#34;bind\u0026#34; configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \u0026#34;bind\u0026#34; configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 loopback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \u0026#34;bind\u0026#34; directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \u0026#34;bind\u0026#34; directive. protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 ################################# TLS/SSL ##################################### # By default, TLS/SSL is disabled. To enable it, the \u0026#34;tls-port\u0026#34; configuration # directive can be used to define TLS-listening ports. To enable TLS on the # default port, use: # # port 0 # tls-port 6379 # Configure a X.509 certificate and private key to use for authenticating the # server to connected clients, masters or cluster peers. These files should be # PEM formatted. # # tls-cert-file redis.crt # tls-key-file redis.key # Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange: # # tls-dh-params-file redis.dh # Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL # clients and peers. Redis requires an explicit configuration of at least one # of these, and will not implicitly use the system wide configuration. # # tls-ca-cert-file ca.crt # tls-ca-cert-dir /etc/ssl/certs # By default, clients (including replica servers) on a TLS port are required # to authenticate using valid client side certificates. # # If \u0026#34;no\u0026#34; is specified, client certificates are not required and not accepted. # If \u0026#34;optional\u0026#34; is specified, client certificates are accepted and must be # valid if provided, but are not required. # # tls-auth-clients no # tls-auth-clients optional # By default, a Redis replica does not attempt to establish a TLS connection # with its master. # # Use the following directive to enable TLS on replication links. # # tls-replication yes # By default, the Redis Cluster bus uses a plain TCP connection. To enable # TLS for the bus protocol, use the following directive: # # tls-cluster yes # Explicitly specify TLS versions to support. Allowed values are case insensitive # and include \u0026#34;TLSv1\u0026#34;, \u0026#34;TLSv1.1\u0026#34;, \u0026#34;TLSv1.2\u0026#34;, \u0026#34;TLSv1.3\u0026#34; (OpenSSL \u0026gt;= 1.1.1) or # any combination. To enable only TLSv1.2 and TLSv1.3, use: # # tls-protocols \u0026#34;TLSv1.2 TLSv1.3\u0026#34; # Configure allowed ciphers. See the ciphers(1ssl) manpage for more information # about the syntax of this string. # # Note: this configuration applies only to \u0026lt;= TLSv1.2. # # tls-ciphers DEFAULT:!MEDIUM # Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more # information about the syntax of this string, and specifically for TLSv1.3 # ciphersuites. # # tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256 # When choosing a cipher, use the server\u0026#39;s preference instead of the client # preference. By default, the server follows the client\u0026#39;s preference. # # tls-prefer-server-ciphers yes # By default, TLS session caching is enabled to allow faster and less expensive # reconnections by clients that support it. Use the following directive to disable # caching. # # tls-session-caching no # Change the default number of TLS sessions cached. A zero value sets the cache # to unlimited size. The default size is 20480. # # tls-session-cache-size 5000 # Change the default timeout of cached TLS sessions. The default timeout is 300 # seconds. # # tls-session-cache-timeout 60 ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use \u0026#39;yes\u0026#39; if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize no # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \u0026#34;process is ready.\u0026#34; # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \u0026#34;/var/run/redis.pid\u0026#34;. # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \u0026#34;\u0026#34; # To enable logging to the system logger, just set \u0026#39;syslog-enabled\u0026#39; to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT \u0026lt;dbid\u0026gt; where # dbid is a number between 0 and \u0026#39;databases\u0026#39;-1 databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \u0026#34;save\u0026#34; lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \u0026#34;\u0026#34; save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that\u0026#39;s set to \u0026#39;yes\u0026#39; as it\u0026#39;s almost always a win. # If you want to save some CPU in the saving child set it to \u0026#39;no\u0026#39; but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # Remove RDB files used by replication in instances without persistence # enabled. By default this option is disabled, however there are environments # where for regulations or other security concerns, RDB files persisted on # disk by masters in order to feed replicas, or stored on disk by replicas # in order to load them for the initial synchronization, should be deleted # ASAP. Note that this option ONLY WORKS in instances that have both AOF # and RDB persistence disabled, otherwise is completely ignored. # # An alternative (and sometimes better) way to obtain the same effect is # to use diskless replication on both master and replicas instances. However # in the case of replicas, diskless is not always an option. rdb-del-sync-files no # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the \u0026#39;dbfilename\u0026#39; configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./ ################################# REPLICATION ################################# # Master-Replica replication. Use replicaof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # +------------------+ +---------------+ # | Master | ---\u0026gt; | Replica | # | (receive writes) | | (exact copy) | # +------------------+ +---------------+ # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of replicas. # 2) Redis replicas are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition replicas automatically try to reconnect to masters # and resynchronize with them. # # replicaof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt; # If the master is password protected (using the \u0026#34;requirepass\u0026#34; configuration # directive below) it is possible to tell the replica to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the replica request. # # masterauth \u0026lt;master-password\u0026gt; # # However this is not enough if you are using Redis ACLs (for Redis version # 6 or greater), and the default user is not capable of running the PSYNC # command and/or other commands needed for replication. In this case it\u0026#39;s # better to configure a special user to use with replication, and specify the # masteruser configuration as such: # # masteruser \u0026lt;username\u0026gt; # # When masteruser is specified, the replica will authenticate against its # master using the new AUTH form: AUTH \u0026lt;username\u0026gt; \u0026lt;password\u0026gt;. # When a replica loses its connection with the master, or when the replication # is still in progress, the replica can act in two different ways: # # 1) if replica-serve-stale-data is set to \u0026#39;yes\u0026#39; (the default) the replica will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if replica-serve-stale-data is set to \u0026#39;no\u0026#39; the replica will reply with # an error \u0026#34;SYNC with master in progress\u0026#34; to all the kind of commands # but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, # SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, # COMMAND, POST, HOST: and LATENCY. # replica-serve-stale-data yes # You can configure a replica instance to accept writes or not. Writing against # a replica instance may be useful to store some ephemeral data (because data # written on a replica will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default replicas are read-only. # # Note: read only replicas are not designed to be exposed to untrusted clients # on the internet. It\u0026#39;s just a protection layer against misuse of the instance. # Still a read only replica exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only replicas using \u0026#39;rename-command\u0026#39; to shadow all the # administrative / dangerous commands. replica-read-only yes # Replication SYNC strategy: disk or socket. # # New replicas and reconnecting replicas that are not able to continue the # replication process just receiving differences, need to do what is called a # \u0026#34;full synchronization\u0026#34;. An RDB file is transmitted from the master to the # replicas. # # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the replicas incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to replica sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more replicas # can be queued and served with the RDB file as soon as the current child # producing the RDB file finishes its work. With diskless replication instead # once the transfer starts, new replicas arriving will be queued and a new # transfer will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple # replicas will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the replicas. # # This is important since once the transfer starts, it is not possible to serve # new replicas arriving, that will be queued for the next RDB transfer, so the # server waits a delay in order to let more replicas arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # ----------------------------------------------------------------------------- # WARNING: RDB diskless load is experimental. Since in this setup the replica # does not immediately store an RDB on disk, it may cause data loss during # failovers. RDB diskless load + Redis modules not handling I/O reads may also # cause Redis to abort in case of I/O errors during the initial synchronization # stage with the master. Use only if your do what you are doing. # ----------------------------------------------------------------------------- # # Replica can load the RDB it reads from the replication link directly from the # socket, or store the RDB to a file and read that file after it was completely # recived from the master. # # In many cases the disk is slower than the network, and storing and loading # the RDB file may increase replication time (and even increase the master\u0026#39;s # Copy on Write memory and salve buffers). # However, parsing the RDB file directly from the socket may mean that we have # to flush the contents of the current database before the full rdb was # received. For this reason we have the following options: # # \u0026#34;disabled\u0026#34; - Don\u0026#39;t use diskless load (store the rdb file to the disk first) # \u0026#34;on-empty-db\u0026#34; - Use diskless load only when it is completely safe. # \u0026#34;swapdb\u0026#34; - Keep a copy of the current db contents in RAM while parsing # the data directly from the socket. note that this requires # sufficient memory, if you don\u0026#39;t have it, you risk an OOM kill. repl-diskless-load disabled # Replicas send PINGs to server in a predefined interval. It\u0026#39;s possible to # change this interval with the repl_ping_replica_period option. The default # value is 10 seconds. # # repl-ping-replica-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of replica. # 2) Master timeout from the point of view of replicas (data, pings). # 3) Replica timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-replica-period otherwise a timeout will be detected # every time there is low traffic between the master and the replica. # # repl-timeout 60 # Disable TCP_NODELAY on the replica socket after SYNC? # # If you select \u0026#34;yes\u0026#34; Redis will use a smaller number of TCP packets and # less bandwidth to send data to replicas. But this can add a delay for # the data to appear on the replica side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \u0026#34;no\u0026#34; the delay for data to appear on the replica side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and replicas are many hops away, turning this to \u0026#34;yes\u0026#34; may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # replica data when replicas are disconnected for some time, so that when a # replica wants to reconnect again, often a full resync is not needed, but a # partial resync is enough, just passing the portion of data the replica # missed while disconnected. # # The bigger the replication backlog, the longer the time the replica can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a replica connected. # # repl-backlog-size 1mb # After a master has no longer connected replicas for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last replica disconnected, for # the backlog buffer to be freed. # # Note that replicas never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \u0026#34;partially # resynchronize\u0026#34; with the replicas: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The replica priority is an integer number published by Redis in the INFO # output. It is used by Redis Sentinel in order to select a replica to promote # into a master if the master is no longer working correctly. # # A replica with a low priority number is considered better for promotion, so # for instance if there are three replicas with priority 10, 100, 25 Sentinel # will pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the replica as not able to perform the # role of master, so a replica with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. replica-priority 100 # It is possible for a master to stop accepting writes if there are less than # N replicas connected, having a lag less or equal than M seconds. # # The N replicas need to be in \u0026#34;online\u0026#34; state. # # The lag in seconds, that must be \u0026lt;= the specified value, is calculated from # the last ping received from the replica, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough replicas # are available, to the specified number of seconds. # # For example to require at least 3 replicas with a lag \u0026lt;= 10 seconds use: # # min-replicas-to-write 3 # min-replicas-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-replicas-to-write is set to 0 (feature disabled) and # min-replicas-max-lag is set to 10. # A Redis master is able to list the address and port of the attached # replicas in different ways. For example the \u0026#34;INFO replication\u0026#34; section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover replica instances. # Another place where this info is available is in the output of the # \u0026#34;ROLE\u0026#34; command of a master. # # The listed IP and address normally reported by a replica is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the replica to connect with the master. # # Port: The port is communicated by the replica during the replication # handshake, and is normally the port that the replica is using to # listen for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the replica may be actually reachable via different IP and port # pairs. The following two options can be used by a replica in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # replica-announce-ip 5.5.5.5 # replica-announce-port 1234 ############################### KEYS TRACKING ################################# # Redis implements server assisted support for client side caching of values. # This is implemented using an invalidation table that remembers, using # 16 millions of slots, what clients may have certain subsets of keys. In turn # this is used in order to send invalidation messages to clients. Please # to understand more about the feature check this page: # # https://redis.io/topics/client-side-caching # # When tracking is enabled for a client, all the read only queries are assumed # to be cached: this will force Redis to store information in the invalidation # table. When keys are modified, such information is flushed away, and # invalidation messages are sent to the clients. However if the workload is # heavily dominated by reads, Redis could use more and more memory in order # to track the keys fetched by many clients. # # For this reason it is possible to configure a maximum fill value for the # invalidation table. By default it is set to 1M of keys, and once this limit # is reached, Redis will start to evict keys in the invalidation table # even if they were not modified, just to reclaim memory: this will in turn # force the clients to invalidate the cached values. Basically the table # maximum size is a trade off between the memory you want to spend server # side to track information about who cached what, and the ability of clients # to retain cached objects in memory. # # If you set the value to 0, it means there are no limits, and Redis will # retain as many keys as needed in the invalidation table. # In the \u0026#34;stats\u0026#34; INFO section, you can find information about the number of # keys in the invalidation table at every given moment. # # Note: when key tracking is used in broadcasting mode, no memory is used # in the server side so this setting is useless. # # tracking-table-max-keys 1000000 ################################## SECURITY ################################### # Warning: since Redis is pretty fast an outside user can try up to # 1 million passwords per second against a modern box. This means that you # should use very strong passwords, otherwise they will be very easy to break. # Note that because the password is really a shared secret between the client # and the server, and should not be memorized by any human, the password # can be easily a long string from /dev/urandom or whatever, so by using a # long and unguessable password no brute force attack will be possible. # Redis ACL users are defined in the following format: # # user \u0026lt;username\u0026gt; ... acl rules ... # # For example: # # user worker +@list +@connection ~jobs:* on \u0026gt;ffa9203c493aa99 # # The special username \u0026#34;default\u0026#34; is used for new connections. If this user # has the \u0026#34;nopass\u0026#34; rule, then new connections will be immediately authenticated # as the \u0026#34;default\u0026#34; user without the need of any password provided via the # AUTH command. Otherwise if the \u0026#34;default\u0026#34; user is not flagged with \u0026#34;nopass\u0026#34; # the connections will start in not authenticated state, and will require # AUTH (or the HELLO command AUTH option) in order to be authenticated and # start to work. # # The ACL rules that describe what an user can do are the following: # # on Enable the user: it is possible to authenticate as this user. # off Disable the user: it\u0026#39;s no longer possible to authenticate # with this user, however the already authenticated connections # will still work. # +\u0026lt;command\u0026gt; Allow the execution of that command # -\u0026lt;command\u0026gt; Disallow the execution of that command # +@\u0026lt;category\u0026gt; Allow the execution of all the commands in such category # with valid categories are like @admin, @set, @sortedset, ... # and so forth, see the full list in the server.c file where # the Redis command table is described and defined. # The special category @all means all the commands, but currently # present in the server, and that will be loaded in the future # via modules. # +\u0026lt;command\u0026gt;|subcommand Allow a specific subcommand of an otherwise # disabled command. Note that this form is not # allowed as negative like -DEBUG|SEGFAULT, but # only additive starting with \u0026#34;+\u0026#34;. # allcommands Alias for +@all. Note that it implies the ability to execute # all the future commands loaded via the modules system. # nocommands Alias for -@all. # ~\u0026lt;pattern\u0026gt; Add a pattern of keys that can be mentioned as part of # commands. For instance ~* allows all the keys. The pattern # is a glob-style pattern like the one of KEYS. # It is possible to specify multiple patterns. # allkeys Alias for ~* # resetkeys Flush the list of allowed keys patterns. # \u0026gt;\u0026lt;password\u0026gt; Add this passowrd to the list of valid password for the user. # For example \u0026gt;mypass will add \u0026#34;mypass\u0026#34; to the list. # This directive clears the \u0026#34;nopass\u0026#34; flag (see later). # \u0026lt;\u0026lt;password\u0026gt; Remove this password from the list of valid passwords. # nopass All the set passwords of the user are removed, and the user # is flagged as requiring no password: it means that every # password will work against this user. If this directive is # used for the default user, every new connection will be # immediately authenticated with the default user without # any explicit AUTH command required. Note that the \u0026#34;resetpass\u0026#34; # directive will clear this condition. # resetpass Flush the list of allowed passwords. Moreover removes the # \u0026#34;nopass\u0026#34; status. After \u0026#34;resetpass\u0026#34; the user has no associated # passwords and there is no way to authenticate without adding # some password (or setting it as \u0026#34;nopass\u0026#34; later). # reset Performs the following actions: resetpass, resetkeys, off, # -@all. The user returns to the same state it has immediately # after its creation. # # ACL rules can be specified in any order: for instance you can start with # passwords, then flags, or key patterns. However note that the additive # and subtractive rules will CHANGE MEANING depending on the ordering. # For instance see the following example: # # user alice on +@all -DEBUG ~* \u0026gt;somepassword # # This will allow \u0026#34;alice\u0026#34; to use all the commands with the exception of the # DEBUG command, since +@all added all the commands to the set of the commands # alice can use, and later DEBUG was removed. However if we invert the order # of two ACL rules the result will be different: # # user alice on -DEBUG +@all ~* \u0026gt;somepassword # # Now DEBUG was removed when alice had yet no commands in the set of allowed # commands, later all the commands are added, so the user will be able to # execute everything. # # Basically ACL rules are processed left-to-right. # # For more information about ACL configuration please refer to # the Redis web site at https://redis.io/topics/acl # ACL LOG # # The ACL Log tracks failed commands and authentication events associated # with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below. acllog-max-len 128 # Using an external ACL file # # Instead of configuring users here in this file, it is possible to use # a stand-alone file just listing users. The two methods cannot be mixed: # if you configure users here and at the same time you activate the exteranl # ACL file, the server will refuse to start. # # The format of the external ACL user file is exactly the same as the # format that is used inside redis.conf to describe users. # # aclfile /etc/redis/users.acl # IMPORTANT NOTE: starting with Redis 6 \u0026#34;requirepass\u0026#34; is just a compatiblity # layer on top of the new ACL system. The option effect will be just setting # the password for the default user. Clients will still authenticate using # AUTH \u0026lt;password\u0026gt; as usually, or more explicitly with AUTH default \u0026lt;password\u0026gt; # if they follow the new protocol: both will work. # # requirepass foobared # Command renaming (DEPRECATED). # # ------------------------------------------------------------------------ # WARNING: avoid using this option if possible. Instead use ACLs to remove # commands from the default user, and put them only in some admin user you # create for administrative purposes. # ------------------------------------------------------------------------ # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \u0026#34;\u0026#34; # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to replicas may cause problems. ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error \u0026#39;max number of clients reached\u0026#39;. # # IMPORTANT: When Redis Cluster is used, the max number of connections is also # shared with the cluster bus: every node in the cluster will use two # connections, one incoming and another outgoing. It is important to size the # limit accordingly in case of very large clusters. # # maxclients 10000 ############################## MEMORY MANAGEMENT ################################ # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can\u0026#39;t remove keys according to the policy, or if the policy is # set to \u0026#39;noeviction\u0026#39;, Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the \u0026#39;noeviction\u0026#39; policy). # # WARNING: If you have replicas attached to an instance with maxmemory on, # the size of the output buffers needed to feed the replicas are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of replicas is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have replicas attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for replica # output buffers (but this is not needed if the policy is \u0026#39;noeviction\u0026#39;). # # maxmemory \u0026lt;bytes\u0026gt; # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select one from the following behaviors: # # volatile-lru -\u0026gt; Evict using approximated LRU, only keys with an expire set. # allkeys-lru -\u0026gt; Evict any key using approximated LRU. # volatile-lfu -\u0026gt; Evict using approximated LFU, only keys with an expire set. # allkeys-lfu -\u0026gt; Evict any key using approximated LFU. # volatile-random -\u0026gt; Remove a random key having an expire set. # allkeys-random -\u0026gt; Remove a random key, any key. # volatile-ttl -\u0026gt; Remove the key with the nearest expire time (minor TTL) # noeviction -\u0026gt; Don\u0026#39;t evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5 # Starting from Redis 5, by default a replica will ignore its maxmemory setting # (unless it is promoted to master after a failover or manually). It means # that the eviction of keys will be just handled by the master, sending the # DEL commands to the replica as keys evict in the master side. # # This behavior ensures that masters and replicas stay consistent, and is usually # what you want, however if your replica is writable, or you want the replica # to have a different memory setting, and you are sure all the writes performed # to the replica are idempotent, then you may change this default (but be sure # to understand what you are doing). # # Note that since the replica by default does not evict, it may end using more # memory than the one set via maxmemory (there are certain buffers that may # be larger on the replica, or data structures may sometimes take more memory # and so forth). So make sure you monitor your replicas and make sure they # have enough memory to never hit a real out-of-memory condition before the # master hits the configured maxmemory setting. # # replica-ignore-maxmemory yes # Redis reclaims expired keys in two ways: upon access when those keys are # found to be expired, and also in background, in what is called the # \u0026#34;active expire key\u0026#34;. The key space is slowly and interactively scanned # looking for expired keys to reclaim, so that it is possible to free memory # of keys that are expired and will never be accessed again in a short time. # # The default effort of the expire cycle will try to avoid having more than # ten percent of expired keys still in memory, and will try to avoid consuming # more than 25% of total memory and to add latency to the system. However # it is possible to increase the expire \u0026#34;effort\u0026#34; that is normally set to # \u0026#34;1\u0026#34;, to a greater value, up to the value \u0026#34;10\u0026#34;. At its maximum value the # system will use more CPU, longer cycles (and technically may introduce # more latency), and will tollerate less already expired keys still present # in the system. It\u0026#39;s a tradeoff betweeen memory, CPU and latecy. # # active-expire-effort 1 ############################# LAZY FREEING #################################### # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It\u0026#39;s up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 4) During replication, when a replica performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transferred. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives. lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no # It is also possible, for the case when to replace the user code DEL calls # with UNLINK calls is not easy, to modify the default behavior of the DEL # command to act exactly like UNLINK, using the following configuration # directive: lazyfree-lazy-user-del no ################################ THREADED I/O ################################# # Redis is mostly single threaded, however there are certain threaded # operations such as UNLINK, slow I/O accesses and other things that are # performed on side threads. # # Now it is also possible to handle Redis clients socket reads and writes # in different I/O threads. Since especially writing is so slow, normally # Redis users use pipelining in order to speedup the Redis performances per # core, and spawn multiple instances in order to scale more. Using I/O # threads it is possible to easily speedup two times Redis without resorting # to pipelining nor sharding of the instance. # # By default threading is disabled, we suggest enabling it only in machines # that have at least 4 or more cores, leaving at least one spare core. # Using more than 8 threads is unlikely to help much. We also recommend using # threaded I/O only if you actually have performance problems, with Redis # instances being able to use a quite big percentage of CPU time, otherwise # there is no point in using this feature. # # So for instance if you have a four cores boxes, try to use 2 or 3 I/O # threads, if you have a 8 cores, try to use 6 threads. In order to # enable I/O threads use the following configuration directive: # # io-threads 4 # # Setting io-threads to 1 will just use the main thread as usually. # When I/O threads are enabled, we only use threads for writes, that is # to thread the write(2) syscall and transfer the client buffers to the # socket. However it is also possible to enable threading of reads and # protocol parsing using the following configuration directive, by setting # it to yes: # # io-threads-do-reads no # # Usually threading reads doesn\u0026#39;t help much. # # NOTE 1: This configuration directive cannot be changed at runtime via # CONFIG SET. Aso this feature currently does not work when SSL is # enabled. # # NOTE 2: If you want to test the Redis speedup using redis-benchmark, make # sure you also run the benchmark itself in threaded mode, using the # --threads option to match the number of Redis theads, otherwise you\u0026#39;ll not # be able to notice the improvements. ############################ KERNEL OOM CONTROL ############################## # On Linux, it is possible to hint the kernel OOM killer on what processes # should be killed first when out of memory. # # Enabling this feature makes Redis actively control the oom_score_adj value # for all its processes, depending on their role. The default scores will # attempt to have background child processes killed before all others, and # replicas killed before masters. oom-score-adj no # When oom-score-adj is used, this directive controls the specific values used # for master, replica and background child processes. Values range -1000 to # 1000 (higher means more likely to be killed). # # Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities) # can freely increase their value, but not decrease it below its initial # settings. # # Values are used relative to the initial value of oom_score_adj when the server # starts. Because typically the initial value is 0, they will often match the # absolute values. oom-score-adj-values 0 200 800 ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \u0026#34;appendonly.aof\u0026#34;) appendfilename \u0026#34;appendonly.aof\u0026#34; # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don\u0026#39;t fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \u0026#34;everysec\u0026#34;, as that\u0026#39;s usually the right compromise between # speed and data safety. It\u0026#39;s up to you to understand if you can relax this to # \u0026#34;no\u0026#34; that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that\u0026#39;s snapshotting), # or on the contrary, use \u0026#34;always\u0026#34; that\u0026#39;s very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \u0026#34;everysec\u0026#34;. # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it\u0026#39;s possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \u0026#34;appendfsync none\u0026#34;. In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \u0026#34;yes\u0026#34;. Otherwise leave it as # \u0026#34;no\u0026#34; that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can\u0026#39;t happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \u0026#34;redis-check-aof\u0026#34; utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \u0026#34;REDIS\u0026#34; # string and loads the prefixed RDB file, and continues loading the AOF # tail. aof-use-rdb-preamble yes ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn\u0026#39;t want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 ################################ REDIS CLUSTER ############################### # Normal Redis instances can\u0026#39;t be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A replica of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a replica to actually have an exact measure of # its \u0026#34;data age\u0026#34;, so the following two checks are performed: # # 1) If there are multiple replicas able to failover, they exchange messages # in order to try to give an advantage to the replica with the best # replication offset (more data from the master processed). # Replicas will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single replica computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \u0026#34;connected\u0026#34; state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the replica will not try to failover # at all. # # The point \u0026#34;2\u0026#34; can be tuned by user. Specifically a replica will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * replica-validity-factor) + repl-ping-replica-period # # So for example if node-timeout is 30 seconds, and the replica-validity-factor # is 10, and assuming a default repl-ping-replica-period of 10 seconds, the # replica will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large replica-validity-factor may allow replicas with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a replica at all. # # For maximum availability, it is possible to set the replica-validity-factor # to a value of 0, which means, that replicas will always try to failover the # master regardless of the last time they interacted with the master. # (However they\u0026#39;ll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-replica-validity-factor 10 # Cluster replicas are able to migrate to orphaned masters, that are masters # that are left without working replicas. This improves the cluster ability # to resist to failures as otherwise an orphaned master can\u0026#39;t be failed over # in case of failure if it has no working replicas. # # Replicas migrate to orphaned masters only if there are still at least a # given number of other working replicas for their old master. This number # is the \u0026#34;migration barrier\u0026#34;. A migration barrier of 1 means that a replica # will migrate only if there is at least 1 other working replica for its master # and so forth. It usually reflects the number of replicas you want for every # master in your cluster. # # Default is 1 (replicas migrate only if their masters remain with at least # one replica). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # This option, when set to yes, prevents replicas from trying to failover its # master during master failures. However the master can still perform a # manual failover, if forced to do so. # # This is useful in different scenarios, especially in the case of multiple # data center operations, where we want one side to never be promoted if not # in the case of a total DC failure. # # cluster-replica-no-failover no # This option, when set to yes, allows nodes to serve read traffic while the # the cluster is in a down state, as long as it believes it owns the slots. # # This is useful for two cases. The first case is for when an application # doesn\u0026#39;t require consistency of data during node failures or network partitions. # One example of this is a cache, where as long as the node has the data it # should be able to serve it. # # The second use case is for configurations that don\u0026#39;t meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the # entire cluster without this option set, with it set there is only a write outage. # Without a quorum of masters, slot ownership will not change automatically. # # cluster-allow-reads-when-down no # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don\u0026#39;t have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \u0026#34;CONFIG SET latency-monitor-threshold \u0026lt;milliseconds\u0026gt;\u0026#34; if needed. latency-monitor-threshold 0 ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \u0026#34;foo\u0026#34; stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@\u0026lt;db\u0026gt;__ prefix. # E Keyevent events, published with __keyevent@\u0026lt;db\u0026gt;__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # t Stream commands # m Key-miss events (Note: It is not included in the \u0026#39;A\u0026#39; class) # A Alias for g$lshzxet, so that the \u0026#34;AKE\u0026#34; string means all the events # (Except key-miss events which are excluded from \u0026#39;A\u0026#39; due to their # unique nature). # # The \u0026#34;notify-keyspace-events\u0026#34; takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don\u0026#39;t need # this feature and the feature has some overhead. Note that if you don\u0026#39;t # specify at least one of K or E, no events will be delivered. notify-keyspace-events \u0026#34;\u0026#34; ############################### GOPHER SERVER ################################# # Redis contains an implementation of the Gopher protocol, as specified in # the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt). # # The Gopher protocol was very popular in the late \u0026#39;90s. It is an alternative # to the web, and the implementation both server and client side is so simple # that the Redis server has just 100 lines of code in order to implement this # support. # # What do you do with Gopher nowadays? Well Gopher never *really* died, and # lately there is a movement in order for the Gopher more hierarchical content # composed of just plain text documents to be resurrected. Some want a simpler # internet, others believe that the mainstream internet became too much # controlled, and it\u0026#39;s cool to create an alternative space for people that # want a bit of fresh air. # # Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol # as a gift. # # --- HOW IT WORKS? --- # # The Redis Gopher support uses the inline protocol of Redis, and specifically # two kind of inline requests that were anyway illegal: an empty request # or any request that starts with \u0026#34;/\u0026#34; (there are no Redis commands starting # with such a slash). Normal RESP2/RESP3 requests are completely out of the # path of the Gopher protocol implementation and are served as usually as well. # # If you open a connection to Redis when Gopher is enabled and send it # a string like \u0026#34;/foo\u0026#34;, if there is a key named \u0026#34;/foo\u0026#34; it is served via the # Gopher protocol. # # In order to create a real Gopher \u0026#34;hole\u0026#34; (the name of a Gopher site in Gopher # talking), you likely need a script like the following: # # https://github.com/antirez/gopher2redis # # --- SECURITY WARNING --- # # If you plan to put Redis on the internet in a publicly accessible address # to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance. # Once a password is set: # # 1. The Gopher server (when enabled, not by default) will still serve # content via Gopher. # 2. However other commands cannot be called before the client will # authenticate. # # So use the \u0026#39;requirepass\u0026#39; option to protect your instance. # # To enable Gopher support uncomment the following line and set # the option from no (the default) to yes. # # gopher-enabled no ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb \u0026lt;-- not recommended for normal workloads # -4: max size: 32 Kb \u0026lt;-- not recommended # -3: max size: 16 Kb \u0026lt;-- probably not recommended # -2: max size: 8 Kb \u0026lt;-- good # -1: max size: 4 Kb \u0026lt;-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \u0026#34;don\u0026#39;t start compressing until after 1 node into the list, # going from either the head or tail\u0026#34; # So: [head]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]-\u0026gt;[next]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[prev]-\u0026gt;[tail] # 2 here means: don\u0026#39;t compress head or head-\u0026gt;next or tail-\u0026gt;prev or tail, # but compress all nodes between them. # 3: [head]-\u0026gt;[next]-\u0026gt;[next]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[prev]-\u0026gt;[prev]-\u0026gt;[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Streams macro node max size / items. The stream data structure is a radix # tree of big nodes that encode multiple items inside. Using this configuration # it is possible to configure how big a single node can be in bytes, and the # maximum number of items it may contain before switching to a new node when # appending new stream entries. If any of the following settings are set to # zero, the limit is ignored, so for instance it is possible to set just a # max entires limit by setting max-bytes to 0 and max-entries to the desired # value. stream-node-max-bytes 4096 stream-node-max-entries 100 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \u0026#34;steps\u0026#34; are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \u0026#34;activerehashing no\u0026#34; if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \u0026#34;activerehashing yes\u0026#34; if you don\u0026#39;t have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can\u0026#39;t consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -\u0026gt; normal clients including MONITOR clients # replica -\u0026gt; replica clients # pubsub -\u0026gt; clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit \u0026lt;class\u0026gt; \u0026lt;hard limit\u0026gt; \u0026lt;soft limit\u0026gt; \u0026lt;soft seconds\u0026gt; # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don\u0026#39;t receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and replica clients, since # subscribers and replicas receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here, but must be 1mb or greater # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \u0026#34;hz\u0026#34; value. # # By default \u0026#34;hz\u0026#34; is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # Normally it is useful to have an HZ value which is proportional to the # number of clients connected. This is useful in order, for instance, to # avoid too many clients are processed for each background task invocation # in order to avoid latency spikes. # # Since the default HZ value by default is conservatively set to 10, Redis # offers, and enables by default, the ability to use an adaptive HZ value # which will temporary raise when there are many connected clients. # # When dynamic HZ is enabled, the actual configured HZ will be used # as a baseline, but multiples of the configured HZ value will be actually # used as needed once more clients are connected. In this way an idle # instance will use very little CPU time while a busy instance will be # more responsive. dynamic-hz yes # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # When redis saves RDB file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. rdb-save-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it\u0026#39;s maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R \u0026lt; P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits | # +--------+------------+------------+------------+------------+------------+ # | 0 | 104 | 255 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 1 | 18 | 49 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 10 | 10 | 18 | 142 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 100 | 8 | 11 | 49 | 143 | 255 | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # # redis-benchmark -n 1000000 incr foo # redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less \u0026lt;= 10). # # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # # lfu-log-factor 10 # lfu-decay-time 1 ########################### ACTIVE DEFRAGMENTATION ####################### # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \u0026#34;hot\u0026#34; way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don\u0026#39;t have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \u0026#34;CONFIG SET activedefrag yes\u0026#34;. # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag no # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage, to be used when the lower # threshold is reached # active-defrag-cycle-min 1 # Maximal effort for defrag in CPU percentage, to be used when the upper # threshold is reached # active-defrag-cycle-max 25 # Maximum number of set/hash/zset/list fields that will be processed from # the main dictionary scan # active-defrag-max-scan-fields 1000 # Jemalloc background thread for purging will be enabled by default jemalloc-bg-thread yes # It is possible to pin different threads and processes of Redis to specific # CPUs in your system, in order to maximize the performances of the server. # This is useful both in order to pin different Redis threads in different # CPUs, but also in order to make sure that multiple Redis instances running # in the same host will be pinned to different CPUs. # # Normally you can do this using the \u0026#34;taskset\u0026#34; command, however it is also # possible to this via Redis configuration directly, both in Linux and FreeBSD. # # You can pin the server/IO threads, bio threads, aof rewrite child process, and # the bgsave child process. The syntax to specify the cpu list is the same as # the taskset command: # # Set redis server/io threads to cpu affinity 0,2,4,6: # server_cpulist 0-7:2 # # Set bio threads to cpu affinity 1,3: # bio_cpulist 1,3 # # Set aof rewrite child process to cpu affinity 8,9,10,11: # aof_rewrite_cpulist 8-11 # # Set bgsave child process to cpu affinity 1,10,11 # bgsave_cpulist 1,10-11 测试 redis-cli连接上来 1 docker exec -it 运行着redis服务容器的ID redis-cli 测试持久化文件生成 第 8 章 将镜像推送到阿里云 本地镜像发布到阿里云流程 镜像生成方法 1、前面的Dockerfile\n2、从容器中创建一个新的镜像\n1 docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] 将本地镜像推送到阿里云 1、本地镜像素材原型 2、阿里云开发者平台\nhttps://promotion.aliyun.com/ntms/act/kubernetes.html\n3、创建镜像仓库 命名空间\n仓库名称\n4、将镜像推送到registry 1 2 3 4 $ sudo docker login --username=white3e registry.cn-shenzhen.aliyuncs.com $ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号] $ sudo docker push registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号] 其中[ImageId][镜像版本]自己填写 5、公有云可以查询得到 6、查看详情 将阿里云上的镜像下载到本地 ","permalink":"https://cold-bin.github.io/post/docker%E5%9F%BA%E7%A1%80/","tags":["docker","dockerfile"],"title":"Docker基础"},{"categories":["汇编"],"contents":"[toc]\n更灵活的定位内存地址的方法 1、and 和 or and指令：逻辑与指令，按位进行与运算。\n1 2 mov al, 01100011B and al, 00111011B 执行后：al=00100011B即都为1才为1\nor指令：逻辑或指令，按位进行或运算。\nmov al, 01100011B or al, 00111011B 执行后：al=01111011B 即只要有一个为1就为1\n关于ASCII码 世界上有很多编码方案，有一种方案叫做ASCII编码，是在计算机系统中通常被采用的。简单地说，所谓编码方案，就是一套规则，它约定了用什么样的信息来表示现实对象。比如说，在ASCII编码方案中，用61H表示“a”，62H表示“b”。一种规则需要人们遵守才有意义。\n在文本编辑过程中，我们按一下键盘的a键，就会在屏幕上看到“a”。我们按下键盘的a键，这个按键的信息被送入计算机，计算机用ASCII码的规则对其进行编码，将其转化为61H存储在内存的指定空间中；文本编辑软件从内存中取出61H，将其送到显卡上的显存中；工作在文本模式下的显卡，用ASCII码的规则解释显存中的内容， 61H被当作字符“a”，显卡驱动显示器，将字符“a”的图像画在屏幕上。我们可以看到，显卡在处理文本信息的时候，是按照ASCII码的规则进行的。这也就是说，如果我们要想在显示器上看到“a”，就要给显卡提供“a”的ASCIⅡ码，61H。如何提供？当然是写入显存中。\n以字符形式给出的数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 assume cs:code,ds:data data segment db \u0026#39;unIx\u0026#39; ;相当于“db 75H，6EH，49H，58H” db \u0026#39;foRK\u0026#39; data ends code segment start:\tmov al, \u0026#39;a\u0026#39; ;相当于“mov al, 61H”，“a”的ASCI码为61H； mov b1, \u0026#39;b\u0026#39; mov ax, 4c00h int 21h code ends end start 大小写转换的问题 小写字母的ASCII码值比大写字母的ASCII码值大20H\n大写字母ASCII码的第5位为0，小写字母的第5位为1(其他一致)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 assume cs:codesg,ds:datasg datasg segment db \u0026#39;BaSiC\u0026#39; db \u0026#39;iNfOrMaTion\u0026#39; datasg end codesg segment start:\tmov ax, datasg mov ds, ax\t;设置ds 指向 datasg段 mov bx, 0\t;设置（bx）=0，ds:bx指向’BaSic’的第一个字母 mov cx, 5 ;设置循环次数5，因为’Basic\u0026#39;有5个字母 s:\tmov al, [bx] ;将ASCII码从ds:bx所指向的单元中取出 and al, 11011111B;将al中的ASCII码的第5位置为0，变为大写字母 mov [bx], al\t;将转变后的ASCII码写回原单元 inc bx\t;（bx）加1，ds:bx指向下一个字母 loop s mov bx, 5\t;设置（bx）=5，ds:bx指向，iNfOrMaTion\u0026#39;的第一个字母 mov cx, 11\t;设置循环次数11，因为‘iNfOrMaTion\u0026#39;有11个字母 s0:\tmov al, [bx] or al, 00100000B;将a1中的ASCII码的第5位置为1，变为小写字母 mov [bx], al inc bx loop s0 mov ax, 4c00h int 21h codesg ends 2、[bx+idata] [bx+idata]表示一个内存单元, 例如：mov ax, [bx+200] 该指令也可以写成如下格式：\n1 2 3 4 5 mov ax, [200+bx] mov ax, 200[bx] mov ax, [bx].200 用[bx+idata]的方式进行数组的处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 assume cs:codesg,ds:datasg datasg segment db \u0026#39;BaSiC\u0026#39;;转为大写 db \u0026#39;MinIx\u0026#39;;转为小写 datasg ends codesg segment start: mov ax, datasg mov ds, ax mov bx, 0 ;初始ds:bx mov cx, 5 s:\tmov al, 0[bx] and al, 11011111b ;转为大写字母 mov 0[bx], al ;写回 mov al, 5[bx] ;[5 + bx] or al, 00100000b ;转为小写字母 mov 5[bx], al inc bx loop s mov ax, 4c00h int 21h codesg ends end start C语言描述\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int main() { char a[] = \u0026#34;BaSic\u0026#34;; char b[] = \u0026#34;MinIX\u0026#34;; int i = 0; do { a[i] = a[i] \u0026amp; 0xDF; b[i] = b[i] | 0x20; i++; } while(i \u0026lt; 5); return 0; } 3、SI 、DI 与 寻址方式的灵活应用 1、si 、di\nsi和di是8086CPU中和bx功能相近的寄存器，si和di不能够分成两个8位寄存器来使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 assume cs: codesg, ds: datasg datasg segment db \u0026#39;welcome to masm!\u0026#39;;用si和di实现将字符串‘welcome to masm！\u0026#34;复制到它后面的数据区中。 db \u0026#39;................\u0026#39; datasg ends codesg segment start:\tmov ax, datasg mov ds, ax mov si, 0 mov cx, 8 s:\tmov ax, 0[si] ;[0 + si] mov 16[si], ax ;[16 + si] 使用[bx +idata]方式代替di，使程序更简洁 add si, 2 loop s mov ax, 4c00h int 21h codesg ends end start 2、[bx + si] 和 [bx + di]\n[bx+si]和[bx+di]的含义相似\n[bx+si]表示一个内存单元，它的偏移地址为（bx）+（si）\n指令mov ax, [bx + si]的含义：将一个内存单元字数据的内容送入ax，段地址在ds中\n该指令也可以写成如下格式：mov ax, [bx][si]\n3、[bx+si+idata]和[bx+di+idata] [bx+si+idata]表示一个内存单元，它的偏移地址为（bx）+（si）+idata\n指令mov ax，[bx+si+idata]的含义：将一个内存单元字数据的内容送入ax，段地址在ds中\n4、不同的寻址方式的灵活应用 [idata]用一个常量来表示地址，可用于直接定位一个内存单元； [bx]用一个变量来表示内存地址，可用于间接定位一个内存单元； [bx+idata]用一个变量和常量表示地址，可在一个起始地址的基础上用变量间接定位一个内存单元； [bx+si]用两个变量表示地址； [bx+si+idata]用两个变量和一个常量表示地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ;将datasg段中每个单词改为大写字母 assume cs:codesg,ds:datasg,ss:stacksg datasg segment db \u0026#39;ibm \u0026#39; ;16 db \u0026#39;dec \u0026#39; db \u0026#39;dos \u0026#39; db \u0026#39;vax \u0026#39; ;看成二维数组 datasg ends stacksg segment ;定义一个段，用来做栈段，容量为16个字节 dw 0, 0, 0, 0, 0, 0, 0, 0 stacksg ends codesg segment start:\tmov ax, stacksg mov ss, ax mov sp, 16 mov ax, datasg mov ds, ax mov bx, 0 ;初始ds:bx ;cx为默认循环计数器，二重循环只有一个计数器，所以外层循环先保存cx值，再恢复，我们采用栈保存 mov cx, 4 s0:\tpush cx\t;将外层循环的cx值入栈 mov si, 0 mov cx, 3\t;cx设置为内层循环的次数 s:\tmov al, [bx+si] and al, 11011111b ;每个字符转为大写字母 mov [bx+si], al inc si loop s add bx, 16 ;下一行 pop cx\t;恢复cx值 loop s0 ;外层循环的loop指令将cx中的计数值减1 mov ax，4c00H int 21H codesg ends end start 4、总结 本章介绍了一些地址定位的方式 对于数据暂存最好不要使用寄存器，因为寄存器资源有限。应该使用栈作为数据暂存的选择 ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/","tags":[],"title":"汇编之其他定位地址方法"},{"categories":["redis"],"contents":"[toc]\nRedis应用 开篇导读 短信登录 这一块我们会使用redis共享session来实现\n商户查询缓存 通过本章节，我们会理解缓存击穿，缓存穿透，缓存雪崩等问题，让小伙伴的对于这些概念的理解不仅仅是停留在概念上，更是能在代码中看到对应的内容\n优惠卷秒杀 通过本章节，我们可以学会Redis的计数器功能， 结合Lua完成高性能的redis操作，同时学会Redis分布式锁的原理，包括Redis的三种消息队列\n附近的商户 我们利用Redis的GEOHash来完成对于地理坐标的操作\nUV统计 主要是使用Redis来完成统计功能\n用户签到 使用Redis的BitMap数据统计功能\n好友关注 基于Set集合的关注、取消关注，共同关注等等功能，这一块知识咱们之前就讲过，这次我们在项目中来使用一下\n打人探店 基于List来完成点赞列表的操作，同时基于SortedSet来完成点赞的排行榜功能\n以上这些内容咱们统统都会给小伙伴们讲解清楚，让大家充分理解如何使用Redis\n1、短信登录 1.1、导入黑马点评项目 1.1.1 、导入SQL 1.1.2、有关当前模型 手机或者app端发起请求，请求我们的nginx服务器，nginx基于七层模型走的事HTTP协议，可以实现基于Lua直接绕开tomcat访问redis，也可以作为静态资源服务器，轻松扛下上万并发， 负载均衡到下游tomcat服务器，打散流量，我们都知道一台4核8G的tomcat，在优化和处理简单业务的加持下，大不了就处理1000左右的并发， 经过nginx的负载均衡分流后，利用集群支撑起整个项目，同时nginx在部署了前端项目后，更是可以做到动静分离，进一步降低tomcat服务的压力，这些功能都得靠nginx起作用，所以nginx是整个项目中重要的一环。\n在tomcat支撑起并发流量后，我们如果让tomcat直接去访问Mysql，根据经验Mysql企业级服务器只要上点并发，一般是16或32 核心cpu，32 或64G内存，像企业级mysql加上固态硬盘能够支撑的并发，大概就是4000起~7000左右，上万并发， 瞬间就会让Mysql服务器的cpu，硬盘全部打满，容易崩溃，所以我们在高并发场景下，会选择使用mysql集群，同时为了进一步降低Mysql的压力，同时增加访问的性能，我们也会加入Redis，同时使用Redis集群使得Redis对外提供更好的服务。\n1.1.3、导入后端项目 在资料中提供了一个项目源码：\n1.1.4、导入前端工程 1.1.5 运行前端项目 1.2 、基于Session实现登录流程 发送验证码：\n用户在提交手机号后，会校验手机号是否合法，如果不合法，则要求用户重新输入手机号\n如果手机号合法，后台此时生成对应的验证码，同时将验证码进行保存，然后再通过短信的方式将验证码发送给用户\n短信验证码登录、注册：\n用户将验证码和手机号进行输入，后台从session中拿到当前验证码，然后和用户输入的验证码进行校验，如果不一致，则无法通过校验，如果一致，则后台根据手机号查询用户，如果用户不存在，则为用户创建账号信息，保存到数据库，无论是否存在，都会将用户信息保存到session中，方便后续获得当前登录信息\n校验登录状态:\n用户在请求时候，会从cookie中携带者JsessionId到后台，后台通过JsessionId从session中拿到用户信息，如果没有session信息，则进行拦截，如果有session信息，则将用户信息保存到threadLocal中，并且放行\n1.3 、实现发送短信验证码功能 页面流程\n具体代码如下\n贴心小提示：\n具体逻辑上文已经分析，我们仅仅只需要按照提示的逻辑写出代码即可。\n发送验证码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Override public Result sendCode(String phone, HttpSession session) { // 1.校验手机号 if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误！\u0026#34;); } // 3.符合，生成验证码 String code = RandomUtil.randomNumbers(6); // 4.保存验证码到 session session.setAttribute(\u0026#34;code\u0026#34;,code); // 5.发送验证码 log.debug(\u0026#34;发送短信验证码成功，验证码：{}\u0026#34;, code); // 返回ok return Result.ok(); } 登录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { // 1.校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误！\u0026#34;); } // 3.校验验证码 Object cacheCode = session.getAttribute(\u0026#34;code\u0026#34;); String code = loginForm.getCode(); if(cacheCode == null || !cacheCode.toString().equals(code)){ //3.不一致，报错 return Result.fail(\u0026#34;验证码错误\u0026#34;); } //一致，根据手机号查询用户 User user = query().eq(\u0026#34;phone\u0026#34;, phone).one(); //5.判断用户是否存在 if(user == null){ //不存在，则创建 user = createUserWithPhone(phone); } //7.保存用户信息到session中 session.setAttribute(\u0026#34;user\u0026#34;,user); return Result.ok(); } 1.4、实现登录拦截功能 java的拦截器类似于gin里的中间件，主要是做一些统一的操作，如：日志记录、权限检查、性能监控、某种状态维护等\n温馨小贴士：tomcat的运行原理\n当用户发起请求时，会访问我们像tomcat注册的端口，任何程序想要运行，都需要有一个线程对当前端口号进行监听，tomcat也不例外，当监听线程知道用户想要和tomcat连接连接时，那会由监听线程创建socket连接，socket都是成对出现的，用户通过socket像互相传递数据，当tomcat端的socket接受到数据后，此时监听线程会从tomcat的线程池中取出一个线程执行用户请求，在我们的服务部署到tomcat后，线程会找到用户想要访问的工程，然后用这个线程转发到工程中的controller，service，dao中，并且访问对应的DB，在用户执行完请求后，再统一返回，再找到tomcat端的socket，再将数据写回到用户端的socket，完成请求和响应\n通过以上讲解，我们可以得知 每个用户其实对应都是去找tomcat线程池中的一个线程来完成工作的， 使用完成后再进行回收，既然每个请求都是独立的，所以在每个用户去访问我们的工程时，我们可以使用threadlocal来做到线程隔离，每个线程操作自己的一份数据\n温馨小贴士：关于threadlocal\n如果小伙伴们看过threadLocal的源码，你会发现在threadLocal中，无论是他的put方法和他的get方法， 都是先从获得当前用户的线程，然后从线程中取出线程的成员变量map，只要线程不一样，map就不一样，所以可以通过这种方式来做到线程隔离\n拦截器代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //1.获取session HttpSession session = request.getSession(); //2.获取session中的用户 Object user = session.getAttribute(\u0026#34;user\u0026#34;); //3.判断用户是否存在 if(user == null){ //4.不存在，拦截，返回401状态码 response.setStatus(401); return false; } //5.存在，保存用户信息到Threadlocal UserHolder.saveUser((User)user); //6.放行 return true; } } 让拦截器生效\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Configuration public class MvcConfig implements WebMvcConfigurer { @Resource private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) { // 登录拦截器 registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( \u0026#34;/shop/**\u0026#34;, \u0026#34;/voucher/**\u0026#34;, \u0026#34;/shop-type/**\u0026#34;, \u0026#34;/upload/**\u0026#34;, \u0026#34;/blog/hot\u0026#34;, \u0026#34;/user/code\u0026#34;, \u0026#34;/user/login\u0026#34; ).order(1); // token刷新的拦截器 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns(\u0026#34;/**\u0026#34;).order(0); } } 1.5、隐藏用户敏感信息 我们通过浏览器观察到此时用户的全部信息都在，这样极为不靠谱，所以我们应当在返回用户信息之前，将用户的敏感信息进行隐藏，采用的核心思路就是书写一个UserDto对象，这个UserDto对象就没有敏感信息了，我们在返回前，将有用户敏感信息的User对象转化成没有敏感信息的UserDto对象，那么就能够避免这个尴尬的问题了\n在登录方法处修改\n1 2 //7.保存用户信息到session中 session.setAttribute(\u0026#34;user\u0026#34;, BeanUtils.copyProperties(user,UserDTO.class)); 在拦截器处：\n1 2 //5.存在，保存用户信息到Threadlocal UserHolder.saveUser((UserDTO) user); 在UserHolder处：将user对象换成UserDTO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class UserHolder { private static final ThreadLocal\u0026lt;UserDTO\u0026gt; tl = new ThreadLocal\u0026lt;\u0026gt;(); public static void saveUser(UserDTO user){ tl.set(user); } public static UserDTO getUser(){ return tl.get(); } public static void removeUser(){ tl.remove(); } } 1.6、session共享问题 核心思路分析：\n每个tomcat中都有一份属于自己的session,假设用户第一次访问第一台tomcat，并且把自己的信息存放到第一台服务器的session中，但是第二次这个用户访问到了第二台tomcat，那么在第二台服务器上，肯定没有第一台服务器存放的session，所以此时 整个登录拦截功能就会出现问题，我们能如何解决这个问题呢？早期的方案是session拷贝，就是说虽然每个tomcat上都有不同的session，但是每当任意一台服务器的session修改时，都会同步给其他的Tomcat服务器的session，这样的话，就可以实现session的共享了\n但是这种方案具有两个大问题\n1、每台服务器中都有完整的一份session数据，服务器压力过大。\n2、session拷贝数据时，可能会出现延迟\n所以咱们后来采用的方案都是基于redis来完成，我们把session换成redis，redis数据本身就是共享的，就可以避免session共享的问题了\n1.7 Redis代替session的业务流程 1.7.1、设计key的结构 首先我们要思考一下利用redis来存储数据，那么到底使用哪种结构呢？由于存入的数据比较简单，我们可以考虑使用String，或者是使用哈希，如下图，如果使用String，同学们注意他的value，用多占用一点空间，如果使用哈希，则他的value中只会存储他数据本身，如果不是特别在意内存，其实使用String就可以啦。\n1.7.2、设计key的具体细节 所以我们可以使用String结构，就是一个简单的key，value键值对的方式，但是关于key的处理，session他是每个用户都有自己的session，但是redis的key是共享的，咱们就不能使用code了\n在设计这个key的时候，我们之前讲过需要满足两点\n1、key要具有唯一性\n2、key要方便携带\n如果我们采用phone：手机号这个的数据来存储当然是可以的，但是如果把这样的敏感数据存储到redis中并且从页面中带过来毕竟不太合适，所以我们在后台生成一个随机串token，然后让前端带来这个token就能完成我们的整体逻辑了\n1.7.3、整体访问流程 当注册完成后，用户去登录会去校验用户提交的手机号和验证码，是否一致，如果一致，则根据手机号查询用户信息，不存在则新建，最后将用户数据保存到redis，并且生成token作为redis的key，当我们校验用户是否登录时，会去携带着token进行访问，从redis中取出token对应的value，判断是否存在这个数据，如果没有则拦截，如果存在则将其保存到threadLocal中，并且放行。\n1.8 基于Redis实现短信登录 这里具体逻辑就不分析了，之前咱们已经重点分析过这个逻辑啦。\nUserServiceImpl代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { // 1.校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误！\u0026#34;); } // 3.从redis获取验证码并校验 String cacheCode = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY + phone); String code = loginForm.getCode(); if (cacheCode == null || !cacheCode.equals(code)) { // 不一致，报错 return Result.fail(\u0026#34;验证码错误\u0026#34;); } // 4.一致，根据手机号查询用户 select * from tb_user where phone = ? User user = query().eq(\u0026#34;phone\u0026#34;, phone).one(); // 5.判断用户是否存在 if (user == null) { // 6.不存在，创建新用户并保存 user = createUserWithPhone(phone); } // 7.保存用户信息到 redis中 // 7.1.随机生成token，作为登录令牌 String token = UUID.randomUUID().toString(true); // 7.2.将User对象转为HashMap存储 UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); Map\u0026lt;String, Object\u0026gt; userMap = BeanUtil.beanToMap(userDTO, new HashMap\u0026lt;\u0026gt;(), CopyOptions.create() .setIgnoreNullValue(true) .setFieldValueEditor((fieldName, fieldValue) -\u0026gt; fieldValue.toString())); // 7.3.存储 String tokenKey = LOGIN_USER_KEY + token; stringRedisTemplate.opsForHash().putAll(tokenKey, userMap); // 7.4.设置token有效期 stringRedisTemplate.expire(tokenKey, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.返回token return Result.ok(token); } 1.9 解决状态登录刷新问题 1.9.1 初始方案思路总结： 在这个方案中，他确实可以使用对应路径的拦截，同时刷新登录token令牌的存活时间，但是现在这个拦截器他只是拦截需要被拦截的路径，假设当前用户访问了一些不需要拦截的路径，那么这个拦截器就不会生效，所以此时令牌刷新的动作实际上就不会执行，所以这个方案他是存在问题的\n1.9.2 优化方案 既然之前的拦截器无法对不需要拦截的路径生效，那么我们可以添加一个拦截器，在第一个拦截器中拦截所有的路径，把第二个拦截器做的事情放入到第一个拦截器中，同时刷新令牌，因为第一个拦截器有了threadLocal的数据，所以此时第二个拦截器只需要判断拦截器中的user对象是否存在即可，完成整体刷新功能。\n1.9.3 代码 RefreshTokenInterceptor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class RefreshTokenInterceptor implements HandlerInterceptor { private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.获取请求头中的token String token = request.getHeader(\u0026#34;authorization\u0026#34;); if (StrUtil.isBlank(token)) { return true; } // 2.基于TOKEN获取redis中的用户 String key = LOGIN_USER_KEY + token; Map\u0026lt;Object, Object\u0026gt; userMap = stringRedisTemplate.opsForHash().entries(key); // 3.判断用户是否存在 if (userMap.isEmpty()) { return true; } // 5.将查询到的hash数据转为UserDTO UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 6.存在，保存用户信息到 ThreadLocal UserHolder.saveUser(userDTO); // 7.刷新token有效期 stringRedisTemplate.expire(key, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.放行 return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { // 移除用户 UserHolder.removeUser(); } } LoginInterceptor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.判断是否需要拦截（ThreadLocal中是否有用户） if (UserHolder.getUser() == null) { // 没有，需要拦截，设置状态码 response.setStatus(401); // 拦截 return false; } // 有用户，则放行 return true; } } 2、商户查询缓存 2.1 什么是缓存? 前言:什么是缓存?\n就像自行车,越野车的避震器\n举个例子:越野车,山地自行车,都拥有\u0026quot;避震器\u0026quot;,防止车体加速后因惯性,在酷似\u0026quot;U\u0026quot;字母的地形上飞跃,硬着陆导致的损害,像个弹簧一样;\n同样,实际开发中,系统也需要\u0026quot;避震器\u0026quot;,防止过高的数据访问猛冲系统,导致其操作线程无法及时处理信息而瘫痪;\n这在实际开发中对企业讲,对产品口碑,用户评价都是致命的;所以企业非常重视缓存技术;\n缓存(Cache),就是数据交换的缓冲区,俗称的缓存就是缓冲区内的数据,一般从数据库中获取,存储于本地代码(例如:\n1 2 3 4 5 6 7 8 //例1: static final ConcurrentHashMap\u0026lt;K,V\u0026gt; map = new ConcurrentHashMap\u0026lt;\u0026gt;(); 本地用于高并发 //例2: static final Cache\u0026lt;K,V\u0026gt; USER_CACHE = CacheBuilder.newBuilder().build(); 用于redis等缓存 //例3: static final Map\u0026lt;K,V\u0026gt; map = new HashMap(); 本地缓存 由于其被Static修饰,所以随着类的加载而被加载到内存之中,作为本地缓存,由于其又被final修饰,所以其引用(例3:map)和对象(例3:new HashMap())之间的关系是固定的,不能改变,因此不用担心赋值(=)导致缓存失效;\n2.1.1 为什么要使用缓存 一句话:因为速度快,好用\n缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力\n实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为\u0026quot;避震器\u0026quot;,系统是几乎撑不住的,所以企业会大量运用到缓存技术;\n但是缓存也会增加代码复杂度和运营的成本:\n2.1.2 如何使用缓存 实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用\n浏览器缓存：主要是存在于浏览器端的缓存\n**应用层缓存：**可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存\n**数据库缓存：**在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中\n**CPU缓存：**当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存\n**磁盘缓存：**现代操作系统做了优化，当程序将数据写入磁盘时，第一步其实是先写入系统 page cache（fsync），之后再由系统线程决定何时真正刷盘（write）\n2.2 添加商户缓存 在我们查询商户信息时，我们是直接操作从数据库中去进行查询的，大致逻辑是这样，直接查询数据库那肯定慢咯，所以我们需要增加缓存\n1 2 3 4 5 @GetMapping(\u0026#34;/{id}\u0026#34;) public Result queryShopById(@PathVariable(\u0026#34;id\u0026#34;) Long id) { //这里是直接查询数据库 return shopService.queryById(id); } 2.2.1 、缓存模型和思路 标准的操作方式就是查询数据库之前先查询缓存，如果缓存数据存在，则直接从缓存中返回，如果缓存数据不存在，再查询数据库，然后将数据存入redis。\n2.1.2、代码如下 代码思路：如果缓存有，则直接返回，如果缓存不存在，则查询数据库，然后存入redis。\n2.3 缓存淘汰策略 缓存淘汰是redis为了节约内存而设计出来的一个东西，主要是因为内存数据宝贵，当我们向redis插入太多数据，此时就可能会导致缓存中的数据过多，所以redis会对部分数据进行淘汰。\n**内存淘汰：**redis自动进行，当redis内存达到咱们设定的max-memery的时候，会自动触发淘汰机制，淘汰掉一些不重要的数据(可以自己设置策略方式)\n**超时剔除：**当我们给redis设置了过期时间ttl之后，redis会将超时的数据进行删除，方便咱们继续使用缓存\n**主动淘汰：**我们可以手动调用方法把缓存删掉，通常用于解决缓存和数据库不一致问题\n低一致性：对于查询结果没有那么严苛，没有要求必须查询最新的数据。这种业务在使用redis作为缓存时，就没必要再查关系型数据库了，直接返回数据可以更好地降低系统的负载。 高一致性：对于查询结果严苛，要求必须查询最新的数据。所以，这种业务在使用redis作为缓存时，需要注意主动更新策略。 2.3.1 数据库缓存不一致解决方案： 由于我们的缓存的数据源来自于数据库,而数据库的数据是会发生变化的,因此,如果当数据库中数据发生变化,而缓存却没有同步,此时就会有一致性问题存在,其后果是:\n用户使用缓存中的过时数据,就会产生类似多线程数据安全问题,从而影响业务,产品口碑等;怎么解决呢？有如下几种方案\nCache Aside Pattern 人工编码方式：缓存调用者在更新完数据库后再去更新缓存，也称之为双写方案\nRead/Write Through Pattern : 由系统本身完成，数据库与缓存的问题交由系统本身去处理\nWrite Behind Caching Pattern ：调用者只操作缓存，其他线程去异步处理数据库，实现最终一致\n2.3.2 数据库和缓存不一致采用什么方案 综合考虑使用方案一，但是方案一调用者如何处理呢？这里有几个问题\n操作缓存和数据库时有三个问题需要考虑：\n删除缓存还是更新缓存？\n~~更新缓存：每次更新数据库都更新缓存，无效写操作较多\u0026ndash;总是期待会有很多查询缓存，类似于悲观锁机制。~~不推荐使用这种方案，悲观锁的机制，性能耗损较多。\n删除缓存：更新数据库时让缓存失效，查询时再更新缓存\n如果采用第一个方案，那么假设我们每次操作数据库后，都操作缓存，但是中间如果没有人查询，那么这个更新动作实际上只有最后一次生效，中间的更新动作意义并不大，我们可以把缓存删除，等待再次查询时，将缓存中的数据加载出来。第二种方案还能腾出缓存空间，属于是主动淘汰策略。\n如何保证缓存与数据库的操作的同时成功或失败？\u0026ndash;事务\n单体系统：将缓存与数据库操作放在一个事务。这个事务的话，可以在代码层面实现逻辑。 分布式系统：利用TCC等分布式事务方案 先操作缓存还是先操作数据库？\n先删除缓存，再操作数据库\n先操作数据库，再删除缓存\n应该具体操作缓存还是操作数据库，我们应当是先操作数据库，再删除缓存，原因在于，如果你选择第一种方案，在两个线程并发来访问时，假设线程1先来，他先把缓存删了，此时线程2过来，他查询缓存数据并不存在，此时他写入缓存，当他写入缓存后，线程1再执行更新动作时，实际上写入的就是旧的数据，新的数据被旧数据覆盖了。\n2.4 实现商铺和缓存与数据库双写一致 核心思路如下：\n修改ShopController中的业务逻辑，满足下面的需求：\n根据id查询店铺时，如果缓存未命中，则查询数据库，将数据库结果写入缓存，并设置超时时间 =\u0026gt; 双写策略\n根据id修改店铺时，先修改数据库，再删除缓存 =\u0026gt; 不一致性的最优解决方案\n修改重点代码1：修改ShopServiceImpl的queryById方法\n设置redis缓存时添加过期时间\n修改重点代码2\n代码分析：通过之前的淘汰，我们确定了采用删除策略，来解决双写问题，当我们修改了数据之后，然后把缓存中的数据进行删除，查询时发现缓存中没有数据，则会从mysql中加载最新的数据，从而避免数据库和缓存不一致的问题\n2.5 缓存穿透问题的解决思路 首先，我们来说说缓存穿透。什么是缓存穿透呢？缓存穿透问题在一定程度上与缓存命中率有关。如果我们的缓存设计的不合理，缓存的命中率非常低，那么，数据访问的绝大部分压力都会集中在后端数据库层面;当然也与请求有关，可能是某些恶意请求绕开了系统的安全机制，将请求打进了系统，但是这个请求的数据是恶意的随机量，那么这些请求就会在缓存和数据库这两层都失效，也就造成了缓存穿透问题\n缓存穿透：如果在请求数据时，在缓存层和数据库层都没有找到符合条件的数据，也就是说，在缓存层和数据库层都没有命中数据，那么，这种情况就叫作缓存穿透。\n造成缓存穿透的主要原因就是：查询某个 Key 对应的数据，Redis 缓存中没有相应的数据，则直接到数据库中查询。数据库中也不存在要查询的数据，则数据库会返回空，而 Redis 也不会缓存这个空结果。这就造成每次通过这样的 Key 去查询数据都会直接到数据库中查询，Redis 不会缓存空结果。这就造成了缓存穿透的问题。\n常见的解决方案有两种：\n缓存空对象 优点：实现简单，维护方便 缺点： 额外的内存消耗 可能造成短期的不一致 布隆过滤 优点：内存占用较少，没有多余key 缺点： 实现复杂 存在误判可能 **缓存空对象思路分析：**当我们客户端访问不存在的数据时，先请求redis，但是此时redis中没有数据，此时会访问到数据库，但是数据库中也没有数据，这个数据穿透了缓存，直击数据库，我们都知道数据库能够承载的并发不如redis这么高，如果大量的请求同时过来访问这种不存在的数据，这些请求就都会访问到数据库，简单的解决方案就是哪怕这个数据在数据库中也不存在，我们也把这个数据存入到redis中去，这样，下次用户过来访问这个不存在的数据，那么在redis中也能找到这个数据就不会进入到数据库了\n**布隆过滤：**布隆过滤器其实采用的是哈希思想来解决这个问题，通过一个庞大的二进制数组，走哈希思想去判断当前这个要查询的这个数据是否存在，如果布隆过滤器判断存在，则放行，这个请求会去访问redis，哪怕此时redis中的数据过期了，但是数据库中一定存在这个数据，在数据库中查询出来这个数据后，再将其放入到redis中。\n假设布隆过滤器判断这个数据不存在，则直接返回\n这种方式优点在于节约内存空间，存在误判，误判原因在于：布隆过滤器走的是哈希思想，只要哈希思想，就可能存在哈希冲突\n2.6 编码解决商品查询的缓存穿透问题： 核心思路如下：\n在原来的逻辑中，我们如果发现这个数据在mysql中不存在，直接就返回404了，这样是会存在缓存穿透问题的\n现在的逻辑中：如果这个数据不存在，我们不会返回404 ，还是会把这个数据写入到Redis中，并且将value设置为空，欧当再次发起查询时，我们如果发现命中之后，判断这个value是否是null，如果是null，则是之前写入的数据，证明是缓存穿透数据，如果不是，则直接返回数据。\n小总结：\n缓存穿透产生的原因是什么？\n用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力 缓存穿透的解决方案有哪些？\n缓存null值，防止后面的相同的穿透请求再次打进来 布隆过滤，识别数据是否存在 增强id的复杂度，避免被猜测id规律，防止大量恶意请求的穿透 做好数据的基础格式校验，减少错误或恶意请求 加强用户权限校验，减少错误或恶意请求 做好热点参数的限流，降低并发量 2.7 缓存雪崩问题及解决思路 缓存雪崩是指在同一时段大量的缓存key同时失效（同一时段，大量请求缓存未命中）或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。\n解决方案：\n给不同的Key的TTL添加随机值 利用Redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多级缓存 2.8 缓存击穿问题及解决思路 缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且**缓存重建业务较复杂（耗费时间较长）的key突然失效了，无数的请求访问会在瞬间（也就是缓存重建阶段内）**给数据库带来巨大的冲击。\n常见的解决方案有两种：\n互斥锁 逻辑过期 逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大\n解决方案一、使用锁来解决：\n因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。\n假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。\n解决方案二、逻辑过期方案\n方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。\n我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。\n这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。\n进行对比\n**互斥锁方案：**由于保证了互斥性，所以数据一致，且实现简单，因为仅仅只需要加一把锁而已，也没其他的事情需要操心，所以没有额外的内存消耗，缺点在于有锁就有死锁问题的发生，且只能串行执行性能肯定受到影响\n逻辑过期方案： 线程读取过程中不需要等待，性能好，有一个额外的线程持有锁去进行重构数据，但是在重构数据完成前，其他的线程只能返回之前的数据，且实现起来麻烦\n2.9 利用互斥锁解决缓存击穿问题 核心思路：相较于原来从缓存中查询不到数据后直接查询数据库而言，现在的方案是：进行查询之后，如果从缓存没有查询到数据，则进行互斥锁的获取，获取互斥锁后，判断是否获得到了锁，如果没有获得到，则休眠，过一会再进行尝试，直到获取到锁为止，才能进行查询\n如果获取到了锁的线程，再去进行查询，查询后将数据写入redis，再释放锁，返回数据，利用互斥锁就能保证只有一个线程去执行操作数据库的逻辑，防止缓存击穿\n操作锁的代码：\n核心思路就是利用redis的setnx方法来表示获取锁，该方法含义是redis中如果没有这个key，则插入成功，返回1，在stringRedisTemplate中返回true， 如果有这个key则插入失败，则返回0，在stringRedisTemplate返回false，我们可以通过true，或者是false，来表示是否有线程成功插入key，成功插入的key的线程我们认为他就是获得到锁的线程。\n1 2 3 4 5 6 7 8 private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \u0026#34;1\u0026#34;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemplate.delete(key); } 操作代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 public Shop queryWithMutex(Long id) { String key = CACHE_SHOP_KEY + id; // 1、从redis中查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2、判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 存在,直接返回 return JSONUtil.toBean(shopJson, Shop.class); } // 判断命中的值是否是空值 if (shopJson != null) { //返回一个错误信息 return null; } // 4.实现缓存重构 // 4.1 获取互斥锁 String lockKey = \u0026#34;lock:shop:\u0026#34; + id; Shop shop = null; try { boolean isLock = tryLock(lockKey); // 4.2 判断否获取成功 if(!isLock){ // 4.3 失败，则休眠重试 Thread.sleep(50); return queryWithMutex(id); } // 4.4 成功，根据id查询数据库 shop = getById(id); // 5.不存在，也就是发生了缓存穿透，返回错误 if(shop == null){ // 将空值写入redis stringRedisTemplate.opsForValue().set(key,\u0026#34;\u0026#34;,CACHE_NULL_TTL,TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.写入redis stringRedisTemplate.opsForValue().set(key,JSONUtil.toJsonStr(shop),CACHE_NULL_TTL,TimeUnit.MINUTES); }catch (Exception e){ throw new RuntimeException(e); } finally { // 7.释放互斥锁 unlock(lockKey); } return shop; } 3.0 利用逻辑过期解决缓存击穿问题 需求：修改根据id查询商铺的业务，基于逻辑过期方式来解决缓存击穿问题\n思路分析：当用户开始查询redis时，判断是否命中，如果没有命中则直接返回空数据，不查询数据库，而一旦命中后，将value取出，判断value中的过期时间是否满足，如果没有过期，则直接返回redis中的数据，如果过期，则在开启独立线程后直接返回之前的数据，独立线程去重构数据，重构完成后释放互斥锁。\n如果封装数据：因为现在redis中存储的数据的value需要带上过期时间，此时要么你去修改原来的实体类，要么你\n步骤一、\n新建一个实体类，我们采用第二个方案，这个方案，对原来代码没有侵入性。\n1 2 3 4 5 @Data public class RedisData { private LocalDateTime expireTime; private Object data; } 步骤二、\n在ShopServiceImpl 新增此方法，利用单元测试进行缓存预热\n在测试类中\n步骤三：正式代码\nShopServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public Shop queryWithLogicalExpire( Long id ) { String key = CACHE_SHOP_KEY + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); Shop shop = JSONUtil.toBean((JSONObject) redisData.getData(), Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return shop; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ // 获取成功就新开线程去做缓存重建 CACHE_REBUILD_EXECUTOR.submit( ()-\u0026gt;{ try{ //重建缓存 this.saveShop2Redis(id,20L); }catch (Exception e){ throw new RuntimeException(e); }finally { unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return shop; } 3.1 封装Redis工具类 基于StringRedisTemplate封装一个缓存工具类，满足下列需求：\n方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间\n方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓存击穿问题\n方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题\n方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题\n将逻辑进行封装\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 @Slf4j @Component public class CacheClient { private final StringRedisTemplate stringRedisTemplate; private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public CacheClient(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public void set(String key, Object value, Long time, TimeUnit unit) { stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit); } public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) { // 设置逻辑过期 RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time))); // 写入Redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); } public \u0026lt;R,ID\u0026gt; R queryWithPassThrough( String keyPrefix, ID id, Class\u0026lt;R\u0026gt; type, Function\u0026lt;ID, R\u0026gt; dbFallback, Long time, TimeUnit unit){ String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(json)) { // 3.存在，直接返回 return JSONUtil.toBean(json, type); } // 判断命中的是否是空值 if (json != null) { // 返回一个错误信息 return null; } // 4.不存在，根据id查询数据库 R r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, \u0026#34;\u0026#34;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); return r; } public \u0026lt;R, ID\u0026gt; R queryWithLogicalExpire( String keyPrefix, ID id, Class\u0026lt;R\u0026gt; type, Function\u0026lt;ID, R\u0026gt; dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); R r = JSONUtil.toBean((JSONObject) redisData.getData(), type); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return r; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ // 6.3.成功，开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -\u0026gt; { try { // 查询数据库 R newR = dbFallback.apply(id); // 重建缓存 this.setWithLogicalExpire(key, newR, time, unit); } catch (Exception e) { throw new RuntimeException(e); }finally { // 释放锁 unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return r; } public \u0026lt;R, ID\u0026gt; R queryWithMutex( String keyPrefix, ID id, Class\u0026lt;R\u0026gt; type, Function\u0026lt;ID, R\u0026gt; dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 3.存在，直接返回 return JSONUtil.toBean(shopJson, type); } // 判断命中的是否是空值 if (shopJson != null) { // 返回一个错误信息 return null; } // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; R r = null; try { boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) { // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit); } // 4.4.获取锁成功，根据id查询数据库 r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, \u0026#34;\u0026#34;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); } catch (InterruptedException e) { throw new RuntimeException(e); }finally { // 7.释放锁 unlock(lockKey); } // 8.返回 return r; } private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \u0026#34;1\u0026#34;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemplate.delete(key); } } 在ShopServiceImpl 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Resource private CacheClient cacheClient; @Override public Result queryById(Long id) { // 解决缓存穿透 Shop shop = cacheClient .queryWithPassThrough(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES); // 互斥锁解决缓存击穿 // Shop shop = cacheClient // .queryWithMutex(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES); // 逻辑过期解决缓存击穿 // Shop shop = cacheClient // .queryWithLogicalExpire(CACHE_SHOP_KEY, id, Shop.class, this::getById, 20L, TimeUnit.SECONDS); if (shop == null) { return Result.fail(\u0026#34;店铺不存在！\u0026#34;); } // 7.返回 return Result.ok(shop); } 3、优惠卷秒杀 3.1 全局唯一ID 每个店铺都可以发布优惠券：\n当用户抢购时，就会生成订单并保存到tb_voucher_order这张表中，而订单表如果使用数据库自增ID就存在一些问题：\nid的规律性太明显 受单表数据量的限制 场景分析：如果我们的id具有太明显的规则，用户或者说商业对手很容易猜测出来我们的一些敏感信息，比如商城在一天时间内，卖出了多少单，这明显不合适。\n场景分析二：随着我们商城规模越来越大，mysql的单表的容量不宜超过500W，数据量过大之后，我们要进行拆库拆表，但拆分表了之后，他们从逻辑上讲他们是同一张表，所以他们的id是不能一样的， 于是乎我们需要保证id的唯一性。\n全局ID生成器，是一种在分布式系统下用来生成全局唯一ID的工具，一般要满足下列特性：\n为了增加ID的安全性，我们可以不直接使用Redis自增的数值，而是拼接一些其它信息：\nID的组成部分：符号位：1bit，永远为0\n时间戳：31bit，以秒为单位，可以使用69年\n序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID\n3.2 Redis实现全局唯一Id 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Component public class RedisIdWorker { /** * 开始时间戳 */ private static final long BEGIN_TIMESTAMP = 1640995200L; /** * 序列号的位数 */ private static final int COUNT_BITS = 32; private StringRedisTemplate stringRedisTemplate; public RedisIdWorker(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public long nextId(String keyPrefix) { // 1.生成时间戳 LocalDateTime now = LocalDateTime.now(); long nowSecond = now.toEpochSecond(ZoneOffset.UTC); long timestamp = nowSecond - BEGIN_TIMESTAMP; // 2.生成序列号 // 2.1.获取当前日期，精确到天 String date = now.format(DateTimeFormatter.ofPattern(\u0026#34;yyyy:MM:dd\u0026#34;)); // 2.2.自增长 long count = stringRedisTemplate.opsForValue().increment(\u0026#34;icr:\u0026#34; + keyPrefix + \u0026#34;:\u0026#34; + date); // 3.拼接并返回 return timestamp \u0026lt;\u0026lt; COUNT_BITS | count; } } 测试类\n知识小贴士：关于countdownlatch\ncountdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题\n我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是分线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatch\nCountDownLatch 中有两个最重要的方法\n1、countDown\n2、await\nawait 方法 是阻塞方法，我们担心分线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch 内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch 维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让分线程和变量绑定， 执行完一个分线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有分线程执行完后的时间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Test void testIdWorker() throws InterruptedException { CountDownLatch latch = new CountDownLatch(300); Runnable task = () -\u0026gt; { for (int i = 0; i \u0026lt; 100; i++) { long id = redisIdWorker.nextId(\u0026#34;order\u0026#34;); System.out.println(\u0026#34;id = \u0026#34; + id); } latch.countDown(); }; long begin = System.currentTimeMillis(); for (int i = 0; i \u0026lt; 300; i++) { es.submit(task); } latch.await(); long end = System.currentTimeMillis(); System.out.println(\u0026#34;time = \u0026#34; + (end - begin)); } 3.3 添加优惠卷 每个店铺都可以发布优惠券，分为平价券和特价券。平价券可以任意购买，而特价券需要秒杀抢购：\ntb_voucher：优惠券的基本信息，优惠金额、使用规则等 tb_seckill_voucher：优惠券的库存、开始抢购时间，结束抢购时间。特价优惠券才需要填写这些信息\n平价卷由于优惠力度并不是很大，所以是可以任意领取\n而代金券由于优惠力度大，所以像第二种卷，就得限制数量，从表结构上也能看出，特价卷除了具有优惠卷的基本信息以外，还具有库存，抢购时间，结束时间等等字段\n**新增普通卷代码： **VoucherController\n1 2 3 4 5 @PostMapping public Result addVoucher(@RequestBody Voucher voucher) { voucherService.save(voucher); return Result.ok(voucher.getId()); } 新增秒杀卷代码：\nVoucherController\n1 2 3 4 5 @PostMapping(\u0026#34;seckill\u0026#34;) public Result addSeckillVoucher(@RequestBody Voucher voucher) { voucherService.addSeckillVoucher(voucher); return Result.ok(voucher.getId()); } VoucherServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } 3.4 实现秒杀下单 下单核心思路：当我们点击抢购时，会触发右侧的请求，我们只需要编写对应的controller即可\n秒杀下单应该思考的内容：\n下单时需要判断两点：\n秒杀是否开始或结束，如果尚未开始或已经结束则无法下单 库存是否充足，不足则无法下单 下单核心逻辑分析：\n当用户开始进行下单，我们应当去查询优惠卷信息，查询到优惠卷信息，判断是否满足秒杀条件\n比如时间是否充足，如果时间充足，则进一步判断库存是否足够，如果两者都满足，则扣减库存，创建订单，然后返回订单id，如果有一个条件不满足则直接结束。\nVoucherOrderServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update(); if (!success) { //扣减库存 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //6.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 6.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 6.2.用户id Long userId = UserHolder.getUser().getId(); voucherOrder.setUserId(userId); // 6.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); return Result.ok(orderId); } 3.5 库存超卖问题分析 有关超卖问题分析：在我们原有代码中是这么写的\n1 2 3 4 5 6 7 8 9 10 11 12 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update(); if (!success) { //扣减库存 return Result.fail(\u0026#34;库存不足！\u0026#34;); } 假设线程1过来查询库存，判断出来库存大于1，正准备去扣减库存，但是还没有来得及去扣减，此时线程2过来，线程2也去查询库存，发现这个数量一定也大于1，那么这两个线程都会去扣减库存，最终多个线程相当于一起去扣减库存，此时就会出现库存的超卖问题。\n超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：而对于加锁，我们通常有两种解决方案：见下图：\n悲观锁：\n悲观锁可以实现对于数据的串行化执行，比如syn，和lock都是悲观锁的代表，同时，悲观锁中又可以再细分为公平锁，非公平锁，可重入锁，等等\n乐观锁：\n乐观锁：会有一个版本号，每次操作数据会对版本号+1，再提交回数据时，会去校验是否比之前的版本大1 ，如果大1 ，则进行操作成功，这套机制的核心逻辑在于，如果在操作过程中，版本号只比原来大1 ，那么就意味着操作过程中没有人对他进行过修改，他的操作就是安全的，如果不大1，则数据被修改过，当然乐观锁还有一些变种的处理方式比如cas\n乐观锁的典型代表：就是cas，利用cas进行无锁化机制加锁，var5 是操作前读取的内存值，while中的var1+var2 是预估值，如果预估值 == 内存值，则代表中间没有被人修改过，此时就将新值去替换 内存值\n其中do while 是为了在操作失败时，再次进行自旋操作，即把之前的逻辑再操作一次。==\u0026gt;一种自旋锁实现\n1 2 3 4 5 6 int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; 课程中的使用方式：\n课程中的使用方式是没有像cas一样带自旋的操作，也没有对version的版本号+1 ，他的操作逻辑是在操作时，对版本号进行+1 操作，然后要求version 如果是1 的情况下，才能操作，那么第一个线程在操作后，数据库中的version变成了2，但是他自己满足version=1 ，所以没有问题，此时线程2执行，线程2 最后也需要加上条件version =1 ，但是现在由于线程1已经操作过了，所以线程2，操作时就不满足version=1 的条件了，所以线程2无法执行成功\n3.6 乐观锁解决超卖问题 修改代码方案一、\nVoucherOrderServiceImpl 在扣减库存时，改为：\n1 2 3 4 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) //set stock = stock -1 .eq(\u0026#34;voucher_id\u0026#34;, voucherId).eq(\u0026#34;stock\u0026#34;,voucher.getStock()).update(); //where id = ？ and stock = ? # 等于先前的库存再做扣减 以上逻辑的核心含义是：只要我扣减库存时的库存和之前我查询到的库存是一样的，就意味着没有人在中间修改过库存，那么此时就是安全的，但是以上这种方式通过测试发现会有很多失败的情况，失败的原因在于：在使用乐观锁过程中假设100个线程同时都拿到了100的库存，然后大家一起去进行扣减，但是100个人中只有1个人能扣减成功，其他的人在处理时，他们在扣减时，库存已经被修改过了，所以此时其他线程都会失败。显然这样的乐观锁实现不能出现在实际生产环境中。\n修改代码方案二、\n之前的方式要修改前后都保持一致，但是这样我们分析过，成功的概率太低，所以我们的乐观锁需要变一下，改成stock大于0 即可\n1 2 3 4 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update().gt(\u0026#34;stock\u0026#34;,0); //where id = ? and stock \u0026gt; 0 解决方案二的实现解释：库存只要剩余，就可以扣减。假设一个场景：当前库存剩余1个，现在有三个线程同时发起请求，同时到达数据库，那么只会有一个线程获得记录的行锁，去执行库存扣减（写锁），其他线程不论是读还是写都会被阻塞。这样库存扣减成功过后，行锁得到释放；其他线程再来更新就会发现当前库存已经清空，就会失败返回。所以归根结底，方案二的实现依赖了MySQL默认存储引擎InnoDB的行锁机制，因为是update语句，MySQL会默认加上行锁，但是有其他非主键或非唯一索引列，行锁会退化成粒度更大的gap lock\n知识小扩展：\n针对cas中的自旋压力过大，我们可以使用Longaddr这个类去解决\nJava8 提供的一个对AtomicLong改进后的一个类，LongAdder\n大量线程并发更新一个原子性的时候，天然的问题就是自旋，会导致并发性问题，当然这也比我们直接使用syn来的好\n所以利用这么一个类，LongAdder来进行优化\n如果获取某个值，则会对cell和base的值进行递增，最后返回一个完整的值\n3.6 优惠券秒杀-一人一单 需求：修改秒杀业务，要求同一个优惠券，一个用户只能下一单\n现在的问题在于：\n优惠卷是为了引流，但是目前的情况是，一个人可以无限制的抢这个优惠卷，所以我们应当增加一层逻辑，让一个用户只能下一个单，而不是让一个用户下多个单\n具体操作逻辑如下：比如时间是否充足，如果时间充足，则进一步判断库存是否足够，然后再根据优惠卷id和用户id查询是否已经下过这个订单，如果下过这个订单，\n不再下单，否则进行下单\nVoucherOrderServiceImpl\n初步代码：增加一人一单逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } // 5.一人一单逻辑 // 5.1.用户id Long userId = UserHolder.getUser().getId(); int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 return Result.fail(\u0026#34;用户已经购买过一次！\u0026#34;); } //6，扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update(); if (!success) { //扣减库存 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); return Result.ok(orderId); } **存在问题：**现在的问题还是和之前一样，并发过来（可能用户贪图优惠劵，会利用科技在一瞬间发起大量并发请求，以获得大量优惠订单），查询数据库，一开始都不存在订单（MySQL查询默认都是不加锁的），所以我们还是需要加锁，但是乐观锁比较适合更新数据，而现在是插入数据，所以我们需要使用悲观锁操作\n**注意：**在这里提到了非常多的问题，我们需要慢慢的来思考，首先我们的初始方案是封装了一个createVoucherOrder方法，同时为了确保他线程安全，在方法上添加了一把synchronized 锁。\n当然经过分析，我们发现并发问题主要出现在查询订单这里，我们可以考虑这样两种处理方案：\n代码层面：在出现并发问题根源的关键代码片段处，加上互斥锁，这样加锁的粒度正好合适\n数据库层面：关键语句上我们使用数据库的锁，来进行阻塞其他线程并发请求带来的问题\n个人认为，最好是数据库层面进行加锁，因为数据库层面的死锁问题会直接报错返回，但是程序层面的死锁问题，服务可能会直接崩溃掉\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @Transactional public synchronized Result createVoucherOrder(Long voucherId) { Long userId = UserHolder.getUser().getId(); // 5.1.查询订单 int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 return Result.fail(\u0026#34;用户已经购买过一次！\u0026#34;); } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) // set stock = stock - 1 .eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, 0) // where id = ? and stock \u0026gt; 0 .update(); if (!success) { // 扣减失败 return Result.fail(\u0026#34;库存不足！\u0026#34;); } // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId); } ，但是这样添加锁，锁的粒度太粗了，在使用锁过程中，控制锁粒度 是一个非常重要的事情，因为如果锁的粒度太大，会导致每个线程进来都会锁住，所以我们需要去控制锁的粒度，以下这段代码需要修改为： intern() 这个方法是从常量池中拿到数据，如果我们直接使用userId.toString() 他拿到的对象实际上是不同的对象，new出来的对象，我们使用锁必须保证锁必须是同一把，所以我们需要使用intern()方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @Transactional public Result createVoucherOrder(Long voucherId) { Long userId = UserHolder.getUser().getId(); synchronized(userId.toString().intern()){ // 5.1.查询订单 int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 return Result.fail(\u0026#34;用户已经购买过一次！\u0026#34;); } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) // set stock = stock - 1 .eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, 0) // where id = ? and stock \u0026gt; 0 .update(); if (!success) { // 扣减失败 return Result.fail(\u0026#34;库存不足！\u0026#34;); } // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId); } } 但是以上代码还是存在问题，问题的原因在于当前方法被spring的事务控制，如果你在方法内部加锁，可能会导致当前方法事务还没有提交，但是锁已经释放也会导致问题，所以我们选择将当前方法整体包裹起来，确保事务不会出现问题：如下：\n在seckillVoucher 方法中，添加以下逻辑，这样就能保证事务的特性，同时也控制了锁的粒度\n但是以上做法依然有问题，因为你调用的方法，其实是this.的方式调用的，事务想要生效，还得利用代理来生效，所以这个地方，我们需要获得原始的事务对象， 来操作事务\n3.7 集群环境下的并发问题 通过加锁可以解决在单机情况下的一人一单安全问题，但是在集群模式下就不行了。\n1、我们将服务启动两份，端口分别为8081和8082：\n2、然后修改nginx的conf目录下的nginx.conf文件，配置反向代理和负载均衡：\n具体操作(略)\n有关锁失效原因分析\n由于现在我们部署了多个tomcat，每个tomcat都有一个属于自己的jvm，那么假设在服务器A的tomcat内部，有两个线程，这两个线程由于使用的是同一份代码，那么他们的锁对象是同一个，是可以实现互斥的，但是如果现在是服务器B的tomcat内部，又有两个线程，但是他们的锁对象写的虽然和服务器A一样，但是锁对象却不是同一个，所以线程3和线程4可以实现互斥，但是却无法和线程1和线程2实现互斥，这就是集群环境下，syn锁失效的原因，在这种情况下，我们就需要使用分布式锁来解决这个问题。\n4、分布式锁 4.1 基本原理和实现方式对比 分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。\n分布式锁的核心思想就是让大家（不同的线程之间，包括跨服务器级的不同线程）都使用同一把锁，只要大家使用的是同一把锁，那么我们就能锁住当前线程，让其他线程进行自旋等待，让程序串行执行，这就是分布式锁的核心思路。虽然降低了程序的并发性能，但是为了数据安全，这是必要的\n那么分布式锁他应该满足一些什么样的条件呢？\n可见性：多个线程都能看到相同的结果，注意：这个地方说的可见性并不是并发编程中指的内存可见性，只是说多个进程之间都能感知到变化的意思\n互斥：互斥是分布式锁的最基本的条件，使得程序串行执行\n高可用：程序不易崩溃，时时刻刻都保证较高的可用性\n高性能：由于加锁本身就让性能降低，所有对于分布式锁本身需要他就较高的加锁性能和释放锁性能\n安全性：安全也是程序中必不可少的一环\n常见的分布式锁有三种\nMysql：mysql本身就带有锁机制，但是由于mysql性能本身一般，所以采用分布式锁的情况下，其实使用mysql作为分布式锁比较少见\nRedis：redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都使用redis或者zookeeper作为分布式锁，利用setnx这个方法，如果插入key成功，则表示获得到了锁，如果有人插入成功，其他人插入失败则表示无法获得到锁，利用这套逻辑来实现分布式锁\nZookeeper：zookeeper也是企业级开发中较好的一个实现分布式锁的方案，由于本套视频并不讲解zookeeper的原理和分布式锁的实现，所以不过多阐述\n4.2 Redis分布式锁的实现核心思路 实现分布式锁时需要实现的两个基本方法：\n获取锁：\n互斥：确保只能有一个线程获取锁 非阻塞：尝试一次，成功返回true，失败返回false 释放锁：\n手动释放 超时释放：获取锁时添加一个超时时间 核心思路：\n我们利用 redis 的 setNx 方法，当有多个线程进入时，我们就利用该方法，第一个线程进入时，redis 中就有这个key 了，返回了1，如果结果是1，则表示他抢到了锁，那么他去执行业务，然后再删除锁，退出锁逻辑，没有抢到锁的哥们，等待一定时间后重试即可\n4.3 实现分布式锁版本一 加锁逻辑 锁的基本接口\nSimpleRedisLock\n利用setnx方法进行加锁，同时增加过期时间，防止死锁，此方法可以保证加锁和增加过期时间具有原子性\n1 2 3 4 5 6 7 8 9 10 private static final String KEY_PREFIX=\u0026#34;lock:\u0026#34; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId + \u0026#34;\u0026#34;, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } 释放锁逻辑 SimpleRedisLock\n释放锁\n1 2 3 4 public void unlock() { //通过del删除锁 stringRedisTemplate.delete(KEY_PREFIX + name); } 修改业务代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 大量进程同时涌入时，由于默认MySQL的InnoDB没有对查询做互斥操作，会导致并发读出相同的那一个数据， // 假设这时库存刚好还剩一件商品，那么后续的业务就会存在超卖问题 // 解决的方案：1.将MySQL数据库的InnoDB存储引擎默认隔离级别REAPETABLE改成串行化级别，永久解决并发问题。 //\t但是这种方案带来的并发性能会骤降，而且也会将一些其他业务的并发性能受到阻塞，所以一般都不会采用这种方案 //\t2.还有一种方案，就是局部加锁。我们在执行这个关键操作时，为了保证不能大量进程并发地读，需要在读的时候加上锁。 //\t当然上锁的方案很多：比如在业务代码层面上锁，或者数据库层面上锁（for update），当然为了控制这个上锁的粒度 //\t，最好选择在数据库层面上锁，可以控制上锁的粒度在那一条sql语句上，同时，死锁问题就不需要我们操心了 // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } Long userId = UserHolder.getUser().getId(); //创建锁对象(新增代码) SimpleRedisLock lock = new SimpleRedisLock(\u0026#34;order:\u0026#34; + userId, stringRedisTemplate); //获取锁对象 boolean isLock = lock.tryLock(1200); //加锁失败 if (!isLock) { return Result.fail(\u0026#34;不允许重复下单\u0026#34;); } try { //获取代理对象(事务) IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 lock.unlock(); } } 4.4 Redis分布式锁误删情况说明 逻辑说明：\n持有锁的线程在锁的内部出现了阻塞，导致他的锁超时自动释放，这时其他线程，线程2来尝试获得锁，就拿到了这把锁，然后线程2在持有锁执行过程中，线程1反应过来，继续执行，而线程1执行过程中，走到了删除锁逻辑，此时就会把本应该属于线程2的锁进行删除，这就是误删别人锁的情况说明\n解决方案：解决方案就是在每个线程释放锁的时候，去判断一下当前这把锁是否属于自己，如果不属于自己，则不进行锁的删除，假设还是上边的情况，线程1卡顿，锁自动释放，线程2进入到锁的内部执行逻辑，此时线程1反应过来，然后删除锁，但是线程1，一看当前这把锁不是属于自己，于是不进行删除锁逻辑，当线程2走到删除锁逻辑时，如果没有卡过自动释放锁的时间点，则判断当前这把锁是属于自己的，于是删除这把锁。\n4.5 解决Redis分布式锁误删问题 需求：修改之前的分布式锁实现，满足：在获取锁时存入线程标示（可以用UUID表示） 在释放锁时先获取锁中的线程标示，判断是否与当前线程标示一致。当然线程的标识应当满足在分布式的服务器架构下线程标识唯一，\n如果一致则释放锁 如果不一致则不释放锁 核心逻辑：在存入锁时，放入自己线程的标识，在删除锁时，判断当前这把锁的标识是不是自己存入的，如果是，则进行删除，如果不是，则不进行删除。\n具体代码如下：加锁\n1 2 3 4 5 6 7 8 9 10 private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \u0026#34;-\u0026#34;; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } 释放锁\n1 2 3 4 5 6 7 8 9 10 11 public void unlock() { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁中的标示 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 判断标示是否一致 if(threadId.equals(id)) { // 释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } } 有关代码实操说明：\n在我们修改完此处代码后，我们重启工程，然后启动两个线程，第一个线程持有锁后，手动释放锁，第二个线程此时进入到锁内部，再放行第一个线程，此时第一个线程由于锁的value值并非是自己，所以不能释放锁，也就无法删除别人的锁，此时第二个线程能够正确释放锁，通过这个案例初步说明我们解决了锁误删的问题。\n4.6 分布式锁的原子性问题 更为极端的误删逻辑说明：\n线程1现在持有锁之后，在执行业务逻辑过程中，他正准备删除锁，而且已经走到了条件判断的过程中，比如他已经拿到了当前这把锁确实是属于他自己的，正准备删除锁，但是此时线程1突然阻塞了，那么此时线程2进来，但是线程1他会接着往后执行，当他阻塞结束后，他直接就会执行删除锁那行代码，相当于条件判断并没有起到作用，这就是删锁时的原子性问题，之所以有这个问题，是因为线程1的拿锁，比锁，删锁，实际上并不是原子性的，我们要防止刚才的情况发生，\n4.7 Lua脚本解决多条命令原子性问题 Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。Lua是一种编程语言，它的基本语法大家可以参考网站：https://www.runoob.com/lua/lua-tutorial.html，这里重点介绍Redis提供的调用函数，我们可以使用lua去操作redis，又能保证他的原子性，这样就可以实现拿锁比锁删锁是一个原子性动作了，作为Java程序员这一块并不作一个简单要求，并不需要大家过于精通，只需要知道他有什么作用即可。\n这里重点介绍Redis提供的调用函数，语法如下：\n1 redis.call(\u0026#39;命令名称\u0026#39;, \u0026#39;key\u0026#39;, \u0026#39;其它参数\u0026#39;, ...) 例如，我们要执行set name jack，则脚本是这样：\n1 2 -- 执行 set name jack redis.call(\u0026#39;set\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;jack\u0026#39;) 例如，我们要先执行set name Rose，再执行get name，则脚本如下：\n1 2 3 4 5 6 -- 先执行 set name jack redis.call(\u0026#39;set\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;Rose\u0026#39;) -- 再执行 get name local name = redis.call(\u0026#39;get\u0026#39;, \u0026#39;name\u0026#39;) -- 返回 return name 写好脚本以后，需要用Redis命令来调用脚本，调用脚本的常见命令如下：\n例如，我们要执行 redis.call('set', 'name', 'jack') 这个脚本，语法如下：\n如果脚本中的key、value不想写死，可以作为参数传递。key类型参数会放入KEYS数组，其它参数会放入ARGV数组，在脚本中可以从KEYS和ARGV数组获取这些参数：\n接下来我们来回一下我们释放锁的逻辑：\n释放锁的业务流程是这样的\n​\t1、获取锁中的线程标示\n​\t2、判断是否与指定的标示（当前线程标示）一致\n​\t3、如果一致则释放锁（删除）\n​\t4、如果不一致则什么都不做\n如果用Lua脚本来表示则是这样的：\n最终我们操作redis的拿锁比锁删锁的lua脚本就会变成这样\n1 2 3 4 5 6 7 8 -- 这里的 KEYS[1] 就是锁的key，这里的ARGV[1] 就是当前线程标示 -- 获取锁中的标示，判断是否与当前线程标示一致 if (redis.call(\u0026#39;GET\u0026#39;, KEYS[1]) == ARGV[1]) then -- 一致，则删除锁 return redis.call(\u0026#39;DEL\u0026#39;, KEYS[1]) end -- 不一致，则直接返回 return 0 4.8 利用Java代码调用Lua脚本改造分布式锁 lua脚本本身并不需要大家花费太多时间去研究，只需要知道如何调用，大致是什么意思即可，所以在笔记中并不会详细的去解释这些lua表达式的含义。\n我们的RedisTemplate中，可以利用execute方法去执行lua脚本，参数对应关系就如下图股\nJava代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 private static final DefaultRedisScript\u0026lt;Long\u0026gt; UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript\u0026lt;\u0026gt;(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(\u0026#34;unlock.lua\u0026#34;)); UNLOCK_SCRIPT.setResultType(Long.class); } public void unlock() { // 调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId()); } 经过以上代码改造后，我们就能够实现 拿锁比锁删锁的原子性动作了~\n小总结：\n基于Redis的分布式锁实现思路：\n利用set nx ex获取锁，并设置过期时间，保存线程标示 释放锁时先判断线程标示是否与自己一致，一致则删除锁 特性： 利用set nx满足互斥性 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性 利用Redis集群保证高可用和高并发特性 笔者总结：我们一路走来，利用添加过期时间，防止死锁问题的发生，但是有了过期时间之后，可能出现误删别人锁的问题，这个问题我们开始是利用删之前 通过拿锁，比锁，删锁这个逻辑来解决的，也就是删之前判断一下当前这把锁是否是属于自己的，但是现在还有原子性问题，也就是我们没法保证拿锁比锁删锁是一个原子性的动作，最后通过lua表达式来解决这个问题\n但是目前还剩下一个问题锁不住（其实就是极端情况，上锁之前都所有线程阻塞了，那么这会导致没有一个线程拿到锁，也就是锁不住），什么是锁不住呢，你想一想，如果当过期时间到了之后，我们可以给他续期一下，比如续个30s，就好像是网吧上网，网费到了之后，然后说，来，网管，再给我来10块的，是不是后边的问题都不会发生了，那么续期问题怎么解决呢，可以依赖于我们接下来要学习redission啦\n测试逻辑：\n第一个线程进来，得到了锁，手动删除锁，模拟锁超时了，其他线程会执行lua来抢锁，当第一个线程利用lua删除锁时，lua能保证他不能删除他的锁，第二个线程删除锁时，利用lua同样可以保证不会删除别人的锁，同时还能保证原子性。\n5、分布式锁-redission 5.1 分布式锁-redission功能介绍 基于setnx实现的分布式锁存在下面的问题：\n重入问题：重入问题是指 获得锁的线程可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁，比如HashTable这样的代码中，他的方法都是使用synchronized修饰的，假如他在一个方法内，调用另一个方法，那么此时如果是不可重入的，不就死锁了吗？所以可重入锁他的主要意义是防止死锁，我们的synchronized和Lock锁都是可重入的。\n不可重试：是指目前的分布式只能尝试一次，我们认为合理的情况是：当线程在获得锁失败后，他应该能再次尝试获得锁。\n**超时释放：**我们在加锁时增加了过期时间，这样的我们可以防止死锁，但是如果卡顿的时间超长，虽然我们采用了lua表达式防止删锁的时候，误删别人的锁，但是毕竟没有锁住，有安全隐患\n主从一致性： 如果Redis提供了主从集群，当我们向集群写数据时，主机需要异步的将数据同步给从机，而万一在同步过去之前，主机宕机了，就会出现死锁问题。\n那么什么是Redission呢\nRedisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。\nRedission提供了分布式锁的多种多样的功能\n5.2 分布式锁-Redission快速入门 引入依赖：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.13.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置Redisson客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Configuration public class RedissonConfig { @Bean public RedissonClient redissonClient(){ // 配置 Config config = new Config(); config.useSingleServer().setAddress(\u0026#34;redis://192.168.150.101:6379\u0026#34;) .setPassword(\u0026#34;123321\u0026#34;); // 创建RedissonClient对象 return Redisson.create(config); } } 如何使用Redission的分布式锁\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Resource private RedissionClient redissonClient; @Test void testRedisson() throws Exception{ //获取锁(可重入)，指定锁的名称 RLock lock = redissonClient.getLock(\u0026#34;anyLock\u0026#34;); //尝试获取锁，参数分别是：获取锁的最大等待时间(期间会重试)，锁自动释放时间，时间单位 boolean isLock = lock.tryLock(1,10,TimeUnit.SECONDS); //判断获取锁成功 if(isLock){ try{ System.out.println(\u0026#34;执行业务\u0026#34;); }finally{ //释放锁 lock.unlock(); } } } 在 VoucherOrderServiceImpl\n注入RedissonClient\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @Resource private RedissonClient redissonClient; @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } Long userId = UserHolder.getUser().getId(); //创建锁对象 这个代码不用了，因为我们现在要使用分布式锁 //SimpleRedisLock lock = new SimpleRedisLock(\u0026#34;order:\u0026#34; + userId, stringRedisTemplate); RLock lock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); //获取锁对象 boolean isLock = lock.tryLock(); //加锁失败 if (!isLock) { return Result.fail(\u0026#34;不允许重复下单\u0026#34;); } try { //获取代理对象(事务) IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 lock.unlock(); } } 5.3 分布式锁-redission可重入锁原理 在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state=0，假如有人持有这把锁，那么state=1，如果持有这把锁的人再次持有这把锁，那么state就会 +1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成 0 时，表示当前这把锁没有被人持有。\n在redission中，也支持可重入锁\n在分布式锁中，他采用hash结构用来存储锁，其中大key表示这把锁是否存在，用小key表示当前这把锁被哪个线程持有，所以接下来我们一起分析一下当前的这个lua表达式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -- 锁是否存在，不存在就初始化这个锁 if (redis.call(\u0026#39;exists\u0026#39;, KEYS[1]) == 0) then redis.call(\u0026#39;hset\u0026#39;, KEYS[1], ARGV[2], 1); redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); return nil; end; -- 如果锁存在，就要判断锁的线程是不是当前线程标识（是否属于自己）。 -- 如果是，就将锁的值+1，然后设置过期值成功返回；如果不是就失败，返回这把锁的失效时间，用于之后的重试 if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); return nil; end; return redis.call(\u0026#39;pttl\u0026#39;, KEYS[1]); end; 这个地方一共有3个参数\nKEYS[1]：锁名称\nARGV[1]：锁失效时间\nARGV[2]：id+\u0026quot;:\u0026quot;+threadId =\u0026gt; 锁的小key\nexists：判断lock是否存在,如果==0，就表示当前这把锁不存在\nredis.call('hset', KEYS[1], ARGV[2], 1)：此时他就开始往redis里边去写数据 ，写成一个hash结构\n1 2 3 Lock{ id+\u0026#34;:\u0026#34;+threadId : 1 } 如果当前这把锁存在，则第一个条件不满足，再判断\nredis.call('hexists', KEYS[1], ARGV[2]) == 1\n此时需要通过大key+小key，判断当前这把锁是否是属于自己的，如果是自己的，则进行\nredis.call('hincrby', KEYS[1], ARGV[2], 1)\n将当前这个锁的value进行+1 ，redis.call('pexpire', KEYS[1], ARGV[1])，然后再对其设置过期时间，如果以上两个条件都不满足，则表示当前这把锁抢锁失败，最后返回pttl，即为当前这把锁的失效时间\n如果小伙帮们看了前边的源码， 你会发现他会去判断当前这个方法的返回值是否为null，如果是null，则对应则前两个if对应的条件，退出抢锁逻辑，如果返回的不是null，即走了第三个分支，在源码处会进行while(true)的自旋抢锁。\n5.4 分布式锁-redission锁重试和WatchDog机制 说明：由于课程中已经说明了有关tryLock的源码解析以及其看门狗原理，所以笔者在这里给大家分析lock()方法的源码解析，希望大家在学习过程中，能够掌握更多的知识\n抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同\n1、先判断当前这把锁是否存在，如果不存在，插入一把锁，返回null\n2、判断当前这把锁是否是属于当前线程，如果是，则返回null\n所以如果返回是null，则代表着当前这哥们已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，同学们可以自行往下翻一点点，你能发现有个while( true) 再次进行tryAcquire进行抢锁\n1 2 3 4 5 6 long threadId = Thread.currentThread().getId(); Long ttl = tryAcquire(-1, leaseTime, unit, threadId); // lock acquired if (ttl == null) { return; } 接下来会有一个条件分支，因为lock方法有重载方法，一个是带参数，一个是不带参数，如果带带参数传入的值是-1，如果传入参数，则leaseTime是他本身，所以如果传入了参数，此时leaseTime != -1 则会进去抢锁，抢锁的逻辑就是之前说的那三个逻辑\n1 2 3 if (leaseTime != -1) { return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } 如果是没有传入时间，则此时也会进行抢锁， 而且抢锁时间是默认看门狗时间 commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout()\nttlRemainingFuture.onComplete((ttlRemaining, e) 这句话相当于对以上抢锁进行了监听，也就是说当上边抢锁完毕后，此方法会被调用，具体调用的逻辑就是去后台开启一个线程，进行续约逻辑，也就是看门狗线程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 RFuture\u0026lt;Long\u0026gt; ttlRemainingFuture = tryLockInnerAsync(waitTime, commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.onComplete((ttlRemaining, e) -\u0026gt; { if (e != null) { return; } // lock acquired if (ttlRemaining == null) { scheduleExpirationRenewal(threadId); } }); return ttlRemainingFuture; 此逻辑就是续约逻辑，注意看commandExecutor.getConnectionManager().newTimeout（） 此方法\nMethod( new TimerTask() {},参数2 ，参数3 )\n指的是：通过参数2，参数3 去描述什么时候去做参数1的事情，现在的情况是：10s之后去做参数一的事情\n因为锁的失效时间是30s，当10s之后，此时这个timeTask 就触发了，他就去进行续约，把当前这把锁续约成30s，如果操作成功，那么此时就会递归调用自己，再重新设置一个timeTask()，于是再过10s后又再设置一个timerTask，完成不停的续约\n那么大家可以想一想，假设我们的线程出现了宕机他还会续约吗？当然不会，因为没有人再去调用renewExpiration这个方法，所以等到时间之后自然就释放了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 private void renewExpiration() { ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ee == null) { return; } Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ent == null) { return; } Long threadId = ent.getFirstThreadId(); if (threadId == null) { return; } RFuture\u0026lt;Boolean\u0026gt; future = renewExpirationAsync(threadId); future.onComplete((res, e) -\u0026gt; { if (e != null) { log.error(\u0026#34;Can\u0026#39;t update lock \u0026#34; + getName() + \u0026#34; expiration\u0026#34;, e); return; } if (res) { // reschedule itself renewExpiration(); } }); } }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); ee.setTimeout(task); } 5.5 分布式锁-redission锁的MutiLock原理 为了提高redis的可用性，我们会搭建集群或者主从，现在以主从为例:\n此时我们去写命令，写在主机上，主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。\n为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。\n虽然说出来容易，但是技术实现上还是存在一定的困难：如何保证redis所有节点之间锁数据的强一致性，不同节点大概都位于不同的服务器上，那么要实现进程隔离级别的数据一致性，可以考虑TCC分布式事务方案。当然也可以使用其他方案，如MultiLock的设计\n那么 MutiLock 加锁原理是什么呢？笔者画了一幅图来说明\n当我们去设置了多个锁时，redission会将多个锁添加到一个集合中，然后用while循环不停地去尝试拿锁，但是会有一个总共的加锁时间，这个时间是用需要加锁的个数 * 1500ms ，假设有3个锁，那么时间就是4500ms，假设在这4500ms内，所有的锁都加锁成功，那么此时才算是加锁成功，如果在4500ms有线程加锁失败，则会再次去进行重试\n6、秒杀优化 6.1 秒杀优化-异步秒杀思路 我们来回顾一下下单流程\n当用户发起请求，此时会请求nginx，nginx会访问到tomcat，而tomcat中的程序，会进行串行操作，分成如下几个步骤\n1、查询优惠卷\n2、判断秒杀库存是否足够\n3、查询订单\n4、校验是否是一人一单\n5、扣减库存\n6、创建订单\n在这六步操作中，又有很多操作是要去操作数据库的，而且还是一个线程串行执行， 这样就会导致我们的程序执行的很慢，所以我们需要异步程序执行，那么如何加速呢？\n在这里笔者想给大家分享一下课程内没有的思路，看看有没有小伙伴这么想，比如，我们可以不可以使用异步编排来做，或者说我开启N多线程，N多个线程，一个线程执行查询优惠卷，一个执行判断扣减库存，一个去创建订单等等，然后再统一做返回，这种做法和课程中有哪种好呢？答案是课程中的好，因为如果你采用我刚说的方式，如果访问的人很多，那么线程池中的线程可能一下子就被消耗完了，而且你使用上述方案，最大的特点在于，你觉得时效性会非常重要，但是你想想是吗？并不是，比如我只要确定他能做这件事，然后我后边慢慢做就可以了，我并不需要他一口气做完这件事，所以我们应当采用的是课程中，类似消息队列的方式来完成我们的需求，而不是使用线程池或者是异步编排的方式来完成这个需求\n优化方案：我们将耗时比较短的逻辑判断放入到redis中，比如是否库存足够，比如是否一人一单，这样的操作，只要这种逻辑可以完成，就意味着我们是一定可以下单完成的，我们只需要进行快速的逻辑判断，根本就不用等下单逻辑走完，我们直接给用户返回成功，再在后台开一个线程，后台线程慢慢的去执行queue里边的消息，这样程序不就超级快了吗？而且也不用担心线程池消耗殆尽的问题，因为这里我们的程序中并没有手动使用任何线程池，当然这里边有两个难点\n第一个难点是我们怎么在redis中去快速校验一人一单，还有库存判断\n第二个难点是由于我们校验和tomct下单是两个线程，那么我们如何知道到底哪个单他最后是否成功，或者是下单完成，为了完成这件事我们在redis操作完之后，我们会将一些信息返回给前端，同时也会把这些信息丢到异步queue中去，后续操作中，可以通过这个id来查询我们tomcat中的下单逻辑是否完成了。\n我们现在来看看整体思路：当用户下单之后，判断库存是否充足只需要到redis中去根据key找对应的value是否大于0即可，如果不充足，则直接结束，如果充足，继续在redis中判断用户是否可以下单，如果set集合中没有这条数据，说明他可以下单，如果set集合中没有这条记录，则将userId和优惠卷存入到redis中，并且返回0，整个过程需要保证是原子性的，我们可以使用lua来操作\n当以上判断逻辑走完之后，我们可以判断当前redis中返回的结果是否是0，如果是0，则表示可以下单，则将之前说的信息存入到到queue中去，然后返回，然后再来个线程异步的下单，前端可以通过返回的订单id来判断是否下单成功。\n6.2 秒杀优化-Redis完成秒杀资格判断 需求：\n新增秒杀优惠券的同时，将优惠券信息保存到Redis中\n基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功\n如果抢购成功，将优惠券id和用户id封装后存入阻塞队列\n开启线程任务，不断从阻塞队列中获取信息，实现异步下单功能\nVoucherServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 //SECKILL_STOCK_KEY 这个变量定义在RedisConstans中 //private static final String SECKILL_STOCK_KEY =\u0026#34;seckill:stock:\u0026#34; stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } 完整lua表达式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 -- 1.参数列表 -- 1.1.优惠券id local voucherId = ARGV[1] -- 1.2.用户id local userId = ARGV[2] -- 1.3.订单id local orderId = ARGV[3] -- 2.数据key -- 2.1.库存key local stockKey = \u0026#39;seckill:stock:\u0026#39; .. voucherId -- 2.2.订单key local orderKey = \u0026#39;seckill:order:\u0026#39; .. voucherId -- 3.脚本业务 -- 3.1.判断库存是否充足 get stockKey if(tonumber(redis.call(\u0026#39;get\u0026#39;, stockKey)) \u0026lt;= 0) then -- 3.2.库存不足，返回1 return 1 end -- 3.2.判断用户是否下单 SISMEMBER orderKey userId if(redis.call(\u0026#39;sismember\u0026#39;, orderKey, userId) == 1) then -- 3.3.存在，说明是重复下单，返回2 return 2 end -- 3.4.扣库存 incrby stockKey -1 redis.call(\u0026#39;incrby\u0026#39;, stockKey, -1) -- 3.5.下单（保存用户）sadd orderKey userId redis.call(\u0026#39;sadd\u0026#39;, orderKey, userId) -- 3.6.发送消息到队列中， XADD stream.orders * k1 v1 k2 v2 ... redis.call(\u0026#39;xadd\u0026#39;, \u0026#39;stream.orders\u0026#39;, \u0026#39;*\u0026#39;, \u0026#39;userId\u0026#39;, userId, \u0026#39;voucherId\u0026#39;, voucherId, \u0026#39;id\u0026#39;, orderId) return 0 当以上lua表达式执行完毕后，剩下的就是根据步骤3,4来执行我们接下来的任务了\nVoucherOrderServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Override public Result seckillVoucher(Long voucherId) { //获取用户 Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) { // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? \u0026#34;库存不足\u0026#34; : \u0026#34;不能重复下单\u0026#34;); } //TODO 保存阻塞队列 // 3.返回订单id return Result.ok(orderId); } 6.3 秒杀优化-基于阻塞队列实现秒杀优化 VoucherOrderServiceImpl\n修改下单动作，现在我们去下单时，是通过lua表达式去原子执行判断逻辑，如果判断我出来不为0 ，则要么是库存不足，要么是重复下单，返回错误信息，如果是0，则把下单的逻辑保存到队列中去，然后异步执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 //异步处理线程池 private static final ExecutorService SECKILL_ORDER_EXECUTOR = Executors.newSingleThreadExecutor(); //在类初始化之后执行，因为当这个类初始化好了之后，随时都是有可能要执行的 @PostConstruct private void init() { SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } // 用于线程池处理的任务 // 当初始化完毕后，就会去从对列中去拿信息 private class VoucherOrderHandler implements Runnable{ @Override public void run() { while (true){ try { // 1.获取队列中的订单信息 VoucherOrder voucherOrder = orderTasks.take(); // 2.创建订单 handleVoucherOrder(voucherOrder); } catch (Exception e) { log.error(\u0026#34;处理订单异常\u0026#34;, e); } } } private void handleVoucherOrder(VoucherOrder voucherOrder) { //1.获取用户 Long userId = voucherOrder.getUserId(); // 2.创建锁对象 RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); // 3.尝试获取锁 boolean isLock = redisLock.lock(); // 4.判断是否获得锁成功 if (!isLock) { // 获取锁失败，直接返回失败或者重试 log.error(\u0026#34;不允许重复下单！\u0026#34;); return; } try { //注意：由于是spring的事务是放在threadLocal中，此时的是多线程，事务会失效 proxy.createVoucherOrder(voucherOrder); } finally { // 释放锁 redisLock.unlock(); } } //a private BlockingQueue\u0026lt;VoucherOrder\u0026gt; orderTasks = new ArrayBlockingQueue\u0026lt;\u0026gt;(1024 * 1024); @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) { // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? \u0026#34;库存不足\u0026#34; : \u0026#34;不能重复下单\u0026#34;); } VoucherOrder voucherOrder = new VoucherOrder(); // 2.3.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 2.4.用户id voucherOrder.setUserId(userId); // 2.5.代金券id voucherOrder.setVoucherId(voucherId); // 2.6.放入阻塞队列 orderTasks.add(voucherOrder); //3.获取代理对象 proxy = (IVoucherOrderService)AopContext.currentProxy(); //4.返回订单id return Result.ok(orderId); } @Transactional public void createVoucherOrder(VoucherOrder voucherOrder) { Long userId = voucherOrder.getUserId(); // 5.1.查询订单 int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherOrder.getVoucherId()).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 log.error(\u0026#34;用户已经购买过了\u0026#34;); return ; } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) // set stock = stock - 1 .eq(\u0026#34;voucher_id\u0026#34;, voucherOrder.getVoucherId()).gt(\u0026#34;stock\u0026#34;, 0) // where id = ? and stock \u0026gt; 0 .update(); if (!success) { // 扣减失败 log.error(\u0026#34;库存不足\u0026#34;); return ; } save(voucherOrder); } 总结优缺点： 秒杀业务的优化思路是什么？\n先利用Redis完成库存余量、一人一单判断，完成抢单业务（即是将“决策逻辑”业务抽离出来决策此业务是否成功，如果成功就将剩余的操作逻辑和所有“决策逻辑”塞入消息队列里去异步执行，自己就直接返回成功了）\n再将下单业务放入阻塞队列，利用独立线程异步下单\n基于阻塞队列的异步秒杀存在哪些问题？\n内存限制问题\n数据安全问题\n还有最大的问题：虽然我们使用了消息队列来对冗杂业务做异步处理，使之运行得更快，但是还是存在一些问题：返回成功，但是异步执行失败。这样就可能导致库存为负得情况出现。\n有问题，也就有解决之法：我们认为异步任务是必须成功的，不允许失败，那么我们可以提供一个重试机制，来使其最终成功\n7、Redis消息队列 7.1 Redis消息队列-认识消息队列 什么是消息队列：字面意思就是存放消息的队列。最简单的消息队列模型包括3个角色：\n消息队列：存储和管理消息，也被称为消息代理（Message Broker） 生产者：发送消息到消息队列 消费者：从消息队列获取消息并处理消息 使用队列的好处在于 **解耦：**所谓解耦，举一个生活中的例子就是：快递员(生产者)把快递放到快递柜里边(Message Queue)去，我们(消费者)从快递柜里边去拿东西，这就是一个异步，如果耦合，那么这个快递员相当于直接把快递交给你，这事固然好，但是万一你不在家，那么快递员就会一直等你，这就浪费了快递员的时间，所以这种思想在我们日常开发中，是非常有必要的。\n这种场景在我们秒杀中就变成了：我们下单之后，利用redis去进行校验下单条件，再通过队列把消息发送出去，然后再启动一个线程去消费这个消息，完成解耦，同时也加快我们的响应速度。\n这里我们可以使用一些现成的mq，比如kafka，rabbitmq等等，但是呢，如果没有安装mq，我们也可以直接使用redis提供的mq方案，降低我们的部署和学习成本。\n7.2 Redis消息队列-基于List实现消息队列 消息队列（Message Queue），字面意思就是存放消息的队列。而Redis的list数据结构是一个双向链表，很容易模拟出队列效果。\n队列是入口和出口不在一边，因此我们可以利用：LPUSH 结合 RPOP、或者 RPUSH 结合 LPOP来实现。 不过要注意的是，当队列中没有消息时RPOP或LPOP操作会返回null，并不像JVM的阻塞队列那样会阻塞并等待消息。因此这里应该使用BRPOP或者BLPOP来实现阻塞效果。\n基于List的消息队列有哪些优缺点？ 优点：\n利用Redis存储，不受限于JVM内存上限 基于Redis的持久化机制，数据安全性有保证 可以满足消息有序性 缺点：\n无法避免消息丢失 只支持单消费者，因为只有一个queue 7.3 Redis消息队列-基于PubSub的消息队列 PubSub（发布订阅）是Redis2.0版本引入的消息传递模型。顾名思义，消费者可以订阅一个或多个channel，生产者向对应channel发送消息后，所有订阅者都能收到相关消息。\nSUBSCRIBE channel [channel]：订阅一个或多个频道 PUBLISH channel msg：向一个频道发送消息 PSUBSCRIBE pattern[pattern]：订阅与pattern格式匹配的所有频道\n基于PubSub的消息队列有哪些优缺点？ 优点：\n采用发布订阅模型，支持多生产、多消费 缺点：\n不支持数据持久化 无法避免消息丢失 消息堆积有上限，超出时数据丢失 7.4 Redis消息队列-基于Stream的消息队列 Stream 是 Redis 5.0 引入的一种新数据类型，可以实现一个功能非常完善的消息队列。\n发送消息的命令：\n例如：\n读取消息的方式之一：XREAD\n例如，使用XREAD读取第一个消息：\nXREAD阻塞方式，读取最新的消息：\n在业务开发中，我们可以循环的调用XREAD阻塞方式来查询最新消息，从而实现持续监听队列的效果，伪代码如下\n注意：当我们指定起始ID为$时，代表读取最新的消息，如果我们处理一条消息的过程中，又有超过1条以上的消息加入队列，则下次获取时也只能获取到最新的一条，会出现漏读消息的问题\nSTREAM类型消息队列的XREAD命令特点：\n消息可回溯 一个消息可以被多个消费者读取，也支持消费者组 可以阻塞读取 有消息漏读的风险 7.5 Redis消息队列-基于Stream的消息队列-消费者组 消费者组（Consumer Group）：将多个消费者划分到一个组中，监听同一个队列。\n具备下列特点：\n创建消费者组： key：队列名称 groupName：消费者组名称 ID：起始ID标示，$代表队列中最后一个消息，0则代表队列中第一个消息 MKSTREAM：队列不存在时自动创建队列 其它常见命令：\n删除指定的消费者组\n1 XGROUP DESTORY key groupName 给指定的消费者组添加消费者\n1 XGROUP CREATECONSUMER key groupname consumername 删除消费者组中的指定消费者\n1 XGROUP DELCONSUMER key groupname consumername 从消费者组读取消息：\n1 XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] group：消费组名称 consumer：消费者名称，如果消费者不存在，会自动创建一个消费者 count：本次查询的最大数量 BLOCK milliseconds：当没有消息时最长等待时间 NOACK：无需手动ACK，获取到消息后自动确认 STREAMS key：指定队列名称 ID：获取消息的起始ID： \u0026ldquo;\u0026gt;\u0026quot;：从下一个未消费的消息开始 其它：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始\n消费者监听消息的基本思路：\nSTREAM类型消息队列的XREADGROUP命令特点：\n消息可回溯 可以多消费者争抢消息，加快消费速度 可以阻塞读取 没有消息漏读的风险 有消息确认机制，保证消息至少被消费一次 最后我们来个小对比\n7.6 基于Redis的Stream结构作为消息队列，实现异步秒杀下单 需求：\n创建一个Stream类型的消息队列，名为stream.orders 修改之前的秒杀下单Lua脚本，在认定有抢购资格后，直接向stream.orders中添加消息，内容包含voucherId、userId、orderId 项目启动时，开启一个线程任务，尝试获取stream.orders中的消息，完成下单 修改lua表达式,新增3.6\nVoucherOrderServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 private class VoucherOrderHandler implements Runnable { @Override public void run() { while (true) { try { // 1.获取消息队列中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 \u0026gt; List\u0026lt;MapRecord\u0026lt;String, Object, Object\u0026gt;\u0026gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(\u0026#34;g1\u0026#34;, \u0026#34;c1\u0026#34;), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(\u0026#34;stream.orders\u0026#34;, ReadOffset.lastConsumed()) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) { // 如果为null，说明没有消息，继续下一次循环 continue; } // 解析数据 MapRecord\u0026lt;String, Object, Object\u0026gt; record = list.get(0); Map\u0026lt;Object, Object\u0026gt; value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(\u0026#34;s1\u0026#34;, \u0026#34;g1\u0026#34;, record.getId()); } catch (Exception e) { log.error(\u0026#34;处理订单异常\u0026#34;, e); //处理异常消息 handlePendingList(); } } } private void handlePendingList() { while (true) { try { // 1.获取pending-list中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 0 List\u0026lt;MapRecord\u0026lt;String, Object, Object\u0026gt;\u0026gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(\u0026#34;g1\u0026#34;, \u0026#34;c1\u0026#34;), StreamReadOptions.empty().count(1), StreamOffset.create(\u0026#34;stream.orders\u0026#34;, ReadOffset.from(\u0026#34;0\u0026#34;)) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) { // 如果为null，说明没有异常消息，结束循环 break; } // 解析数据 MapRecord\u0026lt;String, Object, Object\u0026gt; record = list.get(0); Map\u0026lt;Object, Object\u0026gt; value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(\u0026#34;s1\u0026#34;, \u0026#34;g1\u0026#34;, record.getId()); } catch (Exception e) { log.error(\u0026#34;处理pendding订单异常\u0026#34;, e); try{ Thread.sleep(20); }catch(Exception e){ e.printStackTrace(); } } } } } 8、达人探店 8.1、达人探店-发布探店笔记 发布探店笔记\n探店笔记类似点评网站的评价，往往是图文结合。对应的表有两个： tb_blog：探店笔记表，包含笔记中的标题、文字、图片等 tb_blog_comments：其他用户对探店笔记的评价\n具体发布流程\n上传接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @RestController @RequestMapping(\u0026#34;upload\u0026#34;) public class UploadController { @PostMapping(\u0026#34;blog\u0026#34;) public Result uploadImage(@RequestParam(\u0026#34;file\u0026#34;) MultipartFile image) { try { // 获取原始文件名称 String originalFilename = image.getOriginalFilename(); // 生成新文件名 String fileName = createNewFileName(originalFilename); // 保存文件 image.transferTo(new File(SystemConstants.IMAGE_UPLOAD_DIR, fileName)); // 返回结果 log.debug(\u0026#34;文件上传成功，{}\u0026#34;, fileName); return Result.ok(fileName); } catch (IOException e) { throw new RuntimeException(\u0026#34;文件上传失败\u0026#34;, e); } } } 注意：同学们在操作时，需要修改SystemConstants.IMAGE_UPLOAD_DIR 自己图片所在的地址，在实际开发中图片一般会放在nginx上或者是云存储上。\nBlogController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @RestController @RequestMapping(\u0026#34;/blog\u0026#34;) public class BlogController { @Resource private IBlogService blogService; @PostMapping public Result saveBlog(@RequestBody Blog blog) { //获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUpdateTime(user.getId()); //保存探店博文 blogService.saveBlog(blog); //返回id return Result.ok(blog.getId()); } } 8.2 达人探店-查看探店笔记 实现查看发布探店笔记的接口\n实现代码：\nBlogServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 @Override public Result queryBlogById(Long id) { // 1.查询blog Blog blog = getById(id); if (blog == null) { return Result.fail(\u0026#34;笔记不存在！\u0026#34;); } // 2.查询blog有关的用户 queryBlogUser(blog); return Result.ok(blog); } 8.3 达人探店-点赞功能 初始代码\n1 2 3 4 5 6 @GetMapping(\u0026#34;/likes/{id}\u0026#34;) public Result queryBlogLikes(@PathVariable(\u0026#34;id\u0026#34;) Long id) { //修改点赞数量 blogService.update().setSql(\u0026#34;liked = liked +1 \u0026#34;).eq(\u0026#34;id\u0026#34;,id).update(); return Result.ok(); } 问题分析：这种方式会导致一个用户无限点赞，明显是不合理的\n造成这个问题的原因是，我们现在的逻辑，发起请求只是给数据库+1，所以才会出现这个问题\n完善点赞功能\n需求：\n同一个用户只能点赞一次，再次点击则取消点赞 如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性） 实现步骤：\n给Blog类中添加一个isLike字段，标示是否被当前用户点赞 修改点赞功能，利用Redis的set集合判断是否点赞过，未点赞过则点赞数+1，已点赞过则点赞数-1 修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段 修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段 为什么采用set集合：\n因为我们的数据是不能重复的，当用户操作过之后，无论他怎么操作，都是\n具体步骤：\n1、在Blog 添加一个字段\n1 2 @TableField(exist = false) private Boolean isLike; 2、修改代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Override public Result likeBlog(Long id){ // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Boolean isMember = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); if(BooleanUtil.isFalse(isMember)){ //3.如果未点赞，可以点赞 //3.1 数据库点赞数+1 boolean isSuccess = update().setSql(\u0026#34;liked = liked + 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); //3.2 保存用户到Redis的set集合 if(isSuccess){ stringRedisTemplate.opsForSet().add(key,userId.toString()); } }else{ //4.如果已点赞，取消点赞 //4.1 数据库点赞数-1 boolean isSuccess = update().setSql(\u0026#34;liked = liked - 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); //4.2 把用户从Redis的set集合移除 if(isSuccess){ stringRedisTemplate.opsForSet().remove(key,userId.toString()); } } 8.4 达人探店-点赞排行榜 在探店笔记的详情页面，应该把给该笔记点赞的人显示出来，比如最早点赞的TOP5，形成点赞排行榜：\n之前的点赞是放到set集合，但是set集合是不能排序的，所以这个时候，咱们可以采用一个可以排序的set集合，就是咱们的sortedSet\n我们接下来来对比一下这些集合的区别是什么\n所有点赞的人，需要是唯一的，所以我们应当使用set或者是sortedSet\n其次我们需要排序，就可以直接锁定使用sortedSet啦\n修改代码\nBlogServiceImpl\n点赞逻辑代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Override public Result likeBlog(Long id) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); if (score == null) { // 3.如果未点赞，可以点赞 // 3.1.数据库点赞数 + 1 boolean isSuccess = update().setSql(\u0026#34;liked = liked + 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); // 3.2.保存用户到Redis的set集合 zadd key value score if (isSuccess) { stringRedisTemplate.opsForZSet().add(key, userId.toString(), System.currentTimeMillis()); } } else { // 4.如果已点赞，取消点赞 // 4.1.数据库点赞数 -1 boolean isSuccess = update().setSql(\u0026#34;liked = liked - 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); // 4.2.把用户从Redis的set集合移除 if (isSuccess) { stringRedisTemplate.opsForZSet().remove(key, userId.toString()); } } return Result.ok(); } private void isBlogLiked(Blog blog) { // 1.获取登录用户 UserDTO user = UserHolder.getUser(); if (user == null) { // 用户未登录，无需查询是否点赞 return; } Long userId = user.getId(); // 2.判断当前登录用户是否已经点赞 String key = \u0026#34;blog:liked:\u0026#34; + blog.getId(); Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); blog.setIsLike(score != null); } 点赞列表查询列表\nBlogController\n1 2 3 4 5 @GetMapping(\u0026#34;/likes/{id}\u0026#34;) public Result queryBlogLikes(@PathVariable(\u0026#34;id\u0026#34;) Long id) { return blogService.queryBlogLikes(id); } BlogService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Override public Result queryBlogLikes(Long id) { String key = BLOG_LIKED_KEY + id; // 1.查询top5的点赞用户 zrange key 0 4 Set\u0026lt;String\u0026gt; top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); if (top5 == null || top5.isEmpty()) { return Result.ok(Collections.emptyList()); } // 2.解析出其中的用户id List\u0026lt;Long\u0026gt; ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); String idStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); // 3.根据用户id查询用户 WHERE id IN ( 5 , 1 ) ORDER BY FIELD(id, 5, 1) List\u0026lt;UserDTO\u0026gt; userDTOS = userService.query() .in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + idStr + \u0026#34;)\u0026#34;).list() .stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); // 4.返回 return Result.ok(userDTOS); } 9、好友关注 9.1 好友关注-关注和取消关注 针对用户的操作：可以对用户进行关注和取消关注功能。\n实现思路：\n需求：基于该表数据结构，实现两个接口：\n关注和取关接口 判断是否关注的接口 关注是User之间的关系，是博主与粉丝的关系，数据库中有一张tb_follow表来标示：\n注意: 这里需要把主键修改为自增长，简化开发。\nFollowController\n1 2 3 4 5 6 7 8 9 10 //关注 @PutMapping(\u0026#34;/{id}/{isFollow}\u0026#34;) public Result follow(@PathVariable(\u0026#34;id\u0026#34;) Long followUserId, @PathVariable(\u0026#34;isFollow\u0026#34;) Boolean isFollow) { return followService.follow(followUserId, isFollow); } //取消关注 @GetMapping(\u0026#34;/or/not/{id}\u0026#34;) public Result isFollow(@PathVariable(\u0026#34;id\u0026#34;) Long followUserId) { return followService.isFollow(followUserId); } FollowService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 取消关注service @Override public Result isFollow(Long followUserId) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.查询是否关注 select count(*) from tb_follow where user_id = ? and follow_user_id = ? Integer count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;follow_user_id\u0026#34;, followUserId).count(); // 3.判断 return Result.ok(count \u0026gt; 0); } 关注service @Override public Result follow(Long followUserId, Boolean isFollow) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \u0026#34;follows:\u0026#34; + userId; // 1.判断到底是关注还是取关 if (isFollow) { // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); } else { // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? remove(new QueryWrapper\u0026lt;Follow\u0026gt;() .eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;follow_user_id\u0026#34;, followUserId)); } return Result.ok(); } 9.2 好友关注-共同关注 想要去看共同关注的好友，需要首先进入到这个页面，这个页面会发起两个请求\n1、去查询用户的详情\n2、去查询用户的笔记\n以上两个功能和共同关注没有什么关系，大家可以自行将笔记中的代码拷贝到idea中就可以实现这两个功能了，我们的重点在于共同关注功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // UserController 根据id查询用户 @GetMapping(\u0026#34;/{id}\u0026#34;) public Result queryUserById(@PathVariable(\u0026#34;id\u0026#34;) Long userId){ // 查询详情 User user = userService.getById(userId); if (user == null) { return Result.ok(); } UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); // 返回 return Result.ok(userDTO); } // BlogController 根据id查询博主的探店笔记 @GetMapping(\u0026#34;/of/user\u0026#34;) public Result queryBlogByUserId( @RequestParam(value = \u0026#34;current\u0026#34;, defaultValue = \u0026#34;1\u0026#34;) Integer current, @RequestParam(\u0026#34;id\u0026#34;) Long id) { // 根据用户查询 Page\u0026lt;Blog\u0026gt; page = blogService.query() .eq(\u0026#34;user_id\u0026#34;, id).page(new Page\u0026lt;\u0026gt;(current, SystemConstants.MAX_PAGE_SIZE)); // 获取当前页数据 List\u0026lt;Blog\u0026gt; records = page.getRecords(); return Result.ok(records); } 接下来我们来看看共同关注如何实现：\n需求：利用Redis中恰当的数据结构，实现共同关注功能。在博主个人页面展示出当前用户与博主的共同关注呢。\n当然是使用我们之前学习过的set集合咯，在set集合中，有交集并集补集的api，我们可以把两人的关注的人分别放入到一个set集合中，然后再通过api去查看这两个set集合中的交集数据。\n我们先来改造当前的关注列表\n改造原因是因为我们需要在用户关注了某位用户后，需要将数据放入到set集合中，方便后续进行共同关注，同时当取消关注时，也需要从set集合中进行删除\nFollowServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @Override public Result follow(Long followUserId, Boolean isFollow) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \u0026#34;follows:\u0026#34; + userId; // 1.判断到底是关注还是取关 if (isFollow) { // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); if (isSuccess) { // 把关注用户的id，放入redis的set集合 sadd userId followerUserId stringRedisTemplate.opsForSet().add(key, followUserId.toString()); } } else { // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? boolean isSuccess = remove(new QueryWrapper\u0026lt;Follow\u0026gt;() .eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;follow_user_id\u0026#34;, followUserId)); if (isSuccess) { // 把关注用户的id从Redis集合中移除 stringRedisTemplate.opsForSet().remove(key, followUserId.toString()); } } return Result.ok(); } 具体的关注代码：\nFollowServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Override public Result followCommons(Long id) { // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); String key = \u0026#34;follows:\u0026#34; + userId; // 2.求交集 String key2 = \u0026#34;follows:\u0026#34; + id; Set\u0026lt;String\u0026gt; intersect = stringRedisTemplate.opsForSet().intersect(key, key2); if (intersect == null || intersect.isEmpty()) { // 无交集 return Result.ok(Collections.emptyList()); } // 3.解析id集合 List\u0026lt;Long\u0026gt; ids = intersect.stream().map(Long::valueOf).collect(Collectors.toList()); // 4.查询用户 List\u0026lt;UserDTO\u0026gt; users = userService.listByIds(ids) .stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(users); } 9.3 好友关注-Feed流实现方案 当我们关注了用户后，这个用户发了动态，那么我们应该把这些数据推送给用户，这个需求，其实我们又把他叫做Feed流，关注推送也叫做Feed流，直译为投喂。为用户持续的提供“沉浸式”的体验，通过无限下拉刷新获取新的信息。\n对于传统的模式的内容解锁：我们是需要用户去通过搜索引擎或者是其他的方式去解锁想要看的内容\n对于新型的Feed流的的效果：不需要我们用户再去推送信息，而是系统分析用户到底想要什么，然后直接把内容推送给用户，从而使用户能够更加的节约时间，不用主动去寻找。\nFeed流的实现有两种模式：\nFeed流产品有两种常见模式： Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注。例如朋友圈\n优点：信息全面，不会有缺失。并且实现也相对简单 缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低 智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容。推送用户感兴趣信息来吸引用户\n优点：投喂用户感兴趣信息，用户粘度很高，容易沉迷 缺点：如果算法不精准，可能起到反作用 本例中的个人页面，是基于关注的好友来做Feed流，因此采用Timeline的模式。该模式的实现方案有三种： 我们本次针对好友的操作，采用的就是Timeline的方式，只需要拿到我们关注用户的信息，然后按照时间排序即可\n，因此采用Timeline的模式。该模式的实现方案有三种：\n拉模式 推模式 推拉结合 拉模式：也叫做读扩散 该模式的核心含义就是：当张三和李四和王五发了消息后，都会保存在自己的邮箱中，假设赵六要读取信息，那么他会从读取他自己的收件箱，此时系统会从他关注的人群中，把他关注人的信息全部都进行拉取，然后在进行排序\n优点：比较节约空间，因为赵六在读信息时，并没有重复读取，而且读取完之后可以把他的收件箱进行清楚。\n缺点：比较延迟，当用户读取数据时才去关注的人里边去读取数据，假设用户关注了大量的用户，那么此时就会拉取海量的内容，对服务器压力巨大。\n推模式：也叫做写扩散 推模式是没有写邮箱的，当张三写了一个内容，此时会主动的把张三写的内容发送到他的粉丝收件箱中去，假设此时李四再来读取，就不用再去临时拉取了\n优点：时效快，不用临时拉取\n缺点：内存压力大，假设一个大V写信息，很多人关注他， 就会写很多分数据到粉丝那边去\n推拉结合模式：也叫做读写混合 推拉模式是一个折中的方案，站在发件人这一段，如果是个普通的人，那么我们采用写扩散的方式，直接把数据写入到他的粉丝中去，因为普通的人他的粉丝关注量比较小，所以这样做没有压力，如果是大V，那么他是直接将数据先写入到一份到发件箱里边去，然后再直接写一份到活跃粉丝收件箱里边去，现在站在收件人这端来看，如果是活跃粉丝，那么大V和普通的人发的都会直接写入到自己收件箱里边来，而如果是普通的粉丝，由于他们上线不是很频繁，所以等他们上线时，再从发件箱里边去拉信息。\n9.4 好友关注-推送到粉丝收件箱 需求：\n修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱 收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现 查询收件箱数据时，可以实现分页查询 Feed流中的数据会不断更新，所以数据的角标也在变化，因此不能采用传统的分页模式。\n传统了分页在feed流是不适用的，因为我们的数据会随时发生变化\n假设在t1 时刻，我们去读取第一页，此时page = 1 ，size = 5 ，那么我们拿到的就是10~6 这几条记录，假设现在t2时候又发布了一条记录，此时t3 时刻，我们来读取第二页，读取第二页传入的参数是page=2 ，size=5 ，那么此时读取到的第二页实际上是从6 开始，然后是6~2 ，那么我们就读取到了重复的数据，所以feed流的分页，不能采用原始方案来做。\nFeed流的滚动分页\n我们需要记录每次操作的最后一条，然后从这个位置开始去读取数据\n举个例子：我们从t1时刻开始，拿第一页数据，拿到了10~6，然后记录下当前最后一次拿取的记录，就是6，t2时刻发布了新的记录，此时这个11放到最顶上，但是不会影响我们之前记录的6，此时t3时刻来拿第二页，第二页这个时候拿数据，还是从6后一点的5去拿，就拿到了5-1的记录。我们这个地方可以采用sortedSet来做，可以进行范围查询，并且还可以记录当前获取数据时间戳最小值，就可以实现滚动分页了\n核心的意思：就是我们在保存完探店笔记后，获得到当前笔记的粉丝，然后把数据推送到粉丝的redis中去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Override public Result saveBlog(Blog blog) { // 1.获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 2.保存探店笔记 boolean isSuccess = save(blog); if(!isSuccess){ return Result.fail(\u0026#34;新增笔记失败!\u0026#34;); } // 3.查询笔记作者的所有粉丝 select * from tb_follow where follow_user_id = ? List\u0026lt;Follow\u0026gt; follows = followService.query().eq(\u0026#34;follow_user_id\u0026#34;, user.getId()).list(); // 4.推送笔记id给所有粉丝 for (Follow follow : follows) { // 4.1.获取粉丝id Long userId = follow.getUserId(); // 4.2.推送 String key = FEED_KEY + userId; stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis()); } // 5.返回id return Result.ok(blog.getId()); } 9.5 好友关注-实现分页查询收邮箱 需求：在个人主页的“关注”卡片中，查询并展示推送的Blog信息：\n具体操作如下：\n1、每次查询完成后，我们要分析出查询出数据的最小时间戳，这个值会作为下一次查询的条件\n2、我们需要找到与上一次查询相同的查询个数作为偏移量，下次查询时，跳过这些查询过的数据，拿到我们需要的数据\n综上：我们的请求参数中就需要携带 lastId：上一次查询的最小时间戳 和偏移量这两个参数。\n这两个参数第一次会由前端来指定，以后的查询就根据后台结果作为条件，再次传递到后台。\n一、定义出来具体的返回值实体类\n1 2 3 4 5 6 @Data public class ScrollResult { private List\u0026lt;?\u0026gt; list; private Long minTime; private Integer offset; } BlogController\n注意：RequestParam 表示接受url地址栏传参的注解，当方法上参数的名称和url地址栏不相同时，可以通过RequestParam 来进行指定\n1 2 3 4 5 @GetMapping(\u0026#34;/of/follow\u0026#34;) public Result queryBlogOfFollow( @RequestParam(\u0026#34;lastId\u0026#34;) Long max, @RequestParam(value = \u0026#34;offset\u0026#34;, defaultValue = \u0026#34;0\u0026#34;) Integer offset){ return blogService.queryBlogOfFollow(max, offset); } BlogServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @Override public Result queryBlogOfFollow(Long max, Integer offset) { // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); // 2.查询收件箱 ZREVRANGEBYSCORE key Max Min LIMIT offset count String key = FEED_KEY + userId; Set\u0026lt;ZSetOperations.TypedTuple\u0026lt;String\u0026gt;\u0026gt; typedTuples = stringRedisTemplate.opsForZSet() .reverseRangeByScoreWithScores(key, 0, max, offset, 2); // 3.非空判断 if (typedTuples == null || typedTuples.isEmpty()) { return Result.ok(); } // 4.解析数据：blogId、minTime（时间戳）、offset List\u0026lt;Long\u0026gt; ids = new ArrayList\u0026lt;\u0026gt;(typedTuples.size()); long minTime = 0; // 2 int os = 1; // 2 for (ZSetOperations.TypedTuple\u0026lt;String\u0026gt; tuple : typedTuples) { // 5 4 4 2 2 // 4.1.获取id ids.add(Long.valueOf(tuple.getValue())); // 4.2.获取分数(时间戳） long time = tuple.getScore().longValue(); if(time == minTime){ os++; }else{ minTime = time; os = 1; } } os = minTime == max ? os : os + offset; // 5.根据id查询blog String idStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); List\u0026lt;Blog\u0026gt; blogs = query().in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + idStr + \u0026#34;)\u0026#34;).list(); for (Blog blog : blogs) { // 5.1.查询blog有关的用户 queryBlogUser(blog); // 5.2.查询blog是否被点赞 isBlogLiked(blog); } // 6.封装并返回 ScrollResult r = new ScrollResult(); r.setList(blogs); r.setOffset(os); r.setMinTime(minTime); return Result.ok(r); } 10、附近商户 10.1、附近商户-GEO数据结构的基本用法 GEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有：\nGEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member） GEODIST：计算指定的两个点之间的距离并返回 GEOHASH：将指定member的坐标转为hash字符串形式并返回 GEOPOS：返回指定member的坐标 GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃 GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能 GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能 10.2 附近商户-导入店铺数据到GEO 具体场景说明：\n当我们点击美食之后，会出现一系列的商家，商家中可以按照多种排序方式，我们此时关注的是距离，这个地方就需要使用到我们的GEO，向后台传入当前app收集的地址(我们此处是写死的) ，以当前坐标作为圆心，同时绑定相同的店家类型type，以及分页信息，把这几个条件传入后台，后台查询出对应的数据再返回。\n我们要做的事情是：将数据库表中的数据导入到redis中去，redis中的GEO，GEO在redis中就一个menber和一个经纬度，我们把x和y轴传入到redis做的经纬度位置去，但我们不能把所有的数据都放入到menber中去，毕竟作为redis是一个内存级数据库，如果存海量数据，redis还是力不从心，所以我们在这个地方存储他的id即可。\n但是这个时候还有一个问题，就是在redis中并没有存储type，所以我们无法根据type来对数据进行筛选，所以我们可以按照商户类型做分组，类型相同的商户作为同一组，以typeId为key存入同一个GEO集合中即可\n代码\nHmDianPingApplicationTests\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Test void loadShopData() { // 1.查询店铺信息 List\u0026lt;Shop\u0026gt; list = shopService.list(); // 2.把店铺分组，按照typeId分组，typeId一致的放到一个集合 Map\u0026lt;Long, List\u0026lt;Shop\u0026gt;\u0026gt; map = list.stream().collect(Collectors.groupingBy(Shop::getTypeId)); // 3.分批完成写入Redis for (Map.Entry\u0026lt;Long, List\u0026lt;Shop\u0026gt;\u0026gt; entry : map.entrySet()) { // 3.1.获取类型id Long typeId = entry.getKey(); String key = SHOP_GEO_KEY + typeId; // 3.2.获取同类型的店铺的集合 List\u0026lt;Shop\u0026gt; value = entry.getValue(); List\u0026lt;RedisGeoCommands.GeoLocation\u0026lt;String\u0026gt;\u0026gt; locations = new ArrayList\u0026lt;\u0026gt;(value.size()); // 3.3.写入redis GEOADD key 经度 纬度 member for (Shop shop : value) { // stringRedisTemplate.opsForGeo().add(key, new Point(shop.getX(), shop.getY()), shop.getId().toString()); locations.add(new RedisGeoCommands.GeoLocation\u0026lt;\u0026gt;( shop.getId().toString(), new Point(shop.getX(), shop.getY()) )); } stringRedisTemplate.opsForGeo().add(key, locations); } } 10.3 附近商户-实现附近商户功能 SpringDataRedis的2.3.9版本并不支持Redis 6.2提供的GEOSEARCH命令，因此我们需要提示其版本，修改自己的POM\n第一步：导入pom\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;spring-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.data\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;lettuce-core\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;io.lettuce\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.data\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.lettuce\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lettuce-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.1.6.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 第二步：\nShopController\n1 2 3 4 5 6 7 8 9 @GetMapping(\u0026#34;/of/type\u0026#34;) public Result queryShopByType( @RequestParam(\u0026#34;typeId\u0026#34;) Integer typeId, @RequestParam(value = \u0026#34;current\u0026#34;, defaultValue = \u0026#34;1\u0026#34;) Integer current, @RequestParam(value = \u0026#34;x\u0026#34;, required = false) Double x, @RequestParam(value = \u0026#34;y\u0026#34;, required = false) Double y ) { return shopService.queryShopByType(typeId, current, x, y); } ShopServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @Override public Result queryShopByType(Integer typeId, Integer current, Double x, Double y) { // 1.判断是否需要根据坐标查询 if (x == null || y == null) { // 不需要坐标查询，按数据库查询 Page\u0026lt;Shop\u0026gt; page = query() .eq(\u0026#34;type_id\u0026#34;, typeId) .page(new Page\u0026lt;\u0026gt;(current, SystemConstants.DEFAULT_PAGE_SIZE)); // 返回数据 return Result.ok(page.getRecords()); } // 2.计算分页参数 int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE; int end = current * SystemConstants.DEFAULT_PAGE_SIZE; // 3.查询redis、按照距离排序、分页。结果：shopId、distance String key = SHOP_GEO_KEY + typeId; GeoResults\u0026lt;RedisGeoCommands.GeoLocation\u0026lt;String\u0026gt;\u0026gt; results = stringRedisTemplate.opsForGeo() // GEOSEARCH key BYLONLAT x y BYRADIUS 10 WITHDISTANCE .search( key, GeoReference.fromCoordinate(x, y), new Distance(5000), RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end) ); // 4.解析出id if (results == null) { return Result.ok(Collections.emptyList()); } List\u0026lt;GeoResult\u0026lt;RedisGeoCommands.GeoLocation\u0026lt;String\u0026gt;\u0026gt;\u0026gt; list = results.getContent(); if (list.size() \u0026lt;= from) { // 没有下一页了，结束 return Result.ok(Collections.emptyList()); } // 4.1.截取 from ~ end的部分 List\u0026lt;Long\u0026gt; ids = new ArrayList\u0026lt;\u0026gt;(list.size()); Map\u0026lt;String, Distance\u0026gt; distanceMap = new HashMap\u0026lt;\u0026gt;(list.size()); list.stream().skip(from).forEach(result -\u0026gt; { // 4.2.获取店铺id String shopIdStr = result.getContent().getName(); ids.add(Long.valueOf(shopIdStr)); // 4.3.获取距离 Distance distance = result.getDistance(); distanceMap.put(shopIdStr, distance); }); // 5.根据id查询Shop String idStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); List\u0026lt;Shop\u0026gt; shops = query().in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + idStr + \u0026#34;)\u0026#34;).list(); for (Shop shop : shops) { shop.setDistance(distanceMap.get(shop.getId().toString()).getValue()); } // 6.返回 return Result.ok(shops); } 11、用户签到 11.1、用户签到-BitMap功能演示 我们针对签到功能完全可以通过mysql来完成，比如说以下这张表\n用户一次签到，就是一条记录，假如有1000万用户，平均每人每年签到次数为10次，则这张表一年的数据量为 1亿条\n每签到一次需要使用（8 + 8 + 1 + 1 + 3 + 1）共22 字节的内存，一个月则最多需要600多字节\n我们如何能够简化一点呢？其实可以考虑小时候一个挺常见的方案，就是小时候，咱们准备一张小小的卡片，你只要签到就打上一个勾，我最后判断你是否签到，其实只需要到小卡片上看一看就知道了\n我们可以采用类似这样的方案来实现我们的签到需求。\n我们按月来统计用户签到信息，签到记录为1，未签到则记录为0.\n把每一个bit位对应当月的每一天，形成了映射关系。用0和1标示业务状态，这种思路就称为位图（BitMap）。这样我们就用极小的空间，来实现了大量数据的表示\nRedis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位。\nBitMap的操作命令有：\nSETBIT：向指定位置（offset）存入一个0或1 GETBIT ：获取指定位置（offset）的bit值 BITCOUNT ：统计BitMap中值为1的bit位的数量 BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值 BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回 BITOP ：将多个BitMap的结果做位运算（与 、或、异或） BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置 11.2 用户签到-实现签到功能 需求：实现签到接口，将当前用户当天签到信息保存到Redis中\n思路：我们可以把年和月作为bitMap的key，然后保存到一个bitMap中，每次签到就到对应的位上把数字从0变成1，只要对应是1，就表明说明这一天已经签到了，反之则没有签到。\n我们通过接口文档发现，此接口并没有传递任何的参数，没有参数怎么确实是哪一天签到呢？这个很容易，可以通过后台代码直接获取即可，然后到对应的地址上去修改bitMap。\n代码\nUserController\n1 2 3 4 @PostMapping(\u0026#34;/sign\u0026#34;) public Result sign(){ return userService.sign(); } UserServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Override public Result sign() { // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(\u0026#34;:yyyyMM\u0026#34;)); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.写入Redis SETBIT key offset 1 stringRedisTemplate.opsForValue().setBit(key, dayOfMonth - 1, true); return Result.ok(); } 11.3 用户签到-签到统计 **问题1：**什么叫做连续签到天数？ 从最后一次签到开始向前统计，直到遇到第一次未签到为止，计算总的签到次数，就是连续签到天数。\nJava逻辑代码：获得当前这个月的最后一次签到数据，定义一个计数器，然后不停的向前统计，直到获得第一个非0的数字即可，每得到一个非0的数字计数器+1，直到遍历完所有的数据，就可以获得当前月的签到总天数了\n**问题2：**如何得到本月到今天为止的所有签到数据？\nBITFIELD key GET u[dayOfMonth] 0\n假设今天是10号，那么我们就可以从当前月的第一天开始，获得到当前这一天的位数，是10号，那么就是10位，去拿这段时间的数据，就能拿到所有的数据了，那么这10天里边签到了多少次呢？统计有多少个1即可。\n问题3：如何从后向前遍历每个bit位？\n注意：bitMap返回的数据是10进制，哪假如说返回一个数字8，那么我哪儿知道到底哪些是0，哪些是1呢？我们只需要让得到的10进制数字和1做与运算就可以了，因为1只有遇见1 才是1，其他数字都是0 ，我们把签到结果和1进行与操作，每与一次，就把签到结果向右移动一位，依次内推，我们就能完成逐个遍历的效果了。\n需求：实现下面接口，统计当前用户截止当前时间在本月的连续签到天数\n有用户有时间我们就可以组织出对应的key，此时就能找到这个用户截止这天的所有签到记录，再根据这套算法，就能统计出来他连续签到的次数了\n代码\nUserController\n1 2 3 4 @GetMapping(\u0026#34;/sign/count\u0026#34;) public Result signCount(){ return userService.signCount(); } UserServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Override public Result signCount() { // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(\u0026#34;:yyyyMM\u0026#34;)); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.获取本月截止今天为止的所有的签到记录，返回的是一个十进制的数字 BITFIELD sign:5:202203 GET u14 0 List\u0026lt;Long\u0026gt; result = stringRedisTemplate.opsForValue().bitField( key, BitFieldSubCommands.create() .get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)).valueAt(0) ); if (result == null || result.isEmpty()) { // 没有任何签到结果 return Result.ok(0); } Long num = result.get(0); if (num == null || num == 0) { return Result.ok(0); } // 6.循环遍历 int count = 0; while (true) { // 6.1.让这个数字与1做与运算，得到数字的最后一个bit位 // 判断这个bit位是否为0 if ((num \u0026amp; 1) == 0) { // 如果为0，说明未签到，结束 break; }else { // 如果不为0，说明已签到，计数器+1 count++; } // 把数字右移一位，抛弃最后一个bit位，继续下一个bit位 num \u0026gt;\u0026gt;\u0026gt;= 1; } return Result.ok(count); } 11.4 额外加餐-关于使用bitmap来解决缓存穿透的方案 回顾缓存穿透：\n发起了一个数据库不存在的，redis里边也不存在的数据，通常你可以把他看成一个攻击\n解决方案：\n判断id\u0026lt;0\n如果数据库是空，那么就可以直接往redis里边把这个空数据缓存起来\n第一种解决方案：遇到的问题是如果用户访问的是id不存在的数据，则此时就无法生效\n第二种解决方案：遇到的问题是：如果是不同的id那就可以防止下次过来直击数据\n所以我们如何解决呢？\n我们可以将数据库的数据，所对应的id写入到一个list集合中，当用户过来访问的时候，我们直接去判断list中是否包含当前的要查询的数据，如果说用户要查询的id数据并不在list集合中，则直接返回，如果list中包含对应查询的id数据，则说明不是一次缓存穿透数据，则直接放行。\n现在的问题是这个主键其实并没有那么短，而是很长的一个 主键\n哪怕你单独去提取这个主键，但是在11年左右，淘宝的商品总量就已经超过10亿个\n所以如果采用以上方案，这个list也会很大，所以我们可以使用bitmap来减少list的存储空间\n我们可以把list数据抽象成一个非常大的bitmap，我们不再使用list，而是将db中的id数据利用哈希思想，比如：\nid % bitmap.size = 算出当前这个id对应应该落在bitmap的哪个索引上，然后将这个值从0变成1，然后当用户来查询数据时，此时已经没有了list，让用户用他查询的id去用相同的哈希算法， 算出来当前这个id应当落在bitmap的哪一位，然后判断这一位是0，还是1，如果是0则表明这一位上的数据一定不存在， 采用这种方式来处理，需要重点考虑一个事情，就是误差率，所谓的误差率就是指当发生哈希冲突的时候，产生的误差。\n12、UV统计 12.1 UV统计-HyperLogLog 首先我们搞懂两个概念：\nUV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。 PV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。 通常来说UV会比PV大很多，所以衡量同一个网站的访问量，我们需要综合考虑很多因素，所以我们只是单纯的把这两个值作为一个参考值\nUV统计在服务端做会比较麻烦，因为要判断该用户是否已经统计过了，需要将统计过的用户信息保存。但是如果每个访问的用户都保存到Redis中，数据量会非常恐怖，那怎么处理呢？\nHyperloglog(HLL)是从Loglog算法派生的概率算法，用于确定非常大的集合的基数，而不需要存储其所有值。相关算法原理大家可以参考：https://juejin.cn/post/6844903785744056333#heading-0 Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。\n12.2 UV统计-测试百万数据的统计 测试思路：我们直接利用单元测试，向HyperLogLog中添加100万条数据，看看内存占用和统计效果如何\n经过测试：我们会发生他的误差是在允许范围内，并且内存占用极小\n","permalink":"https://cold-bin.github.io/post/redis%E5%BA%94%E7%94%A8/","tags":["redis超实用的应用"],"title":"Redis应用"},{"categories":["golang"],"contents":"彻底搞懂GOROOT、GOPATH、PATH、mod管理和gopath管理项目的区别 1、GOPATH 和 GOROOT 不同于其他语言，go中没有项目的说法，只有包, 其中有两个重要的路径，GOROOT 和 GOPATH\nGo开发相关的环境变量如下：\nGOROOT：GOROOT就是Go的安装目录，（类似于java的JDK） GOPATH：GOPATH是我们的工作空间,保存go项目代码和第三方依赖包 GOPATH可以设置多个，其中，第一个将会是默认的包目录，使用 go get 下载的包都会在第一个path中的src目录下，使用 go install时，在哪个GOPATH中找到了这个包，就会在哪个GOPATH下的bin目录生成可执行文件\n2、修改 GOPATH 和 GOROOT GOROOT GOROOT是Go的安装路径。GOROOT在绝大多数情况下都不需要修改\nMac中安装Go会自动配置好GOROOT，路径为/usr/local/go。Win中默认的GOROOT是在 C:\\Go中，也可自己指定\n【如下图所示则我的GORROT为：D:\\development\\go】，以下是GOROOT目录的内容：\n可以看到GOROOT下有bin，doc和src目录。bin目录下有我们熟悉的go和gofmt工具。可以认为GOOROOT和Java里的JDK目录类似。\nGOPATH GOPATH是开发时的工作目录。用于：\n保存编译后的二进制文件。 go get和go install命令会下载go代码到GOPATH。 import包时的搜索路径 使用GOPATH时，GO会在以下目录中搜索包：\nGOROOT/src：该目录保存了Go标准库代码。 GOPATH/src：该目录保存了应用自身的代码和第三方依赖的代码。 假设程序中引入了如下的包：\nimport \u0026quot;Go-Player/src/chapter17/models\u0026quot; 第一步：Go会先去GOROOT的scr目录中查找，很显然它不是标准库的包，没找到。 第二步：继续在GOPATH的src目录去找，准确说是GOPATH/src/Go-Player/src/chapter17/models这个目录。如果该目录不存在，会报错找不到package。在使用GOPATH管理项目时，需要按照GO寻找package的规范来合理地保存和组织Go代码。\n3、HelloWord——GOPATH版 （1）设置并查看GOPATH和GOROOT环境变量 安装go SKD目录：D:\\development\\go go项目存放目录：D:\\development\\jetbrains\\goland\\workspace，并且此目录下含有bin、src、pkg三个文件夹，src文件夹用来存放项目代码 当引入module时，首先在GOROOT的src目录下查找，然后再GPOPATH的src目录下查找\n（2）GOLand环境配置 在D:\\development\\jetbrains\\goland\\workspace\\src目录下新建项目GO-Player bin：存放编译后的exe文件\npkg：存放自定义包的目录\nsrc：存放项目源文件的目录\n按如下指令进行配置\n可在Settings中选择SDK和添加GOPATH\n（3）测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 models：Student.go main：hello.go package main import ( //\u0026#34;./models\u0026#34; //相对路径 \u0026#34;Go-Player/src/ademo/models\u0026#34; //根据GOPATH找 //根据GOPATH：D:\\development\\jetbrains\\goland\\workspace，在其src目录下查找 //即GOPATH/src/Go-Player/src/ademo/models \u0026#34;fmt\u0026#34; ) func main() { stu := models.Student{ Name: \u0026#34;张三\u0026#34;, } fmt.Println(stu) } 此篇文章仅介绍网上大部分GOPATH版本。Go语言Hello World都只简单地介绍了GOPATH版本。但是从Go的1.11版本之后，已不再推荐使用GOPATH来构建应用了。也就是说GOPATH被认为是废弃的，错误的做法。\n4、一些踩坑经验 当你开启了GO111MODULE，仍然使用GOPATH模式的方法，在引入自定义模块时会报错。go mod具体使用将在下一篇介绍\n1 2 3 4 5 6 7 GO111MODULE 有三个值：off, on和auto（默认值）。 GO111MODULE=off，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找。 GO111MODULE=on，go命令行会使用modules，而一点也不会去GOPATH目录下查找。 GO111MODULE=auto，默认值，go命令行将会根据当前目录来决定是否启用module功能。这种情况下可以分为两种情形： 当前目录在GOPATH/src之外且该目录包含go.mod文件，即使用go mod对项目的第三方依赖进行管理，不再使用gopath的方式 当前文件在包含go.mod文件的目录下面。 当modules 功能启用时，依赖包的存放位置变更为$GOPATH/pkg，允许同一个package多个版本并存，且多个项目可以共享缓存的 module。\n（1）使用了了相对路径：import \u0026ldquo;./models\u0026rdquo; 报错：build command-line-arguments: cannot find module for path _/D_/dev这里后面一堆本地路径 这是因为在go module下 你源码中 impot …/ 这样的引入形式不支持了， 应该改成 impot 模块名/ 。 这样就ok了\n（2）使用结合了GOPATH的形式：import \u0026quot;Go-Player/src/ademo/models\u0026quot; 于是我们把上面的import改成了结合GOPATH的如上形式\n报错：package Go-Player/src/ademo/models is not in GOROOT D:/development/go/src/GPlayer/src/ademo/models\n（3）彻底解决方法：用go env -u 恢复初始设置 不再使用go mod：\ngo env -w GO111MODULE=off 或者 go env -w GO111MODULE=auto go env -u GO111MODULE 区别在于，如果GO111MODULE=on或者auto，在go get下载包时候，会下载到GOPATH/pkg/mod，引入时也是同样的从这个目录开始。如果这行了上述命令，那么在go get下载包时候，会下载到GOPATH/src 目录下\n4、最佳实践 如果在GOPATH目录下的src文件下编写项目代码，可以不使用mod管理方式，直接沿用旧版本的GOPATH管理；\n如果在GOPATH以外的任意目录下要编译并运行项目代码，为了方便管理第三方依赖，使用go mod进行管理。\n1 2 3 4 5 6 #初始化并创建以“当前项目根目录”为moudles名称的go.mod文件 go mod init 当前项目根目录 #剔除无用包，拿取有用包，准备代码所需环境（第三方依赖） go mod tidy #编译运行即可 #注意：当前mod文件里的moudles名称必须与当前项目目录名保持一致，否则会报错！ ","permalink":"https://cold-bin.github.io/post/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/","tags":["go依赖管理"],"title":"Mod和gopath依赖管理"},{"categories":["golang","docker"],"contents":"如何构建 Go 应用的 Docker 镜像 简介： 在部署 Go 应用时，我们通常会使用 Docker 镜像来部署，那么如何构建一个 Go 应用的 Docker 镜像呢？镜像构建过程中有没有什么最佳实践呢？\n在部署 Go 应用时，我们通常会使用 Docker 镜像来部署，那么如何构建一个 Go 应用的 Docker 镜像呢？镜像构建过程中有没有什么最佳实践呢？\n这正是本文想要讲解的内容。总的来说，本文会包含 Dockerfile 编写、镜像构建、多阶段构建、交叉编译以及使用 Makefile 简化构建流程等知识点。\n创建一个简单的 Go 应用 为了说明整个镜像构建流程，让我们先从一个简单的 Go REST 应用开始。\n该应用主要有以下功能：\n访问 /，返回 Hello, Docker! \u0026lt;3； 访问 /ping ，返回 JSON 字符串 {\u0026quot;Status\u0026quot;:\u0026quot;OK\u0026quot;}； 可以通过环境变量设置 HTTP_PORT，默认值为8080。 应用源码地址在 https://github.com/nodejh/docker-go-server-ping ，你可以直接下载使用，也可以按照下面的步骤从零开始编写代码。\n方式一：下载源码 1 $ git clone git@github.com:nodejh/docker-go-server-ping.git 安装依赖模块：\n1 $ go mod download 方式二：从零编写 Go 应用 新建一个 docker-go-server-ping 目录，然后初始化 Go 模块：\n1 2 $ mkdir docker-go-server-ping \u0026amp;\u0026amp; cd docker-go-server-ping $ go mod init github.com/nodejh/docker-go-server-ping 安装 Echo 模块：\n1 $ go get github.com/labstack/echo/v4 接下来创建一个 main.go 文件，并实现一个简单的 Go 服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/labstack/echo/v4\u0026#34; \u0026#34;github.com/labstack/echo/v4/middleware\u0026#34; ) func main() { e := echo.New() e.Use(middleware.Logger()) e.Use(middleware.Recover()) e.GET(\u0026#34;/\u0026#34;, func(c echo.Context) error { return c.HTML(http.StatusOK, \u0026#34;Hello, Docker! \u0026lt;3\u0026#34;) }) e.GET(\u0026#34;/ping\u0026#34;, func(c echo.Context) error { return c.JSON(http.StatusOK, struct{ Status string }{Status: \u0026#34;OK\u0026#34;}) }) httpPort := os.Getenv(\u0026#34;HTTP_PORT\u0026#34;) if httpPort == \u0026#34;\u0026#34; { httpPort = \u0026#34;8080\u0026#34; } e.Logger.Fatal(e.Start(\u0026#34;:\u0026#34; + httpPort)) } 接下来可能需要执行 go mod tidy 来确保 go.mod 和源码中的模块一致：\n1 $ go mod tidy 测试 Go 应用 让我们启动我们的 Go 应用并确保它正常运行。进入项目目录并通过 go run 命令执行源码：\n1 2 3 4 5 6 7 8 9 10 $ go run main.go ____ __ / __/___/ / ___ / _// __/ _ \\/ _ \\ /___/\\__/_//_/\\___/ v4.6.1 High performance, minimalist Go web framework https://echo.labstack.com ____________________________________O/_______ O\\ ⇨ http server started on [::]:8080 让我们对应用进行一个简单的测试，打开一个新的终端，使用 curl 命令或在浏览器打开 http://localhost:8080/ 进行测试。以 curl 为例：\n1 2 $ curl http://localhost:8080/ Hello, Docker! \u0026lt;3 可以看到应用正常返回了，正如开头描述的那样。\n确定服务器正在运行并且可以访问后，我们就可以继续针对应用构建 Docker 镜像了。\n为 Go 应用创建一个 Dockerfile Dockerfile 是 Docker 镜像的描述文件，是一个文本文件。当我们执行 docker build 构建镜像时，Docker 就会读取 Dockerfile 中的指令来创建 Docker 镜像。\n从零创建 Dockerfile 让我们先来看一下创建 Dockerfile 的详细过程。\n在项目根目录中创建一个名为 Dockerfile 的文件并在编辑器中打开。\n添加到 Dockerfile 的第一行是 # syntax 解析器指令，这是一个可选项，表示 Docker 构建器在解析 Dockerfile 时使用什么语法。解析器指令必须在 Dockerfile 其他任何注释、空格或指令之前，并且应该是 Dockerfile 的第一行。建议使用 docker/dockerfile:1 ，它始终指向版本 1 语法的最新版本。\n1 # syntax=docker/dockerfile:1 接下来在 Dockerfile 中再添加一行，告诉 Docker 我们的应用使用什么基础镜像：\n1 2 # syntax=docker/dockerfile:1 FROM golang:1.16-alpine 这里我们使用了 Golang 官方镜像 中的 1.16-alpine 版本作为基础镜像，alpine 是专门为容器设计的小型 Linux 发行版。使用基础镜像的好处是，基础镜像中内置了 Go 运行环境和工具，我们就不用自己再去安装了。\n为了更好地在镜像中管理我们的应用程序，让我们在镜像中创建一个工作目录，之后源码或编译产物都存放在该目录中：\n1 WORKDIR /app 接下来我们就需要在镜像中编译 Go 应用，这样做是为了保证编译和最终运行的环境一致。\n通常我们编译 Go 应用的第一步是安装依赖，所以要先把 go.mod 和 go.sum 复制到镜像中：\n1 2 COPY go.mod ./ COPY go.sum ./ COPY 命令可以把文件复制到镜像中，这里的 ./ 对应的目录就是上一个命令 WORKDIR 指定的 /app 目录。\n然后通过 RUN 命令在镜像中执行 go mod download 安装依赖，这与我们在本机直接运行命令的作用完全相同，不同的是这次会将依赖安装在镜像中：\n1 RUN go mod download 此时我们已经有了一个基于 Go 1.16 的镜像，并安装了 Go 应用所需的依赖。下一步要做的就是把源码复制到镜像中：\n1 RUN COPY *.go ./ 该 COPY 命令使用通配符将本机当前目录（即 Dockerfile 所在目录）中后缀为 .go 的文件全部复制到镜像中。\n接下来我们就可以编译 Go 应用了，依然使用熟悉的 RUN 命令：\n1 RUN go build -o /docker-go-server-ping 该命令的结果就是产生一个名为 docker-go-server-ping 的二进制文件，并存放在镜像的系统根目录中。当然你也可以将二进制文件放在其他任何位置，根目录在这里没有特殊意义，只是它的路径较短且保持了可读性，使用起来更方便。\n现在，剩下要做的就是告诉 Docker 当我们用该镜像来启动容器时要执行什么命令，这时可以使用 CMD 命令：\n1 CMD [ \u0026#34;/docker-go-server-ping\u0026#34; ] 完整的 Dockerfile 1 2 3 4 5 6 7 8 9 10 # syntax=docker/dockerfile:1 FROM golang:1.16-alpine WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download COPY *.go ./ RUN go build -o /docker-go-server-ping EXPOSE 8080 CMD [ \u0026#34;/docker-go-server-ping\u0026#34; ] 构建镜像 Dockerfile 编写完成后就可以使用 docker build 命令来构建镜像了。\n构建镜像 让我们进入 Dockerfile 所在目录构建镜像，并通过可选的 --tag 给镜像定义一个方便阅读和识别的名字和标签，格式为 \u0026lt;镜像名称\u0026gt;:\u0026lt;标签\u0026gt;，默认是标签 latest：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ docker build --tag docker-go-server-ping:latest . [+] Building 32.2s (16/16) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.1s =\u0026gt; =\u0026gt; transferring dockerfile: 37B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; resolve image config for docker.io/docker/dockerfile:1 15.2s =\u0026gt; CACHED docker-image://docker.io/docker/dockerfile:1@sha256:42399d4635eddd7a9b8a24be879d2f9a930d0ed040a61324cfdf59ef1357b3b2 0.0s =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; [internal] load metadata for docker.io/library/golang:1.16-alpine 15.6s =\u0026gt; [1/7] FROM docker.io/library/golang:1.16-alpine@sha256:45412fe3f5016509fc448b83faefc34e6f9e9bcc8ca1db1c54505d5528264e16 0.0s =\u0026gt; [internal] load build context 0.1s =\u0026gt; =\u0026gt; transferring context: 80B 0.0s =\u0026gt; CACHED [2/7] WORKDIR /app 0.0s =\u0026gt; CACHED [3/7] COPY go.mod ./ 0.0s =\u0026gt; CACHED [4/7] COPY go.sum ./ 0.0s =\u0026gt; CACHED [5/7] RUN go mod download 0.0s =\u0026gt; CACHED [6/7] COPY *.go ./ 0.0s =\u0026gt; CACHED [7/7] RUN go build -o /docker-go-server-ping 0.0s =\u0026gt; exporting to image 0.1s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:17fa05342ccef20e73abc48210fff449cc49683ffd817ccc6aef35500cf91ae2 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/docker-go-server-ping 构建完成后可以通过 docker image ls （或 docker images 简写）来查看镜像列表：\n1 2 3 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE docker-go-server-ping latest 17fa05342cce 9 minutes ago 407MB 为镜像设置标签 我们也可以通过 docker image tag 命令来为镜像设置新的标签，例如：\n1 $ docker image tag docker-go-server-ping:latest docker-go-server-ping:v1.0.0 这时通过 docker image ls 就可以看到 docker-go-server-ping 镜像的两个标签：\n1 2 3 4 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE docker-go-server-ping latest 17fa05342cce 12 minutes ago 407MB docker-go-server-ping v1.0.0 17fa05342cce 1 minutes ago 407MB 我们还可以通过 docker image rm （简写为 docker rmi）删除镜像：\n1 2 $ docker image rm docker-go-server-ping:v1.0.0 Untagged: docker-go-server-ping:v1.0.0 这时再查看镜像列表，v1.0.0 版本的镜像已被删除，只剩下 latest 版本了：\n1 2 3 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE docker-go-server-ping latest 17fa05342cce 15 minutes ago 407MB 单元测试 既然本文主要将 Go 的 Docker 镜像，这样就顺便简单说明如何使用 dockertest 对 Go 应用进行单元测试。\ndockertest 可以在 Docker 容器中启动 Go 应用镜像并执行测试用例。\n相关测试用例可以参考 main_test.go 。\n在 main_test.go 中我们使用了 docker-go-server-ping:latest 镜像来运行 Go 应用：\n1 resource, err := pool.Run(\u0026#34;docker-go-server-ping\u0026#34;, \u0026#34;latest\u0026#34;, []string{}) 所以在执行 go test 之前，需要先构建 docker-go-server-ping:latest 镜像：\n1 2 3 4 5 6 7 8 9 10 11 12 $ docker build --tag docker-go-server-ping:latest . ... $ go test -v ./ === RUN TestRespondsWithLove body Hello, Docker! \u0026lt;3 --- PASS: TestRespondsWithLove (1.12s) === RUN TestHealthCheck main_test.go:85: container not ready, waiting... --- PASS: TestHealthCheck (1.66s) PASS ok command-line-arguments 2.799s 多阶段构建 可能你已经注意到了，docker-go-server-ping 镜像的大小有 407MB，这着实有点大，并且镜像中还有全套的 Go 工具、Go 应用的依赖等，但实际我们应用运行时不需要这些文件，只需要编译后的二进制文件。那么能不能减小镜像的体积呢？\n要减小镜像体积，我们可以使用多阶段构建。Docker 在 17.05 版本以后，新增了多阶段构建功能。多阶段构建实际上是允许一个 Dockerfile 中出现多个 FROM 指令。通常我们使用多阶段构建时，会先使用一个（或多个）镜像来构建一些中间制品，然后再将中间制品放入另一个最新且更小的镜像中，这个最新的镜像就只包含最终需要的构建制品。\n多阶段构建 Dockerfile 项目实例：https://github.com/liuhaibin123456789/last-homework-in-last-term.git\n我们先创建一个多阶段构建的 Dockerfile，名为 Dockerfile.multistage，文件名中的 multistage 没有特殊含义，只是为了和之前的 Dockerfile 作区分，下面是完整的 Dockerfile：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # syntax=docker/dockerfile:1 ## ## Build ## FROM golang:1.16-alpine AS build WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download COPY *.go ./ RUN CGO_ENABLED=0 GOOS=linux go build -o /docker-go-server-ping ## ## Deploy ## FROM scratch WORKDIR / COPY --from=build /docker-go-server-ping /docker-go-server-ping EXPOSE 8080 ENTRYPOINT [\u0026#34;/docker-go-server-ping\u0026#34;] 在 Dockerfile.multistage 中使用了两次 FROM 指令，分别对应两个构建阶段。第一个阶段构建的 FROM 指令依然使用 golang:1.16-alpine 作为基础镜像，并将该阶段命名为 build。第二个构建阶段的 FROM 指令使用 scratch 作为基础镜像，告诉 Docker 接下来从一个全新的基础镜像开始构建，scratch 镜像是 Docker 项目预定义的最小的镜像。第二阶段构建主要是将上个阶段中编译好的二进制文件复制到新的镜像中。\n在 Go 应用中，多阶段构建非常常见，可以减小镜像的体积、节省大量的存储空间。\n在 Dockerfile.multistage 中需要额外关注的是 RUN 指令，这里使用到了交叉编译。\n交叉编译 交叉编译是指在一个平台上生成另一个平台的可执行程序。\n在其他编程语言中进行交叉编译可能要借助第三方工具，但 Go 内置了交叉编译工具，使用起来非常方便，通常设置 CGO_ENABLED、GOOS 和 GOARCH 这几个环境变量就够了。\nCGO_ENABLED 默认值是 1，即默认开启 cgo，允许在 Go 代码中调用 C 代码。\n当 CGO_ENABLED=1 进行编译时，会将文件中引用 libc 的库（比如常用的 net 包）以动态链接的方式生成目标文件； 当 CGO_ENABLED=0 进行编译时，则会把在目标文件中未定义的符号（如外部函数）一起链接到可执行文件中。 所以交叉编译时，我们需要将 CGO_ENABLED 设置为 0。\n注意：编译go应用程序时，需将CGO_ENABLED设置为0，这样多阶段构建镜像时，如果在前一阶段编译成可执行文件时，如果指定该值为0，在下一阶段拉取的可执行文件，会因少了执行时必要的依赖而不能执行。同时，还应指定GOPROXY为七牛云地址，有些mod依赖国外下不了\nGOOS 和 GOARCH GOOS 是目标平台的操作系统，如 linux、windows，注意 macOS 的值是 darwin。默认是当前操作系统。\nGOARCH 是目标平台的 CPU 架构，如 amd64、arm、386 等。默认值是当前平台的 CPU 架构。\nGo 支持的所有操作系统和 CPU 架构可以查看 syslist.go 。\n我们可以使用 go env 命令获取当前 GOOS 和 GOARCH 的值。例如我当前的操作系统是 macOS：\n1 2 3 $ go env GOOS GOARCH darwin amd64 所以在本文的多阶段构建 Dockerfile.multistage 中，构建命令是：\n1 CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o /docker-go-server-ping 构建镜像 由于我们现在有两个 Dockerfile，所以我们必须告诉 Docker 我们要使用新的 Dockerfile 进行构建。\n1 $ docker build -t docker-go-server-ping:multistage -f Dockerfile.multistage . 构建完成后，你会发现 docker-go-server-ping:multistage 只有不到 8MB，比 docker-go-server-ping:latest 小了几十倍。\n1 2 3 4 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE docker-go-server-ping latest 17fa05342cce 20 minutes ago 407MB docker-go-server-ping multistage 55fa4ded3360 1 minutes ago 7.93MB 运行 Go 镜像 现在我们有了 Go 应用的镜像，接下来就可以运行 Go 镜像查看应用程序是否正常运行。\n要在 Docker 容器中运行镜像，我们可以使用 docker run 命令，参数是镜像名称：\n1 2 3 4 5 6 7 8 9 10 $ docker run -p 8080:8080 docker-go-server-ping:multistage ____ __ / __/___/ / ___ / _// __/ _ \\/ _ \\ /___/\\__/_//_/\\___/ v4.6.1 High performance, minimalist Go web framework https://echo.labstack.com ____________________________________O/_______ O\\ ⇨ http server started on [::]:8080 可以看到 Go 应用成功启动了。\n让我们再打开一个新的终端，通过 curl 向 Go 服务器发起一个请求：\n1 2 $ curl http://localhost:8080 Hello, Docker! \u0026lt;3 使用 Makefile 简化构建流程 在前面的步骤中，我们使用到了非常多的命令，维护起来非常麻烦，这时我们就可以使用 make 来简化构建流程。\nmake 是一个自动化构建工具，会在根据当前目录下名为 Makefile（或 makefile）的文件来执行相应的构建任务。\n所以让我们先创建一个 Makefile 文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 APP=docker-go-server-ping all: clean test build-docker-multistage test: go test -v ./ run: clean go build -o ${APP} ./${APP} deps: go mod download clean: go clean build-docker: docker build -t ${APP}:latest -f Dockerfile . build-docker-multistage: docker build -t ${APP}:multistage -f Dockerfile.multistage . 接下来就可以通过 make 命令进行测试或构建了。\n例如：\nmake：执行 all 中定义的命令 make test：执行单元测试 make build-docker：构建 Docker 镜像 build-docker-multistage：多阶段构建镜像，构建的镜像通常用于生产环境 当然你也可以在 Makefile 中定义其他命令。\n总结 在本文中，我们首先开发了一个简单的 Go REST 服务应用，然后针对该应用详细讲解了如何构建 Docker 镜像。要构建镜像首先需要编写 Dockerfile，但基础的 Dockerfile 体积过大，所以我们又学习了如何通过多阶段构建减小镜像体积。在多阶段构建时，由于构建机和部署服务器可能存在操作系统和 CPU 架构的差异，又学习了如何通过交叉编译构建出可在其他平台直接使用的二进制文件。最后由于整个构建流程涉及命令比较多，真实 Go 项目可能构建流程会更复杂，所以学习了如何通过 Makefile 简化构建流程。\n最后感谢你的阅读，希望本文的内容能让你有所收获。\n","permalink":"https://cold-bin.github.io/post/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BAgo%E5%BA%94%E7%94%A8%E7%9A%84docker%E9%95%9C%E5%83%8F/","tags":[],"title":"如何构建Go应用的Docker镜像"},{"categories":["golang","爬虫"],"contents":"基于golang的selenium使用详解 什么是selenium？我理解成：selenium是一种程序员使用地、自动化地、可以通过代码来操控指定浏览器的一种集成工具。在使用go语言colly框架爬取需要登录的网站时，遇到了问题，我必须输入并提交账号密码(colly这个还做不出来)，才能访问网站后面的资源。在网上百度了许久，于是看到了selenium可以做自动化地控制浏览器操作，于是乎便学习了基于go语言实现的selenium-\u0026gt;第三方框架:tebeka/selenium。刚开始学习地时候搜索大量教程，也去官网逛了一圈，觉得这些教程都不是很好，大多只是贴源码，没有更为详尽地解说，于是乎，自己便将自己的学习分享出来。\nselenium使用 安装 目前我正在使用的一个依赖库是 tebeka/selenium，功能较完整且处于维护中。\n1 go get -u github.com/tebeka/selenium 另外，我们需要对应不同类型的浏览器进行安装 WebDriver，Google Chrome 需要安装 ChromeDriver，Firefox 则需要安装 geckodriver。\n使用实例(chorme为例) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/tebeka/selenium\u0026#34; ) const ( //指定对应浏览器的driver chromeDriverPath = \u0026#34;D:/selenium/chromedriver.exe\u0026#34; port = 8080 ) func main() { //ServiceOption 配置服务实例 opts := []selenium.ServiceOption{ selenium.Output(os.Stderr), // Output debug information to STDERR. } //SetDebug 设置调试模式 selenium.SetDebug(true) //在后台启动一个ChromeDriver实例 service, err := selenium.NewChromeDriverService(chromeDriverPath, port, opts...) if err != nil { panic(err) // panic is used only as an example and is not otherwise recommended. } defer service.Stop() //连接到本地运行的 WebDriver 实例 //这里的map键值只能为browserName，源码里需要获取这个键的值，来确定连接的是哪个浏览器 caps := selenium.Capabilities{\u0026#34;browserName\u0026#34;: \u0026#34;chrome\u0026#34;} //NewRemote 创建新的远程客户端，这也将启动一个新会话。 urlPrefix 是 Selenium 服务器的 URL，必须以协议 (http, https, ...) 为前缀。为urlPrefix提供空字符串会导致使用 DefaultURLPrefix,默认访问4444端口，所以最好自定义，避免端口已经被抢占。后面的路由还是照旧DefaultURLPrefix写 wd, err := selenium.NewRemote(caps, fmt.Sprintf(\u0026#34;http://localhost:%d/wd/hub\u0026#34;, port)) if err != nil { panic(err) } defer wd.Quit() // 导航到指定网站页面，浏览器默认get方法 if err := wd.Get(\u0026#34;http://play.golang.org/?simple=1\u0026#34;); err != nil { panic(err) } // 此步操作为定位元素 // 在当前页面的 DOM 中准确找到一个元素，使用css选择器，使用id选择器选择文本框 // 值得注意的是，WebElement接口定义了几乎所有操作Web元素支持的方法，例如清除元素、移动鼠标、截图、鼠标点击、提交按钮等操作 // 此处对于浏览器的自动化操作可以应用于爬虫登录需要输入密码或者其他应用 elem, err := wd.FindElement(selenium.ByCSSSelector, \u0026#34;#code\u0026#34;) if err != nil { panic(err) } // 操作元素，删除文本框中已有的样板代码 if err := elem.Clear(); err != nil { panic(err) } // 在文本框中输入一些新代码 err = elem.SendKeys(` package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello WebDriver!\u0026#34;) } `) if err != nil { panic(err) } // css选择器定位按钮控件 btn, err := wd.FindElement(selenium.ByCSSSelector, \u0026#34;#run\u0026#34;) if err != nil { panic(err) } if err := btn.Click(); err != nil { panic(err) } // 等待程序完成运行并获得输出 outputDiv, err := wd.FindElement(selenium.ByCSSSelector, \u0026#34;#output\u0026#34;) if err != nil { panic(err) } var output string for { output, err = outputDiv.Text() if err != nil { panic(err) } if output != \u0026#34;Waiting for remote server...\u0026#34; { break } time.Sleep(time.Millisecond * 100) } fmt.Printf(\u0026#34;%s\u0026#34;, strings.Replace(output, \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, -1)) // Example Output: // Hello WebDriver! // // Program exited. } 使用css选择器，需要熟悉其语法规则。达到定位的目的。之后就是使用WebElement对应的方法实现业务逻辑处理ok了。\n陷阱 注意：在使用css选择器的同时一定不能使用右键检查查看源代码，这样的源代码是已经经过js渲染的的最终源代码;而查看网页源代码则是服务器发送到浏览器的原封不动的源代码，不包括页面动态渲染的内容。所以，使用css选择器应该使用查看网页源代码。\ncss选择器 选择器 例子 例子描述 .class .intro 选择 class=\u0026quot;intro\u0026quot; 的所有元素。 #id #firstname 选择 id=\u0026quot;firstname\u0026quot; 的所有元素。 * * 选择所有元素。 element p 选择所有 \u0026lt;p\u0026gt; 元素。 element,element div,p 选择所有 \u0026lt;div\u0026gt; 元素和所有 \u0026lt;p\u0026gt; 元素。 element element div p 选择\u0026lt;div\u0026gt;元素内部的所有\u0026lt;p\u0026gt;元素。 element\u0026gt;element div\u0026gt;p 选择父元素为 \u0026lt;div\u0026gt; 元素的所有 \u0026lt;p\u0026gt; 元素。 element+element div+p 选择紧接在 \u0026lt;div\u0026gt; 元素之后的所有 \u0026lt;p\u0026gt; 元素。 [attribute] [target] 选择带有 target 属性所有元素。 [attribute=value] [target=_blank] 选择 target=\u0026quot;_blank\u0026quot; 的所有元素。 [attribute~=value] [title~=flower] 选择title属性包含单词 \u0026quot;flower\u0026quot; 的所有元素。 element1~element2 p~ul 选择前面有 \u0026lt;p\u0026gt; 元素的每个 \u0026lt;ul\u0026gt; 元素。 [attribute^=value] a[src^=\u0026quot;https\u0026quot;] 选择其 src 属性值以 \u0026quot;https\u0026quot; 开头的每个 \u0026lt;a\u0026gt; 元素。 [attribute$=value] a[src$=\u0026quot;.pdf\u0026quot;] 选择其 src 属性以 \u0026quot;.pdf\u0026quot; 结尾的所有\u0026lt;a\u0026gt;元素。 [attribute*=value] a[src*=\u0026quot;abc\u0026quot;] 选择其 src 属性中包含 \u0026quot;abc\u0026quot; 子串的每个\u0026lt;a\u0026gt;元素。 :first-of-type p:first-of-type 选择属于其父元素的首个 \u0026lt;p\u0026gt; 元素的每个 \u0026lt;p\u0026gt; 元素。 :last-of-type p:last-of-type 选择属于其父元素的最后 \u0026lt;p\u0026gt; 元素的每个 \u0026lt;p\u0026gt; 元素。 :only-of-type p:only-of-type 选择属于其父元素唯一的\u0026lt;p\u0026gt;元素的每个\u0026lt;p\u0026gt;元素。 :only-child p:only-child 选择属于其父元素的唯一子元素的每个\u0026lt;p\u0026gt;元素。 :nth-child(n) p:nth-child(2) 选择属于其父元素的第二个子元素的每个 \u0026lt;p\u0026gt; 元素。 :nth-last-child(n) p:nth-last-child(2) 同上，从最后一个子元素开始计数。 :nth-of-type(n) p:nth-of-type(2) 选择属于其父元素第二个 \u0026lt;p\u0026gt; 元素的每个\u0026lt;p\u0026gt;元素。 :nth-last-of-type(n) p:nth-last-of-type( 同上，但是从最后一个子元素开始计数。 :last-child p:last-child 选择属于其父元素最后一个子元素每个 \u0026lt;p\u0026gt; 元素。 :root :root 选择文档的根元素。 :empty p:empty 选择没有子元素的每个 \u0026lt;p\u0026gt; 元素（包括文本节点）。 :not(selector) :not(p) 选择非\u0026lt;p\u0026gt;元素的每个元素。 webElement接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // WebElement defines method supported by web elements. type WebElement interface { // Click clicks on the element. Click() error // SendKeys types into the element. SendKeys(keys string) error // Submit submits the button. Submit() error // Clear clears the element. Clear() error // MoveTo moves the mouse to relative coordinates from center of element, If // the element is not visible, it will be scrolled into view. MoveTo(xOffset, yOffset int) error // FindElement finds a child element. FindElement(by, value string) (WebElement, error) // FindElement finds multiple children elements. FindElements(by, value string) ([]WebElement, error) // TagName returns the element\u0026#39;s name. TagName() (string, error) // Text returns the text of the element. Text() (string, error) // IsSelected returns true if element is selected. IsSelected() (bool, error) // IsEnabled returns true if the element is enabled. IsEnabled() (bool, error) // IsDisplayed returns true if the element is displayed. IsDisplayed() (bool, error) // GetAttribute returns the named attribute of the element. GetAttribute(name string) (string, error) // Location returns the element\u0026#39;s location. Location() (*Point, error) // LocationInView returns the element\u0026#39;s location once it has been scrolled // into view. LocationInView() (*Point, error) // Size returns the element\u0026#39;s size. Size() (*Size, error) // CSSProperty returns the value of the specified CSS property of the // element. CSSProperty(name string) (string, error) // Screenshot takes a screenshot of the attribute scroll\u0026#39;ing if necessary. Screenshot(scroll bool) ([]byte, error) } 源码如上，可见这个webElement封装了一些操作特定元素的一些方法，可以达到自动化操作浏览器的目的，如清除元素，填充元素。。。\n","permalink":"https://cold-bin.github.io/post/%E5%9F%BA%E4%BA%8Ego%E7%9A%84selenium%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/","tags":["selenium","colly"],"title":"基于go的selenium使用详解"},{"categories":["数据结构与算法"],"contents":"[toc]\n常见算法总结 Floyd 判圈算法 Floyd判圈算法(Floyd Cycle Detection Algorithm)，又称龟兔赛跑算法(Tortoise and Hare Algorithm)，是一个可以在有限状态机、迭代函数或者链表上判断是否存在环，求出该环的起点与长度的算法。该算法据高德纳称由美国科学家罗伯特·弗洛伊德发明，但这一算法并没有出现在罗伯特·弗洛伊德公开发表的著作中。 如果有限状态机、迭代函数或者链表上存在环，那么在某个环上以不同速度前进的2个指针必定会在某个时刻相遇。同时显然地，如果从同一个起点(即使这个起点不在某个环上)同时开始以不同速度前进的2个指针最终相遇，那么可以判定存在一个环，且可以求出二者相遇处所在的环的起点与长度。\n算法描述 判断是否存在环路 如果有限状态机、迭代函数或者链表存在环，那么一定存在一个起点可以到达某个环的某处(这个起点也可以在某个环上)。 初始状态下，假设已知某个起点节点为节点S。现设两个指针t和h，将它们均指向S。接着，同时让t和h往前推进，但是二者的速度不同：t每前进1步，h前进2步。只要二者都可以前进而且没有相遇，就如此保持二者的推进。当h无法前进，即到达某个没有后继的节点时，就可以确定从S出发不会遇到环。反之当t与h再次相遇时，就可以确定从S出发一定会进入某个环，设其为环C。如果确定了存在某个环，就可以求此环的起点与长度。\n求解环路的长度 上述算法刚判断出存在环C时，显然t和h位于同一节点，设其为节点M。显然，仅需令h不动，而t不断推进，最终又会返回节点M，统计这一次t推进的步数，显然这就是环C的长度。\n求解环路的起点 为了求出环C的起点，只要令h仍均位于节点M，而令t移动至起点节点S，此时h与t之间距为环C长度的整数倍。随后，同时让t和h往前推进，且保持二者的速度相同：t每前进1步，h前进1步。持续该过程直至t与h再一次相遇，设此次相遇时位于同一节点P，则节点P即为从节点S出发所到达的环C的第一个节点，即环C的一个起点。\n对于环路起点算法的解释：\n假设出发起点到环起点的距离为m，已经确定有环，环的周长为n，（第一次）相遇点距离环的起点的距离是k。那么当两者相遇时，慢指针（t）移动的总距离i = m + a * n + k，快指针（h）的移动距离为2i，2i = m + b * n + k。其中，a和b分别为t和h在第一次相遇时转过的圈数。让两者相减（快减慢），那么有i = (b - a) * n。即i是圈长度的倍数，b*n和a*n都是环长度的倍数。\n​\ti=(b-a)*n = m+a*n+k ==\u0026gt; 3i = b*n+i+m+k = m+k+i ==\u0026gt; 2i = m+k，所以得出m+k是环长的整数倍\n将一个指针移到出发起点S，另一个指针仍呆在相遇节点M处，两者同时移动，每次移动一步。当第一个指针前进了m，即到达环起点时，另一个指针距离链表起点为m+k 。考虑到m+k为圈长度的倍数，可以理解为指针从链表起点出发，走到环起点，然后绕环转了几圈，所以第二个指针也必然在环的起点。即两者相遇点就是环的起点。\n伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 t := \u0026amp;S h := \u0026amp;S //令指针t和h均指向起点节点S。 repeat t := t-\u0026gt;next h := h-\u0026gt;next if h is not NULL //要注意这一判断一般不能省略 h := h-\u0026gt;next until t = h or h = NULL if h != NULL //如果存在环的话 n := 0 repeat //求环的长度 t := t-\u0026gt;next n := n+1 until t = h t := \u0026amp;S //求环的一个起点 while t != h t := t-\u0026gt;next h := h-\u0026gt;next P := *t 算法复杂度\n**时间复杂度：**注意到当指针t到达环C的一个起点节点P时(此时指针h显然在环C上)，之后指针t最多仅可能走1圈。若设节点S到P距离为 m，环C的长度为 n，则时间复杂度为 O(m+n)，是线性时间的算法。\n**空间复杂度：**仅需要创立指针t、指针h，保存环长n、环的一个起点P。空间复杂度为 O(1)，是常数空间的算法\nKMP算法 博客参考 =\u0026gt; 很详尽的KMP算法\u0026amp;详解KMP中next数组求解\n暴力匹配算法 假设现在我们面7899临这样一个问题：有一个0410文本串S，和一个模式串P，现在要查找P在S中的位置，怎么查找呢？\n如果用暴力匹配的思路，并假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置，则有：\n如果当前字符匹配成功（即S[i] == P[j]），则i++，j++，继续匹配下一个字符； 如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0。相当于每次匹配失败时，i回溯到下一个字符，j被置为0。 暴力匹配算法咋一看好像也没啥，能解决问题。但是如果需要匹配的字符串重复字符太多的话会发生什么呢？\n例如，文本串txt=BBC ABCDAB ABCDABCDABDE，子串p=ABCDABD。\nKMP算法 在上面那个例子里，如果采用暴力匹配时，每次子串p匹配失败时，都会使得文本串txt回溯到下一个字符，显然这样回溯时间复杂度很高，而且可以发现其实我们没必要回溯到下一个字符，也就是要减少匹配的趟数。\n那么，如何减少匹配的趟数呢？其实在每一次匹配过程中，我们就能够判断后续几次匹配是否会成功，算法的核心就是每次匹配过程中推断出后续完全不可能匹配成功的匹配过程，从而减少比较的趟数，如图所示：\n因此，第一次匹配过之后，就可以得出可以直接跳到第四趟再进行判断的结论了。因为第一次匹配的时候，前5个序列和主串相同，只需要对模式串进行分析，模式串出现了重复单元(即AB)，在第一次匹配失败后就可以直接跳跃到出现重复单元的位置。\nnext数组\nnext数组实质上就是找出模式串中前后字符重复出现的个数，为了能够跳跃不可能匹配的步骤。 next数组的定义为：next[i]表示模式串A[0]至A[i]这个字串，使得前k个字符等于后k个字符的最大值，特别的k不能取i+i,因为字串一共才i+1个字符，自己跟自己相等毫无意义。\n最终得到next数组为：\n如何确定在移动过程中需要跳过多少步呢？下图更直观的体现了跳跃的过程：\n对于上述红色部分的计算跳过长度的公式为跳过的趟数=匹配上字符串中间字符长度-重复字符串长度\n跳过这些步骤后并非再从头开始匹配，而是从重复位置开始匹配：\n最终，我们不难得出如下结论：\n快速幂算法 递归思想 「快速幂算法」的本质是分治算法。举个例子，如果我们要计算 x^64^，我们可以按照： $$ x \\to x^2 \\to x^4 \\to x^8 \\to x^{16} \\to x^{32} \\to x^{64} $$ 的顺序，从 x 开始，每次直接把上一次的结果进行平方，计算 66 次就可以得到 x^64^ 的值，而不需要对 x 乘 63 次 x。\n再举一个例子，如果我们要计算 x^77^，我们可以按照： $$ x \\to x^2 \\to x^4 \\to x^9 \\to x^{19} \\to x^{38} \\to x^{77} $$ 的顺序，在 $$x \\to x^2，x^2 \\to x^4，x^{19} \\to x^{38}$$ 这些步骤中，我们直接把上一次的结果进行平方，而在 $$x^4 \\to x^9，x^9 \\to x^{19}，x^{38} \\to x^{77}$$ 这些步骤中，我们把上一次的结果进行平方后，还要额外乘一个 x。\n直接从左到右进行推导看上去很困难，因为在每一步中，我们不知道在将上一次的结果平方之后，还需不需要额外乘 x。但如果我们从右往左看，分治的思想就十分明显了：\n当我们要计算 x^n^ 时，我们可以先递归地计算出 $$y = x^{\\lfloor n/2 \\rfloor}$$，其中 $$\\lfloor a \\rfloor$$ 表示对 a 进行下取整； 根据递归计算的结果，如果 n 为偶数，那么 x^n^ = y^2^；如果 n 为奇数，那么 x^n^ = y^2^ ； 递归的边界为 n = 0，任意数的 0 次方均为 1。 由于每次递归都会使得指数减少一半，因此递归的层数为 O(log n)，算法可以在很快的时间内得到结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public double myPow(double x, int n) { long N = n; return N \u0026gt;= 0 ? quickMul(x, N) : 1.0 / quickMul(x, -N); } public double quickMul(double x, long N) { if (N == 0) { return 1.0; } double y = quickMul(x, N / 2); return N % 2 == 0 ? y * y : y * y * x; } } 复杂度分析\n时间复杂度：O(log n)，即为递归的层数。 空间复杂度：O(log n)，即为递归的层数。这是由于递归的函数调用会使用栈空间。 迭代思想 由于递归需要使用额外的栈空间，我们试着将递归转写为迭代。在方法一中，我们也提到过，从左到右进行推导是不容易的，因为我们不知道是否需要额外乘 x。但我们不妨找一找规律，看看哪些地方额外乘了 x，并且它们对答案产生了什么影响。\n我们还是以 x^77^ 作为例子： $$ x \\to x^2 \\to x^4 \\to^+ x^9 \\to^+ x^{19} \\to x^{38} \\to^+ x^{77} $$ 并且把需要额外乘 x 的步骤打上了 + 标记。可以发现：\n$$x^{38} \\to^+ x^{77}$$ 中额外乘的 x 在 x^77^ 中贡献了 x； $$x^9 \\to^+ x^{19}$$ 中额外乘的 x 在之后被平方了 2 次，因此在 x^77^ 中贡献了 $$x^{2^2} = x^4$$； $$x^4 \\to^+ x^9$$ 中额外乘的 x 在之后被平方了 3 次，因此在 x^77^ 中贡献了 $$x^{2^3} = x^8$$； 最初的 x 在之后被平方了 6 次，因此在 x^77^ 中贡献了 $$x^{2^6} = x^{64}$$。 我们把这些贡献相乘，$$x \\times x^4 \\times x^8 \\times x^{64}$$ 恰好等于 x^77^。而这些贡献的指数部分又是什么呢？它们都是 2 的幂次，这是因为每个额外乘的 x 在之后都会被平方若干次。而这些指数 1，4，8 和 64，**恰好就对应了 77 的二进制表示 (1001101)~2~ 中的每个 1！\n因此我们借助整数的二进制拆分，就可以得到迭代计算的方法，一般地，如果整数 n 的二进制拆分为 $$ n = 2^{i_0} + 2^{i_1} + \\cdots + 2^{i_k} $$ 那么 $$ x^n = x^{2^{i_0}} \\times x^{2^{i_1}} \\times \\cdots \\times x^{2^{i_k}} $$ 这样以来，我们从 x 开始不断地进行平方，得到 $$x^2, x^4, x^8, x^{16}, \\cdots$$，如果 n 的第 k 个（从右往左，从 0 开始计数）二进制位为 1，那么我们就将对应的贡献 $$x^{2^k}$$计入答案。\n下面的代码给出了详细的注释。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public double myPow(double x, int n) { long N = n; return N \u0026gt;= 0 ? quickMul(x, N) : 1.0 / quickMul(x, -N); } public double quickMul(double x, long N) { double ans = 1.0; // 贡献的初始值为 x double x_contribute = x; // 在对 N 进行二进制拆分的同时计算答案 while (N \u0026gt; 0) { if (N % 2 == 1) { // 如果 N 二进制表示的最低位为 1，那么需要计入贡献 ans *= x_contribute; } // 将贡献不断地平方 x_contribute *= x_contribute; // 舍弃 N 二进制表示的最低位，这样我们每次只要判断最低位即可 N /= 2; } return ans; } } 贪心算法 转载 =\u0026gt; https://mp.weixin.qq.com/s?__biz=MzI0NjAxMDU5NA==\u0026amp;mid=2475930307\u0026amp;idx=1\u0026amp;sn=243c7b43b5aa3919129d0ac40a1b2fab\u0026amp;chksm=ff22d54ec8555c586a07dbcfe671bdf6d79278ebcf42c706008211f54361a4556149ad911681\u0026amp;token=1439040973\u0026amp;lang=zh_CN#rd\n大家好呀，我是帅蛋。\n今天我们来学习贪心算法，它和我之前带大家玩儿的【递归算法】、【分治算法】类似，带着算法的名儿，但实际上是一种解决问题的思想策略。\n在正式开始之前，我想先说几句题外话：\n我知道贪心算法对很多同学来说有点难，这个难不是难在对概念的理解上，而是一看就会，一做题就废，接着半途而废。\n这个我想说很正常，因为贪心算法是一种算法【思想】，但凡是这种的，就没什么套路可讲，不像我们在上个专题学二叉树的时候，解题就是递归 + 迭代，可以由上到下、由下到上、由左到右的整，套路明显。\n更不用说，后面碰到动态规划的时候，更容易贪心算法和动态规划用哪个傻傻分不清楚。\n碰到这些问题怎么办？就是干，唯做题 + 总结。\n写这篇文章应该篇幅不会很多，我会带你了解一下什么是贪心算法，重头戏是会在后面的实战题板块，我来带你做题和做总结，所以跟着搞就完事了。\nR u ready？gogogo！\n贪心算法\n学习贪心算法，首先我们得从它的概念学起。\n贪心算法（greedy algorithm）是指从问题初始转状态出发，通过在每一步选择中都采取最好或者最优（最有利）的选择，从而得到结果的最优值（或较优值）。\n通过概念我们能知道贪心算法的 2 个关键点：\n贪心算法在对问题进行求解时，总是做出当前看来最好的选择。 通过贪心算法所得到的结果不一定是最优的结果，但肯定都是相对接近最优解的结果。 看起来这 2 点可能不好理解，我用两个例子你就懂了。\n例 1：我们现在有 20、10、5、1 这 4 种数额的钱币，如果想要凑齐 36 元，那我最少需要几张钱币？\n如果根据贪心算法的话，我们上来肯定是看需要几张 20 的，这道题需要 1 张，那还剩 36 - 20 = 16。\n看完 20 的我们再来看 10 元的，需要 1 张 10 元，现在还剩 16 - 10 = 6。\n下面继续是看 5 和 1，分别就需要 1 张。\n最后我们得到的答案是，如果想要凑齐 36 元我们最少需要 4 张纸币。\n这个例子，每次都是用最大的纸币去匹配，剩下的余额再用较小点的面额去匹配，这个就是第 1 点我们说的，在对问题进行求解时，每次都是做出当前看来最好的选择。\n例 2：我们还是以撒币为例，现在我们有 10、9、1 这 3 种金额的钱币，如果想要凑齐 18 元，咱最少需要几张钱币？\n在这个例子，如果我们还是用上面的贪心策略，那就完蛋了。\n我上来就看需要几张 10 元，那这道题需要 1 张，剩余金额是 8 元，那我无法用 9 的纸币，只能用 8 张 1 元的纸币，那这最后的结果是用了 9 张纸币。\n而通过小学知识，我们肉眼就能看出用 2 张 9 元的纸币就 ok 了。\n通过这个例子就是说明了第 2 点：通过贪心算法所得到的结果不一定是最优的结过。\n看到这是不是懵了？懵了就对了。\n你现在就先记住一点：贪心算法只是在部分情况下有用。至于什么是部分情况，这个就得靠多做题了~\n诶诶诶，你先别动手，那你看嘛，就比如上面的 2 个例子，你要看看数之间的规律，例 1 的中的币互相成倍数，例 2 中就没啥规律。\n这个就得是靠多做题多总结好伐~\n什么时候用贪心？\n说实话，这就是贪心算法对我们来说”难“的地方，即没有模式化的东西直接了当的告诉我们”这样就是用贪心“。\n如果硬要说的话，绝大多数用在像上一节中举的例子那种【组合优化】问题，求解的过程涉及到多步判断。\n碰到这样的题，你的想法可以往贪心算法上靠一靠，但也只是”可以“而已。\n因为你看到类似这种问题，你想到贪心算法，首先就要自己先搞出个贪心策略，之后你要验证你所用的贪心策略产生的结果是不是最优的，如果不是最优的，那可能就要用到我们下一个专题要学的【动态规划】。\n大家估计会问如何验证，直接就举几个靠谱的例子验证就好了。\n当然这里我说的是”靠谱“，就是一些特例，不然你随便整了几个例子，发现都对，这个时候你就觉得你做的就是对的，恰恰也可能你举的这几个例子正好巧了。\n这里我还想多说几句：\n其实按照正常来说呢，像这种验证贪心算法的正确性，最靠谱的就是通过数学推导来弄，一般像什么数学归纳法、反正法这种方法。\n但是怎么说呢，数学推导虽然很对，结果也很正确，但是对于我们来说完全没有必要，且不说很多人根本不会推导，就算会，对我们来说意义也不大，毕竟目的性不一样，这样搞就走远了。\n从我们刷题的角度来看，我们完全就可以靠举一些特例就能验证绝大多数问题。\n当然像这种举特例的能力，你要问我怎么举，我只能告诉你：多做题就有了。\n贪心法解题步骤\n贪心算法的解题步骤，其实和分治算法很像的。\n我在之前讲分治算法的时候讲过分治算法的 3 个步骤：\n划分（Divide）：将原问题划分为规模较小的子问题，子问题相互独立，与原问题形式相同。\n求解（Conquer）：递归的求解划分之后的子问题。\n合并（Combine）：这一步非必须。有些问题涉及合并子问题的解，将子问题的解合并成原问题的解。有的问题则不需要，只是求出子问题的解即可。\n贪心算法的步骤也类似，如果你确定是贪心算法可解，也是 3 个步骤：\n(1) 将问题分解为多个子问题。\n(2) 选择合适的贪心策略，得到每一个子问题的局部最优解。\n(3) 将子问题的局部最优解合并成原问题的最优解。\n是不是这么看觉得还挺简单的？嘿嘿嘿嘿，等做题的时候你就知道有时候看到的并不就是真实的感受~\n贪心算法的内容到这就说的差不多了，单纯理论上的东西确实不多，看完了也只是让你有个印象。\n关于理论上的东西你只要了解就行，你想学好贪心别无他法。就是要多做题多练习，见的多了就有感觉了。\n其实一连套的动作就是：诶，这道题看着能用贪心，试试，得出个结果，找几个特例验一验，是最优解万事大吉，不是最优解，就再想想是不是动态规划啥的可以解。\n最后再说一句，贪心算法不用慌，退一万步讲，贪心就算学的不好，问题也不大，面试的时候贪心考的一般比较少。\n毕竟生活啊，总归是要全局考虑，哪能只盯着眼前~\n我是帅蛋，我们下次见！\n动态规划算法 转载 =\u0026gt; https://mp.weixin.qq.com/s?__biz=MzI0NjAxMDU5NA==\u0026amp;mid=2475932264\u0026amp;idx=1\u0026amp;sn=465412392bab644f2391c62148fcac2c\u0026amp;chksm=ff22dde5c85554f3b66668a590b63c6ed27859ca83cf4018ede7653d25adf24764574adde5fc\u0026amp;token=239743856\u0026amp;lang=zh_CN#rd\n大家好呀，我是帅蛋！\n很多同学在动态规划和贪心算法上有些傻傻分不清楚，今天我就来解决动态规划和贪心算法的比较分析，当作加餐。\n前面我先比较概念，后面我会用两个例子带大家一步步分析。\n我尽量以我的理解给大家讲清楚！\n动态规划 vs 贪心算法\n我在【动态规划】的文章中写过：\n动态规划将一个大的复杂的问题，拆成一个个的子问题，子问题再拆成更小的子问题，直至拆到到子问题可以用确定的条件解答，之后通过这些子问题的解反向得到原问题的解。\n它可以解决的问题包含 4 个特点：\n求最优性质问题 有重叠子问题 最优子结构 无后效性 同样，我在【贪心算法】的文章中也解释过贪心的概念：\n**贪心算法（greedy algorithm）**是指从问题初始转状态出发，通过在每一步选择中都采取最好或者最优（最有利）的选择，从而得到结果的最优值（或较优值）。\n通过概念我们能知道贪心算法的 2 个关键点：\n贪心算法在对问题进行求解时，总是做出当前看来最好的选择。 通过贪心算法所得到的结果不一定是最优的结果，但肯定都是相对接近最优解的结果。 从贪心的概念提炼一下，其实贪心能解决的问题也包含 4 个特点：\n求最优性质问题 贪心选择 最优子结构 无后效性：无后向性是指 以前出现状态 和 以前状态的变化过程 不会影响 将来的变化。 综合可以看出，贪心和动态规划：\n相似之处在于，都是求最优性质问题，问题具有”无后效性“，解决办法都是将问题拆成子问题，都有“最优子结构”。\n但区别是，贪心算法独有的”贪心选择“，它选的是当前最优解，而动态规划是通过子问题的最优解推出当前的最优解。\n又是说了一堆概念上的东西，可能你看了还是觉得有点抽象很难解释。\n![图片](data:image/svg+xml,%3C%3Fxml version=\u0026lsquo;1.0\u0026rsquo; encoding=\u0026lsquo;UTF-8\u0026rsquo;%3F%3E%3Csvg width=\u0026lsquo;1px\u0026rsquo; height=\u0026lsquo;1px\u0026rsquo; viewBox=\u0026lsquo;0 0 1 1\u0026rsquo; version=\u0026lsquo;1.1\u0026rsquo; xmlns=\u0026lsquo;http://www.w3.org/2000/svg' xmlns:xlink=\u0026lsquo;http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke=\u0026lsquo;none\u0026rsquo; stroke-width=\u0026lsquo;1\u0026rsquo; fill=\u0026lsquo;none\u0026rsquo; fill-rule=\u0026lsquo;evenodd\u0026rsquo; fill-opacity=\u0026lsquo;0\u0026rsquo;%3E%3Cg transform=\u0026lsquo;translate(-249.000000, -126.000000)\u0026rsquo; fill=\u0026rsquo;%23FFFFFF\u0026rsquo;%3E%3Crect x=\u0026lsquo;249\u0026rsquo; y=\u0026lsquo;126\u0026rsquo; width=\u0026lsquo;1\u0026rsquo; height=\u0026lsquo;1\u0026rsquo;%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n诶诶诶，别动手，下面讲具体的了！我就用两个例子来解释一下~\n![图片](data:image/svg+xml,%3C%3Fxml version=\u0026lsquo;1.0\u0026rsquo; encoding=\u0026lsquo;UTF-8\u0026rsquo;%3F%3E%3Csvg width=\u0026lsquo;1px\u0026rsquo; height=\u0026lsquo;1px\u0026rsquo; viewBox=\u0026lsquo;0 0 1 1\u0026rsquo; version=\u0026lsquo;1.1\u0026rsquo; xmlns=\u0026lsquo;http://www.w3.org/2000/svg' xmlns:xlink=\u0026lsquo;http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke=\u0026lsquo;none\u0026rsquo; stroke-width=\u0026lsquo;1\u0026rsquo; fill=\u0026lsquo;none\u0026rsquo; fill-rule=\u0026lsquo;evenodd\u0026rsquo; fill-opacity=\u0026lsquo;0\u0026rsquo;%3E%3Cg transform=\u0026lsquo;translate(-249.000000, -126.000000)\u0026rsquo; fill=\u0026rsquo;%23FFFFFF\u0026rsquo;%3E%3Crect x=\u0026lsquo;249\u0026rsquo; y=\u0026lsquo;126\u0026rsquo; width=\u0026lsquo;1\u0026rsquo; height=\u0026lsquo;1\u0026rsquo;%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n**问题：**我们现在有 20、10、5、1 这 4 种数额的钱币，如果想要凑齐 36 元，那我最少需要几张钱币？\n如果根据贪心算法的话，我们上来肯定是看需要几张 20 的，这道题需要 1 张，那还剩 36 - 20 = 16。\n看完 20 的我们再来看 10 元的，需要 1 张 10 元，现在还剩 16 - 10 = 6。\n下面继续是看 5 和 1，分别就需要 1 张。\n最后我们得到的答案是，如果想要凑齐 36 元我们最少需要 4 张纸币。\n这个例子，每次都是用最大的纸币去匹配，剩下的余额再用较小点的面额去匹配，这个就是贪心算法中我们说的，在对问题进行求解时，每次都是做出当前看来最好的选择。\n上面这个问题你看贪心可以，还挺得瑟，那你看我换一下。\n**问题：**还是以撒币为例，现在我们有 10、9、1 这 3 种金额的钱币，如果想要凑齐 18 元，咱最少需要几张钱币？\n如果用贪心的话，我上来就看需要几张 10 元，那这道题需要 1 张，剩余金额是 8 元，那我无法用 9 的纸币，只能用 8 张 1 元的纸币，那这最后的结果是用了 9 张纸币。\n但是明明我们一眼就能看出用 2 张 9 元的纸币就 ok 了。\n这就是贪心算法”贪心选择“局限的地方，只考虑眼前的情况。\n我们重新分析这个例子。\n想要凑齐 18 元，如果我们取的金额钱币为 10，下面我们就要凑齐 18 - 10 = 8 元；如果取得金额钱币为 9，下面我们就要凑齐 18 - 9 = 9 元。\n这些问题其实都有一个相同的形式：为了凑齐 xx 元，最少需要几张钱币。这里我用 dp[n] 表示“凑出 n 元需要的最少钱币数”。\n对于凑齐 18 元，如果我们选了 10 元金额，那：\ndp[18] = dp[8] + 1 = 8 + 1 = 9。\n意思是利用 10 元凑出 18，相当于是“凑齐 8 元需要的纸币数 + 这一张 10 元钱币”。至于 dp[8] 怎么求出来的我们先不管。\n同理：\n选 9 元钱币，dp[18] = dp[9] + 1 = 1 + 1 = 2。\n选 1 元钱币，dp[18] = dp[17] + 1 = 8 + 1 = 9。\n显而易见的是，凑出 18 需要的最小钱币数是 2，是选取 9 这张钱币的方案。\n我们通过上面三个公式做出了正确的决策。\n可以看出 dp[18] 是与 dp[18 - 10]、dp[18 - 9]、dp[18 - 1] 有关，放大来看，即 dp[n] 与 dp[n - 10]、dp[n - 9]、dp[n - 1] 有关：\ndp[n] = min(dp[n - 10], dp[n - 9], dp[n - 1]) + 1。\n你看，我们想要求出 dp[n]，只需要管 dp[n - 10]、dp[n - 9]、dp[n - 1] ，至于更小的值是怎么求的，和我 dp[n] 就没关系了。\n这也是动态规划的“无后效性”的完美解释。\n动态规划保证了我们解决问题的结果的正确性，比起贪心只关心眼前，动态规划会分别算出每种情况的最优解，从而得出自己的最优解。\n现在你再看这句话：\n贪心算法选的是当前最优解，而动态规划****是通过子问题的最优解推出当前的最优解。\n是不是就突然理解了呢，这就是最关键的地方，下次再有人问你，别和他扯别的，就说这句话！\n背包问题 原文地址： https://zhuanlan.zhihu.com/p/93857890\n背包问题是一类经典的动态规划问题，它非常灵活，需要仔细琢磨体会，本文先对背包问题的几种常见类型作一个总结，然后再看看LeetCode上几个相关题目。\n本文首发于我的博客，传送门\n根据维基百科，背包问题（Knapsack problem）是一种组合优化的NP完全（NP-Complete，NPC）问题。问题可以描述为：给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择，才能使得物品的总价格最高。NPC问题是没有多项式时间复杂度的解法的，但是利用动态规划，我们可以以伪多项式时间复杂度求解背包问题。一般来讲，背包问题有以下几种分类：\n01背包问题 完全背包问题 多重背包问题 此外，还存在一些其他考法，例如恰好装满、求方案总数、求所有的方案等。本文接下来就分别讨论一下这些问题。\n1. 01背包 1.1 题目\n最基本的背包问题就是01背包问题（01 knapsack problem）：一共有N件物品，第i（i从1开始）件物品的重量为w[i]，价值为v[i]。在总重量不超过背包承载上限W的情况下，能够装入背包的最大价值是多少？\n1.2 分析\n如果采用暴力穷举的方式，每件物品都存在装入和不装入两种情况，所以总的时间复杂度是O(2^N)，这是不可接受的。而使用动态规划可以将复杂度降至O(NW)。我们的目标是书包内物品的总价值，而变量是物品和书包的限重，所以我们可定义状态dp:\n1 dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值, 0\u0026lt;=i\u0026lt;=N, 0\u0026lt;=j\u0026lt;=W 那么我们可以将dp[0][0\u0026hellip;W]初始化为0，表示将前0个物品（即没有物品）装入书包的最大价值为0。那么当 i \u0026gt; 0 时dp[i][j]有两种情况：\n不装入第i件物品，即dp[i−1][j]； 装入第i件物品（前提是能装下），即dp[i−1][j−w[i]] + v[i]。 即状态转移方程为\n1 dp[i][j] = max(dp[i−1][j], dp[i−1][j−w[i]]+v[i]) // j \u0026gt;= w[i] 由上述状态转移方程可知，dp[i][j]的值只与dp[i-1][0,...,j-1]有关，所以我们可以采用动态规划常用的方法（滚动数组）对空间进行优化（即去掉dp的第一维）。需要注意的是，为了防止上一层循环的dp[0,...,j-1]被覆盖，循环的时候 j 只能逆向枚举（空间优化前没有这个限制），伪代码为：\n1 2 3 4 5 // 01背包问题伪代码(空间优化版) dp[0,...,W] = 0 for i = 1,...,N for j = W,...,w[i] // 必须逆向枚举!!! dp[j] = max(dp[j], dp[j−w[i]]+v[i]) 时间复杂度为O(NW), 空间复杂度为O(W)。由于W的值是W的位数的幂，所以这个时间复杂度是伪多项式时间。\n动态规划的核心思想避免重复计算在01背包问题中体现得淋漓尽致。第i件物品装入或者不装入而获得的最大价值完全可以由前面i-1件物品的最大价值决定，暴力枚举忽略了这个事实。\n2. 完全背包 2.1 题目\n完全背包（unbounded knapsack problem）与01背包不同就是每种物品可以有无限多个：一共有N种物品，每种物品有无限多个，第i（i从1开始）种物品的重量为w[i]，价值为v[i]。在总重量不超过背包承载上限W的情况下，能够装入背包的最大价值是多少？\n2.2 分析一\n我们的目标和变量和01背包没有区别，所以我们可定义与01背包问题几乎完全相同的状态dp:\n1 dp[i][j]表示将前i种物品装进限重为j的背包可以获得的最大价值, 0\u0026lt;=i\u0026lt;=N, 0\u0026lt;=j\u0026lt;=W 初始状态也是一样的，我们将dp[0][0\u0026hellip;W]初始化为0，表示将前0种物品（即没有物品）装入书包的最大价值为0。那么当 i \u0026gt; 0 时dp[i][j]也有两种情况：\n不装入第i种物品，即dp[i−1][j]，同01背包； 装入第i种物品，此时和01背包不太一样，因为每种物品有无限个（但注意书包限重是有限的），所以此时不应该转移到dp[i−1][j−w[i]]而应该转移到dp[i][j−w[i]]，即装入第i种商品后还可以再继续装入第种商品。 所以状态转移方程为\n1 dp[i][j] = max(dp[i−1][j], dp[i][j−w[i]]+v[i]) // j \u0026gt;= w[i] 这个状态转移方程与01背包问题唯一不同就是max第二项不是dp[i-1]而是dp[i]。\n和01背包问题类似，也可进行空间优化，优化后不同点在于这里的 j 只能正向枚举而01背包只能逆向枚举，因为这里的max第二项是dp[i]而01背包是dp[i-1]，即这里就是需要覆盖而01背包需要避免覆盖。所以伪代码如下：\n1 2 3 4 5 // 完全背包问题思路一伪代码(空间优化版) dp[0,...,W] = 0 for i = 1,...,N for j = w[i],...,W // 必须正向枚举!!! dp[j] = max(dp[j], dp[j−w[i]]+v[i]) 由上述伪代码看出，01背包和完全背包问题此解法的空间优化版解法唯一不同就是前者的 j 只能逆向枚举而后者的 j 只能正向枚举，这是由二者的状态转移方程决定的。此解法时间复杂度为O(NW), 空间复杂度为O(W)。\n2.3 分析二\n除了分析一的思路外，完全背包还有一种常见的思路，但是复杂度高一些。我们从装入第 i 种物品多少件出发，01背包只有两种情况即取0件和取1件，而这里是取0件、1件、2件\u0026hellip;直到超过限重（k \u0026gt; j/w[i]），所以状态转移方程为：\n1 2 # k为装入第i种物品的件数, k \u0026lt;= j/w[i] dp[i][j] = max{(dp[i-1][j − k*w[i]] + k*v[i]) for every k} 同理也可以进行空间优化，需要注意的是，这里max里面是dp[i-1]，和01背包一样，所以 j 必须逆向枚举，优化后伪代码为\n1 2 3 4 5 6 // 完全背包问题思路二伪代码(空间优化版) dp[0,...,W] = 0 for i = 1,...,N for j = W,...,w[i] // 必须逆向枚举!!! for k = [0, 1,..., j/w[i]] dp[j] = max(dp[j], dp[j−k*w[i]]+k*v[i]) 相比于分析一，此种方法不是在O(1)时间求得dp[i][j]，所以总的时间复杂度就比分析一大些了，为 �(����¯)级别。\n2.4 分析三、转换成01背包\n01背包问题是最基本的背包问题，我们可以考虑把完全背包问题转化为01背包问题来解：将一种物品转换成若干件只能装入0件或者1件的01背包中的物品。\n最简单的想法是，考虑到第 i 种物品最多装入 W/w[i] 件，于是可以把第 i 种物品转化为 W/w[i] 件费用及价值均不变的物品，然后求解这个01背包问题。\n更高效的转化方法是采用二进制的思想：把第 i 种物品拆成重量为 ��2� 、价值为 ��2� 的若干件物品，其中 k 取遍满足 ��2�≤� 的非负整数。这是因为不管最优策略选几件第 i 种物品，总可以表示成若干个刚才这些物品的和（例：13 = 1 + 4 + 8）。这样就将转换后的物品数目降成了对数级别。\n3. 多重背包 3.1 题目\n多重背包（bounded knapsack problem）与前面不同就是每种物品是有限个：一共有N种物品，第i（i从1开始）种物品的数量为n[i]，重量为w[i]，价值为v[i]。在总重量不超过背包承载上限W的情况下，能够装入背包的最大价值是多少？\n3.2 分析一\n此时的分析和完全背包的分析二差不多，也是从装入第 i 种物品多少件出发：装入第i种物品0件、1件、\u0026hellip;n[i]件（还要满足不超过限重）。所以状态方程为：\n1 2 # k为装入第i种物品的件数, k \u0026lt;= min(n[i], j/w[i]) dp[i][j] = max{(dp[i-1][j − k*w[i]] + k*v[i]) for every k} 同理也可以进行空间优化，而且 j 也必须逆向枚举，优化后伪代码为\n1 2 3 4 5 6 // 完全背包问题思路二伪代码(空间优化版) dp[0,...,W] = 0 for i = 1,...,N for j = W,...,w[i] // 必须逆向枚举!!! for k = [0, 1,..., min(n[i], j/w[i])] dp[j] = max(dp[j], dp[j−k*w[i]]+k*v[i]) 3.3 分析二、转换成01背包\n采用2.4节类似的思路可以将多重背包转换成01背包问题，采用二进制思路将第 i 种物品分成了 �(�����) 件物品，将原问题转化为了复杂度为 �(�∑������) 的 01 背包问题，相对于分析一是很大的改进。\n4. 其他情形 除了上述三种基本的背包问题外，还有一些其他的变种，如下图所示（图片来源）。\n本节列举几种比较常见的。\n4.1 恰好装满 背包问题有时候还有一个限制就是必须恰好装满背包，此时基本思路没有区别，只是在初始化的时候有所不同。\n如果没有恰好装满背包的限制，我们将dp全部初始化成0就可以了。因为任何容量的背包都有一个合法解“什么都不装”，这个解的价值为0，所以初始时状态的值也就全部为0了。如果有恰好装满的限制，那只应该将dp[0,\u0026hellip;,N][0]初始为0，其它dp值均初始化为-inf，因为此时只有容量为0的背包可以在什么也不装情况下被“恰好装满”，其它容量的背包初始均没有合法的解，应该被初始化为-inf。\n4.2 求方案总数 除了在给定每个物品的价值后求可得到的最大价值外，还有一类问题是问装满背包或将背包装至某一指定容量的方案总数。对于这类问题，需要将状态转移方程中的 max 改成 sum ，大体思路是不变的。例如若每件物品均是完全背包中的物品，转移方程即为\n1 dp[i][j] = sum(dp[i−1][j], dp[i][j−w[i]]) // j \u0026gt;= w[i] 4.3 二维背包 前面讨论的背包容量都是一个量：重量。二维背包问题是指每个背包有两个限制条件（比如重量和体积限制），选择物品必须要满足这两个条件。此类问题的解法和一维背包问题不同就是dp数组要多开一维，其他和一维背包完全一样，例如5.4节。\n4.4 求最优方案 一般而言，背包问题是要求一个最优值，如果要求输出这个最优值的方案，可以参照一般动态规划问题输出方案的方法：记录下每个状态的最优值是由哪一个策略推出来的，这样便可根据这条策略找到上一个状态，从上一个状态接着向前推即可。\n以01背包为例，我们可以再用一个数组G[i][j]来记录方案，设 G[i][j] = 0表示计算 dp[i][j] 的值时是采用了max中的前一项(也即dp[i−1][j])，G[i][j] = 1 表示采用了方程的后一项。即分别表示了两种策略: 未装入第 i 个物品及装了第 i 个物品。其实我们也可以直接从求好的dp[i][j]反推方案：若 dp[i][j] = dp[i−1][j] 说明未选第i个物品，反之说明选了。\n5. LeetCode相关题目 本节对LeetCode上面的背包问题进行讨论。\n5.1 Partition Equal Subset Sum（分割等和子集） Loading\u0026hellip;leetcode.com/problems/partition-equal-subset-sum/\n题目给定一个只包含正整数的非空数组。问是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。\n由于所有元素的和sum已知，所以两个子集的和都应该是sum/2（所以前提是sum不能是奇数），即题目转换成从这个数组里面选取一些元素使这些元素和为sum/2。如果我们将所有元素的值看做是物品的重量，每件物品价值都为1，所以这就是一个恰好装满的01背包问题。\n我们定义空间优化后的状态数组dp，由于是恰好装满，所以应该将dp[0]初始化为0而将其他全部初始化为INT_MIN，然后按照类似1.2节的伪代码更新dp：\n1 2 3 4 5 6 int capacity = sum / 2; vector\u0026lt;int\u0026gt;dp(capacity + 1, INT_MIN); dp[0] = 0; for(int i = 1; i \u0026lt;= n; i++) for(int j = capacity; j \u0026gt;= nums[i-1]; j--) dp[j] = max(dp[j], 1 + dp[j - nums[i-1]]); 更新完毕后，如果dp[sum/2]大于0说明满足题意。\n由于此题最后求的是能不能进行划分，所以dp的每个元素定义成bool型就可以了，然后将dp[0]初始为true其他初始化为false，而转移方程就应该是用或操作而不是max操作。完整代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 bool canPartition(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int sum = 0, n = nums.size(); for(int \u0026amp;num: nums) sum += num; if(sum % 2) return false; int capacity = sum / 2; vector\u0026lt;bool\u0026gt;dp(capacity + 1, false); dp[0] = true; for(int i = 1; i \u0026lt;= n; i++) for(int j = capacity; j \u0026gt;= nums[i-1]; j--) dp[j] = dp[j] || dp[j - nums[i-1]]; return dp[capacity]; } 另外此题还有一个更巧妙更快的解法，基本思路是用一个bisets来记录所有可能子集的和，详见我的Github。\n5.2 Coin Change（零钱兑换） Loading\u0026hellip;leetcode.com/problems/coin-change/\n题目给定一个价值amount和一些面值，假设每个面值的硬币数都是无限的，问我们最少能用几个硬币组成给定的价值。\n如果我们将面值看作是物品，面值金额看成是物品的重量，每件物品的价值均为1，这样此题就是是一个恰好装满的完全背包问题了。不过这里不是求最多装入多少物品而是求最少，我们只需要将2.2节的转态转移方程中的max改成min即可，又由于是恰好装满，所以除了dp[0]，其他都应初始化为INT_MAX。完整代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 int coinChange(vector\u0026lt;int\u0026gt;\u0026amp; coins, int amount) { vector\u0026lt;int\u0026gt;dp(amount + 1, INT_MAX); dp[0] = 0; for(int i = 1; i \u0026lt;= coins.size(); i++) for(int j = coins[i-1]; j \u0026lt;= amount; j++){ // 下行代码会在 1+INT_MAX 时溢出 // dp[j] = min(dp[j], 1 + dp[j - coins[i-1]]); if(dp[j] - 1 \u0026gt; dp[j - coins[i-1]]) dp[j] = 1 + dp[j - coins[i-1]]; } return dp[amount] == INT_MAX ? -1 : dp[amount]; } 注意上面1 + dp[j - coins[i-1]]会存在溢出的风险，所以我们换了个写法。\n另外此题还可以进行搜索所有可能然后保持一个全局的结果res，但是直接搜索会超时，所以需要进行精心剪枝，剪枝后可击败99%。详见我的Github。\n5.3 Target Sum（目标和） Loading\u0026hellip;leetcode.com/problems/target-sum/\n这道题给了我们一个数组（元素非负），和一个目标值，要求给数组中每个数字前添加正号或负号所组成的表达式结果与目标值S相等，求有多少种情况。\n假设所有元素和为sum，所有添加正号的元素的和为A，所有添加负号的元素和为B，则有sum = A + B 且 S = A - B，解方程得A = (sum + S)/2。即题目转换成：从数组中选取一些元素使和恰好为(sum + S) / 2。可见这是一个恰好装满的01背包问题，要求所有方案数，将1.2节状态转移方程中的max改成求和即可。需要注意的是，虽然这里是恰好装满，但是dp初始值不应该是inf，因为这里求的不是总价值而是方案数，应该全部初始为0（除了dp[0]初始化为1）。所以代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int findTargetSumWays(vector\u0026lt;int\u0026gt;\u0026amp; nums, int S) { int sum = 0; // for(int \u0026amp;num: nums) sum += num; sum = accumulate(nums.begin(), nums.end(), 0); if(S \u0026gt; sum || sum \u0026lt; -S) return 0; // 肯定不行 if((S + sum) \u0026amp; 1) return 0; // 奇数 int target = (S + sum) \u0026gt;\u0026gt; 1; vector\u0026lt;int\u0026gt;dp(target + 1, 0); dp[0] = 1; for(int i = 1; i \u0026lt;= nums.size(); i++) for(int j = target; j \u0026gt;= nums[i-1]; j--) dp[j] = dp[j] + dp[j - nums[i-1]]; return dp[target]; } 5.4 Ones and Zeros（一和零） Loading\u0026hellip;leetcode.com/problems/ones-and-zeroes/\n题目给定一个仅包含 0 和 1 字符串的数组。任务是从数组中选取尽可能多的字符串，使这些字符串包含的0和1的数目分别不超过m和n。\n我们把每个字符串看做是一件物品，把字符串中0的数目和1的数目看做是两种“重量”，所以就变成了一个二维01背包问题，书包的两个限重分别是 m 和 n，要求书包能装下的物品的最大数目（也相当于价值最大，设每个物品价值为1）。\n我们可以提前把每个字符串的两个“重量” w0和w1算出来用数组存放，但是注意到只需要用一次这两个值，所以我们只需在用到的时候计算w0和w1就行了，这样就不用额外的数组存放。完整代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 int findMaxForm(vector\u0026lt;string\u0026gt;\u0026amp; strs, int m, int n) { int num = strs.size(); int w0, w1; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;dp(m+1, vector\u0026lt;int\u0026gt;(n+1, 0)); for(int i = 1; i \u0026lt;= num; i++){ w0 = 0; w1 = 0; // 计算第i-1个字符串的两个重量 for(char \u0026amp;c: strs[i - 1]){ if(c == \u0026#39;0\u0026#39;) w0 += 1; else w1 += 1; } // 01背包, 逆向迭代更新dp for(int j = m; j \u0026gt;= w0; j--) for(int k = n; k \u0026gt;= w1; k--) dp[j][k] = max(dp[j][k], 1+dp[j-w0][k-w1]); } return dp[m][n]; } 6. 总结 本文讨论了几类背包问题及LeetCode相关题目，其中01背包问题和完全背包问题是最常考的，另外还需要注意一些其他变种例如恰好装满、二维背包、求方案总数等等。除了本文讨论的这些背包问题之外，还存在一些其他的变种，但只要深刻领会本文所列的背包问题的思路和状态转移方程，遇到其它的变形问题，应该也不难想出算法。如果想更加详细地理解背包问题，推荐阅读经典的背包问题九讲。\n","permalink":"https://cold-bin.github.io/post/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/","tags":["算法","kmp","floyd","快速幂","背包问题"],"title":"常见算法总结"},{"categories":["golang"],"contents":"swag 目录 快速开始 支持的Web框架 如何与Gin集成 格式化说明 开发现状 声明式注释格式 通用API信息 API操作 安全性 样例 多行的描述 用户自定义的具有数组类型的结构 响应对象中的模型组合 在响应中增加头字段 使用多路径参数 结构体的示例值 结构体描述 使用swaggertype标签更改字段类型 使用swaggerignore标签排除字段 将扩展信息添加到结构字段 对展示的模型重命名 如何使用安全性注释 项目相关 快速开始 将注释添加到API源代码中，请参阅声明性注释格式。 使用如下命令下载swag： 1 2 3 4 $ go get -u github.com/swaggo/swag/cmd/swag # 1.16 及以上版本 $ go install github.com/swaggo/swag/cmd/swag@latest 从源码开始构建的话，需要有Go环境（1.15及以上版本）。\n或者从github的release页面下载预编译好的二进制文件。\n在包含main.go文件的项目根目录运行swag init。这将会解析注释并生成需要的文件（docs文件夹和docs/docs.go）。 1 swag init 确保导入了生成的docs/docs.go文件，这样特定的配置文件才会被初始化。如果通用API指数没有写在main.go中，可以使用-g标识符来告知swag。\n1 swag init -g http/api.go (可选) 使用fmt格式化 SWAG 注释。(请先升级到最新版本) 1 swag fmt swag cli 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 swag init -h NAME: swag init - Create docs.go USAGE: swag init [command options] [arguments...] OPTIONS: --generalInfo value, -g value API通用信息所在的go源文件路径，如果是相对路径则基于API解析目录 (默认: \u0026#34;main.go\u0026#34;) --dir value, -d value API解析目录 (默认: \u0026#34;./\u0026#34;) --exclude value 解析扫描时排除的目录，多个目录可用逗号分隔（默认：空） --propertyStrategy value, -p value 结构体字段命名规则，三种：snakecase,camelcase,pascalcase (默认: \u0026#34;camelcase\u0026#34;) --output value, -o value 文件(swagger.json, swagger.yaml and doc.go)输出目录 (默认: \u0026#34;./docs\u0026#34;) --parseVendor 是否解析vendor目录里的go源文件，默认不 --parseDependency 是否解析依赖目录中的go源文件，默认不 --markdownFiles value, --md value 指定API的描述信息所使用的markdown文件所在的目录 --generatedTime 是否输出时间到输出文件docs.go的顶部，默认是 --codeExampleFiles value, --cef value 解析包含用于 x-codeSamples 扩展的代码示例文件的文件夹，默认禁用 --parseInternal 解析 internal 包中的go文件，默认禁用 --parseDepth value 依赖解析深度 (默认: 100) --instanceName value 设置文档实例名 (默认: \u0026#34;swagger\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 swag fmt -h NAME: swag fmt - format swag comments USAGE: swag fmt [command options] [arguments...] OPTIONS: --dir value, -d value API解析目录 (默认: \u0026#34;./\u0026#34;) --exclude value 解析扫描时排除的目录，多个目录可用逗号分隔（默认：空） --generalInfo value, -g value API通用信息所在的go源文件路径，如果是相对路径则基于API解析目录 (默认: \u0026#34;main.go\u0026#34;) --help, -h show help (default: false) 支持的Web框架 gin echo buffalo net/http 如何与Gin集成 点击此处查看示例源代码。\n使用swag init生成Swagger2.0文档后，导入如下代码包： 1 2 import \u0026#34;github.com/swaggo/gin-swagger\u0026#34; // gin-swagger middleware import \u0026#34;github.com/swaggo/files\u0026#34; // swagger embed files 在main.go源代码中添加通用的API注释： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // @title Swagger Example API // @version 1.0 // @description This is a sample server celler server. // @termsOfService http://swagger.io/terms/ // @contact.name API Support // @contact.url http://www.swagger.io/support // @contact.email support@swagger.io // @license.name Apache 2.0 // @license.url http://www.apache.org/licenses/LICENSE-2.0.html // @host localhost:8080 // @BasePath /api/v1 // @securityDefinitions.basic BasicAuth func main() { r := gin.Default() c := controller.NewController() v1 := r.Group(\u0026#34;/api/v1\u0026#34;) { accounts := v1.Group(\u0026#34;/accounts\u0026#34;) { accounts.GET(\u0026#34;:id\u0026#34;, c.ShowAccount) accounts.GET(\u0026#34;\u0026#34;, c.ListAccounts) accounts.POST(\u0026#34;\u0026#34;, c.AddAccount) accounts.DELETE(\u0026#34;:id\u0026#34;, c.DeleteAccount) accounts.PATCH(\u0026#34;:id\u0026#34;, c.UpdateAccount) accounts.POST(\u0026#34;:id/images\u0026#34;, c.UploadAccountImage) } //... } r.GET(\u0026#34;/swagger/*any\u0026#34;, ginSwagger.WrapHandler(swaggerFiles.Handler)) r.Run(\u0026#34;:8080\u0026#34;) } //... 此外，可以动态设置一些通用的API信息。生成的代码包docs导出SwaggerInfo变量，使用该变量可以通过编码的方式设置标题、描述、版本、主机和基础路径。使用Gin的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/swaggo/files\u0026#34; \u0026#34;github.com/swaggo/gin-swagger\u0026#34; \u0026#34;./docs\u0026#34; // docs is generated by Swag CLI, you have to import it. ) // @contact.name API Support // @contact.url http://www.swagger.io/support // @contact.email support@swagger.io // @license.name Apache 2.0 // @license.url http://www.apache.org/licenses/LICENSE-2.0.html func main() { // programatically set swagger info docs.SwaggerInfo.Title = \u0026#34;Swagger Example API\u0026#34; docs.SwaggerInfo.Description = \u0026#34;This is a sample server Petstore server.\u0026#34; docs.SwaggerInfo.Version = \u0026#34;1.0\u0026#34; docs.SwaggerInfo.Host = \u0026#34;petstore.swagger.io\u0026#34; docs.SwaggerInfo.BasePath = \u0026#34;/v2\u0026#34; docs.SwaggerInfo.Schemes = []string{\u0026#34;http\u0026#34;, \u0026#34;https\u0026#34;} r := gin.New() // use ginSwagger middleware to serve the API docs r.GET(\u0026#34;/swagger/*any\u0026#34;, ginSwagger.WrapHandler(swaggerFiles.Handler)) r.Run() } 在controller代码中添加API操作注释： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 package controller import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/swaggo/swag/example/celler/httputil\u0026#34; \u0026#34;github.com/swaggo/swag/example/celler/model\u0026#34; ) // ShowAccount godoc // @Summary Show an account // @Description get string by ID // @Tags accounts // @Accept json // @Produce json // @Param id path int true \u0026#34;Account ID\u0026#34; // @Success 200 {object} model.Account // @Failure 400 {object} httputil.HTTPError // @Failure 404 {object} httputil.HTTPError // @Failure 500 {object} httputil.HTTPError // @Router /accounts/{id} [get] func (c *Controller) ShowAccount(ctx *gin.Context) { id := ctx.Param(\u0026#34;id\u0026#34;) aid, err := strconv.Atoi(id) if err != nil { httputil.NewError(ctx, http.StatusBadRequest, err) return } account, err := model.AccountOne(aid) if err != nil { httputil.NewError(ctx, http.StatusNotFound, err) return } ctx.JSON(http.StatusOK, account) } // ListAccounts godoc // @Summary List accounts // @Description get accounts // @Tags accounts // @Accept json // @Produce json // @Param q query string false \u0026#34;name search by q\u0026#34; Format(email) // @Success 200 {array} model.Account // @Failure 400 {object} httputil.HTTPError // @Failure 404 {object} httputil.HTTPError // @Failure 500 {object} httputil.HTTPError // @Router /accounts [get] func (c *Controller) ListAccounts(ctx *gin.Context) { q := ctx.Request.URL.Query().Get(\u0026#34;q\u0026#34;) accounts, err := model.AccountsAll(q) if err != nil { httputil.NewError(ctx, http.StatusNotFound, err) return } ctx.JSON(http.StatusOK, accounts) } //... 1 swag init 运行程序，然后在浏览器中访问 http://localhost:8080/swagger/index.html 。将看到Swagger 2.0 Api文档，如下所示： 格式化说明 可以针对Swag的注释自动格式化，就像go fmt。\n此处查看格式化结果 here.\n示例：\n1 swag fmt 排除目录（不扫描）示例：\n1 swag fmt -d ./ --exclude ./internal 开发现状 Swagger 2.0 文档\nBasic Structure API Host and Base Path Paths and Operations Describing Parameters Describing Request Body Describing Responses MIME Types Authentication Basic Authentication API Keys Adding Examples File Upload Enums Grouping Operations With Tags Swagger Extensions 声明式注释格式 通用API信息 示例 celler/main.go\n注释 说明 示例 title 必填 应用程序的名称。 // @title Swagger Example API version 必填 提供应用程序API的版本。 // @version 1.0 description 应用程序的简短描述。 // @description This is a sample server celler server. tag.name 标签的名称。 // @tag.name This is the name of the tag tag.description 标签的描述。 // @tag.description Cool Description tag.docs.url 标签的外部文档的URL。 // @tag.docs.url https://example.com tag.docs.description 标签的外部文档说明。 // @tag.docs.description Best example documentation termsOfService API的服务条款。 // @termsOfService http://swagger.io/terms/ contact.name 公开的API的联系信息。 // @contact.name API Support contact.url 联系信息的URL。 必须采用网址格式。 // @contact.url http://www.swagger.io/support contact.email 联系人/组织的电子邮件地址。 必须采用电子邮件地址的格式。 // @contact.email support@swagger.io license.name 必填 用于API的许可证名称。 // @license.name Apache 2.0 license.url 用于API的许可证的URL。 必须采用网址格式。 // @license.url http://www.apache.org/licenses/LICENSE-2.0.html host 运行API的主机（主机名或IP地址）。 // @host localhost:8080 BasePath 运行API的基本路径。 // @BasePath /api/v1 accept API 可以使用的 MIME 类型列表。 请注意，Accept 仅影响具有请求正文的操作，例如 POST、PUT 和 PATCH。 值必须如“Mime类型”中所述。 // @accept json produce API可以生成的MIME类型的列表。值必须如“Mime类型”中所述。 // @produce json query.collection.format 请求URI query里数组参数的默认格式：csv，multi，pipes，tsv，ssv。 如果未设置，则默认为csv。 // @query.collection.format multi schemes 用空格分隔的请求的传输协议。 // @schemes http https x-name 扩展的键必须以x-开头，并且只能使用json值 // @x-example-key {\u0026ldquo;key\u0026rdquo;: \u0026ldquo;value\u0026rdquo;} 使用Markdown描述 如果文档中的短字符串不足以完整表达，或者需要展示图片，代码示例等类似的内容，则可能需要使用Markdown描述。要使用Markdown描述，请使用一下注释。\n注释 说明 示例 title 必填 应用程序的名称。 // @title Swagger Example API version 必填 提供应用程序API的版本。 // @version 1.0 description.markdown 应用程序的简短描述。 从api.md文件中解析。 这是@description的替代用法。 // @description.markdown No value needed, this parses the description from api.md tag.name 标签的名称。 // @tag.name This is the name of the tag tag.description.markdown 标签说明，这是tag.description的替代用法。 该描述将从名为tagname.md的文件中读取。 // @tag.description.markdown API操作 Example celler/controller\n注释 描述 description 操作行为的详细说明。 description.markdown 应用程序的简短描述。该描述将从名为endpointname.md的文件中读取。 id 用于标识操作的唯一字符串。在所有API操作中必须唯一。 tags 每个API操作的标签列表，以逗号分隔。 summary 该操作的简短摘要。 accept API 可以使用的 MIME 类型列表。 请注意，Accept 仅影响具有请求正文的操作，例如 POST、PUT 和 PATCH。 值必须如“Mime类型”中所述。 produce API可以生成的MIME类型的列表。值必须如“Mime类型”中所述。 param 用空格分隔的参数。param name,param type,data type,is mandatory?,comment attribute(optional) security 每个API操作的安全性。 success 以空格分隔的成功响应。return code,{param type},data type,comment failure 以空格分隔的故障响应。return code,{param type},data type,comment response 与success、failure作用相同 header 以空格分隔的头字段。 return code,{param type},data type,comment router 以空格分隔的路径定义。 path,[httpMethod] x-name 扩展字段必须以x-开头，并且只能使用json值。 Mime类型 swag 接受所有格式正确的MIME类型, 即使匹配 */*。除此之外，swag还接受某些MIME类型的别名，如下所示：\nAlias MIME Type json application/json xml text/xml plain text/plain html text/html mpfd multipart/form-data x-www-form-urlencoded application/x-www-form-urlencoded json-api application/vnd.api+json json-stream application/x-json-stream octet-stream application/octet-stream png image/png jpeg image/jpeg gif image/gif 参数类型 query： 表示带在 url 之后的参数\npath ：表示请求路径上得url变量\nheader ： 表示带在 header 信息中得参数\nbody ：表示是一个 raw 数据请求，当Accept是JSON格式时，我们使用该字段指定接收的JSON类型\nformData ：表示是form表单请求的数据，需要加对应的api注解：// @accept mpfd，表单数据提交一般不会用于GET方法，GET方法常见于url参数或变量的使用中。\n数据类型 string (string) integer (int, uint, uint32, uint64) number (float32) boolean (bool) user defined struct 安全性 注释 描述 参数 示例 securitydefinitions.basic Basic auth. // @securityDefinitions.basic BasicAuth securitydefinitions.apikey API key auth. in, name // @securityDefinitions.apikey ApiKeyAuth securitydefinitions.oauth2.application OAuth2 application auth. tokenUrl, scope // @securitydefinitions.oauth2.application OAuth2Application securitydefinitions.oauth2.implicit OAuth2 implicit auth. authorizationUrl, scope // @securitydefinitions.oauth2.implicit OAuth2Implicit securitydefinitions.oauth2.password OAuth2 password auth. tokenUrl, scope // @securitydefinitions.oauth2.password OAuth2Password securitydefinitions.oauth2.accessCode OAuth2 access code auth. tokenUrl, authorizationUrl, scope // @securitydefinitions.oauth2.accessCode OAuth2AccessCode 参数注释 示例 in // @in header name // @name Authorization tokenUrl // @tokenUrl https://example.com/oauth/token authorizationurl // @authorizationurl https://example.com/oauth/authorize scope.hoge // @scope.write Grants write access 属性 1 2 3 4 5 6 7 8 // @Param enumstring query string false \u0026#34;string enums\u0026#34; Enums(A, B, C) // @Param enumint query int false \u0026#34;int enums\u0026#34; Enums(1, 2, 3) // @Param enumnumber query number false \u0026#34;int enums\u0026#34; Enums(1.1, 1.2, 1.3) // @Param string query string false \u0026#34;string valid\u0026#34; minlength(5) maxlength(10) // @Param int query int false \u0026#34;int valid\u0026#34; minimum(1) maximum(10) // @Param default query string false \u0026#34;string default\u0026#34; default(A) // @Param collection query []string false \u0026#34;string collection\u0026#34; collectionFormat(multi) // @Param extensions query []string false \u0026#34;string collection\u0026#34; extensions(x-example=test,x-nullable) 也适用于结构体字段：\n1 2 3 4 5 type Foo struct { Bar string `minLength:\u0026#34;4\u0026#34; maxLength:\u0026#34;16\u0026#34;` Baz int `minimum:\u0026#34;10\u0026#34; maximum:\u0026#34;20\u0026#34; default:\u0026#34;15\u0026#34;` Qux []string `enums:\u0026#34;foo,bar,baz\u0026#34;` } 当前可用的 字段名 类型 描述 default * 声明如果未提供任何参数，则服务器将使用的默认参数值，例如，如果请求中的客户端未提供该参数，则用于控制每页结果数的“计数”可能默认为100。 （注意：“default”对于必需的参数没有意义）。参看 https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-6.2。 与JSON模式不同，此值务必符合此参数的定义类型。 maximum number 参看 https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.1.2. minimum number 参看 https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.1.3. maxLength integer 参看 https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.2.1. minLength integer 参看 https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.2.2. enums [*] 参看 https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.5.1. format string 上面提到的类型的扩展格式。有关更多详细信息，请参见数据类型格式。 collectionFormat string 指定query数组参数的格式。 可能的值为： csv - 逗号分隔值 foo,bar. ssv - 空格分隔值 foo bar. tsv - 制表符分隔值 foo\\tbar. pipes - 管道符分隔值 foo|bar. multi - 对应于多个参数实例，而不是单个实例 foo=bar＆foo=baz 的多个值。这仅对“query”或“formData”中的参数有效。 默认值是 csv。 进一步的 字段名 类型 描述 multipleOf number See https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.1.1. pattern string See https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.2.3. maxItems integer See https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.3.2. minItems integer See https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.3.3. uniqueItems boolean See https://tools.ietf.org/html/draft-fge-json-schema-validation-00#section-5.3.4. 样例 多行的描述 可以在常规api描述或路由定义中添加跨越多行的描述，如下所示：\n1 2 3 // @description This is the first line // @description This is the second line // @description And so forth. 用户自定义的具有数组类型的结构 1 // @Success 200 {array} model.Account \u0026lt;-- This is a user defined struct. 1 2 3 4 5 6 package model type Account struct { ID int `json:\u0026#34;id\u0026#34; example:\u0026#34;1\u0026#34;` Name string `json:\u0026#34;name\u0026#34; example:\u0026#34;account name\u0026#34;` } 响应对象中的模型组合 1 2 // JSONResult的data字段类型将被proto.Order类型替换 @success 200 {object} jsonresult.JSONResult{data=proto.Order} \u0026#34;desc\u0026#34; 1 2 3 4 5 6 7 8 9 type JSONResult struct { Code int `json:\u0026#34;code\u0026#34; ` Message string `json:\u0026#34;message\u0026#34;` Data interface{} `json:\u0026#34;data\u0026#34;` } type Order struct { //in `proto` package ... } 还支持对象数组和原始类型作为嵌套响应 1 2 3 @success 200 {object} jsonresult.JSONResult{data=[]proto.Order} \u0026#34;desc\u0026#34; @success 200 {object} jsonresult.JSONResult{data=string} \u0026#34;desc\u0026#34; @success 200 {object} jsonresult.JSONResult{data=[]string} \u0026#34;desc\u0026#34; 替换多个字段的类型。如果某字段不存在，将添加该字段。 1 @success 200 {object} jsonresult.JSONResult{data1=string,data2=[]string,data3=proto.Order,data4=[]proto.Order} \u0026#34;desc\u0026#34; 在响应中增加头字段 1 2 3 4 5 6 // @Success 200 {string} string \u0026#34;ok\u0026#34; // @failure 400 {string} string \u0026#34;error\u0026#34; // @response default {string} string \u0026#34;other error\u0026#34; // @Header 200 {string} Location \u0026#34;/entity/1\u0026#34; // @Header 200,400,default {string} Token \u0026#34;token\u0026#34; // @Header all {string} Token2 \u0026#34;token2\u0026#34; 使用多路径参数 1 2 3 4 5 /// ... // @Param group_id path int true \u0026#34;Group ID\u0026#34; // @Param account_id path int true \u0026#34;Account ID\u0026#34; // ... // @Router /examples/groups/{group_id}/accounts/{account_id} [get] 结构体的示例值 1 2 3 4 5 type Account struct { ID int `json:\u0026#34;id\u0026#34; example:\u0026#34;1\u0026#34;` Name string `json:\u0026#34;name\u0026#34; example:\u0026#34;account name\u0026#34;` PhotoUrls []string `json:\u0026#34;photo_urls\u0026#34; example:\u0026#34;http://test/image/1.jpg,http://test/image/2.jpg\u0026#34;` } 结构体描述 1 2 3 4 5 type Account struct { // ID this is userid ID int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` // This is Name } 使用swaggertype标签更改字段类型 #201\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 type TimestampTime struct { time.Time } ///实现encoding.JSON.Marshaler接口 func (t *TimestampTime) MarshalJSON() ([]byte, error) { bin := make([]byte, 16) bin = strconv.AppendInt(bin[:0], t.Time.Unix(), 10) return bin, nil } ///实现encoding.JSON.Unmarshaler接口 func (t *TimestampTime) UnmarshalJSON(bin []byte) error { v, err := strconv.ParseInt(string(bin), 10, 64) if err != nil { return err } t.Time = time.Unix(v, 0) return nil } /// type Account struct { // 使用`swaggertype`标签将别名类型更改为内置类型integer ID sql.NullInt64 `json:\u0026#34;id\u0026#34; swaggertype:\u0026#34;integer\u0026#34;` // 使用`swaggertype`标签更改struct类型为内置类型integer RegisterTime TimestampTime `json:\u0026#34;register_time\u0026#34; swaggertype:\u0026#34;primitive,integer\u0026#34;` // Array types can be overridden using \u0026#34;array,\u0026lt;prim_type\u0026gt;\u0026#34; format Coeffs []big.Float `json:\u0026#34;coeffs\u0026#34; swaggertype:\u0026#34;array,number\u0026#34;` } #379\n1 2 3 4 type CerticateKeyPair struct { Crt []byte `json:\u0026#34;crt\u0026#34; swaggertype:\u0026#34;string\u0026#34; format:\u0026#34;base64\u0026#34; example:\u0026#34;U3dhZ2dlciByb2Nrcw==\u0026#34;` Key []byte `json:\u0026#34;key\u0026#34; swaggertype:\u0026#34;string\u0026#34; format:\u0026#34;base64\u0026#34; example:\u0026#34;U3dhZ2dlciByb2Nrcw==\u0026#34;` } 生成的swagger文档如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026#34;api.MyBinding\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;:{ \u0026#34;crt\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;base64\u0026#34;, \u0026#34;example\u0026#34;:\u0026#34;U3dhZ2dlciByb2Nrcw==\u0026#34; }, \u0026#34;key\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;base64\u0026#34;, \u0026#34;example\u0026#34;:\u0026#34;U3dhZ2dlciByb2Nrcw==\u0026#34; } } } 使用swaggerignore标签排除字段 1 2 3 4 5 type Account struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Ignored int `swaggerignore:\u0026#34;true\u0026#34;` } 将扩展信息添加到结构字段 1 2 3 type Account struct { ID string `json:\u0026#34;id\u0026#34; extensions:\u0026#34;x-nullable,x-abc=def,!x-omitempty\u0026#34;` // 扩展字段必须以\u0026#34;x-\u0026#34;开头 } 生成swagger文档，如下所示：\n1 2 3 4 5 6 7 8 9 10 11 \u0026#34;Account\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;x-nullable\u0026#34;: true, \u0026#34;x-abc\u0026#34;: \u0026#34;def\u0026#34;, \u0026#34;x-omitempty\u0026#34;: false } } } 对展示的模型重命名 1 2 3 type Resp struct { Code int }//@name Response 如何使用安全性注释 通用API信息。\n1 2 3 4 5 6 // @securityDefinitions.basic BasicAuth // @securitydefinitions.oauth2.application OAuth2Application // @tokenUrl https://example.com/oauth/token // @scope.write Grants write access // @scope.admin Grants read and write access to administrative information 每个API操作。\n1 // @Security ApiKeyAuth 使用AND条件。\n1 2 // @Security ApiKeyAuth // @Security OAuth2Application[write, admin] token的api注解 swagger除了可以查看文档外，还可以直接try it out 调用接口，有些接口是需要登录后才能调用。有个token鉴权。\n校验token 的api接口 header中必须有正确的 Authorization\n可以通过添加通用的API注释来设置全局接口的header值\n1 2 3 4 5 // @securityDefinitions.apikey CoreAPI // @name Authorization // @in header //设置header字段 Authorization 然后在需要token接口注释上增加\n1 2 // @Security CoreAPI //并且需要携带对应的token，一般token放在Bear开头，所以需要如下图操作：（Bear{token}，注意两者之间有空格） ","permalink":"https://cold-bin.github.io/post/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3swag%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","tags":["swagger","接口文档自动化"],"title":"自动化接口文档swag使用指南"},{"categories":["golang","爬虫"],"contents":"Go爬虫 爬虫就是模拟客户端程序访问服务端某个url下的资源，再将资源进行部分提取（使用正则筛选html等），然后在存储至某端，或者放在某网页。 流程： 明确目标url 发送请求，获取应答数据包 保存、过滤数据。提取有用信息（考虑使用正则表达式对html元素进行筛选） 使用、分析得到的数据信息 一、入门案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) //该例子用于简单的模拟了浏览器的请求，即爬虫原理 func main() { //创建一个http请求 req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;https://www.baidu.com/\u0026#34;, nil) //编写请求头 req.Header.Add(\u0026#34;UserAgent\u0026#34;, \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\u0026#34;) //初始化一个客户端 client := \u0026amp;http.Client{} //客户端发起请求 resp, err := client.Do(req) if err != nil { return } //获得回应 fmt.Println(resp) } 二、正则表达式 正则表达式教程\n三、常用爬虫正则表达式 1. 爬取html标签里的内容 1 2 3 4 5 6 7 8 9 \u0026lt;div class=\u0026#34;navbar-header\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/\u0026#34; class=\u0026#34;navbar-brand\u0026#34; title=\u0026#34;Go语言中文网\u0026#34;\u0026gt;\u0026lt;img alt=\u0026#34;Go语言中文网\u0026#34; src=\u0026#34;https://static.studygolang.com/static/img/logo.png\u0026#34; style=\u0026#34;margin-top: -7px; height: 45px;\u0026#34;\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;button class=\u0026#34;navbar-toggle\u0026#34; type=\u0026#34;button\u0026#34; data-toggle=\u0026#34;collapse\u0026#34; data-target=\u0026#34;#navbar-main\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 //此正则表示匹配div标签里的所有数据，及时里面是空 //?s-\u0026gt;是正则表达式的模式修饰符，即单行模式。表示更改【.】的含义，使其与每一个字符匹配，包括换行符。 //*？表示非贪婪，即重复\u0026gt;=0次匹配x，越少越好（优先跳出重复） // html := \u0026#34;\u0026lt;a\u0026gt;hhh\u0026lt;/a\u0026gt;\u0026lt;div\u0026gt;你好，我来自哪里？\\n\\r我来自火星。\u0026lt;/div\u0026gt;\u0026lt;/br\u0026gt;\u0026lt;div\u0026gt;你好，我来自哪里？\\n\\n我来自域外。\u0026lt;/div\u0026gt;\u0026lt;/br\u0026gt;\u0026lt;div\u0026gt;你好，我来自哪里？\\n\\n我来自你不知道的地方。\u0026lt;/div\u0026gt;\u0026lt;div\u0026gt;\u0026lt;/div\u0026gt;\u0026#34; regex := `\u0026lt;div\u0026gt;(?s:(.+?))\u0026lt;/div\u0026gt;` c, err := regexp.Compile(regex) if err != nil { fmt.Println(fmt.Sprint(err)) return } fmt.Println(c.FindAllStringSubmatch(html, -1)) //\u0026lt;div\u0026gt;?s:(.+?)\u0026lt;/div\u0026gt;只表示一次正则提取。 //\u0026lt;div\u0026gt;(?s:(.+?))\u0026lt;/div\u0026gt;表示先提取整个表达式匹配的，然后再提取括号内满足的表达式 2. 双向爬取 ①横向爬取 所谓横向爬取，是指在爬取的网站页面中，以“页”为单位，找寻该网站的分页规律。一页一页的爬取网站数据信息。大多数网站，采用分页管理模式。针对这类网站，首先需要确立横向爬取方法\n②纵向爬取 纵向爬取，是指在一个网页中，按不同的“条目”为单位，找寻个条目之间的规律。一条一条的爬取一个网页的数据信息。也就是爬取一个页面内不同类别数据\n四、使用colly框架 上文举一个例子，通过go的http库实现了获取指定网站的html数据，但是不是每个网站都能这样爬取，有些网站会有一些防爬措施来辨别访问对象是否是爬虫还是正常客户端。按照正常地流程:伪造客户端，设置合理的请求头属性，提交某种特定格式的参数才能对网站资源进行访问，而且在访问后还需要使用正则表达式对获取的html内容进行筛选。❌\n我们简化一下流程，让我们可以更加轻松且快速的使用爬虫：\n使用colly框架：分析爬取网页html结构；选择合适的selector来筛选网页数据\ncolly框架内部的OnHtml方法实现了对html的监听，需要使用goquery库实现的jquery go语法。这样使用特定的选择器可以实现对特定html标签元素的监听，达到过滤数据的目的。当然还有提取xml标签内容的库\n详见xml的go实现\ngoquery对爬取到的HTML进行选择和查找匹配的内容时，goquery的选择器让解析html操作更简洁，而且还有很多不常用但又很有用的选择器，这里总结下，以供参考。\n以下内容转载自飞雪无情的 golang goquery selector(选择器) 示例大全\n如果大家以前做过前端开发，对jquery不会陌生，goquery类似jquery，它是jquery的go版本实现。使用它，可以很方便的对HTML进行处理。\n基于HTML Element元素的选择器 这个比较简单，就是基于a,p等这些HTML的基本元素进行选择，这种直接使用Element名称作为选择器即可。比如dom.Find(\u0026quot;div\u0026quot;)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt;SPAN\u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 以上示例，可以把div元素筛选出来，而body,span并不会被筛选。\nID 选择器 这个是使用频次最多的，类似于上面的例子，有两个div元素，其实我们只需要其中的一个，那么我们只需要给这个标记一个唯一的id即可，这样我们就可以使用id选择器，精确定位了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;div1\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt;SPAN\u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;#div1\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } Element ID 选择器 id选择器以#开头，紧跟着元素id的值，使用语法为dom.Find(#id),后面的例子我会简写为Find(#id),大家知道这是代表goquery选择器的即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;div1\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt;SPAN\u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;#div1\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 如果有相同的ID，但是它们又分别属于不同的HTML元素怎么办？有好办法，和Element结合起来。比如我们筛选元素为div,并且id是div1的元素，就可以使用Find(div#div1)这样的筛选器进行筛选。\n所以这类筛选器的语法为Find(element#id)，这是常用的组合方法，比如后面讲的过滤器也可以采用这种方式组合使用。\nClass选择器 class也是HTML中常用的属性，我们可以通过class选择器来快速的筛选需要的HTML元素，它的用法和ID选择器类似，为Find(\u0026quot;.class\u0026quot;)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;div1\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;name\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt;SPAN\u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;.name\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 以上示例中，就筛选出来class为name的这个div元素。\nElement Class 选择器 class选择器和id选择器一样，也可以结合着HTML元素使用，他们的语法也类似Find(element.class)，这样就可以筛选特定element、并且指定class的元素。\n属性选择器 一个HTML元素都有自己的属性以及属性值，所以我们也可以通过属性和值筛选元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;name\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt;SPAN\u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div[class]\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 示例中我们通过div[class]这个选择器，筛选出Element为div并且有class这个属性的，所以第一个div没有被筛选到。\n刚刚上面这个示例是采用是否存在某个属性为筛选器，同理，我们可以筛选出属性为某个值的元素。\n1 2 3 dom.Find(\u0026#34;div[class=name]\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) 这样我们就可以筛选出class这个属性值为name的div元素。\n当然我们这里以class属性为例，还可以用其他属性，比如href等很多，自定义属性也是可以的。\n除了完全相等，还有其他匹配方式，使用方式类似，这里统一列举下，不再举例\n选择器 说明 Find(“div[lang]“) 筛选含有lang属性的div元素 Find(“div[lang=zh]“) 筛选lang属性为zh的div元素 Find(“div[lang!=zh]“) 筛选lang属性不等于zh的div元素 Find(“div[lang¦=zh]“) 筛选lang属性为zh或者zh-开头的div元素 Find(“div[lang*=zh]“) 筛选lang属性包含zh这个字符串的div元素 Find(“div[lang~=zh]“) 筛选lang属性包含zh这个单词的div元素，单词以空格分开的 Find(“div[lang$=zh]“) 筛选lang属性以zh结尾的div元素，区分大小写 Find(“div[lang^=zh]“) 筛选lang属性以zh开头的div元素，区分大小写 （如果=后面的值是拼接的用单引号包裹即可） 示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func main() { //spiderTop250() //spoderMoiveInfo(\u0026#34;https://movie.douban.com/subject/3011235/\u0026#34;) html := `\u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt;JSDKK\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; ` dom, err := goquery.NewDocumentFromReader(strings.NewReader(html)) if err != nil { log.Fatalln(err) } dom.Find(\u0026#34;script[type=\u0026#39;application/ld+json\u0026#39;]\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 以上是属性筛选器的用法，都是以一个属性筛选器为例，当然你也可以使用多个属性筛选器组合使用，比如： Find(\u0026quot;div[id][lang=zh]\u0026quot;),用多个中括号连起来即可。当有多个属性筛选器的时候，要同时满足这些筛选器的元素才能被筛选出来。\nparent\u0026gt;child选择器 如果我们想筛选出某个元素下符合条件的子元素，我们就可以使用子元素筛选器，它的语法为Find(\u0026quot;parent\u0026gt;child\u0026quot;),表示筛选parent这个父元素下，符合child这个条件的最直接（一级）的子元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;ZH\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;div lang=\u0026#34;zh-cn\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;div lang=\u0026#34;en\u0026#34;\u0026gt;DIV3\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;div\u0026gt;DIV4\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;body\u0026gt;div\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 以上示例，筛选出body这个父元素下，符合条件的最直接的子元素div，结果是DIV1、DIV2、DIV3，虽然DIV4也是body的子元素，但不是一级的，所以不会被筛选到。\n那么问题来了，我就是想把DIV4也筛选出来怎么办?就是要筛选body下所有的div元素，不管是一级、二级还是N级。有办法的，goquery考虑到了，只需要把大于号(\u0026gt;)改为空格就好了。比如上面的例子，改为如下选择器即可。\n1 2 3 dom.Find(\u0026#34;body div\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) prev+next相邻选择器 假设我们要筛选的元素没有规律，但是该元素的上一个元素有规律，我们就可以使用这种下一个相邻选择器来进行选择。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;zh\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;P1\u0026lt;/p\u0026gt; \u0026lt;div lang=\u0026#34;zh-cn\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;div lang=\u0026#34;en\u0026#34;\u0026gt;DIV3\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;div\u0026gt;DIV4\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;p\u0026gt;P2\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div[lang=zh]+p\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) } 这个示例演示了这种用法，我们想选择\u0026lt;p\u0026gt;P1\u0026lt;/p\u0026gt;这个元素，但是没啥规律，我们发现它前面的\u0026lt;div lang=\u0026quot;zh\u0026quot;\u0026gt;DIV1\u0026lt;/div\u0026gt;很有规律，可以选择，所以我们就可以采用Find(\u0026quot;div[lang=zh]+p\u0026quot;)达到选择P元素的目的。\n这种选择器的语法是(\u0026quot;prev+next\u0026quot;)，中间是一个加号(+)，+号前后也是选择器。\nprev~next兄弟选择器 有相邻就有兄弟，兄弟选择器就不一定要求相邻了，只要他们共有一个父元素就可以。\n1 2 3 dom.Find(\u0026#34;div[lang=zh]~p\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) 刚刚的例子，只需要把+号换成~号,就可以把P2也筛选出来，因为P2、P1和DIV1都是兄弟。\n兄弟选择器的语法是(\u0026quot;prev~next\u0026quot;),也就是相邻选择器的+换成了~。\n内容过滤器 有时候我们使用选择器选择出来后后，希望再过滤一下，这时候就用到过滤器了，过滤器有很多，我们先讲内容过滤器这一种。\n1 2 3 dom.Find(\u0026#34;div:contains(DIV2)\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) Find(\u0026quot;:contains(text)\u0026quot;)表示筛选出的元素要包含指定的文本，我们例子中要求选择出的div元素要包含DIV2文本，那么只有一个DIV2元素满足要求。\n此外还有Find(\u0026quot;:empty\u0026quot;)表示筛选出的元素都不能有子元素（包括文本元素），只筛选那些不包含任何子元素的元素。\nFind(\u0026quot;:has(selector)\u0026quot;)和contains差不多，只不过这个是包含的是元素节点。\n1 2 3 dom.Find(\u0026#34;span:has(div)\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Text()) }) 以上示例表示筛选出包含div元素的span节点。\n:first-child过滤器 :first-child过滤器，语法为Find(\u0026quot;:first-child\u0026quot;)，表示筛选出的元素，要是他们的父元素的第一个子元素，如果不是，则不会被筛选出来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;zh\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;P1\u0026lt;/p\u0026gt; \u0026lt;div lang=\u0026#34;zh-cn\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;div lang=\u0026#34;en\u0026#34;\u0026gt;DIV3\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;div style=\u0026#34;display:none;\u0026#34;\u0026gt;DIV4\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;DIV5\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;p\u0026gt;P2\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div:first-child\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Html()) }) } 以上例子中，我们使用Find(\u0026quot;div\u0026quot;)会筛选出所有的div元素，但是我们加了:first-child后，就只有DIV1和DIV4了，因为只有这两个是他们父元素的第一个子元素，其他的DIV都不满足。\n:first-of-type过滤器 :first-child选择器限制的比较死，必须得是第一个子元素，如果该元素前有其他在前面，就不能用:first-child了，这时候:first-of-type就派上用场了，它要求只要是这个类型的第一个就可以，我们把上面的例子微调下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;zh\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;P1\u0026lt;/p\u0026gt; \u0026lt;div lang=\u0026#34;zh-cn\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;div lang=\u0026#34;en\u0026#34;\u0026gt;DIV3\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;p\u0026gt;P2\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;DIV5\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div:first-of-type\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Html()) }) } 改动很简单，把原来的DIV4换成了P2，如果我们还使用:first-child,DIV5是不能被筛选出来的，因为它不是第一个子元素，它前面还有一个P2。这时候我们使用:first-of-type就可以达到目的，因为它要求是同类型第一个就可以。DIV5就是这个div类型的第一个元素，P2不是div类型，被忽略。\n:last-child 和 :last-of-type过滤器 这两个正好和上面的:first-child、:first-of-type相反，表示最后一个，这里不再举例，大家可以自己试试。\n:nth-child(n) 过滤器 这个表示筛选出的元素是其父元素的第n个元素，n以1开始。所以我们可以知道:first-child和:nth-child(1)是相等的。通过指定n，我们就很灵活的筛选出我们需要的元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;zh\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;P1\u0026lt;/p\u0026gt; \u0026lt;div lang=\u0026#34;zh-cn\u0026#34;\u0026gt;DIV2\u0026lt;/div\u0026gt; \u0026lt;div lang=\u0026#34;en\u0026#34;\u0026gt;DIV3\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;p\u0026gt;P2\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;DIV5\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div:nth-child(3)\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Html()) }) } 这个示例会筛选出DIV2，因为DIV2是其父元素body的第三个子元素。\n:nth-of-type(n) 过滤器 :nth-of-type(n)和 :nth-child(n)类似，只不过它表示的是同类型元素的第n个,所以:nth-of-type(1)和:first-of-type是相等的，大家可以自己试试，这里不再举例。\nnth-last-child(n) 和:nth-last-of-type(n) 过滤器 这两个和上面的类似，只不过是倒序开始计算的，最后一个元素被当成了第一个。大家自己测试下看看效果，很明显。\n:only-child 过滤器 Find(\u0026quot;:only-child\u0026quot;)过滤器，从字面上看，可以猜测出来，它表示筛选的元素，在其父元素中，只有它自己，它的父元素没有其他子元素，才会被匹配筛选出来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;zh\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;div\u0026gt;DIV5\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div:only-child\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Html()) }) } 示例中DIV5就可以被筛选出来，因为它是它的父元素span达到唯一子元素，但DIV1就不是，所以不能被筛选出来。\n:only-of-type 过滤器 上面的例子，如果想筛选出DIV1怎么办？可以使用Find(\u0026quot;:only-of-type\u0026quot;),因为它是它的父元素中，唯一的div元素，这就是:only-of-type过滤器所要做的，同类型元素只要只有一个，就可以被筛选出来。大家把上面的例子改成:only-of-type试试，看看是否有DIV1。\n选择器或(|)运算 如果我们想同时筛选出div,span等元素怎么办？这时候可以采用多个选择器进行组合使用，并且以逗号(,)分割，Find(\u0026quot;selector1, selector2, selectorN\u0026quot;)表示，只要满足其中一个选择器就可以被筛选出来，也就是选择器的或(|)运算操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { html := `\u0026lt;body\u0026gt; \u0026lt;div lang=\u0026#34;zh\u0026#34;\u0026gt;DIV1\u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;div\u0026gt;DIV5\u0026lt;/div\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; ` dom,err:=goquery.NewDocumentFromReader(strings.NewReader(html)) if err!=nil{ log.Fatalln(err) } dom.Find(\u0026#34;div,span\u0026#34;).Each(func(i int, selection *goquery.Selection) { fmt.Println(selection.Html()) }) } 小结 goquery 是解析HTML网页必备的利器，在爬虫抓取网页的过程中，灵活的使用goquery不同的选择器，可以让我们的抓取工作事半功倍，大大提升爬虫的效率。\n选择器问题解决了。再看看关于爬虫的一些设置，以让爬虫可以更好的伪造客户端进行数据爬取。\n下面就是一些伪造方法。\ncolly常见配置及存储 1. 爬取简书首页文章列表 通过浏览器开发者工具查看jianshu.com结构如下\ncolly-jianshu-dom.png\n文章列表为ul标签，中间每一项是li标签，li中包含content，content中包含title，abstract和meta标签\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gocolly/colly\u0026#34; \u0026#34;github.com/gocolly/colly/debug\u0026#34; ) func main() { c := colly.NewCollector(colly.UserAgent(\u0026#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u0026#34;), colly.MaxDepth(1), colly.Debugger(\u0026amp;debug.LogDebugger{})) //文章列表 c.OnHTML(\u0026#34;ul[class=\u0026#39;note-list\u0026#39;]\u0026#34;, func(e *colly.HTMLElement) { //列表中每一项 e.ForEach(\u0026#34;li\u0026#34;, func(i int, item *colly.HTMLElement) { //文章链接 href := item.ChildAttr(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; a[class=\u0026#39;title\u0026#39;]\u0026#34;, \u0026#34;href\u0026#34;) //文章标题 title := item.ChildText(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; a[class=\u0026#39;title\u0026#39;]\u0026#34;) //文章摘要 summary := item.ChildText(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; p[class=\u0026#39;abstract\u0026#39;]\u0026#34;) fmt.Println(title, href) fmt.Println(summary) fmt.Println() }) }) err := c.Visit(\u0026#34;https://www.jianshu.com\u0026#34;) if err != nil { fmt.Println(err.Error()) } } 2. 爬取文章列表和详情 文章列表和1方式一样，文章详情通过创建新的采集器访问详情页面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gocolly/colly\u0026#34; \u0026#34;time\u0026#34; ) func main() { c1 := colly.NewCollector(colly.UserAgent(\u0026#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u0026#34;), colly.MaxDepth(1)) c2 := c1.Clone() //异步 c2.Async = true //限速 c2.Limit(\u0026amp;colly.LimitRule{ DomainRegexp: \u0026#34;\u0026#34;, DomainGlob: \u0026#34;*.jianshu.com/p/*\u0026#34;, Delay: 10 * time.Second, RandomDelay: 0, Parallelism: 1, }) //采集器1，获取文章列表 c1.OnHTML(\u0026#34;ul[class=\u0026#39;note-list\u0026#39;]\u0026#34;, func(e *colly.HTMLElement) { e.ForEach(\u0026#34;li\u0026#34;, func(i int, item *colly.HTMLElement) { href := item.ChildAttr(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; a[class=\u0026#39;title\u0026#39;]\u0026#34;, \u0026#34;href\u0026#34;) title := item.ChildText(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; a[class=\u0026#39;title\u0026#39;]\u0026#34;) summary := item.ChildText(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; p[class=\u0026#39;abstract\u0026#39;]\u0026#34;) ctx := colly.NewContext() ctx.Put(\u0026#34;href\u0026#34;, href) ctx.Put(\u0026#34;title\u0026#34;, title) ctx.Put(\u0026#34;summary\u0026#34;, summary) //通过Context上下文对象将采集器1采集到的数据传递到采集器2 c2.Request(\u0026#34;GET\u0026#34;, \u0026#34;https://www.jianshu.com\u0026#34; + href, nil, ctx, nil) }) }) //采集器2，获取文章详情 c2.OnHTML(\u0026#34;article\u0026#34;, func(e *colly.HTMLElement) { href := e.Request.Ctx.Get(\u0026#34;href\u0026#34;) title := e.Request.Ctx.Get(\u0026#34;title\u0026#34;) summary := e.Request.Ctx.Get(\u0026#34;summary\u0026#34;) detail := e.Text fmt.Println(\u0026#34;----------\u0026#34; + title + \u0026#34;----------\u0026#34;) fmt.Println(href) fmt.Println(summary) fmt.Println(detail) fmt.Println() }) c2.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;c2爬取页面：\u0026#34;, r.URL) }) c1.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;c1爬取页面：\u0026#34;, r.URL) }) c1.OnError(func(r *colly.Response, err error) { fmt.Println(\u0026#34;Request URL:\u0026#34;, r.Request.URL, \u0026#34;failed with response:\u0026#34;, r, \u0026#34;\\nError:\u0026#34;, err) }) err := c1.Visit(\u0026#34;https://www.jianshu.com\u0026#34;) if err != nil { fmt.Println(err.Error()) } c2.Wait() } 3. 爬取需要登录的网页 官网提供登录页处理的例子，但是大多数涉及验证码，不好处理，目前方式是手动登录，复制cookie写到爬虫请求头里\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gocolly/colly\u0026#34; \u0026#34;github.com/gocolly/colly/debug\u0026#34; \u0026#34;github.com/gocolly/colly/extensions\u0026#34; _ \u0026#34;github.com/gocolly/colly/extensions\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { url := \u0026#34;https://www.jianshu.com\u0026#34; c := colly.NewCollector(colly.UserAgent(\u0026#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u0026#34;), colly.MaxDepth(1), colly.Debugger(\u0026amp;debug.LogDebugger{})) c.OnHTML(\u0026#34;ul[class=\u0026#39;note-list\u0026#39;]\u0026#34;, func(e *colly.HTMLElement) { e.ForEach(\u0026#34;li\u0026#34;, func(i int, item *colly.HTMLElement) { href := item.ChildAttr(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; a[class=\u0026#39;title\u0026#39;]\u0026#34;, \u0026#34;href\u0026#34;) title := item.ChildText(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; a[class=\u0026#39;title\u0026#39;]\u0026#34;) summary := item.ChildText(\u0026#34;div[class=\u0026#39;content\u0026#39;] \u0026gt; p[class=\u0026#39;abstract\u0026#39;]\u0026#34;) fmt.Println(title, href) fmt.Println(summary) fmt.Println() }) }) //设置随机useragent extensions.RandomUserAgent(c) //设置登录cookie c.SetCookies(url, []*http.Cookie{ \u0026amp;http.Cookie{ Name: \u0026#34;remember_user_token\u0026#34;, Value: \u0026#34;wNDUxOV0sIiQyYSQxMSRwdkhqWVhHYmxXaDJ6dEU3NzJwbmsuIiwiMTU\u0026#34;, Path: \u0026#34;/\u0026#34;, Domain: \u0026#34;.jianshu.com\u0026#34;, Secure: true, HttpOnly: true, }, }) c.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;爬取页面：\u0026#34;, r.URL) }) c.OnError(func(r *colly.Response, err error) { fmt.Println(\u0026#34;Request URL:\u0026#34;, r.Request.URL, \u0026#34;failed with response:\u0026#34;, r, \u0026#34;\\nError:\u0026#34;, err) }) err := c.Visit(url) if err != nil { fmt.Println(err.Error()) } } 4. 内存任务队列 将需要爬取的连接放入队列中，设置队列并发数，可以并行爬取连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gocolly/colly\u0026#34; \u0026#34;github.com/gocolly/colly/debug\u0026#34; \u0026#34;github.com/gocolly/colly/queue\u0026#34; ) func main() { c := colly.NewCollector(colly.UserAgent(\u0026#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u0026#34;), colly.MaxDepth(3), colly.Debugger(\u0026amp;debug.LogDebugger{})) //创建内存队列，大小10000，goroutine数量 5 q, _ := queue.New(5, \u0026amp;queue.InMemoryQueueStorage{MaxSize: 10000}) c.OnHTML(\u0026#34;a\u0026#34;, func(element *colly.HTMLElement) { element.Request.Visit(element.Attr(\u0026#34;href\u0026#34;)) }) c.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;爬取页面：\u0026#34;, r.URL) }) c.OnError(func(r *colly.Response, err error) { fmt.Println(\u0026#34;Request URL:\u0026#34;, r.Request.URL, \u0026#34;failed with response:\u0026#34;, r, \u0026#34;\\nError:\u0026#34;, err) }) q.AddURL(\u0026#34;https://www.jianshu.com\u0026#34;) q.Run(c) } 5. redis任务队列 设置redis存储后，队列中URL存储在redis中，访问页面的cookie及访问记录也会保存在redis中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gocolly/colly\u0026#34; \u0026#34;github.com/gocolly/colly/debug\u0026#34; \u0026#34;github.com/gocolly/colly/queue\u0026#34; \u0026#34;github.com/gocolly/redisstorage\u0026#34; ) func main() { c := colly.NewCollector(colly.UserAgent(\u0026#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u0026#34;), colly.MaxDepth(3), colly.Debugger(\u0026amp;debug.LogDebugger{})) storage := \u0026amp;redisstorage.Storage{ Address: \u0026#34;192.168.1.10:6379\u0026#34;, Password: \u0026#34;123456\u0026#34;, DB: 0, Prefix: \u0026#34;colly\u0026#34;, Client: nil, Expires: 0, } c.SetStorage(storage) err := storage.Clear() if err != nil{ panic(err) } defer storage.Client.Close() q, _ := queue.New(5, storage) c.OnHTML(\u0026#34;a\u0026#34;, func(element *colly.HTMLElement) { element.Request.Visit(element.Attr(\u0026#34;href\u0026#34;)) }) c.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;爬取页面：\u0026#34;, r.URL) }) c.OnError(func(r *colly.Response, err error) { fmt.Println(\u0026#34;Request URL:\u0026#34;, r.Request.URL, \u0026#34;failed with response:\u0026#34;, r, \u0026#34;\\nError:\u0026#34;, err) }) q.AddURL(\u0026#34;https://www.jianshu.com\u0026#34;) q.Run(c) } redis中数据\n6.配置代理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/gocolly/colly\u0026#34; \u0026#34;github.com/gocolly/colly/proxy\u0026#34; ) func main() { c := colly.NewCollector() //配置两个代理 rp, err := proxy.RoundRobinProxySwitcher(\u0026#34;http://127.0.0.1:1080\u0026#34;, \u0026#34;socks5://127.0.0.1:1338\u0026#34;) if err != nil { log.Fatal(err) } c.SetProxyFunc(rp) c.OnResponse(func(r *colly.Response) { log.Printf(\u0026#34;%s\\n\u0026#34;, bytes.Replace(r.Body, []byte(\u0026#34;\\n\u0026#34;), nil, -1)) }) c.Visit(\u0026#34;https://httpbin.org/ip\u0026#34;) } ","permalink":"https://cold-bin.github.io/post/go%E7%88%AC%E8%99%AB/","tags":["colly框架","goquery框架"],"title":"Go爬虫"},{"categories":["数据结构与算法","Leecode"],"contents":"Leetcode刷题笔记 [toc]\nday01 移除元素 题目\n给你一个数组 nums 和一个值val，你需要 原地 移除所有数值等于val的元素，并返回移除后数组的新长度。\n不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组。\n元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。\n暴力题解\n这个题目暴力的解法就是两层for循环，一个for循环遍历数组元素 ，第二个for循环更新数组。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public int removeElement(int[] nums, int val) { int j=0; int i=0; int len=nums.length; for(i=0;i\u0026lt;len;i++){ if(val==nums[i]){ //将之后的元素全部往前挪移覆盖一位 for(j=i+1;j\u0026lt;len;j++){ nums[j-1]=nums[j]; } //将数组索引也往前挪 i--; len--; } } return len; } } 双指针法求解\n由于题目要求删除数组中等于val的元素，因此输出数组的长度一定小于等于输入数组的长度，我们可以把输出的数组直接写在输入数组上。可以使用双指针：右指针right 指向当前将要处理的元素，左指针 left 指向下一个将要赋值的位置。\n如果右指针指向的元素不等于 val，它一定是输出数组的一个元素，我们就将右指针指向的元素复制到左指针位置，然后将左右指针同时右移；\n如果右指针指向的元素等于 val，它不能在输出数组里，此时左指针不动，右指针右移一位。\n整个过程保持不变的性质是：区间 [0,left)中的元素都不等于 val。当左右指针遍历完输入数组以后，left 的值就是输出数组的长度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public int removeElement(int[] nums, int val) { int n = nums.length; int left = 0;//指输出元素索引 for (int right = 0; right \u0026lt; n; right++){ if (nums[right] != val) { //不等于删除值，则将这个元素添加至输出元素 nums[left] = nums[right]; left++; } } return left; } } day02 最大子数组和 题目\n给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。子数组 是数组中的一个连续部分。\n暴力破解\n找规律\n对于含有正数的序列而言,最大子序列肯定是正数,所以头尾肯定都是正数.我们可以从第一个正数开始算起,每往后加一个数便更新一次和的最大值;当当前和成为负数时,则表明此前序列无法为后面提供最大子序列和,因此必须重新确定序列首项\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int maxSubArray(int[] nums) { int maxSum = Integer.MIN_VALUE;//最大和 int thisSum = 0;//当前和 int len = nums.length; for(int i = 0; i \u0026lt; len; i++) { thisSum += nums[i]; if(maxSum \u0026lt; thisSum) { maxSum = thisSum; } //如果当前和小于0则归零，因为对于后面的元素来说这些是减小的。于是归零，意即从此处算开始最大和 if(thisSum \u0026lt; 0) { thisSum = 0; } } return maxSum; } } 加一 题目\n给定一个由 整数 组成的 非空 数组所表示的非负整数，在该数的基础上加一。最高位数字存放在数组的首位， 数组中每个元素只存储单个数字。你可以假设除了整数 0 之外，这个整数不会以零开头。\n找规律\n将这个看作是两个数组相加，辅助数组始终要比给定数组多一位，用来承载可能两数之和比给定数组多一位。两数相加，先从末尾相加，依次往前，满十进一\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Solution { public int[] plusOne(int[] digits) { int len1=digits.length; int len2=len1+1; int i,j; int[] arr=new int[len2]; arr[len2-1]=1; for(i=len1-1,j=len2-1;;i--,j--){ int sum=digits[i]+arr[j]; if(sum\u0026gt;9){ //进1 arr[j-1]++; //取余 arr[j]=(sum)%10; }else{ arr[j]=sum; } //跳出条件放到此处，防止少算一次 if (i==0){ break; } } //根据首位判断是否输出首位 if (arr[0]==0){ for (i=1,j=0;i\u0026lt;len2;){ digits[j++]=arr[i++]; } return digits; } return arr; } } day03 合并两个有序数组 题目\n给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。请你 合并 nums2 到 nums1 中，使合并后的数组同样按 非递减顺序 排列。\n注意：最终，合并后数组不应由函数返回，而是存储在数组 nums1 中。为了应对这种情况，nums1 的初始长度为 m + n，其中前 m 个元素表示应合并的元素，后 n 个元素为 0 ，应忽略。nums2 的长度为 n 。\n方法一：先合并再排序（缺点，没有利用两数组已经排序的条件）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { //先合并末尾数据 for (int i = m, j = 0; i \u0026lt; nums1.length \u0026amp;\u0026amp; j \u0026lt; n; i++, j++) { nums1[i] = nums2[j]; } //按递增排序(希尔排序) for (int gap = nums1.length / 2; gap \u0026gt; 0; gap /= 2) {//步长 //对每组进行直接插入排序 for (int i = gap; i \u0026lt; nums1.length; i++) { int index = i; int value = nums1[i]; while (index - gap \u0026gt;= 0 \u0026amp;\u0026amp; nums1[index - gap] \u0026gt; value) { nums1[index] = nums1[index - gap]; index -= gap; } nums1[index] = value; } } } } 方法二：双指针\n方法一没有利用数组nums1与 nums2已经被排序的性质。为了利用这一性质，我们可以使用双指针方法。这一方法将两个数组看作队列，每次从两个数组头部取出比较小的数字放到结果中。如下面的动画所示：\n我们为两个数组分别设置一个指针p1与p2来作为队列的头部指针。代码实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { int p1 = 0, p2 = 0; int[] sorted = new int[m + n]; int cur; while (p1 \u0026lt; m || p2 \u0026lt; n) { if (p1 == m) { cur = nums2[p2++]; } else if (p2 == n) { cur = nums1[p1++]; } else if (nums1[p1] \u0026lt; nums2[p2]) { cur = nums1[p1++]; } else { cur = nums2[p2++]; } sorted[p1 + p2 - 1] = cur; } for (int i = 0; i != m + n; ++i) { nums1[i] = sorted[i]; } } } day04 59. 螺旋矩阵 II 题目\n给你一个正整数 n ，生成一个包含 1 到 n2 所有元素，且元素按顺时针顺序螺旋排列的 n x n 正方形矩阵 matrix 。\n示例 1：\n1 2 输入：n = 3 输出：[[1,2,3],[8,9,4],[7,6,5]] 示例 2：\n1 2 输入：n = 1 输出：[[1]] 提示：\n1 \u0026lt;= n \u0026lt;= 20 方法一：暴力破解，环形遍历\u0026ndash;\u0026gt;也即按层模拟\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 package other; import java.util.Arrays; class Solution5 { public static void main(String[] args) { System.out.println(Arrays.deepToString(generateMatrix(4))); } public static int[][] generateMatrix(int n) { int[][] arr = new int[n][n]; int val = 1;//表示填入值 int i = 0, j = 0;//arr数组索引，i为行标，j列标 int tempN1 = n;//表示每列或每行在某环的行走最大值 int tempN2 = 0;//表示每列或每行在某环的行走最小值 for (int l = 0; l \u0026lt; n - n / 2; l++) {//环数 while (j \u0026lt; tempN1) { arr[i][j] = val++; j++; } j--;//还原j值 i++;//下一个数 while (i \u0026lt; tempN1) { arr[i][j] = val++; i++; } i--;//还原i值 j--;//下一个数 while (j \u0026gt;= tempN2) { arr[i][j] = val++; j--; } j++;//还原j值 i--;//下一个数 while (i \u0026gt; tempN2) { arr[i][j] = val++; i--; } i++;//还原i数 j++;//指向下一个数字 tempN1--; tempN2++; } return arr; } } day05 209. 长度最小的子数组 题目\n给定一个含有 n 个正整数的数组和一个正整数 target 。\n找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, \u0026hellip;, numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。\n方法一：暴力破解（超时）\n题目要求求出和大于等于target的长度最小的连续子数组，于是乎，就使用一个“窗口”从长度最小的1开始到整个数组的长度遍历，每次遍历都要“框出”子数组的和并比较，满足要求就返回。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public int minSubArrayLen(int target, int[] nums) { int left=0;//每个窗口的左索引 int sum=0; for(int cap=1;cap\u0026lt;=nums.length;cap++){//滑动窗口大小 for(left=0;left\u0026lt;nums.length-cap+1;left++){//左索引位置 sum=0; //计算窗口内的和 for(int i=left;i\u0026lt;left+cap;i++){ sum+=nums[i]; } if (sum\u0026gt;=target) return cap; } } return 0; } 方法二：滑动窗口\n使用左右指针 left 和 right，left 和 right 之间的长度即为滑动窗口的大小（即连续数组的大小）。如果滑动窗口内的值 sum \u0026gt;= target，维护连续数组最短长度，left 向右移动，缩小滑动窗口。如果滑动窗口内的值 sum \u0026lt; target，则 right 向有移动，扩大滑动窗口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public int minSubArrayLen(int target, int[] nums) { int left=0; int right=0; int sum=0; int len=Integer.MAX_VALUE; while(right\u0026lt;nums.length){ sum+=nums[right]; while(sum\u0026gt;=target){ if(len\u0026gt;right-left+1) len=right-left+1;//与上次的进行比较，保留最短结果 sum-=nums[left++]; } right++; } if (len==Integer.MAX_VALUE) return 0; return len; } day06 977. 有序数组的平方 题目\n给你一个按 非递减顺序 排序的整数数组 nums，返回 每个数字的平方 组成的新数组，要求也按 非递减顺序 排序。\n示例 1：\n1 2 3 4 输入：nums = [-4,-1,0,3,10] 输出：[0,1,9,16,100] 解释：平方后，数组变为 [16,1,0,9,100] 排序后，数组变为 [0,1,9,16,100] 示例2：\n1 2 输入：nums = [-7,-3,2,3,11] 输出：[4,9,9,49,121] 方法一：直接排序\n最简单的方法就是将数组nums中的数平方后直接排序。\n1 2 3 4 5 6 7 8 9 10 class Solution { public int[] sortedSquares(int[] nums) { int[] ans = new int[nums.length]; for (int i = 0; i \u0026lt; nums.length; ++i) { ans[i] = nums[i] * nums[i]; } Arrays.sort(ans); return ans; } } 方法二：双指针\n同样地，我们可以使用两个指针分别指向位置 0 和 n-1，每次比较两个指针对应的数，选择较大的那个逆序放入答案并移动指针。这种方法无需处理某一指针移动至边界的情况，读者可以仔细思考其精髓所在。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public int[] sortedSquares(int[] nums) { int n = nums.length; int[] ans = new int[n]; for (int i = 0, j = n - 1, pos = n - 1; i \u0026lt;= j;) { if (nums[i] * nums[i] \u0026gt; nums[j] * nums[j]) { ans[pos] = nums[i] * nums[i]; ++i; } else { ans[pos] = nums[j] * nums[j]; --j; } --pos; } return ans; } } day07 19. 删除链表的倒数第 N 个结点 题目\n给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。\n示例 1：\n1 2 输入：head = [1,2,3,4,5], n = 2 输出：[1,2,3,5] 示例 2：\n1 2 输入：head = [1], n = 1 输出：[] 示例 3：\n1 2 输入：head = [1,2], n = 1 输出：[1] 提示：\n链表中结点的数目为 sz 1 \u0026lt;= sz \u0026lt;= 30 0 \u0026lt;= Node.val \u0026lt;= 100 1 \u0026lt;= n \u0026lt;= sz **进阶：**你能尝试使用一趟扫描实现吗？\n解决思路\n前言 在对链表进行操作时，一种常用的技巧是添加一个哑节点（dummy node），它的 next 指针指向链表的头节点。这样一来，我们就不需要对头节点进行特殊的判断了。\n例如，在本题中，如果我们要删除节点 y，我们需要知道节点 y 的前驱节点 x，并将 x 的指针指向 y 的后继节点。但由于头节点不存在前驱节点，因此我们需要在删除头节点时进行特殊判断。但如果我们添加了哑节点，那么头节点的前驱节点就是哑节点本身，此时我们就只需要考虑通用的情况即可。（注意：此处头节点存放了有意义的数据）\n特别地，在某些语言中，由于需要自行对内存进行管理。因此在实际的面试中，对于「是否需要释放被删除节点对应的空间」这一问题，我们需要和面试官进行积极的沟通以达成一致。下面的代码中默认不释放空间。\n方法一：计算链表长度 思路与算法\n一种容易想到的方法是，我们首先从头节点开始对链表进行一次遍历，得到链表的长度 LL。随后我们再从头节点开始对链表进行一次遍历，当遍历到第 L-n+1个节点时，它就是我们需要删除的节点。\n为了与题目中的 n 保持一致，节点的编号从 1 开始，头节点为编号 1 的节点。\n为了方便删除操作，我们可以从哑节点开始遍历 L-n+1个节点。当遍历到第 L-n+1 个节点时，它的下一个节点就是我们需要删除的节点，这样我们只需要修改一次指针，就能完成删除操作。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head); int length = getLength(head); ListNode cur = dummy; for (int i = 1; i \u0026lt; length - n + 1; ++i) { cur = cur.next; } cur.next = cur.next.next; ListNode ans = dummy.next; return ans; } public int getLength(ListNode head) { int length = 0; while (head != null) { ++length; head = head.next; } return length; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { l:=0 heads:=\u0026amp;ListNode{Next:head} tmp:=heads for tmp!=nil{ l++ tmp=tmp.Next } tmp = heads no:=0 for tmp!=nil { // 找到了n节点 if no==l-n-1 { if tmp.Next!=nil{ tmp.Next = tmp.Next.Next }else{ tmp.Next = nil } } no++ tmp=tmp.Next } return heads.Next } 复杂度分析\n时间复杂度：O(L)，其中 L是链表的长度。 空间复杂度：O(1)。 方法二：栈 思路与算法\n我们也可以在遍历链表的同时将所有节点依次入栈。根据栈「先进后出」的原则，我们弹出栈的第 n 个节点就是需要删除的节点，并且目前栈顶的节点就是待删除节点的前驱节点。这样一来，删除操作就变得十分方便了。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head); Deque\u0026lt;ListNode\u0026gt; stack = new LinkedList\u0026lt;ListNode\u0026gt;(); ListNode cur = dummy; while (cur != null) { stack.push(cur); cur = cur.next; } for (int i = 0; i \u0026lt; n; ++i) { stack.pop(); } ListNode prev = stack.peek(); prev.next = prev.next.next; ListNode ans = dummy.next; return ans; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { heads:=\u0026amp;ListNode{Next:head} stack := []*ListNode{} // 先所有节点入栈 tmp:=heads for tmp!=nil { stack = append(stack,tmp) tmp = tmp.Next } var lastN *ListNode for n\u0026gt;0{ lastN = stack[len(stack)-1] stack = stack[:len(stack)-1] n-- } stack[len(stack)-1].Next = lastN.Next return heads.Next } 复杂度分析\n时间复杂度：O(L)，其中 L 是链表的长度。 空间复杂度：O(L)，其中 L 是链表的长度。主要为栈的开销。 方法三：双指针 思路与算法\n我们也可以在不预处理出链表的长度，以及使用常数空间的前提下解决本题。\n由于我们需要找到倒数第 n 个节点，因此我们可以使用两个指针first 和 second 同时对链表进行遍历，并且 first 比 second 超前 n 个节点。当 first 遍历到链表的末尾时，second 就恰好处于倒数第 n 个节点。\n具体地，初始时 first 和second 均指向头节点。我们首先使用first 对链表进行遍历，遍历的次数为 n。此时，first 和 second 之间间隔了 n-1个节点，即first 比 second 超前了 n个节点。\n在这之后，我们同时使用 first和 second 对链表进行遍历。当first 遍历到链表的末尾（即first 为空指针）时，second 恰好指向倒数第 n 个节点。\n根据方法一和方法二，如果我们能够得到的是倒数第 n 个节点的前驱节点而不是倒数第 n 个节点的话，删除操作会更加方便。因此我们可以考虑在初始时将 second 指向哑节点，其余的操作步骤不变。这样一来，当 first 遍历到链表的末尾时，second 的下一个节点就是我们需要删除的节点。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head); ListNode first = head; ListNode second = dummy; for (int i = 0; i \u0026lt; n; ++i) { first = first.next; } while (first != null) { first = first.next; second = second.next; } second.next = second.next.next; ListNode ans = dummy.next; return ans; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { heads:=\u0026amp;ListNode{Next:head} first:=heads second:=heads for n\u0026gt;0 { first = first.Next n-- } for first.Next!=nil{ first=first.Next second = second.Next } // fmt.Println(second.Val) second.Next = second.Next.Next return heads.Next } 复杂度分析\n时间复杂度：O(L)，其中 L 是链表的长度。 空间复杂度：O(1)。 day08 24. 两两交换链表中的节点 题目\n给你一个链表，两两交换其中相邻的节点，并返回交换后链表的头节点。你必须在不修改节点内部的值的情况下完成本题（即，只能进行节点交换）。\n示例 1：\n1 2 输入：head = [1,2,3,4] 输出：[2,1,4,3] 示例 2：\n1 2 输入：head = [] 输出：[] 示例 3：\n1 2 输入：head = [1] 输出：[1] 提示：\n链表中节点的数目在范围 [0, 100] 内 0 \u0026lt;= Node.val \u0026lt;= 100 方法\n方法一：递归\n思路与算法\n可以通过递归的方式实现两两交换链表中的节点。\n递归的终止条件是链表中没有节点，或者链表中只有一个节点，此时无法进行交换。\n如果链表中至少有两个节点，则在两两交换链表中的节点之后，原始链表的头节点变成新的链表的第二个节点，原始链表的第二个节点变成新的链表的头节点。链表中的其余节点的两两交换可以递归地实现。在对链表中的其余节点递归地两两交换之后，更新节点之间的指针关系，即可完成整个链表的两两交换。\n用 head 表示原始链表的头节点，新的链表的第二个节点，用 newHead 表示新的链表的头节点，原始链表的第二个节点，则原始链表中的其余节点的头节点是 newHead.next。令 head.next = swapPairs(newHead.next)，表示将其余节点进行两两交换，交换后的新的头节点为 head 的下一个节点。然后令 newHead.next = head，即完成了所有节点的交换。最后返回新的链表的头节点 newHead。\n代码\n1 2 3 4 5 6 7 8 9 10 11 class Solution { public ListNode swapPairs(ListNode head) { if (head == null || head.next == null) { return head; } ListNode newHead = head.next; head.next = swapPairs(newHead.next); newHead.next = head; return newHead; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是链表的节点数量。需要对每个节点进行更新指针的操作。 空间复杂度：O(n)，其中 n 是链表的节点数量。空间复杂度主要取决于递归调用的栈空间。 方法二：迭代\n思路与算法\n也可以通过迭代的方式实现两两交换链表中的节点。\n创建哑结点 dummyHead，令 dummyHead.next = head。令 temp 表示当前到达的节点，初始时 temp = dummyHead。每次需要交换 temp 后面的两个节点。\n如果 temp 的后面没有节点或者只有一个节点，则没有更多的节点需要交换，因此结束交换。否则，获得 temp 后面的两个节点 node1 和 node2，通过更新节点的指针关系实现两两交换节点。\n具体而言，交换之前的节点关系是 temp -\u0026gt; node1 -\u0026gt; node2，交换之后的节点关系要变成 temp -\u0026gt; node2 -\u0026gt; node1，因此需要进行如下操作。\n1 2 3 temp.next = node2 node1.next = node2.next node2.next = node1 完成上述操作之后，节点关系即变成 temp -\u0026gt; node2 -\u0026gt; node1。再令 temp = node1，对链表中的其余节点进行两两交换，直到全部节点都被两两交换。\n两两交换链表中的节点之后，新的链表的头节点是 dummyHead.next，返回新的链表的头节点即可。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class Solution { public ListNode swapPairs(ListNode head) { ListNode dummyHead = new ListNode(0); dummyHead.next = head; ListNode temp = dummyHead; while (temp.next != null \u0026amp;\u0026amp; temp.next.next != null) { ListNode node1 = temp.next; ListNode node2 = temp.next.next; temp.next = node2; node1.next = node2.next; node2.next = node1; temp = node1; } return dummyHead.next; } } //the other /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ class Solution { public ListNode swapPairs(ListNode head) { //哑节点 ListNode dummyNode = new ListNode(-1,head); //每两个节点的遍历链表，当不足两个时则不交换 ListNode temp=dummyNode; ListNode pre=temp.next; if(temp.next==null){ return dummyNode.next; } ListNode post=temp.next.next; while(pre!=null\u0026amp;\u0026amp;post!=null){ temp.next=post; pre.next=post.next; post.next=pre; //移动指针 temp=pre; pre=temp.next; if(temp.next!=null){ post=temp.next.next; } } return dummyNode.next; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是链表的节点数量。需要对每个节点进行更新指针的操作。 空间复杂度：O(1)。 day09 25. K 个一组翻转链表 题目\n给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。\n示例 1：\n1 2 输入：head = [1,2,3,4,5], k = 2 输出：[2,1,4,3,5] 示例 2：\n1 2 输入：head = [1,2,3,4,5], k = 3 输出：[3,2,1,4,5] 提示：\n链表中的节点数目为 n 1 \u0026lt;= k \u0026lt;= n \u0026lt;= 5000 0 \u0026lt;= Node.val \u0026lt;= 1000 **进阶：**你可以设计一个只用 O(1) 额外内存空间的算法解决此问题吗？\n方法\n方法一：模拟 思路与算法\n本题的目标非常清晰易懂，不涉及复杂的算法，但是实现过程中需要考虑的细节比较多，容易写出冗长的代码。主要考查面试者设计的能力。\n我们需要把链表节点按照 k 个一组分组，所以可以使用一个指针 head 依次指向每组的头节点。这个指针每次向前移动 k 步，直至链表结尾。对于每个分组，我们先判断它的长度是否大于等于 k。若是，我们就翻转这部分链表，否则不需要翻转。\n接下来的问题就是如何翻转一个分组内的子链表。翻转一个链表并不难，过程可以参考「206. 反转链表」。但是对于一个子链表，除了翻转其本身之外，还需要将子链表的头部与上一个子链表连接，以及子链表的尾部与下一个子链表连接。如下图所示：\n因此，在翻转子链表的时候，我们不仅需要子链表头节点 head，还需要有 head 的上一个节点 pre，以便翻转完后把子链表再接回 pre。\n但是对于第一个子链表，它的头节点 head 前面是没有节点 pre 的。太麻烦了！难道只能特判了吗？答案是否定的。没有条件，我们就创造条件；没有节点，我们就创建一个节点。我们新建一个节点（哑节点），把它接到链表的头部，让它作为 pre 的初始值，这样 head 前面就有了一个节点，我们就可以避开链表头部的边界条件。这么做还有一个好处，下面我们会看到。\n反复移动指针 head 与 pre，对 head 所指向的子链表进行翻转，直到结尾，我们就得到了答案。下面我们该返回函数值了。\n有的同学可能发现这又是一件麻烦事：链表翻转之后，链表的头节点发生了变化，那么应该返回哪个节点呢？照理来说，前 k 个节点翻转之后，链表的头节点应该是第 k 个节点。那么要在遍历过程中记录第 k 个节点吗？但是如果链表里面没有 k 个节点，答案又还是原来的头节点。我们又多了一大堆循环和判断要写，太崩溃了！\n等等！还记得我们创建了节点 pre 吗？这个节点一开始被连接到了头节点的前面，而无论之后链表有没有翻转，它的 next 指针都会指向正确的头节点。那么我们只要返回它的下一个节点就好了。至此，问题解决。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Solution { public ListNode reverseKGroup(ListNode head, int k) { ListNode hair = new ListNode(0); hair.next = head; ListNode pre = hair; while (head != null) { ListNode tail = pre; // 查看剩余部分长度是否大于等于 k for (int i = 0; i \u0026lt; k; ++i) { tail = tail.next; if (tail == null) { return hair.next; } } ListNode nex = tail.next; ListNode[] reverse = myReverse(head, tail); head = reverse[0]; tail = reverse[1]; // 把子链表重新接回原链表 pre.next = head; tail.next = nex; pre = tail; head = tail.next; } return hair.next; } public ListNode[] myReverse(ListNode head, ListNode tail) { ListNode prev = tail.next; ListNode p = head; while (prev != tail) { ListNode nex = p.next; p.next = prev; prev = p; p = nex; } return new ListNode[]{tail, head}; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为链表的长度。head 指针会在 $O(\\lfloor \\frac{n}{k} \\rfloor) $个节点上停留，每次停留需要进行一次 O(k) 的翻转操作。 空间复杂度：O(1)，我们只需要建立常数个变量。 day10 141. 环形链表 题目\n给你一个链表的头节点 head ，判断链表中是否有环。\n如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos 不作为参数进行传递 。仅仅是为了标识链表的实际情况。\n如果链表中存在环 ，则返回 true 。 否则，返回 false 。\n示例 1：\n1 2 3 输入：head = [3,2,0,-4], pos = 1 输出：true 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2：\n1 2 3 输入：head = [1,2], pos = 0 输出：true 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3：\n1 2 3 输入：head = [1], pos = -1 输出：false 解释：链表中没有环。 提示：\n链表中节点的数目范围是 [0, 104] -105 \u0026lt;= Node.val \u0026lt;= 105 pos 为 -1 或者链表中的一个 有效索引 。 **进阶：**你能用 O(1)（即，常量）内存解决此问题吗？\n解法\n方法一：哈希表 思路及算法\n最容易想到的方法是遍历所有节点，每次遍历到一个节点时，判断该节点此前是否被访问过。\n具体地，我们可以使用哈希表来存储所有已经访问过的节点。每次我们到达一个节点，如果该节点已经存在于哈希表中，则说明该链表是环形链表，否则就将该节点加入哈希表中。重复这一过程，直到我们遍历完整个链表即可。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 public class Solution { public boolean hasCycle(ListNode head) { Set\u0026lt;ListNode\u0026gt; seen = new HashSet\u0026lt;ListNode\u0026gt;(); while (head != null) { if (!seen.add(head)) { return true; } head = head.next; } return false; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是链表中的节点数。最坏情况下我们需要遍历每个节点一次。 空间复杂度：O(N)，其中 N 是链表中的节点数。主要为哈希表的开销，最坏情况下我们需要将每个节点插入到哈希表中一次。 方法二：快慢指针 思路及算法\n本方法需要读者对「Floyd 判圈算法」（又称龟兔赛跑算法）有所了解。\n假想「乌龟」和「兔子」在链表上移动，「兔子」跑得快，「乌龟」跑得慢。当「乌龟」和「兔子」从链表上的同一个节点开始移动时，如果该链表中没有环，那么「兔子」将一直处于「乌龟」的前方；如果该链表中有环，那么「兔子」会先于「乌龟」进入环，并且一直在环内移动。等到「乌龟」进入环时，由于「兔子」的速度快，它一定会在某个时刻与乌龟相遇，即套了「乌龟」若干圈。\n我们可以根据上述思路来解决本题。具体地，我们定义两个指针，一快一满。慢指针每次只移动一步，而快指针每次移动两步。初始时，慢指针在位置 head，而快指针在位置 head.next。这样一来，如果在移动的过程中，快指针反过来追上慢指针，就说明该链表为环形链表。否则快指针将到达链表尾部，该链表不为环形链表。\n细节\n为什么我们要规定初始时慢指针在位置 head，快指针在位置 head.next，而不是两个指针都在位置 head（即与「乌龟」和「兔子」中的叙述相同）？\n观察下面的代码，我们使用的是 while 循环，循环条件先于循环体。由于循环条件一定是判断快慢指针是否重合，如果我们将两个指针初始都置于 head，那么 while 循环就不会执行。因此，我们可以假想一个在 head 之前的虚拟节点，慢指针从虚拟节点移动一步到达 head，快指针从虚拟节点移动两步到达 head.next，这样我们就可以使用 while 循环了。 当然，我们也可以使用 do-while 循环。此时，我们就可以把快慢指针的初始值都置为 head。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Solution { public boolean hasCycle(ListNode head) { if (head == null || head.next == null) { return false; } ListNode slow = head; ListNode fast = head.next; while (slow != fast) { if (fast == null || fast.next == null) { return false; } slow = slow.next; fast = fast.next.next; } return true; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是链表中的节点数。 当链表中不存在环时，快指针将先于慢指针到达链表尾部，链表中每个节点至多被访问两次。 当链表中存在环时，每一轮移动后，快慢指针的距离将减小一。而初始距离为环的长度，因此至多移动 N 轮。 空间复杂度：O(1)。我们只使用了两个指针的额外空间。 day11 142. 环形链表 II 题目\n给定一个链表的头节点 head ，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。\n如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。如果 pos 是 -1，则在该链表中没有环。注意：pos 不作为参数进行传递，仅仅是为了标识链表的实际情况。\n不允许修改 链表。\n示例 1：\n1 2 3 输入：head = [3,2,0,-4], pos = 1 输出：返回索引为 1 的链表节点 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2：\n1 2 3 输入：head = [1,2], pos = 0 输出：返回索引为 0 的链表节点 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3：\n1 2 3 输入：head = [1], pos = -1 输出：返回 null 解释：链表中没有环。 提示：\n链表中节点的数目范围在范围 [0, 104] 内 -105 \u0026lt;= Node.val \u0026lt;= 105 pos 的值为 -1 或者链表中的一个有效索引 **进阶：**你是否可以使用 O(1) 空间解决此题？\n方法\n方法一：哈希表\n思路与算法\n一个非常直观的思路是：我们遍历链表中的每个节点，并将它记录下来；一旦遇到了此前遍历过的节点，就可以判定链表中存在环。借助哈希表可以很方便地实现。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class Solution { public ListNode detectCycle(ListNode head) { ListNode pos = head; Set\u0026lt;ListNode\u0026gt; visited = new HashSet\u0026lt;ListNode\u0026gt;(); while (pos != null) { if (visited.contains(pos)) { return pos; } else { visited.add(pos); } pos = pos.next; } return null; } } 复杂度分析\n时间复杂度：O(N)，其中 N 为链表中节点的数目。我们恰好需要访问链表中的每一个节点。\n空间复杂度：O(N)，其中 N 为链表中节点的数目。我们需要将链表中的每个节点都保存在哈希表当中。\n方法二：快慢指针\n思路与算法\n我们使用两个指针，fast 与 slow。它们起始都位于链表的头部。随后，slow 指针每次向后移动一个位置，而 fast 指针向后移动两个位置。如果链表中存在环，则 fast 指针最终将再次与 slow 指针在环中相遇。\n如下图所示，设链表中环外部分的长度为 a。slow 指针进入环后，又走了 b 的距离与 fast 相遇。此时，fast 指针已经走完了环的 n 圈，因此它走过的总距离为 a+n*(b+c)+b = a+(n+1)*b+n*c。\n根据题意，任意时刻，fast 指针走过的距离都为 slow 指针的 2 倍。因此，我们有\n`a+(n+1)*b+n*c = 2*(a+b) ==\u0026gt; a = c+(n-1)*(b+c)`\r有了 a=c+(n-1)(b+c) 的等量关系，我们会发现：从相遇点到入环点的距离加上 n-1 圈的环长，恰好等于从链表头部到入环点的距离。\n因此，当发现 slow 与 fast 相遇时，我们再额外使用一个指针 ptr。起始，它指向链表头部；随后，它和 slow 每次向后移动一个位置。最终，它们会在入环点相遇。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public class Solution { public ListNode detectCycle(ListNode head) { if (head == null) { return null; } ListNode slow = head, fast = head; while (fast != null) { slow = slow.next; if (fast.next != null) { fast = fast.next.next; } else { return null; } if (fast == slow) { ListNode ptr = head; while (ptr != slow) { ptr = ptr.next; slow = slow.next; } return ptr; } } return null; } } 复杂度分析\n时间复杂度：O(N)，其中 N 为链表中节点的数目。在最初判断快慢指针是否相遇时，slow 指针走过的距离不会超过链表的总长度；随后寻找入环点时，走过的距离也不会超过链表的总长度。因此，总的执行时间为 O(N)+O(N)=O(N)。 空间复杂度：O(1)。我们只使用了 slow,fast,ptr 三个指针。 day12 160. 相交链表 题目\n给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 null 。\n图示两个链表在节点 c1 开始相交**：**\n题目数据 保证 整个链式结构中不存在环。\n注意，函数返回结果后，链表必须 保持其原始结构 。\n自定义评测：\n评测系统 的输入如下（你设计的程序 不适用 此输入）：\nintersectVal - 相交的起始节点的值。如果不存在相交节点，这一值为 0 listA - 第一个链表 listB - 第二个链表 skipA - 在 listA 中（从头节点开始）跳到交叉节点的节点数 skipB - 在 listB 中（从头节点开始）跳到交叉节点的节点数 评测系统将根据这些输入创建链式数据结构，并将两个头节点 headA 和 headB 传递给你的程序。如果程序能够正确返回相交节点，那么你的解决方案将被 视作正确答案 。\n示例 1：\n1 2 3 4 5 输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,6,1,8,4,5], skipA = 2, skipB = 3 输出：Intersected at \u0026#39;8\u0026#39; 解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。 从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,6,1,8,4,5]。 在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2：\n1 2 3 4 5 输入：intersectVal = 2, listA = [1,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1 输出：Intersected at \u0026#39;2\u0026#39; 解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。 从各自的表头开始算起，链表 A 为 [1,9,1,2,4]，链表 B 为 [3,2,4]。 在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3：\n1 2 3 4 5 输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2 输出：null 解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。 由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。 这两个链表不相交，因此返回 null 。 提示：\nlistA 中节点数目为 m listB 中节点数目为 n 1 \u0026lt;= m, n \u0026lt;= 3 * 104 1 \u0026lt;= Node.val \u0026lt;= 105 0 \u0026lt;= skipA \u0026lt;= m 0 \u0026lt;= skipB \u0026lt;= n 如果 listA 和 listB 没有交点，intersectVal 为 0 如果 listA 和 listB 有交点，intersectVal == listA[skipA] == listB[skipB] **进阶：**你能否设计一个时间复杂度 O(m + n) 、仅用 O(1) 内存的解决方案？\n方法\n方法一：哈希集合 思路和算法\n判断两个链表是否相交，可以使用哈希集合存储链表节点。\n首先遍历链表 headA，并将链表 headA 中的每个节点加入哈希集合中。然后遍历链表 headB，对于遍历到的每个节点，判断该节点是否在哈希集合中：\n如果当前节点不在哈希集合中，则继续遍历下一个节点； 如果当前节点在哈希集合中，则后面的节点都在哈希集合中，即从当前节点开始的所有节点都在两个链表的相交部分，因此在链表 headB 中遍历到的第一个在哈希集合中的节点就是两个链表相交的节点，返回该节点。 如果链表 headB 中的所有节点都不在哈希集合中，则两个链表不相交，返回 null。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { Set\u0026lt;ListNode\u0026gt; visited = new HashSet\u0026lt;ListNode\u0026gt;(); ListNode temp = headA; while (temp != null) { visited.add(temp); temp = temp.next; } temp = headB; while (temp != null) { if (visited.contains(temp)) { return temp; } temp = temp.next; } return null; } } 复杂度分析\n时间复杂度：O(m+n)，其中 m 和 n 是分别是链表 headA 和 headB 的长度。需要遍历两个链表各一次。 空间复杂度：O(m)，其中 m 是链表 headA 的长度。需要使用哈希集合存储链表 headA 中的全部节点。 方法二：双指针 思路和算法\n使用双指针的方法，可以将空间复杂度降至 O(1)O(1)。\n只有当链表 headA 和 headB 都不为空时，两个链表才可能相交。因此首先判断链表 headA 和 headB 是否为空，如果其中至少有一个链表为空，则两个链表一定不相交，返回 null。\n当链表 headA 和 headB 都不为空时，创建两个指针 pA 和 pB*，初始时分别指向两个链表的头节点 headA 和 headB，然后将两个指针依次遍历两个链表的每个节点。具体做法如下：\n每步操作需要同时更新指针 pA 和 pB。 如果指针 pA 不为空，则将指针 pA 移到下一个节点；如果指针 pB 不为空，则将指针 pB 移到下一个节点。 如果指针 pA 为空，则将指针 pA 移到链表 headB 的头节点；如果指针 pB 为空，则将指针 pB 移到链表 headA 的头节点。 当指针 pA 和 pB 指向同一个节点或者都为空时，返回它们指向的节点或者 null。 证明\n下面提供双指针方法的正确性证明。考虑两种情况，第一种情况是两个链表相交，第二种情况是两个链表不相交。\n情况一：两个链表相交\n链表 headA 和 headB 的长度分别是 m 和 n*。假设链表 headA 的不相交部分有 a 个节点，链表 headB 的不相交部分有 b 个节点，两个链表相交的部分有 c 个节点，则有 a+c=m，b+c=n。\n如果 a=b，则两个指针会同时到达两个链表相交的节点，此时返回相交的节点； 如果 a!=b，则指针 pA 会遍历完链表 headA，指针 pB 会遍历完链表 headB，两个指针不会同时到达链表的尾节点，然后指针 pA 移到链表 headB 的头节点，指针 pB 移到链表 headA 的头节点，然后两个指针继续移动，在指针 pA 移动了 a+c+b 次、指针 pB 移动了 b+c+a 次之后，两个指针会同时到达两个链表相交的节点，该节点也是两个指针第一次同时指向的节点，此时返回相交的节点。 情况二：两个链表不相交\n链表 headA 和 headB 的长度分别是 m 和 n。考虑当 m=n 和 m != n 时，两个指针分别会如何移动：\n如果 m=n，则两个指针会同时到达两个链表的尾节点，然后同时变成空值 null，此时返回 null； 如果 m != n，则由于两个链表没有公共节点，两个指针也不会同时到达两个链表的尾节点，因此两个指针都会遍历完两个链表，在指针 pA 移动了 m+n 次、指针 pB 移动了 n+m 次之后，两个指针会同时变成空值 null，此时返回 null。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if (headA == null || headB == null) { return null; } ListNode pA = headA, pB = headB; while (pA != pB) { pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; } return pA; } } 复杂度分析\n时间复杂度：O(m+n)，其中 m 和 n 是分别是链表 headA 和 headB 的长度。两个指针同时遍历两个链表，每个指针遍历两个链表各一次。 空间复杂度：O(1)。 day13 206. 反转链表 题目\n给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。\n示例 1：\n1 2 输入：head = [1,2,3,4,5] 输出：[5,4,3,2,1] 示例 2：\n1 2 输入：head = [1,2] 输出：[2,1] 示例 3：\n1 2 输入：head = [] 输出：[] 提示：\n链表中节点的数目范围是 [0, 5000] -5000 \u0026lt;= Node.val \u0026lt;= 5000 解法\n方法一：迭代 假设存在链表 1\u0026ndash;\u0026gt;2\u0026ndash;\u0026gt;3\u0026ndash;\u0026gt;∅，我们想要把它改成 ∅\u0026lt;\u0026ndash;1\u0026lt;\u0026ndash;2\u0026lt;\u0026ndash;3。\n在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null) { ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; } return prev; } } 复杂度分析\n时间复杂度：O(n)，假设 n 是列表的长度，时间复杂度是 O(n)。 空间复杂度：O(1)。 方法二：递归 递归版本稍微复杂一些，其关键在于反向工作。假设列表的其余部分已经被反转，现在我们应该如何反转它前面的部分？\n假设列表为：\nn~1~--\u0026gt;…--\u0026gt;n~k−1~--\u0026gt;n~k~--\u0026gt;n~k+1~--\u0026gt;…--\u0026gt;n~m~--\u0026gt;∅\r若从节点 n~k+1~ 到 n~k~\nn~1~--\u0026gt;…--\u0026gt;n~k−1~--\u0026gt;n~k~--\u0026gt;n~k+1~\u0026lt;--…\u0026lt;--n~m~\r我们希望 n~k+1~ 的下一个节点指向 n~k~\n所以，n~k~.next.next*=*n~k~\n要小心的是 n~1~ 的下一个必须指向 ∅ 。如果你忽略了这一点，你的链表中可能会产生循环。如果使用大小为 22 的链表测试代码，则可能会捕获此错误。\n1 2 3 4 5 6 7 8 9 10 11 class Solution { public ListNode reverseList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode p = reverseList(head.next); head.next.next = head; head.next = null; return p; } } 复杂度分析\n时间复杂度：O(n)，假设 n 是列表的长度，那么时间复杂度为 O(n)。 空间复杂度：O(n)，由于使用递归，将会使用隐式栈空间。递归深度可能会达到 n 层。 day14 20. 有效的括号 题目\n给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串 s ，判断字符串是否有效。\n有效字符串需满足：\n左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 示例 1：\n1 2 输入：s = \u0026#34;()\u0026#34; 输出：true 示例 2：\n1 2 输入：s = \u0026#34;()[]{}\u0026#34; 输出：true 示例 3：\n1 2 输入：s = \u0026#34;(]\u0026#34; 输出：false 示例 4：\n1 2 输入：s = \u0026#34;([)]\u0026#34; 输出：false 示例 5：\n1 2 输入：s = \u0026#34;{[]}\u0026#34; 输出：true 提示：\n1 \u0026lt;= s.length \u0026lt;= 104 s 仅由括号 '()[]{}' 组成 题解\n方法一：栈 判断括号的有效性可以使用「栈」这一数据结构来解决。\n我们遍历给定的字符串 s。当我们遇到一个左括号时，我们会期望在后续的遍历中，有一个相同类型的右括号将其闭合。由于后遇到的左括号要先闭合，因此我们可以将这个左括号放入栈顶。\n当我们遇到一个右括号时，我们需要将一个相同类型的左括号闭合。此时，我们可以取出栈顶的左括号并判断它们是否是相同类型的括号。如果不是相同的类型，或者栈中并没有左括号，那么字符串 s 无效，返回 False。为了快速判断括号的类型，我们可以使用哈希表存储每一种括号。哈希表的键为右括号，值为相同类型的左括号。\n在遍历结束后，如果栈中没有左括号，说明我们将字符串 s 中的所有左括号闭合，返回 True，否则返回 False。\n注意到有效字符串的长度一定为偶数，因此如果字符串的长度为奇数，我们可以直接返回 False，省去后续的遍历判断过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public boolean isValid(String s) { int n = s.length(); if (n % 2 == 1) { return false; } Map\u0026lt;Character, Character\u0026gt; pairs = new HashMap\u0026lt;Character, Character\u0026gt;() {{ put(\u0026#39;)\u0026#39;, \u0026#39;(\u0026#39;); put(\u0026#39;]\u0026#39;, \u0026#39;[\u0026#39;); put(\u0026#39;}\u0026#39;, \u0026#39;{\u0026#39;); }}; Deque\u0026lt;Character\u0026gt; stack = new LinkedList\u0026lt;Character\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { char ch = s.charAt(i); if (pairs.containsKey(ch)) { if (stack.isEmpty() || stack.peek() != pairs.get(ch)) { return false; } stack.pop(); } else { stack.push(ch); } } return stack.isEmpty(); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func isValid(s string) bool { stack := []byte{} leftOk:=func(ch byte) bool{ return ch==\u0026#39;(\u0026#39;||ch==\u0026#39;[\u0026#39;||ch==\u0026#39;{\u0026#39; } check :=func(topc,c byte) bool { return (c==\u0026#39;)\u0026#39;\u0026amp;\u0026amp;topc==\u0026#39;(\u0026#39;)||(c==\u0026#39;]\u0026#39;\u0026amp;\u0026amp;topc==\u0026#39;[\u0026#39;)||(c==\u0026#39;}\u0026#39;\u0026amp;\u0026amp;topc==\u0026#39;{\u0026#39;) } // 遇到左括号就压入栈中，遇到右括号就从栈顶取出括号是否配对，如果不配对就返回false，配对继续遍历 for i:=0;i\u0026lt;len(s);i++{ if leftOk(s[i]){ stack = append(stack,s[i]) }else{ if len(stack)!=0\u0026amp;\u0026amp;check(stack[len(stack)-1],s[i]){ stack = stack[:len(stack)-1] }else{ return false } } } // fmt.Println(string(stack)) return len(stack)==0 } 复杂度分析\n时间复杂度：O(n)，其中 n 是字符串 s 的长度。 空间复杂度：O*(*n+∣Σ∣)，其中 Σ 表示字符集，本题中字符串只包含 66 种括号，∣Σ∣=6。栈中的字符数量为 O(n)，而哈希表使用的空间为 O(∣Σ∣)，相加即可得到总空间复杂度。 day15 225. 用队列实现栈 题目\n请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。\n实现 MyStack 类：\nvoid push(int x) 将元素 x 压入栈顶。 int pop() 移除并返回栈顶元素。 int top() 返回栈顶元素。 boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。 注意：\n你只能使用队列的基本操作 —— 也就是 push to back、peek/pop from front、size 和 is empty 这些操作。 你所使用的语言也许不支持队列。 你可以使用 list （列表）或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 输入： [\u0026#34;MyStack\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;top\u0026#34;, \u0026#34;pop\u0026#34;, \u0026#34;empty\u0026#34;] [[], [1], [2], [], [], []] 输出： [null, null, null, 2, 2, false] 解释： MyStack myStack = new MyStack(); myStack.push(1); myStack.push(2); myStack.top(); // 返回 2 myStack.pop(); // 返回 2 myStack.empty(); // 返回 False 提示：\n1 \u0026lt;= x \u0026lt;= 9 最多调用100 次 push、pop、top 和 empty 每次调用 pop 和 top 都保证栈不为空 **进阶：**你能否仅用一个队列来实现栈。\n方法\n方法一：两个队列 为了满足栈的特性，即最后入栈的元素最先出栈，在使用队列实现栈时，应满足队列前端的元素是最后入栈的元素。可以使用两个队列实现栈的操作，其中 queue1 用于存储栈内的元素，queue2 作为入栈操作的辅助队列。\n入栈操作时，首先将元素入队到 queue2，然后将 queue1 的全部元素依次出队并入队到 queue2，此时 queue2 的前端的元素即为新入栈的元素，再将 queue1 和 queue2 互换，则 queue1 的元素即为栈内的元素，queue1 的前端和后端分别对应栈顶和栈底。\n由于每次入栈操作都确保 queue1 的前端元素为栈顶元素，因此出栈操作和获得栈顶元素操作都可以简单实现。出栈操作只需要移除 queue1 的前端元素并返回即可，获得栈顶元素操作只需要获得 queue1 的前端元素并返回即可（不移除元素）。\n由于 queue1 用于存储栈内的元素，判断栈是否为空时，只需要判断 queue1 是否为空即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MyStack { Queue\u0026lt;Integer\u0026gt; queue1; Queue\u0026lt;Integer\u0026gt; queue2; /** Initialize your data structure here. */ public MyStack() { queue1 = new LinkedList\u0026lt;Integer\u0026gt;(); queue2 = new LinkedList\u0026lt;Integer\u0026gt;(); } /** Push element x onto stack. */ public void push(int x) { queue2.offer(x); while (!queue1.isEmpty()) { queue2.offer(queue1.poll()); } Queue\u0026lt;Integer\u0026gt; temp = queue1; queue1 = queue2; queue2 = temp; } /** Removes the element on top of the stack and returns that element. */ public int pop() { return queue1.poll(); } /** Get the top element. */ public int top() { return queue1.peek(); } /** Returns whether the stack is empty. */ public boolean empty() { return queue1.isEmpty(); } } 复杂度分析\n时间复杂度：入栈操作 O(n)，其余操作都是 O(1)，其中 n 是栈内的元素个数。 入栈操作需要将 queue1 中的 n 个元素出队，并入队 n+1 个元素到 queue2，共有 2n+1 次操作，每次出队和入队操作的时间复杂度都是 O(1)，因此入栈操作的时间复杂度是 O(n)。 出栈操作对应将 queue1 的前端元素出队，时间复杂度是 O(1)。 获得栈顶元素操作对应获得 queue1 的前端元素，时间复杂度是 O(1)。 判断栈是否为空操作只需要判断 queue1 是否为空，时间复杂度是 O(1)。 空间复杂度：O(n)，其中 n 是栈内的元素个数。需要使用两个队列存储栈内的元素。 方法二：一个队列 方法一使用了两个队列实现栈的操作，也可以使用一个队列实现栈的操作。\n使用一个队列时，为了满足栈的特性，即最后入栈的元素最先出栈，同样需要满足队列前端的元素是最后入栈的元素。\n入栈操作时，首先获得入栈前的元素个数 n，然后将元素入队到队列，再将队列中的前 n 个元素（即除了新入栈的元素之外的全部元素）依次出队并入队到队列，此时队列的前端的元素即为新入栈的元素，且队列的前端和后端分别对应栈顶和栈底。\n由于每次入栈操作都确保队列的前端元素为栈顶元素，因此出栈操作和获得栈顶元素操作都可以简单实现。出栈操作只需要移除队列的前端元素并返回即可，获得栈顶元素操作只需要获得队列的前端元素并返回即可（不移除元素）。\n由于队列用于存储栈内的元素，判断栈是否为空时，只需要判断队列是否为空即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class MyStack { Queue\u0026lt;Integer\u0026gt; queue; /** Initialize your data structure here. */ public MyStack() { queue = new LinkedList\u0026lt;Integer\u0026gt;(); } /** Push element x onto stack. */ public void push(int x) { int n = queue.size(); queue.offer(x); for (int i = 0; i \u0026lt; n; i++) { queue.offer(queue.poll()); } } /** Removes the element on top of the stack and returns that element. */ public int pop() { return queue.poll(); } /** Get the top element. */ public int top() { return queue.peek(); } /** Returns whether the stack is empty. */ public boolean empty() { return queue.isEmpty(); } } 复杂度分析\n时间复杂度：入栈操作 O(n)，其余操作都是 O(1)，其中 n 是栈内的元素个数。 入栈操作需要将队列中的 n 个元素出队，并入队 n+1 个元素到队列，共有 2n+1 次操作，每次出队和入队操作的时间复杂度都是 O(1)，因此入栈操作的时间复杂度是 O(n)。 出栈操作对应将队列的前端元素出队，时间复杂度是 O(1)。 获得栈顶元素操作对应获得队列的前端元素，时间复杂度是 O(1)。 判断栈是否为空操作只需要判断队列是否为空，时间复杂度是 O(1)。 空间复杂度：O(n)，其中 n 是栈内的元素个数。需要使用一个队列存储栈内的元素。 day16 232. 用栈实现队列 题目\n请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty）：\n实现 MyQueue 类：\nvoid push(int x) 将元素 x 推到队列的末尾 int pop() 从队列的开头移除并返回元素 int peek() 返回队列开头的元素 boolean empty() 如果队列为空，返回 true ；否则，返回 false 说明：\n你 只能 使用标准的栈操作 —— 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。 示例 1：\n1 2 3 4 5 6 7 8 9 10 11 12 13 输入： [\u0026#34;MyQueue\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;peek\u0026#34;, \u0026#34;pop\u0026#34;, \u0026#34;empty\u0026#34;] [[], [1], [2], [], [], []] 输出： [null, null, null, 1, 1, false] 解释： MyQueue myQueue = new MyQueue(); myQueue.push(1); // queue is: [1] myQueue.push(2); // queue is: [1, 2] (leftmost is front of the queue) myQueue.peek(); // return 1 myQueue.pop(); // return 1, queue is [2] myQueue.empty(); // return false 提示：\n1 \u0026lt;= x \u0026lt;= 9 最多调用 100 次 push、pop、peek 和 empty 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作） 进阶：\n你能否实现每个操作均摊时间复杂度为 O(1) 的队列？换句话说，执行 n 个操作的总时间复杂度为 O(n) ，即使其中一个操作可能花费较长时间。 方法\nhttps://leetcode.cn/problems/implement-queue-using-stacks/solution/yong-zhan-shi-xian-dui-lie-by-leetcode/\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class MyQueue { Stack\u0026lt;Integer\u0026gt; s1; Stack\u0026lt;Integer\u0026gt; s2; public MyQueue() { this.s1=new Stack\u0026lt;\u0026gt;(); this.s2=new Stack\u0026lt;\u0026gt;(); } public void push(int x) { s1.push(x); } public int pop() { while(s1.size()!=0){ s2.push(s1.pop()); } int res=s2.pop(); while(s2.size()!=0){ s1.push(s2.pop()); } return res; } public int peek() { while(s1.size()!=0){ s2.push(s1.pop()); } int res=s2.peek(); while(s2.size()!=0){ s1.push(s2.pop()); } return res; } public boolean empty() { return s1.empty(); } } day17 239. 滑动窗口最大值 题目\n给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。\n返回 滑动窗口中的最大值 。\n示例 1：\n1 2 3 4 5 6 7 8 9 10 11 输入：nums = [1,3,-1,-3,5,3,6,7], k = 3 输出：[3,3,5,5,6,7] 解释： 滑动窗口的位置 最大值 --------------- ----- [1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7 示例 2：\n1 2 输入：nums = [1], k = 1 输出：[1] 提示：\n1 \u0026lt;= nums.length \u0026lt;= 105 -104 \u0026lt;= nums[i] \u0026lt;= 104 1 \u0026lt;= k \u0026lt;= nums.length 方法一（超时）\n暴力解法，依次移动窗口，不断寻找最大值，当然可以取巧：每一次只滑动一个索引位置，因此，每一次滑动窗口内的数已经知道了最大的数。如果最大的数还在下一次滑动窗口内，那么就可以利用这个属性，只需要比较上次滑动窗口的最大值和这次滑动窗口的末尾索引处的值，就可以得出下一次滑动窗口的最大值了，而不用傻乎乎的重复比较。当然，还是超时了\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public int[] maxSlidingWindow(int[] nums, int k) { int len=nums.length; int max=Integer.MIN_VALUE; int index=-1; int[] arr = new int[len-k+1]; for(int i=0;i\u0026lt;len-k+1;i++){ if(index!=0){ if(max\u0026lt;nums[i+k-1]){ max=nums[i+k-1]; } } max=Integer.MIN_VALUE; index=-1; for(int j=i;j\u0026lt;i+k;j++){ if(max\u0026lt;nums[j]){ max=nums[j]; index=j; } } arr[i]=max; } return arr; } } /** 已经知道上一个最大的数了， 记录这个最大的数的位置如果不是窗口的第一位，显然具有利用价值： 无需比较上次窗口的最后一位，只需要比较窗口的下一位即可。 */ 其他方法\n优先队列、单调队列（双端队列的特殊情况）\nhttps://leetcode.cn/problems/sliding-window-maximum/solution/hua-dong-chuang-kou-zui-da-zhi-by-leetco-ki6m/\nday18 1047. 删除字符串中的所有相邻重复项 题目\n给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。\n在 S 上反复执行重复项删除操作，直到无法继续删除。\n在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。\n示例：\n1 2 3 4 输入：\u0026#34;abbaca\u0026#34; 输出：\u0026#34;ca\u0026#34; 解释： 例如，在 \u0026#34;abbaca\u0026#34; 中，我们可以删除 \u0026#34;bb\u0026#34; 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 \u0026#34;aaca\u0026#34;，其中又只有 \u0026#34;aa\u0026#34; 可以执行重复项删除操作，所以最后的字符串为 \u0026#34;ca\u0026#34;。 提示：\n1 \u0026lt;= S.length \u0026lt;= 20000 S 仅由小写英文字母组成。 解法\n方法一：栈\n充分理解题意后，我们可以发现，当字符串中同时有多组相邻重复项时，我们无论是先删除哪一个，都不会影响最终的结果。因此我们可以从左向右顺次处理该字符串。\n而消除一对相邻重复项可能会导致新的相邻重复项出现，如从字符串 abba 中删除 bb 会导致出现新的相邻重复项 aa 出现。因此我们需要保存当前还未被删除的字符。一种显而易见的数据结构呼之欲出：栈。我们只需要遍历该字符串，如果当前字符和栈顶字符相同，我们就贪心地将其消去，否则就将其入栈即可。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public String removeDuplicates(String s) { StringBuffer stack = new StringBuffer(); int top = -1; for (int i = 0; i \u0026lt; s.length(); ++i) { char ch = s.charAt(i); if (top \u0026gt;= 0 \u0026amp;\u0026amp; stack.charAt(top) == ch) { stack.deleteCharAt(top); --top; } else { stack.append(ch); ++top; } } return stack.toString(); } } 复杂度分析\n时间复杂度：O(n)，其中 n 是字符串的长度。我们只需要遍历该字符串一次。 空间复杂度：O(n) 或 O(1))，取决于使用的语言提供的字符串类是否提供了类似「入栈」和「出栈」的接口。注意返回值不计入空间复杂度。 day19 151. 颠倒字符串中的单词 题目\n给你一个字符串 s ，颠倒字符串中 单词 的顺序。\n单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。\n返回 单词 顺序颠倒且 单词 之间用单个空格连接的结果字符串。\n**注意：**输入字符串 s中可能会存在前导空格、尾随空格或者单词间的多个空格。返回的结果字符串中，单词间应当仅用单个空格分隔，且不包含任何额外的空格。\n示例 1：\n1 2 输入：s = \u0026#34;the sky is blue\u0026#34; 输出：\u0026#34;blue is sky the\u0026#34; 示例 2：\n1 2 3 输入：s = \u0026#34; hello world \u0026#34; 输出：\u0026#34;world hello\u0026#34; 解释：颠倒后的字符串中不能存在前导空格和尾随空格。 示例 3：\n1 2 3 输入：s = \u0026#34;a good example\u0026#34; 输出：\u0026#34;example good a\u0026#34; 解释：如果两个单词间有多余的空格，颠倒后的字符串需要将单词间的空格减少到仅有一个。 提示：\n1 \u0026lt;= s.length \u0026lt;= 104 s 包含英文大小写字母、数字和空格 ' ' s 中 至少存在一个 单词 方法一\n思路：\n去除首部空格 将空格之间的子串追加到字符串累加器并附带一个空格 追加完过后，再以空格分隔字符串 倒序数组，追加空格即可 代码：（空间复杂度较高）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Solution { public String reverseWords(String s) { char[] chs = s.toCharArray(); char prev = 0; StringBuilder stringBuilder = new StringBuilder(); int index=-1; for (int i = 0; i \u0026lt; s.length(); i++) { if (chs[i]!=\u0026#39; \u0026#39;){ index=i; break; } } for (int i = index; i \u0026lt; chs.length; i++) { if (chs[i] == \u0026#39; \u0026#39; \u0026amp;\u0026amp; prev == \u0026#39; \u0026#39;) {//如果当前字符和上一个字符为\u0026#39; \u0026#39;, continue; } stringBuilder.append(chs[i]); prev = chs[i]; } String str = stringBuilder.toString(); String[] split = str.split(\u0026#34; \u0026#34;); stringBuilder = new StringBuilder(); for (int i = split.length - 1; i \u0026gt; 0; i--) { stringBuilder.append(split[i]).append(\u0026#39; \u0026#39;); } stringBuilder.append(split[0]); return stringBuilder.toString(); } } 方法二\nhttps://leetcode.cn/problems/reverse-words-in-a-string/solution/fan-zhuan-zi-fu-chuan-li-de-dan-ci-by-leetcode-sol/\nday20 541. 反转字符串 II 题目\n给定一个字符串 s 和一个整数 k，从字符串开头算起，每计数至 2k 个字符，就反转这 2k 字符中的前 k 个字符。\n如果剩余字符少于 k 个，则将剩余字符全部反转。 如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。 示例 1：\n1 2 输入：s = \u0026#34;abcdefg\u0026#34;, k = 2 输出：\u0026#34;bacdfeg\u0026#34; 示例 2：\n1 2 输入：s = \u0026#34;abcd\u0026#34;, k = 2 输出：\u0026#34;bacd\u0026#34; 提示：\n1 \u0026lt;= s.length \u0026lt;= 104 s 仅由小写英文组成 1 \u0026lt;= k \u0026lt;= 104 方法\n方法一：模拟 我们直接按题意进行模拟：反转每个下标从 2k2k 的倍数开始的，长度为 kk 的子串。若该子串长度不足 kk，则反转整个子串。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public String reverseStr(String s, int k) { int n = s.length(); char[] arr = s.toCharArray(); for (int i = 0; i \u0026lt; n; i += 2 * k) { reverse(arr, i, Math.min(i + k, n) - 1); } return new String(arr); } public void reverse(char[] arr, int left, int right) { while (left \u0026lt; right) { char temp = arr[left]; arr[left] = arr[right]; arr[right] = temp; left++; right--; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 是字符串 s 的长度。 空间复杂度：O(1) 或 O(n)，取决于使用的语言中字符串类型的性质。如果字符串是可修改的，那么我们可以直接在原字符串上修改，空间复杂度为 O(1)，否则需要使用 O(n) 的空间将字符串临时转换为可以修改的数据结构（例如数组），空间复杂度为 O(n)。 day21 1. 两数之和 题目\n给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。\n你可以按任意顺序返回答案。\n示例 1：\n1 2 3 输入：nums = [2,7,11,15], target = 9 输出：[0,1] 解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2：\n1 2 输入：nums = [3,2,4], target = 6 输出：[1,2] 示例 3：\n1 2 输入：nums = [3,3], target = 6 输出：[0,1] 提示：\n2 \u0026lt;= nums.length \u0026lt;= 104 -109 \u0026lt;= nums[i] \u0026lt;= 109 -109 \u0026lt;= target \u0026lt;= 109 只会存在一个有效答案 **进阶：**你可以想出一个时间复杂度小于 O(n2) 的算法吗？\n方法\n方法一：暴力枚举\n思路及算法\n最容易想到的方法是枚举数组中的每一个数 x，寻找数组中是否存在 target - x。当我们使用遍历整个数组的方式寻找 target - x 时，需要注意到每一个位于 x 之前的元素都已经和 x 匹配过，因此不需要再进行匹配。而每一个元素不能被使用两次，所以我们只需要在 x 后面的元素中寻找 target - x。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public int[] twoSum(int[] nums, int target) { int n = nums.length; for (int i = 0; i \u0026lt; n; ++i) { for (int j = i + 1; j \u0026lt; n; ++j) { if (nums[i] + nums[j] == target) { return new int[]{i, j}; } } } return new int[0]; } } 复杂度分析\n时间复杂度：O(N^2^)，其中 N 是数组中的元素数量。最坏情况下数组中任意两个数都要被匹配一次。 空间复杂度：O(1)。 方法二：哈希表\n思路及算法\n此题，如果使用方法一，显然时间复杂度过高。结合题意：在一堆值里找一个值，应该考虑转化为hash的数据结构，hash的时间复杂度为O(1)\n注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 O(N) 降低到 O(1)。这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public int[] twoSum(int[] nums, int target) { Map\u0026lt;Integer, Integer\u0026gt; hashtable = new HashMap\u0026lt;Integer, Integer\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; ++i) { // 将此处的n次循环变为1次即可得到结果，这就是hash的好处 if (hashtable.containsKey(target - nums[i])) { return new int[]{hashtable.get(target - nums[i]), i}; } hashtable.put(nums[i], i); } return new int[0]; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。 空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。 day22 15. 三数之和 题目\n给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 *a，b，c ，*使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。\n**注意：**答案中不可以包含重复的三元组。\n示例 1：\n1 2 输入：nums = [-1,0,1,2,-1,-4] 输出：[[-1,-1,2],[-1,0,1]] 示例 2：\n1 2 输入：nums = [] 输出：[] 示例 3：\n1 2 输入：nums = [0] 输出：[] 提示：\n0 \u0026lt;= nums.length \u0026lt;= 3000 -105 \u0026lt;= nums[i] \u0026lt;= 105 解法\n方法一：排序 + 双指针\n题目中要求找到所有「不重复」且和为 0 的三元组，这个「不重复」的要求使得我们无法简单地使用三重循环枚举所有的三元组。这是因为在最坏的情况下，数组中的元素全部为 0，即\n1 [0, 0, 0, 0, 0, ..., 0, 0, 0] 任意一个三元组的和都为 00。如果我直接使用三重循环枚举三元组，会得到 O(N^3^) 个满足题目要求的三元组（其中 N 是数组的长度）时间复杂度至少为 O(N^3^)。在这之后，我们还需要使用哈希表进行去重操作，得到不包含重复三元组的最终答案，又消耗了大量的空间。这个做法的时间复杂度和空间复杂度都很高，因此我们要换一种思路来考虑这个问题。\n「不重复」的本质是什么？我们保持三重循环的大框架不变，只需要保证：\n第二重循环枚举到的元素不小于当前第一重循环枚举到的元素； 第三重循环枚举到的元素不小于当前第二重循环枚举到的元素。 也就是说，我们枚举的三元组 (a, b, c) 满足 a≤b≤c，保证了只有 (a, b, c) 这个顺序会被枚举到，而 (b, a, c)、(c, b, a) 等等这些不会，这样就减少了重复。要实现这一点，我们可以将数组中的元素从小到大进行排序，随后使用普通的三重循环就可以满足上面的要求。\n同时，对于每一重循环而言，相邻两次枚举的元素不能相同，否则也会造成重复。举个例子，如果排完序的数组为\n1 2 [0, 1, 2, 2, 2, 3] ^ ^ ^ 我们使用三重循环枚举到的第一个三元组为 (0, 1, 2)，如果第三重循环继续枚举下一个元素，那么仍然是三元组 (0, 1, 2)，产生了重复。因此我们需要将第三重循环「跳到」下一个不相同的元素，即数组中的最后一个元素 3，枚举三元组 (0, 1, 3)。\n下面给出了改进的方法的伪代码实现：\n1 2 3 4 5 6 7 8 9 10 nums.sort() for first = 0 .. n-1 // 只有和上一次枚举的元素不相同，我们才会进行枚举 if first == 0 or nums[first] != nums[first-1] then for second = first+1 .. n-1 if second == first+1 or nums[second] != nums[second-1] then for third = second+1 .. n-1 if third == second+1 or nums[third] != nums[third-1] then // 判断是否有 a+b+c==0 check(first, second, third) 这种方法的时间复杂度仍然为 O(N^3^)，毕竟我们还是没有跳出三重循环的大框架。然而它是很容易继续优化的，可以发现，如果我们固定了前两重循环枚举到的元素 a 和 b，那么只有唯一的 c 满足 a+b+c=0。当第二重循环往后枚举一个元素 b\u0026rsquo; 时，由于 b\u0026rsquo; \u0026gt; b，那么满足 a+b\u0026rsquo;+c\u0026rsquo;=0 的 c\u0026rsquo; 一定有 c\u0026rsquo; \u0026lt; c，即 c\u0026rsquo; 在数组中一定出现在 c 的左侧。也就是说，我们可以从小到大枚举 b，同时从大到小枚举 c，即第二重循环和第三重循环实际上是并列的关系。\n有了这样的发现，我们就可以保持第二重循环不变，而将第三重循环变成一个从数组最右端开始向左移动的指针，从而得到下面的伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 nums.sort() for first = 0 .. n-1 if first == 0 or nums[first] != nums[first-1] then // 第三重循环对应的指针 third = n-1 for second = first+1 .. n-1 if second == first+1 or nums[second] != nums[second-1] then // 向左移动指针，直到 a+b+c 不大于 0 while nums[first]+nums[second]+nums[third] \u0026gt; 0 third = third-1 // 判断是否有 a+b+c==0 check(first, second, third) 这个方法就是我们常说的「双指针」，当我们需要枚举数组中的两个元素时，如果我们发现随着第一个元素的递增，第二个元素是递减的，那么就可以使用双指针的方法，将枚举的时间复杂度从 O(N^2)O(N2) 减少至 O(N)。为什么是 O(N) 呢？这是因为在枚举的过程每一步中，「左指针」会向右移动一个位置（也就是题目中的 b），而「右指针」会向左移动若干个位置，这个与数组的元素有关，但我们知道它一共会移动的位置数为 O(N)，均摊下来，每次也向左移动一个位置，因此时间复杂度为 O(N)。\n注意到我们的伪代码中还有第一重循环，时间复杂度为 O(N)O(N)，因此枚举的总时间复杂度为 O(N^2^)。由于排序的时间复杂度为 O(NlogN)，在渐进意义下小于前者，因此算法的总时间复杂度为 O(N^2^)。\n上述的伪代码中还有一些细节需要补充，例如我们需要保持左指针一直在右指针的左侧（即满足 b≤c），具体可以参考下面的代码，均给出了详细的注释。\n代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] nums) { int n = nums.length; Arrays.sort(nums); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ans = new ArrayList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt;(); // 枚举 a for (int first = 0; first \u0026lt; n; ++first) { // 需要和上一次枚举的数不相同 if (first \u0026gt; 0 \u0026amp;\u0026amp; nums[first] == nums[first - 1]) { continue; } // c 对应的指针初始指向数组的最右端 int third = n - 1; int target = -nums[first]; // 枚举 b for (int second = first + 1; second \u0026lt; n; ++second) { // 需要和上一次枚举的数不相同 if (second \u0026gt; first + 1 \u0026amp;\u0026amp; nums[second] == nums[second - 1]) { continue; } // 需要保证 b 的指针在 c 的指针的左侧 while (second \u0026lt; third \u0026amp;\u0026amp; nums[second] + nums[third] \u0026gt; target) { --third; } // 如果指针重合，随着 b 后续的增加 // 就不会有满足 a+b+c=0 并且 b\u0026lt;c 的 c 了，可以退出循环 if (second == third) { break; } if (nums[second] + nums[third] == target) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;Integer\u0026gt;(); list.add(nums[first]); list.add(nums[second]); list.add(nums[third]); ans.add(list); } } } return ans; } } 复杂度分析\n时间复杂度：O(N^2^)，其中 N 是数组 nums 的长度。 空间复杂度：O(logN)。我们忽略存储答案的空间，额外的排序的空间复杂度为 O(logN)。然而我们修改了输入的数组 nums，在实际情况下不一定允许，因此也可以看成使用了一个额外的数组存储了 nums 的副本并进行排序，空间复杂度为 O(N)。 day23 202. 快乐数 题目\n编写一个算法来判断一个数 n 是不是快乐数。\n「快乐数」 定义为：\n对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和。 然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。 如果这个过程 结果为 1，那么这个数就是快乐数。 如果 n 是 快乐数 就返回 true ；不是，则返回 false 。\n示例 1：\n1 2 3 4 5 6 7 输入：n = 19 输出：true 解释： 12 + 92 = 82 82 + 22 = 68 62 + 82 = 100 12 + 02 + 02 = 1 示例 2：\n1 2 输入：n = 2 输出：false 提示：\n1 \u0026lt;= n \u0026lt;= 231 - 1 解法\n暴力错误且超时\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public boolean isHappy(int n) { int tmp=n; while(true){ tmp=get(tmp); if(tmp==n) return false; else if(tmp==1) return true; } } public int get(int n) { int tmp=n; int sum=0; int r=0; while(tmp!=0){ r = tmp%10; sum = r*r+sum; tmp = tmp/10; } return sum; } } 超时的原因在于：条件判断错误，不是一定要出现到初始数字才会陷入死循环，有可能是中途的某个节点出现循环，例如：数字116\n显然，需要使用弗洛伊德判圈算法（龟兔赛跑算法）\u0026ndash;\u0026gt; 快慢指针法：一个快指针和一个慢指针同时出发，如果快指针追上了慢指针说明是一个死循环，就返回false\n快慢指针法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public boolean isHappy(int n) { int slowRunner = n; int fastRunner = getNext(n); while (fastRunner != 1 \u0026amp;\u0026amp; slowRunner != fastRunner) { slowRunner = getNext(slowRunner); fastRunner = getNext(getNext(fastRunner)); } return fastRunner == 1; } public int getNext(int n) { int tmp=n; int sum=0; int r=0; while(tmp!=0){ r = tmp%10; sum = r*r+sum; tmp = tmp/10; } return sum; } } day24 454. 四数相加 II 题目\n给你四个整数数组 nums1、nums2、nums3 和 nums4 ，数组长度都是 n ，请你计算有多少个元组 (i, j, k, l) 能满足：\n0 \u0026lt;= i, j, k, l \u0026lt; n nums1[i] + nums2[j] + nums3[k] + nums4[l] == 0 示例 1：\n1 2 3 4 5 6 输入：nums1 = [1,2], nums2 = [-2,-1], nums3 = [-1,2], nums4 = [0,2] 输出：2 解释： 两个元组如下： 1. (0, 0, 0, 1) -\u0026gt; nums1[0] + nums2[0] + nums3[0] + nums4[1] = 1 + (-2) + (-1) + 2 = 0 2. (1, 1, 0, 0) -\u0026gt; nums1[1] + nums2[1] + nums3[0] + nums4[0] = 2 + (-1) + (-1) + 0 = 0 示例 2：\n1 2 输入：nums1 = [0], nums2 = [0], nums3 = [0], nums4 = [0] 输出：1 提示：\nn == nums1.length n == nums2.length n == nums3.length n == nums4.length 1 \u0026lt;= n \u0026lt;= 200 -228 \u0026lt;= nums1[i], nums2[i], nums3[i], nums4[i] \u0026lt;= 228 解法：分组+hash表\n思路与算法\n我们可以将四个数组分成两部分，A 和 B 为一组，C 和 D 为另外一组。\n对于 A 和 B，我们使用二重循环对它们进行遍历，得到所有 A[i]+B[j] 的值并存入哈希映射中。对于哈希映射中的每个键值对，每个键表示一种 A[i]+B[j]，对应的值为 A[i]+B[j] 出现的次数。\n对于 CC 和 DD，我们同样使用二重循环对它们进行遍历。当遍历到 C[k]+D[l] 时，如果 -(C[k]+D[l]) 出现在哈希映射中，那么将 -(C[k]+D[l]) 对应的值累加进答案中。\n最终即可得到满足 A[i]+B[j]+C[k]+D[l]=0 的四元组数目。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public int fourSumCount(int[] A, int[] B, int[] C, int[] D) { Map\u0026lt;Integer, Integer\u0026gt; countAB = new HashMap\u0026lt;Integer, Integer\u0026gt;(); for (int u : A) { for (int v : B) { countAB.put(u + v, countAB.getOrDefault(u + v, 0) + 1); } } int ans = 0; for (int u : C) { for (int v : D) { if (countAB.containsKey(-u - v)) { ans += countAB.get(-u - v); } } } return ans; } } 复杂度分析\n时间复杂度：O(n^2^)。我们使用了两次二重循环，时间复杂度均为 O(n^2^)。在循环中对哈希映射进行的修改以及查询操作的期望时间复杂度均为 O(1)，因此总时间复杂度为 O(n^2^)。 空间复杂度：O(n^2^)，即为哈希映射需要使用的空间。在最坏的情况下，A[i]+B[j] 的值均不相同，因此值的个数为 n^2^，也就需要 O(n^2^) 的空间。 day25 100. 相同的树 题目\n给你两棵二叉树的根节点 p 和 q ，编写一个函数来检验这两棵树是否相同。\n如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。\n示例 1：\n1 2 输入：p = [1,2,3], q = [1,2,3] 输出：true 示例 2：\n1 2 输入：p = [1,2], q = [1,null,2] 输出：false 示例 3：\n1 2 输入：p = [1,2,1], q = [1,1,2] 输出：false 提示：\n两棵树上的节点数目都在范围 [0, 100] 内 -104 \u0026lt;= Node.val \u0026lt;= 104 解法\n两个二叉树相同，当且仅当两个二叉树的结构完全相同，且所有对应节点的值相同。因此，可以通过搜索的方式判断两个二叉树是否相同。\n方法一：深度优先搜索\n如果两个二叉树都为空，则两个二叉树相同。如果两个二叉树中有且只有一个为空，则两个二叉树一定不相同。\n如果两个二叉树都不为空，那么首先判断它们的根节点的值是否相同，若不相同则两个二叉树一定不同，若相同，再分别判断两个二叉树的左子树是否相同以及右子树是否相同。这是一个递归的过程，因此可以使用深度优先搜索，递归地判断两个二叉树是否相同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null \u0026amp;\u0026amp; q == null) { return true; } else if (p == null || q == null) { return false; } else if (p.val != q.val) { return false; } else { return isSameTree(p.left, q.left) \u0026amp;\u0026amp; isSameTree(p.right, q.right); } } } 复杂度分析\n时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。对两个二叉树同时进行深度优先搜索，只有当两个二叉树中的对应节点都不为空时才会访问到该节点，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。空间复杂度取决于递归调用的层数，递归调用的层数不会超过较小的二叉树的最大高度，最坏情况下，二叉树的高度等于节点数。 方法二：广度优先搜索\n也可以通过广度优先搜索判断两个二叉树是否相同。同样首先判断两个二叉树是否为空，如果两个二叉树都不为空，则从两个二叉树的根节点开始广度优先搜索。\n使用两个队列分别存储两个二叉树的节点。初始时将两个二叉树的根节点分别加入两个队列。每次从两个队列各取出一个节点，进行如下比较操作。\n比较两个节点的值，如果两个节点的值不相同则两个二叉树一定不同； 如果两个节点的值相同，则判断两个节点的子节点是否为空，如果只有一个节点的左子节点为空，或者只有一个节点的右子节点为空，则两个二叉树的结构不同，因此两个二叉树一定不同； 如果两个节点的子节点的结构相同，则将两个节点的非空子节点分别加入两个队列，子节点加入队列时需要注意顺序，如果左右子节点都不为空，则先加入左子节点，后加入右子节点。 如果搜索结束时两个队列同时为空，则两个二叉树相同。如果只有一个队列为空，则两个二叉树的结构不同，因此两个二叉树不同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null \u0026amp;\u0026amp; q == null) { return true; } else if (p == null || q == null) { return false; } Queue\u0026lt;TreeNode\u0026gt; queue1 = new LinkedList\u0026lt;TreeNode\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue2 = new LinkedList\u0026lt;TreeNode\u0026gt;(); queue1.offer(p); queue2.offer(q); while (!queue1.isEmpty() \u0026amp;\u0026amp; !queue2.isEmpty()) { TreeNode node1 = queue1.poll(); TreeNode node2 = queue2.poll(); if (node1.val != node2.val) { return false; } TreeNode left1 = node1.left, right1 = node1.right, left2 = node2.left, right2 = node2.right; if (left1 == null ^ left2 == null) { return false; } if (right1 == null ^ right2 == null) { return false; } if (left1 != null) { queue1.offer(left1); } if (right1 != null) { queue1.offer(right1); } if (left2 != null) { queue2.offer(left2); } if (right2 != null) { queue2.offer(right2); } } return queue1.isEmpty() \u0026amp;\u0026amp; queue2.isEmpty(); } } 复杂度分析\n时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。对两个二叉树同时进行广度优先搜索，只有当两个二叉树中的对应节点都不为空时才会访问到该节点，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。空间复杂度取决于队列中的元素个数，队列中的元素个数不会超过较小的二叉树的节点数。 day26 101. 对称二叉树 题目\n给你一个二叉树的根节点 root ， 检查它是否轴对称。\n示例 1：\n1 2 输入：root = [1,2,2,3,4,4,3] 输出：true 示例 2：\n1 2 输入：root = [1,2,2,null,3,null,3] 输出：false 提示：\n树中节点数目在范围 [1, 1000] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 **进阶：**你可以运用递归和迭代两种方法解决这个问题吗？\n解法\n方法一：递归\n思路和算法\n如果一个树的左子树与右子树镜像对称，那么这个树是对称的。\n因此，该问题可以转化为：两个树在什么情况下互为镜像？\n如果同时满足下面的条件，两个树互为镜像：\n它们的两个根结点具有相同的值 每个树的右子树都与另一个树的左子树镜像对称 我们可以实现这样一个递归函数，通过「同步移动」两个指针的方法来遍历这棵树，p 指针和 q 指针一开始都指向这棵树的根，随后 p 右移时，q 左移，p 左移时，q 右移。每次检查当前 p 和 q 节点的值是否相等，如果相等再判断左右子树是否对称。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public boolean isSymmetric(TreeNode root) { return check(root.left, root.right); } public boolean check(TreeNode p, TreeNode q) { if (p == null \u0026amp;\u0026amp; q == null) { return true; }else if (p == null || q == null) { return false; }else if (p.val != q.val ){ return false; } return check(p.left, q.right) \u0026amp;\u0026amp; check(p.right, q.left); // 就是看左子树和右子树节点是否相同 } } 复杂度分析\n假设树上一共 n 个节点。\n时间复杂度：这里遍历了这棵树，渐进时间复杂度为 O(n)。 空间复杂度：这里的空间复杂度和递归使用的栈空间有关，这里递归层数不超过 n，故渐进空间复杂度为 O(n)。 方法二：迭代\n思路和算法\n「方法一」中我们用递归的方法实现了对称性的判断，那么如何用迭代的方法实现呢？首先我们引入一个队列，这是把递归程序改写成迭代程序的常用方法。初始化时我们把根节点入队两次。每次提取两个结点并比较它们的值（队列中每两个连续的结点应该是相等的，而且它们的子树互为镜像），然后将两个结点的左右子结点按相反的顺序插入队列中。当队列为空时，或者我们检测到树不对称（即从队列中取出两个不相等的连续结点）时，该算法结束。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public boolean isSymmetric(TreeNode root) { return check(root, root); } public boolean check(TreeNode u, TreeNode v) { Queue\u0026lt;TreeNode\u0026gt; q = new LinkedList\u0026lt;TreeNode\u0026gt;(); q.offer(u); q.offer(v); while (!q.isEmpty()) { u = q.poll(); v = q.poll(); if (u == null \u0026amp;\u0026amp; v == null) { continue; } if ((u == null || v == null) || (u.val != v.val)) { return false; } // 核心代码 q.offer(u.left); q.offer(v.right); q.offer(u.right); q.offer(v.left); } return true; } } 复杂度分析\n时间复杂度：O(n)，同「方法一」。 空间复杂度：这里需要用一个队列来维护节点，每个节点最多进队一次，出队一次，队列中最多不会超过 n 个点，故渐进空间复杂度为 O(n)。 day27 104. 二叉树的最大深度 题目\n给定一个二叉树，找出其最大深度。\n二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。\n说明: 叶子节点是指没有子节点的节点。\n示例： 给定二叉树 [3,9,20,null,null,15,7]，\n1 2 3 4 5 3 / \\ 9 20 / \\ 15 7 返回它的最大深度 3 。\n解法\n方法一：深度优先搜索\n思路与算法\n如果我们知道了左子树和右子树的最大深度 l 和 r，那么该二叉树的最大深度即为max(l,r)+1\n而左子树和右子树的最大深度又可以以同样的方式进行计算。因此我们可以用「深度优先搜索」的方法来计算二叉树的最大深度。具体而言，在计算当前二叉树的最大深度时，可以先递归计算出其左子树和右子树的最大深度，然后在 O(1) 时间内计算出当前二叉树的最大深度。递归在访问到空节点时退出。\n1 2 3 4 5 6 7 8 9 10 11 class Solution { public int maxDepth(TreeNode root) { if (root == null) { return 0; } else { int leftHeight = maxDepth(root.left); int rightHeight = maxDepth(root.right); return Math.max(leftHeight, rightHeight) + 1; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树节点的个数。每个节点在递归中只被遍历一次。 空间复杂度：O(height)，其中 height 表示二叉树的高度。递归函数需要栈空间，而栈空间取决于递归的深度，因此空间复杂度等价于二叉树的高度。 方法二：广度优先搜索\n思路与算法\n我们也可以用「广度优先搜索」的方法来解决这道题目，但我们需要对其进行一些修改，此时我们广度优先搜索的队列里存放的是「当前层的所有节点」。每次拓展下一层的时候，不同于广度优先搜索的每次只从队列里拿出一个节点，我们需要将队列里的所有节点都拿出来进行拓展，这样能保证每次拓展完的时候队列里存放的是当前层的所有节点，即我们是一层一层地进行拓展，最后我们用一个变量 ans 来维护拓展的次数，该二叉树的最大深度即为 ans。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { public int maxDepth(TreeNode root) { if (root == null) { return 0; } Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;TreeNode\u0026gt;(); queue.offer(root); int ans = 0; while (!queue.isEmpty()) { int size = queue.size(); while (size \u0026gt; 0) { TreeNode node = queue.poll(); if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } size--; } ans++; } return ans; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func maxDepth(root *TreeNode) int { if root==nil { return 0 } depth:=0 queue:=[]*TreeNode{root} for len(queue)!=0{ n:=len(queue) for i:=0;i\u0026lt;n;i++{ node:=queue[i] // fmt.Println(node.Val) if node.Left!=nil{ queue=append(queue,node.Left) } if node.Right!=nil{ queue=append(queue,node.Right) } } queue=queue[n:] depth++ } return depth } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树的节点个数。与方法一同样的分析，每个节点只会被访问一次。 空间复杂度：此方法空间的消耗取决于队列存储的元素数量，其在最坏情况下会达到 O(n)。 day28 111. 二叉树的最小深度 题目\n给定一个二叉树，找出其最小深度。\n最小深度是从根节点到最近叶子节点的最短路径上的节点数量。\n**说明：**叶子节点是指没有子节点的节点。\n示例 1：\n1 2 输入：root = [3,9,20,null,null,15,7] 输出：2 示例 2：\n1 2 输入：root = [2,null,3,null,4,null,5,null,6] 输出：5 提示：\n树中节点数的范围在 [0, 105] 内 -1000 \u0026lt;= Node.val \u0026lt;= 1000 解法\n方法一：深度优先搜索\n思路及解法\n首先可以想到使用深度优先搜索的方法，遍历整棵树，记录最小深度。\n对于每一个非叶子节点，我们只需要分别计算其左右子树的最小叶子节点深度。这样就将一个大问题转化为了小问题，可以递归地解决该问题。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public int minDepth(TreeNode root) { if (root==null) return 0; int l=Integer.MAX_VALUE; int r=Integer.MAX_VALUE; if (root.left==null\u0026amp;\u0026amp;root.right==null) return 1; if (root.left!=null){ l=minDepth(root.left)+1; } if(root.right!=null){ // System.out.println(root.val); r=minDepth(root.right)+1; } return l\u0026gt;r?r:l; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(H)，其中 H 是树的高度。空间复杂度主要取决于递归时栈空间的开销，最坏情况下，树呈现链状，空间复杂度为 O(N)。平均情况下树的高度与节点数的对数正相关，空间复杂度为 O(log N)。 方法二：广度优先搜索\n思路及解法\n同样，我们可以想到使用广度优先搜索的方法，遍历整棵树。\n当我们找到一个叶子节点时，直接返回这个叶子节点的深度。广度优先搜索的性质保证了最先搜索到的叶子节点的深度一定最小。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public int minDepth(TreeNode root) { if (root==null) return 0; if (root.left==null\u0026amp;\u0026amp;root.right==null) return 1; int layerNum=1; Queue\u0026lt;TreeNode\u0026gt; queue=new LinkedList\u0026lt;\u0026gt;(); queue.offer(root); while(!queue.isEmpty()){ int size = queue.size(); while(size\u0026gt;0){ TreeNode node = queue.poll(); if(node.left==null\u0026amp;\u0026amp;node.right==null) return layerNum; if(node.left!=null) queue.offer(node.left); if(node.right!=null) queue.offer(node.right); size--; } layerNum++; } return layerNum; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(N)，其中 N 是树的节点数。空间复杂度主要取决于队列的开销，队列中的元素个数不会超过树的节点数。 day29 112. 路径总和 题目\n给你二叉树的根节点 root 和一个表示目标和的整数 targetSum 。判断该树中是否存在 根节点到叶子节点 的路径，这条路径上所有节点值相加等于目标和 targetSum 。如果存在，返回 true ；否则，返回 false 。\n叶子节点 是指没有子节点的节点。\n示例 1：\n1 2 3 输入：root = [5,4,8,11,null,13,4,7,2,null,null,null,1], targetSum = 22 输出：true 解释：等于目标和的根节点到叶节点路径如上图所示。 示例 2：\n1 2 3 4 5 6 输入：root = [1,2,3], targetSum = 5 输出：false 解释：树中存在两条根节点到叶子节点的路径： (1 --\u0026gt; 2): 和为 3 (1 --\u0026gt; 3): 和为 4 不存在 sum = 5 的根节点到叶子节点的路径。 示例 3：\n1 2 3 输入：root = [], targetSum = 0 输出：false 解释：由于树是空的，所以不存在根节点到叶子节点的路径。 提示：\n树中节点的数目在范围 [0, 5000] 内 -1000 \u0026lt;= Node.val \u0026lt;= 1000 -1000 \u0026lt;= targetSum \u0026lt;= 1000 解法\n写在前面\n注意到本题的要求是，询问是否有从「根节点」到某个「叶子节点」经过的路径上的节点之和等于目标和。核心思想是对树进行一次遍历，在遍历时记录从根节点到当前节点的路径和，以防止重复计算。\n需要特别注意的是，给定的 root 可能为空。\n方法一：广度优先搜索\n思路及算法\n首先我们可以想到使用广度优先搜索的方式，记录从根节点到当前节点的路径和，以防止重复计算。\n这样我们使用两个队列，分别存储将要遍历的节点，以及根节点到这些节点的路径和即可。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public boolean hasPathSum(TreeNode root, int sum) { if (root == null) { return false; } Queue\u0026lt;TreeNode\u0026gt; queNode = new LinkedList\u0026lt;\u0026gt;(); Queue\u0026lt;Integer\u0026gt; queVal = new LinkedList\u0026lt;\u0026gt;(); queNode.offer(root); queVal.offer(root.val); while (!queNode.isEmpty()) { TreeNode now = queNode.poll(); int temp = queVal.poll(); if (now.left == null \u0026amp;\u0026amp; now.right == null) { if (temp == sum) { return true; } continue; } if (now.left != null) { queNode.offer(now.left); queVal.offer(now.left.val + temp); } if (now.right != null) { queNode.offer(now.right); queVal.offer(now.right.val + temp); } } return false; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(N)，其中 N 是树的节点数。空间复杂度主要取决于队列的开销，队列中的元素个数不会超过树的节点数。 方法二：递归 思路及算法\n观察要求我们完成的函数，我们可以归纳出它的功能：询问是否存在从当前节点 root 到叶子节点的路径，满足其路径和为 sum。\n假定从根节点到当前节点的值之和为 val，我们可以将这个大问题转化为一个小问题：是否存在从当前节点的子节点到叶子的路径，满足其路径和为 sum - val。\n不难发现这满足递归的性质，若当前节点就是叶子节点，那么我们直接判断 sum 是否等于 val 即可（因为路径和已经确定，就是当前节点的值，我们只需要判断该路径和是否满足条件）。若当前节点不是叶子节点，我们只需要递归地询问它的子节点是否能满足条件即可。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Solution { public boolean hasPathSum(TreeNode root, int sum) { if (root == null) { return false; } if (root.left == null \u0026amp;\u0026amp; root.right == null) { return sum == root.val; } return hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val); } } // 或者：正向思维，遍历二叉树，将到达叶子节点的路径和累加与 targetSum 比较，如果相等就返回true class Solution { public boolean hasPathSum(TreeNode root, int targetSum) { return doSome(root,targetSum,0); } public boolean doSome(TreeNode root,int targetSum,int tmp){ if (root==null) return false; // 叶子节点 if(root.left==null\u0026amp;\u0026amp;root.right==null) { if(tmp+root.val==targetSum){ return true; } else { tmp=0; return false; } } if(root.left!=null) { boolean l =doSome(root.left,targetSum,tmp+root.val); if (l) return true; } if(root.right!=null) { boolean r = doSome(root.right,targetSum,tmp+root.val); if (r) return true; } return false; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(H)，其中 H 是树的高度。空间复杂度主要取决于递归时栈空间的开销，最坏情况下，树呈现链状，空间复杂度为 O(N)。平均情况下树的高度与节点数的对数正相关，空间复杂度为 O(log N)。 day30 222. 完全二叉树的节点个数 题目\n给你一棵 完全二叉树 的根节点 root ，求出该树的节点个数。\n完全二叉树 的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2h 个节点。\n示例 1：\n1 2 输入：root = [1,2,3,4,5,6] 输出：6 示例 2：\n1 2 输入：root = [] 输出：0 示例 3：\n1 2 输入：root = [1] 输出：1 提示：\n树中节点的数目范围是[0, 5 * 104] 0 \u0026lt;= Node.val \u0026lt;= 5 * 104 题目数据保证输入的树是 完全二叉树 **进阶：**遍历树来统计节点是一种时间复杂度为 O(n) 的简单解决方案。你可以设计一个更快的算法吗？\n解法\n方法一：深度优先遍历\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public int countNodes(TreeNode root) { if(root == null) { return 0; } int left = countNodes(root.left); int right = countNodes(root.right); return left+right+1; } } 方法二：广度优先遍历\n一如既往，广度优先貌似硬是要比深度优先慢一些\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public int countNodes(TreeNode root) { if (root==null) return 0; Queue\u0026lt;TreeNode\u0026gt; queue=new LinkedList\u0026lt;\u0026gt;(); int count=0; queue.offer(root); while(!queue.isEmpty()){ int size = queue.size(); while(size\u0026gt;0){ TreeNode n = queue.poll(); if (n!=null) count++; if (n.left!=null) queue.offer(n.left); if (n.right!=null) queue.offer(n.right); size--; } } return count; } } 方法三：二分查找 + 位运算\n对于任意二叉树，都可以通过广度优先搜索或深度优先搜索计算节点个数，时间复杂度和空间复杂度都是 O(n)，其中 n 是二叉树的节点个数。这道题规定了给出的是完全二叉树，因此可以利用完全二叉树的特性计算节点个数。\n规定根节点位于第 0 层，完全二叉树的最大层数为 h。根据完全二叉树的特性可知，完全二叉树的最左边的节点一定位于最底层，因此从根节点出发，每次访问左子节点，直到遇到叶子节点，该叶子节点即为完全二叉树的最左边的节点，经过的路径长度即为最大层数 h。\n当 0≤i\u0026lt;h 时，第 i 层包含 2^i^ 个节点，最底层包含的节点数最少为 1，最多为 2^h^。\n当最底层包含 1 个节点时，完全二叉树的节点个数是 $$ \\sum_{i=0}^{h-1}2^i+1=2^h $$ 当最底层包含 2^h^ 个节点时，完全二叉树的节点个数是 $$ \\sum_{i=0}^{h}2^i=2^{h+1}-1 $$ 因此对于最大层数为 h 的完全二叉树，节点个数一定在 [2^h^,2^h+1^-1] 的范围内，可以在该范围内通过二分查找的方式得到完全二叉树的节点个数。\n具体做法是，根据节点个数范围的上下界得到当前需要判断的节点个数 k，如果第 k 个节点存在，则节点个数一定大于或等于 k，如果第 k 个节点不存在，则节点个数一定小于 k，由此可以将查找的范围缩小一半，直到得到节点个数。\n如何判断第 k 个节点是否存在呢？如果第 k 个节点位于第 h 层，则 k 的二进制表示包含 h+1 位，其中最高位是 1，其余各位从高到低表示从根节点到第 k 个节点的路径，0 表示移动到左子节点，1 表示移动到右子节点。通过位运算得到第 k 个节点对应的路径，判断该路径对应的节点是否存在，即可判断第 k 个节点是否存在。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Solution { public int countNodes(TreeNode root) { if (root == null) { return 0; } int level = 0; TreeNode node = root; while (node.left != null) { level++; node = node.left; } int low = 1 \u0026lt;\u0026lt; level, high = (1 \u0026lt;\u0026lt; (level + 1)) - 1; while (low \u0026lt; high) { int mid = (high - low + 1) / 2 + low; if (exists(root, level, mid)) { low = mid; } else { high = mid - 1; } } return low; } public boolean exists(TreeNode root, int level, int k) { int bits = 1 \u0026lt;\u0026lt; (level - 1); TreeNode node = root; while (node != null \u0026amp;\u0026amp; bits \u0026gt; 0) { if ((bits \u0026amp; k) == 0) { node = node.left; } else { node = node.right; } bits \u0026gt;\u0026gt;= 1; } return node != null; } } 复杂度分析\n时间复杂度：O(log^2^n)，其中 n 是完全二叉树的节点数。 首先需要 O(h) 的时间得到完全二叉树的最大层数，其中 h 是完全二叉树的最大层数。 使用二分查找确定节点个数时，需要查找的次数为 O(log 2^h^)=O(h)，每次查找需要遍历从根节点开始的一条长度为 h 的路径，需要 O(h) 的时间，因此二分查找的总时间复杂度是 O(h^2^)。 因此总时间复杂度是 O(h^2^)。由于完全二叉树满足 2^h^≤n\u0026lt;2^h+1^，因此有 O(h)=O(log n)，O(h^2^)=O(log^2^ n)。 空间复杂度：O(1)。只需要维护有限的额外空间。 day31 226. 翻转二叉树 题目\n给你一棵二叉树的根节点 root ，翻转这棵二叉树，并返回其根节点。\n示例 1：\n1 2 输入：root = [4,2,7,1,3,6,9] 输出：[4,7,2,9,6,3,1] 示例 2：\n1 2 输入：root = [2,1,3] 输出：[2,3,1] 示例 3：\n1 2 输入：root = [] 输出：[] 提示：\n树中节点数目范围在 [0, 100] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 解法\n方法一：深度优先遍历\n思路与算法\n这是一道很经典的二叉树问题。显然，我们从根节点开始，递归地对树进行遍历，并从叶子节点先开始翻转。如果当前遍历到的节点 root 的左右两棵子树都已经翻转，那么我们只需要交换两棵子树的位置，即可完成以 root 为根节点的整棵子树的翻转。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public TreeNode invertTree(TreeNode root) { if(root==null) return null; // 先交换一下左右子树 TreeNode tmp = root.right; root.right = root.left; root.left = tmp; // 此时已经交换过子树了，所以分别遍历左右子树 root.left = invertTree(root.left); root.right = invertTree(root.right); return root; } } 复杂度分析\n时间复杂度：O(N)，其中 N 为二叉树节点的数目。我们会遍历二叉树中的每一个节点，对每个节点而言，我们在常数时间内交换其两棵子树。 空间复杂度：O(N)。使用的空间由递归栈的深度决定，它等于当前节点在二叉树中的高度。在平均情况下，二叉树的高度与节点个数为对数关系，即 O*(logN)。而在最坏情况下，树形成链状，空间复杂度为 O(N)。 方法二：广度优先遍历\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public TreeNode invertTree(TreeNode root) { if (root == null) return root; Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(root);//相当于把数据加入到队列尾部 while (!queue.isEmpty()) { //poll方法相当于移除队列头部的元素 TreeNode node = queue.poll(); //先交换子节点 TreeNode left = node.left; node.left = node.right; node.right = left; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } return root; } day32 236. 二叉树的最近公共祖先 题目\n给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个节点 p、q，最近公共祖先表示为一个节点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n示例 1：\n1 2 3 输入：root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出：3 解释：节点 5 和节点 1 的最近公共祖先是节点 3 。 示例 2：\n1 2 3 输入：root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 输出：5 解释：节点 5 和节点 4 的最近公共祖先是节点 5 。因为根据定义最近公共祖先节点可以为节点本身。 示例 3：\n1 2 输入：root = [1,2], p = 1, q = 2 输出：1 提示：\n树中节点数目在范围 [2, 105] 内。 -109 \u0026lt;= Node.val \u0026lt;= 109 所有 Node.val 互不相同 。 p != q p 和 q 均存在于给定的二叉树中。 解法\n方法零：两层递归\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root==null) return null; //左子树是否含有这两个节点，如果有，就继续递归，直到左子树不再同时含有两个节点 if (doSome(root.left,p)\u0026amp;\u0026amp;doSome(root.left,q)) { return lowestCommonAncestor(root.left,p,q); } if (doSome(root.right,p)\u0026amp;\u0026amp;doSome(root.right,q)){ return lowestCommonAncestor(root.right,p,q); } return root; } // 检查 node子树里是否含有n节点 public boolean doSome(TreeNode node,TreeNode n){ if (node==null)return false; if (node.val==n.val){ return true; } boolean l = doSome(node.left,n); boolean r = doSome(node.right,n); return l||r; } } 方法一：递归\n思路和算法\n我们递归遍历整棵二叉树，定义 f~x~ 表示 x 节点的子树中是否包含 p 节点或 q 节点，如果包含为 true，否则为 false。那么符合条件的最近公共祖先 x 一定满足如下条件：(f^lson^ \u0026amp;\u0026amp; f^rson^) ∣∣ ((x = p ∣∣ x = q) \u0026amp;\u0026amp; (f^lson^ ∣∣ f^rson^))\n其中 lson 和 rson 分别代表 x 节点的左孩子和右孩子。初看可能会感觉条件判断有点复杂，我们来一条条看，f^lson^ \u0026amp;\u0026amp; f^rson^` 说明左子树和右子树均包含 p 节点或 q 节点，如果左子树包含的是 p 节点，那么右子树只能包含 q 节点，反之亦然，因为 p 节点和 q 节点都是不同且唯一的节点，因此如果满足这个判断条件即可说明 x 就是我们要找的最近公共祖先。再来看第二条判断条件，这个判断条件即是考虑了 x 恰好是 p 节点或 q 节点且它的左子树或右子树有一个包含了另一个节点的情况，因此如果满足这个判断条件亦可说明 x 就是我们要找的最近公共祖先。\n你可能会疑惑这样找出来的公共祖先深度是否是最大的。其实是最大的，因为我们是自底向上从叶子节点开始更新的，所以在所有满足条件的公共祖先中一定是深度最大的祖先先被访问到，且由于 f~x~ 本身的定义很巧妙，在找到最近公共祖先 x 以后，f~x~ 按定义被设置为 true ，即假定了这个子树中只有一个 p 节点或 q 节点，因此其他公共祖先不会再被判断为符合条件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { private TreeNode ans; public Solution() { this.ans = null; } private boolean dfs(TreeNode root, TreeNode p, TreeNode q) { if (root == null) return false; boolean lson = dfs(root.left, p, q); boolean rson = dfs(root.right, p, q); if ((lson \u0026amp;\u0026amp; rson) || ((root.val == p.val || root.val == q.val) \u0026amp;\u0026amp; (lson || rson))) { ans = root; } return lson || rson || (root.val == p.val || root.val == q.val); } public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { this.dfs(root, p, q); return this.ans; } } 复杂度分析\n时间复杂度：O(N) ，其中 N 是二叉树的节点数。二叉树的所有节点有且只会被访问一次，因此时间复杂度为 O(N)。 空间复杂度：O(N) ，其中 N 是二叉树的节点数。递归调用的栈深度取决于二叉树的高度，二叉树最坏情况下为一条链，此时高度为 N ，因此空间复杂度为 O(N)。 方法二：存储父节点\n思路\n我们可以用哈希表存储所有节点的父节点，然后我们就可以利用节点的父节点信息从 p 结点开始不断往上跳，并记录已经访问过的节点，再从 q 节点开始不断往上跳，如果碰到已经访问过的节点，那么这个节点就是我们要找的最近公共祖先。\n算法\n从根节点开始遍历整棵二叉树，用哈希表记录每个节点的父节点指针。 从 p 节点开始不断往它的祖先移动，并用数据结构记录已经访问过的祖先节点。 同样，我们再从 q 节点开始不断往它的祖先移动，如果有祖先已经被访问过，即意味着这是 p 和 q 的深度最深的公共祖先，即 LCA 节点。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { Map\u0026lt;Integer, TreeNode\u0026gt; parent = new HashMap\u0026lt;Integer, TreeNode\u0026gt;(); Set\u0026lt;Integer\u0026gt; visited = new HashSet\u0026lt;Integer\u0026gt;(); public void dfs(TreeNode root) { if (root.left != null) { parent.put(root.left.val, root); dfs(root.left); } if (root.right != null) { parent.put(root.right.val, root); dfs(root.right); } } public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { dfs(root); while (p != null) { visited.add(p.val); p = parent.get(p.val); } while (q != null) { if (visited.contains(q.val)) { return q; } q = parent.get(q.val); } return null; } } 复杂度分析\n时间复杂度：O(N)，其中 N 是二叉树的节点数。二叉树的所有节点有且只会被访问一次，从 p 和 q 节点往上跳经过的祖先节点个数不会超过 N，因此总的时间复杂度为 O(N)。 空间复杂度：O(N) ，其中 N 是二叉树的节点数。递归调用的栈深度取决于二叉树的高度，二叉树最坏情况下为一条链，此时高度为 N，因此空间复杂度为 O(N)，哈希表存储每个节点的父节点也需要 O(N) 的空间复杂度，因此最后总的空间复杂度为 O(N)。 day33 257. 二叉树的所有路径 题目\n给你一个二叉树的根节点 root ，按 任意顺序 ，返回所有从根节点到叶子节点的路径。\n叶子节点 是指没有子节点的节点。\n示例 1：\n1 2 输入：root = [1,2,3,null,5] 输出：[\u0026#34;1-\u0026gt;2-\u0026gt;5\u0026#34;,\u0026#34;1-\u0026gt;3\u0026#34;] 示例 2：\n1 2 输入：root = [1] 输出：[\u0026#34;1\u0026#34;] 提示：\n树中节点的数目在范围 [1, 100] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 解法\n方法一：深度优先搜索\n思路与算法\n最直观的方法是使用深度优先搜索。在深度优先搜索遍历二叉树时，我们需要考虑当前的节点以及它的孩子节点。\n如果当前节点不是叶子节点，则在当前的路径末尾添加该节点，并继续递归遍历该节点的每一个孩子节点。 如果当前节点是叶子节点，则在当前路径末尾添加该节点后我们就得到了一条从根节点到叶子节点的路径，将该路径加入到答案即可。 如此，当遍历完整棵二叉树以后我们就得到了所有从根节点到叶子节点的路径。当然，深度优先搜索也可以使用非递归的方式实现，这里不再赘述。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public List\u0026lt;String\u0026gt; binaryTreePaths(TreeNode root) { List\u0026lt;String\u0026gt; paths = new ArrayList\u0026lt;String\u0026gt;(); constructPaths(root, \u0026#34;\u0026#34;, paths); return paths; } public void constructPaths(TreeNode root, String path, List\u0026lt;String\u0026gt; paths) { if (root != null) { StringBuffer pathSB = new StringBuffer(path); pathSB.append(Integer.toString(root.val)); if (root.left == null \u0026amp;\u0026amp; root.right == null) { // 当前节点是叶子节点 paths.add(pathSB.toString()); // 把路径加入到答案中 } else { pathSB.append(\u0026#34;-\u0026gt;\u0026#34;); // 当前节点不是叶子节点，继续递归遍历 constructPaths(root.left, pathSB.toString(), paths); constructPaths(root.right, pathSB.toString(), paths); } } } } 复杂度分析\n时间复杂度：O(N^2^)，其中 N 表示节点数目。在深度优先搜索中每个节点会被访问一次且只会被访问一次，每一次会对 path 变量进行拷贝构造，时间代价为 O(N) ，故时间复杂度为 O(N^2^)。\n空间复杂度：O(N^2^)，其中 N 表示节点数目。除答案数组外我们需要考虑递归调用的栈空间。在最坏情况下，当二叉树中每个节点只有一个孩子节点时，即整棵二叉树呈一个链状，此时递归的层数为 N，此时每一层的 path 变量的空间代价的总和为 $$ O(\\sum_{i = 1}^{N} i) = O(N^2) $$ 空间复杂度为 O(N^2^)。最好情况下，当二叉树为平衡二叉树时，它的高度为 log N ，此时空间复杂度为 $$ O((\\log {N})^2) $$ 。\n方法二：广度优先搜索\n思路与算法\n我们也可以用广度优先搜索来实现。我们维护一个队列，存储节点以及根到该节点的路径。一开始这个队列里只有根节点。在每一步迭代中，我们取出队列中的首节点，如果它是叶子节点，则将它对应的路径加入到答案中。如果它不是叶子节点，则将它的所有孩子节点加入到队列的末尾。当队列为空时广度优先搜索结束，我们即能得到答案。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Solution { public List\u0026lt;String\u0026gt; binaryTreePaths(TreeNode root) { List\u0026lt;String\u0026gt; paths = new ArrayList\u0026lt;String\u0026gt;(); if (root == null) { return paths; } Queue\u0026lt;TreeNode\u0026gt; nodeQueue = new LinkedList\u0026lt;TreeNode\u0026gt;(); Queue\u0026lt;String\u0026gt; pathQueue = new LinkedList\u0026lt;String\u0026gt;(); nodeQueue.offer(root); pathQueue.offer(Integer.toString(root.val)); while (!nodeQueue.isEmpty()) { TreeNode node = nodeQueue.poll(); String path = pathQueue.poll(); if (node.left == null \u0026amp;\u0026amp; node.right == null) { paths.add(path); } else { if (node.left != null) { nodeQueue.offer(node.left); pathQueue.offer(new StringBuffer(path).append(\u0026#34;-\u0026gt;\u0026#34;).append(node.left.val).toString()); } if (node.right != null) { nodeQueue.offer(node.right); pathQueue.offer(new StringBuffer(path).append(\u0026#34;-\u0026gt;\u0026#34;).append(node.right.val).toString()); } } } return paths; } } 复杂度分析\n时间复杂度：O(N^2^)，其中 N 表示节点数目。分析同方法一。 空间复杂度：O(N^2^)，其中 N 表示节点数目。在最坏情况下，队列中会存在 N 个节点，保存字符串的队列中每个节点的最大长度为 N，故空间复杂度为 O(N^2^)。 day34 404. 左叶子之和 题目\n给定二叉树的根节点 root ，返回所有左叶子之和。\n示例 1：\n1 2 3 输入: root = [3,9,20,null,null,15,7] 输出: 24 解释: 在这个二叉树中，有两个左叶子，分别是 9 和 15，所以返回 24 示例 2:\n1 2 输入: root = [1] 输出: 0 提示:\n节点数在 [1, 1000] 范围内 -1000 \u0026lt;= Node.val \u0026lt;= 1000 解法\n一个节点为「左叶子」节点，当且仅当它是某个节点的左子节点，并且它是一个叶子结点。因此我们可以考虑对整棵树进行遍历，当我们遍历到节点 node 时，如果它的左子节点是一个叶子结点，那么就将它的左子节点的值累加计入答案。\n遍历整棵树的方法有深度优先搜索和广度优先搜索，下面分别给出了实现代码。\n方法一：深度优先搜索\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public int sumOfLeftLeaves(TreeNode root) { return root != null ? dfs(root) : 0; } public int dfs(TreeNode node) { int ans = 0; if (node.left != null) { ans += isLeafNode(node.left) ? node.left.val : dfs(node.left); } if (node.right != null \u0026amp;\u0026amp; !isLeafNode(node.right)) { ans += dfs(node.right); } return ans; } public boolean isLeafNode(TreeNode node) { return node.left == null \u0026amp;\u0026amp; node.right == null; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)。空间复杂度与深度优先搜索使用的栈的最大深度相关。在最坏的情况下，树呈现链式结构，深度为 O(n)，对应的空间复杂度也为 O(n)。 方法二：广度优先搜索\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public int sumOfLeftLeaves(TreeNode root) { if (root == null) { return 0; } Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;TreeNode\u0026gt;(); queue.offer(root); int ans = 0; while (!queue.isEmpty()) { TreeNode node = queue.poll(); if (node.left != null) { if (isLeafNode(node.left)) { ans += node.left.val; } else { queue.offer(node.left); } } if (node.right != null) { if (!isLeafNode(node.right)) { queue.offer(node.right); } } } return ans; } public boolean isLeafNode(TreeNode node) { return node.left == null \u0026amp;\u0026amp; node.right == null; } } 复杂度分析\n时间复杂度：O(n) ，其中 n 是树中的节点个数。 空间复杂度：O(n) 。空间复杂度与广度优先搜索使用的队列需要的容量相关，为 O(n)。 day35 513. 找树左下角的值 题目\n给定一个二叉树的 根节点 root，请找出该二叉树的 最底层 最左边 节点的值。\n假设二叉树中至少有一个节点。\n示例 1:\n1 2 输入: root = [2,1,3] 输出: 1 示例 2:\n1 2 输入: [1,2,3,4,null,5,6,null,null,7] 输出: 7 提示:\n二叉树的节点个数的范围是 [1,104] -231 \u0026lt;= Node.val \u0026lt;= 231 - 1 解法\n方法一：深度优先搜索\n使用 height 记录遍历到的节点的高度，curVal 记录高度在 curHeight 的最左节点的值。在深度优先搜索时，我们先搜索当前节点的左子节点，再搜索当前节点的右子节点，然后判断当前节点的高度 height 是否大于 curHeight，如果是，那么将 curVal 设置为当前结点的值,curHeight 设置为 height。\n因为我们先遍历左子树，然后再遍历右子树，所以对同一高度的所有节点，最左节点肯定是最先被遍历到的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { int curVal = 0; int curHeight = 0; public int findBottomLeftValue(TreeNode root) { int curHeight = 0; dfs(root, 0); return curVal; } public void dfs(TreeNode root, int height) { if (root == null) { return; } height++; dfs(root.left, height); dfs(root.right, height); // 相当于后续遍历麻，最先遍历的一定是左子节点，然后是右、根子节点 if (height \u0026gt; curHeight) { curHeight = height; curVal = root.val; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树的节点数目。需要遍历 n 个节点。 空间复杂度：O(n)。递归栈需要占用 O(n) 的空间。 方法二：广度优先搜索\n使用广度优先搜索遍历每一层的节点。在遍历一个节点时，需要先把它的非空右子节点放入队列，然后再把它的非空左子节点放入队列，这样才能保证从右到左遍历每一层的节点。广度优先搜索所遍历的最后一个节点的值就是最底层最左边节点的值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int findBottomLeftValue(TreeNode root) { int ret = 0; Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;TreeNode\u0026gt;(); queue.offer(root); while (!queue.isEmpty()) { TreeNode p = queue.poll(); if (p.right != null) { queue.offer(p.right); } if (p.left != null) { queue.offer(p.left); } ret = p.val; } return ret; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树的节点数目。 空间复杂度：O(n)。如果二叉树是满完全二叉树，那么队列 q 最多保存 2/n 个节点。 day36 559. N 叉树的最大深度 题目\n给定一个 N 叉树，找到其最大深度。\n最大深度是指从根节点到最远叶子节点的最长路径上的节点总数。\nN 叉树输入按层序遍历序列化表示，每组子节点由空值分隔（请参见示例）。\n示例 1：\n1 2 输入：root = [1,null,3,2,4,null,5,6] 输出：3 示例 2：\n1 2 输入：root = [1,null,2,3,4,5,null,null,6,7,null,8,null,9,10,null,null,11,null,12,null,13,null,null,14] 输出：5 提示：\n树的深度不会超过 1000 。 树的节点数目位于 [0, 104] 之间。 解法\n方法一：深度优先搜索\n如果根节点有 N 个子节点，则这 N 个子节点对应 N 个子树。记 N 个子树的最大深度中的最大值为 maxChildDepth，则该 N 叉树的最大深度为 maxChildDepth+1。\n每个子树的最大深度又可以以同样的方式进行计算。因此我们可以用「深度优先搜索」的方法计算 N 叉树的最大深度。具体而言，在计算当前 N 叉树的最大深度时，可以先递归计算出其每个子树的最大深度，然后在 O(1) 的时间内计算出当前 N 叉树的最大深度。递归在访问到空节点时退出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public int maxDepth(Node root) { if (root == null) { return 0; } int maxChildDepth = 0; List\u0026lt;Node\u0026gt; children = root.children; for (Node child : children) { int childDepth = maxDepth(child); maxChildDepth = Math.max(maxChildDepth, childDepth); } return maxChildDepth + 1; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为 N 叉树节点的个数。每个节点在递归中只被遍历一次。 空间复杂度：O(height)，其中 height 表示 N 叉树的高度。递归函数需要栈空间，而栈空间取决于递归的深度，因此空间复杂度等价于 N 叉树的高度。 方法二：广度优先搜索\n我们也可以用「广度优先搜索」的方法来解决这道题目，但我们需要对其进行一些修改，此时我们广度优先搜索的队列里存放的是「当前层的所有节点」。每次拓展下一层的时候，不同于广度优先搜索的每次只从队列里拿出一个节点，我们需要将队列里的所有节点都拿出来进行拓展，这样能保证每次拓展完的时候队列里存放的是当前层的所有节点，即我们是一层一层地进行拓展。最后我们用一个变量 ans 来维护拓展的次数，该 N 叉树的最大深度即为 ans。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public int maxDepth(Node root) { if (root == null) { return 0; } Queue\u0026lt;Node\u0026gt; queue = new LinkedList\u0026lt;Node\u0026gt;(); queue.offer(root); int ans = 0; while (!queue.isEmpty()) { int size = queue.size(); while (size \u0026gt; 0) { Node node = queue.poll(); List\u0026lt;Node\u0026gt; children = node.children; for (Node child : children) { queue.offer(child); } size--; } ans++; } return ans; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为 N 叉树的节点个数。与方法一同样的分析，每个节点只会被访问一次。 空间复杂度：此方法空间的消耗取决于队列存储的元素数量，其在最坏情况下会达到 O(n)。 day37 617. 合并二叉树 题目\n给你两棵二叉树： root1 和 root2 。\n想象一下，当你将其中一棵覆盖到另一棵之上时，两棵树上的一些节点将会重叠（而另一些不会）。你需要将这两棵树合并成一棵新二叉树。合并的规则是：如果两个节点重叠，那么将这两个节点的值相加作为合并后节点的新值；否则，不为 null 的节点将直接作为新二叉树的节点。\n返回合并后的二叉树。\n注意: 合并过程必须从两个树的根节点开始。\n示例 1：\n1 2 输入：root1 = [1,3,2,5], root2 = [2,1,3,null,4,null,7] 输出：[3,4,5,5,4,null,7] 示例 2：\n1 2 输入：root1 = [1], root2 = [1,2] 输出：[2,2] 提示：\n两棵树中的节点数目在范围 [0, 2000] 内 -104 \u0026lt;= Node.val \u0026lt;= 104 解法\n方法一：深度优先搜索\n可以使用深度优先搜索合并两个二叉树。从根节点开始同时遍历两个二叉树，并将对应的节点进行合并。\n两个二叉树的对应节点可能存在以下三种情况，对于每种情况使用不同的合并方式。\n如果两个二叉树的对应节点都为空，则合并后的二叉树的对应节点也为空； 如果两个二叉树的对应节点只有一个为空，则合并后的二叉树的对应节点为其中的非空节点； 如果两个二叉树的对应节点都不为空，则合并后的二叉树的对应节点的值为两个二叉树的对应节点的值之和，此时需要显性合并两个节点。 对一个节点进行合并之后，还要对该节点的左右子树分别进行合并。这是一个递归的过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public TreeNode mergeTrees(TreeNode root1, TreeNode root2) { TreeNode node; // 定义一个局部变量用于递归时，本层节点记录 if(root1==null\u0026amp;\u0026amp;root2==null){// 当两个根节点都是null，直接返回null return null; }else if(root1==null){// 当两个根节点有一个为空时，也直接返回非空节点，没必要加 return root2; }else if(root2==null){// 当两个根节点有一个为空时，也直接返回非空节点，没必要加 return root1; }else{// 当两个根节点都不为空的时候，先记录合并后的根节点 node = new TreeNode(root1.val+root2.val); } // 然后再递归的遍历左右子树 TreeNode left = mergeTrees(root1.left,root2.left); TreeNode right = mergeTrees(root1.right,root2.right); // 将左右子树的结果挂在新的根节点上 node.left = left; node.right = right; return node; } } 复杂度分析\n时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。对两个二叉树同时进行深度优先搜索，只有当两个二叉树中的对应节点都不为空时才会对该节点进行显性合并操作，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。空间复杂度取决于递归调用的层数，递归调用的层数不会超过较小的二叉树的最大高度，最坏情况下，二叉树的高度等于节点数。 方法二：广度优先搜索\n也可以使用广度优先搜索合并两个二叉树。首先判断两个二叉树是否为空，如果两个二叉树都为空，则合并后的二叉树也为空，如果只有一个二叉树为空，则合并后的二叉树为另一个非空的二叉树。\n如果两个二叉树都不为空，则首先计算合并后的根节点的值，然后从合并后的二叉树与两个原始二叉树的根节点开始广度优先搜索，从根节点开始同时遍历每个二叉树，并将对应的节点进行合并。\n使用三个队列分别存储合并后的二叉树的节点以及两个原始二叉树的节点。初始时将每个二叉树的根节点分别加入相应的队列。每次从每个队列中取出一个节点，判断两个原始二叉树的节点的左右子节点是否为空。如果两个原始二叉树的当前节点中至少有一个节点的左子节点不为空，则合并后的二叉树的对应节点的左子节点也不为空。对于右子节点同理。\n如果合并后的二叉树的左子节点不为空，则需要根据两个原始二叉树的左子节点计算合并后的二叉树的左子节点以及整个左子树。考虑以下两种情况：\n如果两个原始二叉树的左子节点都不为空，则合并后的二叉树的左子节点的值为两个原始二叉树的左子节点的值之和，在创建合并后的二叉树的左子节点之后，将每个二叉树中的左子节点都加入相应的队列； 如果两个原始二叉树的左子节点有一个为空，即有一个原始二叉树的左子树为空，则合并后的二叉树的左子树即为另一个原始二叉树的左子树，此时也不需要对非空左子树继续遍历，因此不需要将左子节点加入队列。 对于右子节点和右子树，处理方法与左子节点和左子树相同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null) { return t2; } if (t2 == null) { return t1; } TreeNode merged = new TreeNode(t1.val + t2.val); Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;TreeNode\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue1 = new LinkedList\u0026lt;TreeNode\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue2 = new LinkedList\u0026lt;TreeNode\u0026gt;(); queue.offer(merged); queue1.offer(t1); queue2.offer(t2); while (!queue1.isEmpty() \u0026amp;\u0026amp; !queue2.isEmpty()) { TreeNode node = queue.poll(), node1 = queue1.poll(), node2 = queue2.poll(); TreeNode left1 = node1.left, left2 = node2.left, right1 = node1.right, right2 = node2.right; if (left1 != null || left2 != null) { if (left1 != null \u0026amp;\u0026amp; left2 != null) { TreeNode left = new TreeNode(left1.val + left2.val); node.left = left; queue.offer(left); queue1.offer(left1); queue2.offer(left2); } else if (left1 != null) { node.left = left1; } else if (left2 != null) { node.left = left2; } } if (right1 != null || right2 != null) { if (right1 != null \u0026amp;\u0026amp; right2 != null) { TreeNode right = new TreeNode(right1.val + right2.val); node.right = right; queue.offer(right); queue1.offer(right1); queue2.offer(right2); } else if (right1 != null) { node.right = right1; } else { node.right = right2; } } } return merged; } } 复杂度分析\n时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。对两个二叉树同时进行广度优先搜索，只有当两个二叉树中的对应节点都不为空时才会访问到该节点，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。空间复杂度取决于队列中的元素个数，队列中的元素个数不会超过较小的二叉树的节点数。 day38 654. 最大二叉树 题目\n给定一个不重复的整数数组 nums 。 最大二叉树 可以用下面的算法从 nums 递归地构建:\n创建一个根节点，其值为 nums 中的最大值。 递归地在最大值 左边 的 子数组前缀上 构建左子树。 递归地在最大值 右边 的 子数组后缀上 构建右子树。 返回 nums 构建的 *最大二叉树* 。\n示例 1：\n1 2 3 4 5 6 7 8 9 10 11 12 输入：nums = [3,2,1,6,0,5] 输出：[6,3,5,null,2,0,null,null,1] 解释：递归调用如下所示： - [3,2,1,6,0,5] 中的最大值是 6 ，左边部分是 [3,2,1] ，右边部分是 [0,5] 。 - [3,2,1] 中的最大值是 3 ，左边部分是 [] ，右边部分是 [2,1] 。 - 空数组，无子节点。 - [2,1] 中的最大值是 2 ，左边部分是 [] ，右边部分是 [1] 。 - 空数组，无子节点。 - 只有一个元素，所以子节点是一个值为 1 的节点。 - [0,5] 中的最大值是 5 ，左边部分是 [0] ，右边部分是 [] 。 - 只有一个元素，所以子节点是一个值为 0 的节点。 - 空数组，无子节点。 示例 2：\n1 2 输入：nums = [3,2,1] 输出：[3,null,2,null,1] 提示：\n1 \u0026lt;= nums.length \u0026lt;= 1000 0 \u0026lt;= nums[i] \u0026lt;= 1000 nums 中的所有整数 互不相同 解法\n方法一：递归\n思路与算法\n最简单的方法是直接按照题目描述进行模拟。\n我们用递归函数 construct(nums,left,right) 表示对数组 nums 中从 nums 到 的元素构建一棵树。我们首先找到这一区间中的最大值，记为 nums 中从 nums[best]，这样就确定了根节点的值。随后我们就可以进行递归：\n左子树为 construct(nums, left, best-1)； 右子树为 construct(nums,best+1,right)。 当递归到一个无效的区间（即 left*\u0026gt;*right）时，便可以返回一棵空的树。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public TreeNode constructMaximumBinaryTree(int[] nums) { return construct(nums, 0, nums.length - 1); } public TreeNode construct(int[] nums, int left, int right) { if (left \u0026gt; right) { return null; } int best = left; for (int i = left + 1; i \u0026lt;= right; ++i) { if (nums[i] \u0026gt; nums[best]) { best = i; } } TreeNode node = new TreeNode(nums[best]); node.left = construct(nums, left, best - 1); node.right = construct(nums, best + 1, right); return node; } } 复杂度分析\n时间复杂度：O(n^2^)，其中 n 是数组 nums 的长度。在最坏的情况下，数组严格递增或递减，需要递归 n 层，第 i (0≤i\u0026lt;n) 层需要遍历 n-i 个元素以找出最大值，总时间复杂度为 O(n^2^)。 空间复杂度：O(n)，即为最坏情况下需要使用的栈空间。 方法二：单调栈\n思路与算法\n我们可以将题目中构造树的过程等价转换为下面的构造过程：\n初始时，我们只有一个根节点，其中存储了整个数组； 在每一步操作中，我们可以「任选」一个存储了超过一个数的节点，找出其中的最大值并存储在该节点。最大值左侧的数组部分下放到该节点的左子节点，右侧的数组部分下放到该节点的右子节点； 如果所有的节点都恰好存储了一个数，那么构造结束。 由于最终构造出的是一棵树，因此无需按照题目的要求「递归」地进行构造，而是每次可以「任选」一个节点进行构造。这里可以类比一棵树的「深度优先搜索」和「广度优先搜索」，二者都可以起到遍历整棵树的效果。\n既然可以任意进行选择，那么我们不妨每次选择数组中最大值最大的那个节点进行构造。这样一来，我们就可以保证按照数组中元素降序排序的顺序依次构造每个节点。因此：\n当我们选择的节点中数组的最大值为 nums[i] 时，所有大于 nums[i] 的元素已经被构造过（即被单独放入某一个节点中），所有小于 nums[i] 的元素还没有被构造过。\n这就说明：\n在最终构造出的树上，以 nums[i] 为根节点的子树，在原数组中对应的区间，左边界为 nums[i] 左侧第一个比它大的元素所在的位置，右边界为 nums[i] 右侧第一个比它大的元素所在的位置。左右边界均为开边界。\n如果某一侧边界不存在，则那一侧边界为数组的边界。如果两侧边界均不存在，说明其为最大值，即根节点。\n并且：\nnums[i] 的父结点是两个边界中较小的那个元素对应的节点。\n因此，我们的任务变为：找出每一个元素左侧和右侧第一个比它大的元素所在的位置。这就是一个经典的单调栈问题了，可以参考 503. 下一个更大元素 II。如果左侧的元素较小，那么该元素就是左侧元素的右子节点；如果右侧的元素较小，那么该元素就是右侧元素的左子节点。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Solution { public TreeNode constructMaximumBinaryTree(int[] nums) { int n = nums.length; Deque\u0026lt;Integer\u0026gt; stack = new ArrayDeque\u0026lt;Integer\u0026gt;(); int[] left = new int[n]; int[] right = new int[n]; Arrays.fill(left, -1); Arrays.fill(right, -1); TreeNode[] tree = new TreeNode[n]; for (int i = 0; i \u0026lt; n; ++i) { tree[i] = new TreeNode(nums[i]); while (!stack.isEmpty() \u0026amp;\u0026amp; nums[i] \u0026gt; nums[stack.peek()]) { right[stack.pop()] = i; } if (!stack.isEmpty()) { left[i] = stack.peek(); } stack.push(i); } TreeNode root = null; for (int i = 0; i \u0026lt; n; ++i) { if (left[i] == -1 \u0026amp;\u0026amp; right[i] == -1) { root = tree[i]; } else if (right[i] == -1 || (left[i] != -1 \u0026amp;\u0026amp; nums[left[i]] \u0026lt; nums[right[i]])) { tree[left[i]].right = tree[i]; } else { tree[right[i]].left = tree[i]; } } return root; } } 我们还可以把最后构造树的过程放进单调栈求解的步骤中，省去用来存储左右边界的数组。下面的代码理解起来较为困难，同一个节点的左右子树会被多次赋值，读者可以仔细品味其妙处所在。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public TreeNode constructMaximumBinaryTree(int[] nums) { int n = nums.length; List\u0026lt;Integer\u0026gt; stack = new ArrayList\u0026lt;Integer\u0026gt;(); TreeNode[] tree = new TreeNode[n]; for (int i = 0; i \u0026lt; n; ++i) { tree[i] = new TreeNode(nums[i]); while (!stack.isEmpty() \u0026amp;\u0026amp; nums[i] \u0026gt; nums[stack.get(stack.size() - 1)]) { tree[i].left = tree[stack.get(stack.size() - 1)]; stack.remove(stack.size() - 1); } if (!stack.isEmpty()) { tree[stack.get(stack.size() - 1)].right = tree[i]; } stack.add(i); } return tree[stack.get(0)]; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是数组 nums 的长度。单调栈求解左右边界和构造树均需要 O(n) 的时间。 空间复杂度：O(n)，即为单调栈和数组 tree 需要使用的空间。 day39 144. 二叉树的前序遍历 题目\n给你二叉树的根节点 root ，返回它节点值的 前序 遍历。\n示例 1：\n1 2 输入：root = [1,null,2,3] 输出：[1,2,3] 示例 2：\n1 2 输入：root = [] 输出：[] 示例 3：\n1 2 输入：root = [1] 输出：[1] 示例 4：\n1 2 输入：root = [1,2] 输出：[1,2] 示例 5：\n1 2 输入：root = [1,null,2] 输出：[1,2] 提示：\n树中节点数目在范围 [0, 100] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 **进阶：**递归算法很简单，你可以通过迭代算法完成吗？\n解法\n方法一：递归\n思路与算法\n首先我们需要了解什么是二叉树的前序遍历：按照访问根节点——左子树——右子树的方式遍历这棵树，而在访问左子树或者右子树的时候，我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。\n定义 preorder(root) 表示当前遍历到 root 节点的答案。按照定义，我们只要首先将 root 节点的值加入答案，然后递归调用 preorder(root.left) 来遍历 root 节点的左子树，最后递归调用 preorder(root.right) 来遍历 root 节点的右子树即可，递归终止的条件为碰到空节点。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); preorder(root, res); return res; } public void preorder(TreeNode root, List\u0026lt;Integer\u0026gt; res) { if (root == null) { return; } res.add(root.val); preorder(root.left, res); preorder(root.right, res); } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为递归过程中栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法二：迭代\n思路与算法\n我们也可以用迭代的方式实现方法一的递归函数，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其余的实现与细节都相同，具体可以参考下面的代码。\n初始化维护一个栈，将根节点入栈。\n当栈不为空时\n弹出栈顶元素 node，将节点值加入结果数组中。 若 node 的右子树不为空，右子树入栈。 若 node 的左子树不为空，左子树入栈。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); if (root == null) { return res; } Deque\u0026lt;TreeNode\u0026gt; stack = new LinkedList\u0026lt;TreeNode\u0026gt;(); TreeNode node = root; while (!stack.isEmpty() || node != null) { while (node != null) { res.add(node.val); stack.push(node); node = node.left; } node = stack.pop(); node = node.right; } return res; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为迭代过程中显式栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法三：Morris 遍历\n思路与算法\n有一种巧妙的方法可以在线性时间内，只占用常数空间来实现前序遍历。这种方法由 J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出，因此被称为 Morris 遍历。\nMorris 遍历的核心思想是利用树的大量空闲指针，实现空间开销的极限缩减。其前序遍历规则总结如下：\n新建临时节点，令该节点为 root； 如果当前节点的左子节点为空，将当前节点加入答案，并遍历当前节点的右子节点； 如果当前节点的左子节点不为空，在当前节点的左子树中找到当前节点在中序遍历下的前驱节点： 如果前驱节点的右子节点为空，将前驱节点的右子节点设置为当前节点。然后将当前节点加入答案，并将前驱节点的右子节点更新为当前节点。当前节点更新为当前节点的左子节点。 如果前驱节点的右子节点为当前节点，将它的右子节点重新设为空。当前节点更新为当前节点的右子节点。 重复步骤 2 和步骤 3，直到遍历结束。 这样我们利用 Morris 遍历的方法，前序遍历该二叉树，即可实现线性时间与常数空间的遍历。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); if (root == null) { return res; } TreeNode p1 = root, p2 = null; while (p1 != null) { p2 = p1.left; if (p2 != null) { while (p2.right != null \u0026amp;\u0026amp; p2.right != p1) { p2 = p2.right; } if (p2.right == null) { res.add(p1.val); p2.right = p1; p1 = p1.left; continue; } else { p2.right = null; } } else { res.add(p1.val); } p1 = p1.right; } return res; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树的节点数。没有左子树的节点只被访问一次，有左子树的节点被访问两次。 空间复杂度：O(1)。只操作已经存在的指针（树的空闲指针），因此只需要常数的额外空间。 day40 94. 二叉树的中序遍历 题目\n给定一个二叉树的根节点 root ，返回 它的 中序 遍历 。\n示例 1：\n1 2 输入：root = [1,null,2,3] 输出：[1,3,2] 示例 2：\n1 2 输入：root = [] 输出：[] 示例 3：\n1 2 输入：root = [1] 输出：[1] 提示：\n树中节点数目在范围 [0, 100] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 进阶: 递归算法很简单，你可以通过迭代算法完成吗？\n解法\n方法一：递归\n思路与算法\n首先我们需要了解什么是二叉树的中序遍历：按照访问左子树——根节点——右子树的方式遍历这棵树，而在访问左子树或者右子树的时候我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。\n定义 inorder(root) 表示当前遍历到 root 节点的答案，那么按照定义，我们只要递归调用 inorder(root.left) 来遍历 root 节点的左子树，然后将 root 节点的值加入答案，再递归调用inorder(root.right) 来遍历 root 节点的右子树即可，递归终止的条件为碰到空节点。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); inorder(root, res); return res; } public void inorder(TreeNode root, List\u0026lt;Integer\u0026gt; res) { if (root == null) { return; } inorder(root.left, res); res.add(root.val); inorder(root.right, res); } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树节点的个数。二叉树的遍历中每个节点会被访问一次且只会被访问一次。 空间复杂度：O(n)。空间复杂度取决于递归的栈深度，而栈深度在二叉树为一条链的情况下会达到 O(n) 的级别。 方法二：迭代\n思路与算法\n方法一的递归函数我们也可以用迭代的方式实现，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其他都相同，具体实现可以看下面的代码。\n初始化一个空栈。\n当【根节点不为空】或者【栈不为空】时，从根节点开始\n若当前节点有左子树，一直遍历左子树，每次将当前节点压入栈中。 若当前节点无左子树，从栈中弹出该节点，尝试访问该节点的右子树。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); Deque\u0026lt;TreeNode\u0026gt; stk = new LinkedList\u0026lt;TreeNode\u0026gt;(); while (root != null || !stk.isEmpty()) { while (root != null) { stk.push(root); root = root.left; } root = stk.pop(); res.add(root.val); root = root.right; } return res; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树节点的个数。二叉树的遍历中每个节点会被访问一次且只会被访问一次。 空间复杂度：O(n)。空间复杂度取决于栈深度，而栈深度在二叉树为一条链的情况下会达到 O(n) 的级别。 方法三：Morris 中序遍历\n思路与算法\nMorris 遍历算法是另一种遍历二叉树的方法，它能将非递归的中序遍历空间复杂度降为 O(1)。\nMorris 遍历算法整体步骤如下（假设当前遍历到的节点为 x）：\n如果 x 无左孩子，先将 x 的值加入答案数组，再访问 x 的右孩子，即x*=*x.right。 如果 x 有左孩子，则找到 x 左子树上最右的节点（即左子树中序遍历的最后一个节点，x 在中序遍历中的前驱节点），我们记为predecessor. 根据predecessor的右孩子是否为空，进行如下操作。 如果 predecessor 的右孩子为空，则将其右孩子指向 x，然后访问 x 的左孩子，即 x = x.left。 如果 predecessor 的右孩子不为空，则此时其右孩子指向 x，说明我们已经遍历完 x 的左子树，我们将 predecessor 的右孩子置空，将 x 的值加入答案数组，然后访问 x 的右孩子，即 x*=*x.right。 重复上述操作，直至访问完整棵树。 其实整个过程我们就多做一步：假设当前遍历到的节点为 x，将 x 的左子树中最右边的节点的右孩子指向 x，这样在左子树遍历完成后我们通过这个指向走回了 x，且能通过这个指向知晓我们已经遍历完成了左子树，而不用再通过栈来维护，省去了栈的空间复杂度。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); TreeNode predecessor = null; while (root != null) { if (root.left != null) { // predecessor 节点就是当前 root 节点向左走一步，然后一直向右走至无法走为止 predecessor = root.left; while (predecessor.right != null \u0026amp;\u0026amp; predecessor.right != root) { predecessor = predecessor.right; } // 让 predecessor 的右指针指向 root，继续遍历左子树 if (predecessor.right == null) { predecessor.right = root; root = root.left; } // 说明左子树已经访问完了，我们需要断开链接 else { res.add(root.val); predecessor.right = null; root = root.right; } } // 如果没有左孩子，则直接访问右孩子 else { res.add(root.val); root = root.right; } } return res; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉搜索树的节点个数。Morris 遍历中每个节点会被访问两次，因此总时间复杂度为 O(2n)=O(n)。 空间复杂度：O(1)。 day41 105. 从前序与中序遍历序列构造二叉树 题目\n给定两个整数数组 preorder 和 inorder ，其中 preorder 是二叉树的先序遍历， inorder 是同一棵树的中序遍历，请构造二叉树并返回其根节点。\n示例 1:\n1 2 输入: preorder = [3,9,20,15,7], inorder = [9,3,15,20,7] 输出: [3,9,20,null,null,15,7] 示例 2:\n1 2 输入: preorder = [-1], inorder = [-1] 输出: [-1] 提示:\n1 \u0026lt;= preorder.length \u0026lt;= 3000 inorder.length == preorder.length -3000 \u0026lt;= preorder[i], inorder[i] \u0026lt;= 3000 preorder 和 inorder 均 无重复 元素 inorder 均出现在 preorder preorder 保证 为二叉树的前序遍历序列 inorder 保证 为二叉树的中序遍历序列 解法\n二叉树前序遍历的顺序为：\n先遍历根节点； 随后递归地遍历左子树； 最后递归地遍历右子树。 二叉树中序遍历的顺序为：\n先递归地遍历左子树； 随后遍历根节点； 最后递归地遍历右子树。 在「递归」地遍历某个子树的过程中，我们也是将这颗子树看成一颗全新的树，按照上述的顺序进行遍历。挖掘「前序遍历」和「中序遍历」的性质，我们就可以得出本题的做法。\n方法一：递归\n思路\n对于任意一颗树而言，前序遍历的形式总是\n1 [ 根节点, [左子树的前序遍历结果], [右子树的前序遍历结果] ] 即根节点总是前序遍历中的第一个节点。而中序遍历的形式总是\n1 [ [左子树的中序遍历结果], 根节点, [右子树的中序遍历结果] ] 只要我们在中序遍历中定位到根节点，那么我们就可以分别知道左子树和右子树中的节点数目。由于同一颗子树的前序遍历和中序遍历的长度显然是相同的，因此我们就可以对应到前序遍历的结果中，对上述形式中的所有左右括号进行定位。\n这样以来，我们就知道了左子树的前序遍历和中序遍历结果，以及右子树的前序遍历和中序遍历结果，我们就可以递归地对构造出左子树和右子树，再将这两颗子树接到根节点的左右位置。\n细节\n在中序遍历中对根节点进行定位时，一种简单的方法是直接扫描整个中序遍历的结果并找出根节点，但这样做的时间复杂度较高。我们可以考虑使用哈希表来帮助我们快速地定位根节点。对于哈希映射中的每个键值对，键表示一个元素（节点的值），值表示其在中序遍历中的出现位置。在构造二叉树的过程之前，我们可以对中序遍历的列表进行一遍扫描，就可以构造出这个哈希映射。在此后构造二叉树的过程中，我们就只需要 O(1) 的时间对根节点进行定位了。\n下面的代码给出了详细的注释。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Solution { private Map\u0026lt;Integer, Integer\u0026gt; indexMap; public TreeNode myBuildTree(int[] preorder, int[] inorder, int preorder_left, int preorder_right, int inorder_left, int inorder_right) { if (preorder_left \u0026gt; preorder_right) { return null; } // 前序遍历中的第一个节点就是根节点 int preorder_root = preorder_left; // 在中序遍历中定位根节点 int inorder_root = indexMap.get(preorder[preorder_root]); // 先把根节点建立出来 TreeNode root = new TreeNode(preorder[preorder_root]); // 得到左子树中的节点数目 int size_left_subtree = inorder_root - inorder_left; // 递归地构造左子树，并连接到根节点 // 先序遍历中「从 左边界+1 开始的 size_left_subtree」个元素就对应了中序遍历中「从 左边界 开始到 根节点定位-1」的元素 root.left = myBuildTree(preorder, inorder, preorder_left + 1, preorder_left + size_left_subtree, inorder_left, inorder_root - 1); // 递归地构造右子树，并连接到根节点 // 先序遍历中「从 左边界+1+左子树节点数目 开始到 右边界」的元素就对应了中序遍历中「从 根节点定位+1 到 右边界」的元素 root.right = myBuildTree(preorder, inorder, preorder_left + size_left_subtree + 1, preorder_right, inorder_root + 1, inorder_right); return root; } public TreeNode buildTree(int[] preorder, int[] inorder) { int n = preorder.length; // 构造哈希映射，帮助我们快速定位根节点 indexMap = new HashMap\u0026lt;Integer, Integer\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { indexMap.put(inorder[i], i); } return myBuildTree(preorder, inorder, 0, n - 1, 0, n - 1); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(preorder []int, inorder []int) *TreeNode { return dfs(preorder,inorder) } func dfs(preorder,inorder []int)*TreeNode{ if len(preorder)==0{ return nil } // fmt.Println(preorder[prel]) root:=\u0026amp;TreeNode{Val:preorder[0]}// 创建根节点 inRootIdx:=0//取中序遍历根节点索引 for ;inRootIdx\u0026lt;len(inorder);inRootIdx++{ if inorder[inRootIdx]==preorder[0]{ break } } root.Left = dfs(preorder[1:len(inorder[:inRootIdx])+1],inorder[:inRootIdx]) root.Right = dfs(preorder[len(inorder[:inRootIdx])+1:],inorder[inRootIdx+1:]) return root } 复杂度分析\n时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)，除去返回的答案需要的 O(n) 空间之外，我们还需要使用 O(n) 的空间存储哈希映射，以及 O(h)（其中 h 是树的高度）的空间表示递归时栈空间。这里 h \u0026lt; n，所以总空间复杂度为 O(n)。 方法二：迭代\n思路\n迭代法是一种非常巧妙的实现方法。\n对于前序遍历中的任意两个连续节点 u 和 v，根据前序遍历的流程，我们可以知道 u 和 v 只有两种可能的关系：\nv 是 u 的左儿子。这是因为在遍历到 u 之后，下一个遍历的节点就是 u 的左儿子，即 v； u 没有左儿子，并且 v 是 u 的某个祖先节点（或者 u 本身）的右儿子。如果 u 没有左儿子，那么下一个遍历的节点就是 u 的右儿子。如果 u 没有右儿子，我们就会向上回溯，直到遇到第一个有右儿子（且 u 不在它的右儿子的子树中）的节点 u_a，那么 v 就是 u_a 的右儿子。 第二种关系看上去有些复杂。我们举一个例子来说明其正确性，并在例子中给出我们的迭代算法。\n例子\n我们以树\n1 2 3 4 5 6 7 8 9 3 / \\ 9 20 / / \\ 8 15 7 / \\ 5 10 / 4 为例，它的前序遍历和中序遍历分别为\n1 2 preorder = [3, 9, 8, 5, 4, 10, 20, 15, 7] inorder = [4, 5, 8, 10, 9, 3, 15, 20, 7] 我们用一个栈 stack 来维护「当前节点的所有还没有考虑过右儿子的祖先节点」，栈顶就是当前节点。也就是说，只有在栈中的节点才可能连接一个新的右儿子。同时，我们用一个指针 index 指向中序遍历的某个位置，初始值为 0。index 对应的节点是「当前节点不断往左走达到的最终节点」，这也是符合中序遍历的，它的作用在下面的过程中会有所体现。\n首先我们将根节点 3 入栈，再初始化 index 所指向的节点为 4，随后对于前序遍历中的每个节点，我们依次判断它是栈顶节点的左儿子，还是栈中某个节点的右儿子。\n我们遍历 9。9 一定是栈顶节点 3 的左儿子。我们使用反证法，假设 9 是 3 的右儿子，那么 3 没有左儿子，index 应该恰好指向 3，但实际上为 4，因此产生了矛盾。所以我们将 9 作为 3 的左儿子，并将 9 入栈。\nstack = [3, 9] index -\u0026gt; inorder[0] = 4 我们遍历 8，5 和 4。同理可得它们都是上一个节点（栈顶节点）的左儿子，所以它们会依次入栈。\nstack = [3, 9, 8, 5, 4] index -\u0026gt; inorder[0] = 4 我们遍历 10，这时情况就不一样了。我们发现 index 恰好指向当前的栈顶节点 4，也就是说 4 没有左儿子，那么 10 必须为栈中某个节点的右儿子。那么如何找到这个节点呢？栈中的节点的顺序和它们在前序遍历中出现的顺序是一致的，而且每一个节点的右儿子都还没有被遍历过，那么这些节点的顺序和它们在中序遍历中出现的顺序一定是相反的。\n这是因为栈中的任意两个相邻的节点，前者都是后者的某个祖先。并且我们知道，栈中的任意一个节点的右儿子还没有被遍历过，说明后者一定是前者左儿子的子树中的节点，那么后者就先于前者出现在中序遍历中。\n因此我们可以把 index 不断向右移动，并与栈顶节点进行比较。如果 index 对应的元素恰好等于栈顶节点，那么说明我们在中序遍历中找到了栈顶节点，所以将 index 增加 1 并弹出栈顶节点，直到 index 对应的元素不等于栈顶节点。按照这样的过程，我们弹出的最后一个节点 x 就是 10 的双亲节点，这是因为 10 出现在了 x 与 x 在栈中的下一个节点的中序遍历之间，因此 10 就是 x 的右儿子。\n回到我们的例子，我们会依次从栈顶弹出 4，5 和 8，并且将 index 向右移动了三次。我们将 10 作为最后弹出的节点 8 的右儿子，并将 10 入栈。\nstack = [3, 9, 10] index -\u0026gt; inorder[3] = 10 我们遍历 20。同理，index 恰好指向当前栈顶节点 10，那么我们会依次从栈顶弹出 10，9 和 3，并且将 index 向右移动了三次。我们将 20 作为最后弹出的节点 3 的右儿子，并将 20 入栈。\nstack = [20] index -\u0026gt; inorder[6] = 15 我们遍历 15，将 15 作为栈顶节点 20 的左儿子，并将 15 入栈。\nstack = [20, 15] index -\u0026gt; inorder[6] = 15 我们遍历 7。index 恰好指向当前栈顶节点 15，那么我们会依次从栈顶弹出 15 和 20，并且将 index 向右移动了两次。我们将 7 作为最后弹出的节点 20 的右儿子，并将 7 入栈。\nstack = [7] index -\u0026gt; inorder[8] = 7 此时遍历结束，我们就构造出了正确的二叉树。\n算法\n我们归纳出上述例子中的算法流程：\n我们用一个栈和一个指针辅助进行二叉树的构造。初始时栈中存放了根节点（前序遍历的第一个节点），指针指向中序遍历的第一个节点； 我们依次枚举前序遍历中除了第一个节点以外的每个节点。如果 index 恰好指向栈顶节点，那么我们不断地弹出栈顶节点并向右移动 index，并将当前节点作为最后一个弹出的节点的右儿子；如果 index 和栈顶节点不同，我们将当前节点作为栈顶节点的左儿子； 无论是哪一种情况，我们最后都将当前的节点入栈。 最后得到的二叉树即为答案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public TreeNode buildTree(int[] preorder, int[] inorder) { if (preorder == null || preorder.length == 0) { return null; } TreeNode root = new TreeNode(preorder[0]); Deque\u0026lt;TreeNode\u0026gt; stack = new LinkedList\u0026lt;TreeNode\u0026gt;(); stack.push(root); int inorderIndex = 0; for (int i = 1; i \u0026lt; preorder.length; i++) { int preorderVal = preorder[i]; TreeNode node = stack.peek(); if (node.val != inorder[inorderIndex]) { node.left = new TreeNode(preorderVal); stack.push(node.left); } else { while (!stack.isEmpty() \u0026amp;\u0026amp; stack.peek().val == inorder[inorderIndex]) { node = stack.pop(); inorderIndex++; } node.right = new TreeNode(preorderVal); stack.push(node.right); } } return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)，除去返回的答案需要的 O(n) 空间之外，我们还需要使用 O(h)（其中 h 是树的高度）的空间存储栈。这里 h \u0026lt; n，所以（在最坏情况下）总空间复杂度为 O(n)。 day42 106. 从中序与后序遍历序列构造二叉树 题目\n给定两个整数数组 inorder 和 postorder ，其中 inorder 是二叉树的中序遍历， postorder 是同一棵树的后序遍历，请你构造并返回这颗 二叉树 。\n示例 1:\n1 2 输入：inorder = [9,3,15,20,7], postorder = [9,15,7,20,3] 输出：[3,9,20,null,null,15,7] 示例 2:\n1 2 输入：inorder = [-1], postorder = [-1] 输出：[-1] 提示:\n1 \u0026lt;= inorder.length \u0026lt;= 3000 postorder.length == inorder.length -3000 \u0026lt;= inorder[i], postorder[i] \u0026lt;= 3000 inorder 和 postorder 都由 不同 的值组成 postorder 中每一个值都在 inorder 中 inorder 保证是树的中序遍历 postorder 保证是树的后序遍历 解法\n方法一：递归\n我们可以发现后序遍历的数组最后一个元素代表的即为根节点。知道这个性质后，我们可以利用已知的根节点信息在中序遍历的数组中找到根节点所在的下标，然后根据其将中序遍历的数组分成左右两部分，左边部分即左子树，右边部分为右子树，针对每个部分可以用同样的方法继续递归下去构造。\n算法\n为了高效查找根节点元素在中序遍历数组中的下标，我们选择创建哈希表来存储中序序列，即建立一个（元素，下标）键值对的哈希表。 定义递归函数 helper(in_left, in_right) 表示当前递归到中序序列中当前子树的左右边界，递归入口为helper(0, n - 1) ： 如果 in_left \u0026gt; in_right，说明子树为空，返回空节点。 选择后序遍历的最后一个节点作为根节点。 利用哈希表 O(1) 查询当根节点在中序遍历中下标为 index。从 in_left 到 index - 1 属于左子树，从 index + 1 到 in_right 属于右子树。 根据后序遍历逻辑，递归创建右子树 helper(index + 1, in_right) 和左子树 helper(in_left, index - 1)。注意这里有需要先创建右子树，再创建左子树的依赖关系。可以理解为在后序遍历的数组中整个数组是先存储左子树的节点，再存储右子树的节点，最后存储根节点，如果按每次选择「后序遍历的最后一个节点」为根节点，则先被构造出来的应该为右子树。 返回根节点 root 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class Solution { int post_idx; int[] postorder; int[] inorder; Map\u0026lt;Integer, Integer\u0026gt; idx_map = new HashMap\u0026lt;Integer, Integer\u0026gt;(); public TreeNode helper(int in_left, int in_right) { // 如果这里没有节点构造二叉树了，就结束 if (in_left \u0026gt; in_right) { return null; } // 选择 post_idx 位置的元素作为当前子树根节点 int root_val = postorder[post_idx]; TreeNode root = new TreeNode(root_val); // 根据 root 所在位置分成左右两棵子树 int index = idx_map.get(root_val); // 下标减一 post_idx--; // 构造右子树 root.right = helper(index + 1, in_right); // 构造左子树 root.left = helper(in_left, index - 1); return root; } public TreeNode buildTree(int[] inorder, int[] postorder) { this.postorder = postorder; this.inorder = inorder; // 从后序遍历的最后一个元素开始 post_idx = postorder.length - 1; // 建立（元素，下标）键值对的哈希表 int idx = 0; for (Integer val : inorder) { idx_map.put(val, idx++); } return helper(0, inorder.length - 1); } } // 或者 复杂度分析\n时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)。我们需要使用 O(n) 的空间存储哈希表，以及 O(h)（其中 h 是树的高度）的空间表示递归时栈空间。这里 h \u0026lt; n，所以总空间复杂度为 O(n)。 方法二：迭代\n思路\n迭代法是一种非常巧妙的实现方法。迭代法的实现基于以下两点发现。\n如果将中序遍历反序，则得到反向的中序遍历，即每次遍历右孩子，再遍历根节点，最后遍历左孩子。 如果将后序遍历反序，则得到反向的前序遍历，即每次遍历根节点，再遍历右孩子，最后遍历左孩子。 「反向」的意思是交换遍历左孩子和右孩子的顺序，即反向的遍历中，右孩子在左孩子之前被遍历。\n因此可以使用和「105. 从前序与中序遍历序列构造二叉树」的迭代方法类似的方法构造二叉树。\n对于后序遍历中的任意两个连续节点 u 和 v（在后序遍历中，u 在 v 的前面），根据后序遍历的流程，我们可以知道 u 和 v 只有两种可能的关系：\nu 是 v 的右儿子。这是因为在遍历到 u 之后，下一个遍历的节点就是 u 的双亲节点，即 v； v 没有右儿子，并且 u 是 v 的某个祖先节点（或者 v 本身）的左儿子。如果 v 没有右儿子，那么上一个遍历的节点就是 v 的左儿子。如果 v 没有左儿子，则从 v 开始向上遍历 v 的祖先节点，直到遇到一个有左儿子（且 v 不在它的左儿子的子树中）的节点 v_a，那么 u 就是 v_a 的左儿子。 第二种关系看上去有些复杂。我们举一个例子来说明其正确性，并在例子中给出我们的迭代算法。\n例子\n我们以树\n1 2 3 4 5 6 7 8 9 3 / \\ 9 20 / \\ \\ 15 10 7 / \\ 5 8 \\ 4 为例，它的中序遍历和后序遍历分别为\n1 2 inorder = [15, 9, 10, 3, 20, 5, 7, 8, 4] postorder = [15, 10, 9, 5, 4, 8, 7, 20, 3] 我们用一个栈 stack 来维护「当前节点的所有还没有考虑过左儿子的祖先节点」，栈顶就是当前节点。也就是说，只有在栈中的节点才可能连接一个新的左儿子。同时，我们用一个指针 index 指向中序遍历的某个位置，初始值为 n - 1，其中 n 为数组的长度。index 对应的节点是「当前节点不断往右走达到的最终节点」，这也是符合反向中序遍历的，它的作用在下面的过程中会有所体现。\n首先我们将根节点 3 入栈，再初始化 index 所指向的节点为 4，随后对于后序遍历中的每个节点，我们依次判断它是栈顶节点的右儿子，还是栈中某个节点的左儿子。\n我们遍历 20。20 一定是栈顶节点 3 的右儿子。我们使用反证法，假设 20 是 3 的左儿子，因为 20 和 3 中间不存在其他的节点，那么 3 没有右儿子，index 应该恰好指向 3，但实际上为 4，因此产生了矛盾。所以我们将 20 作为 3 的右儿子，并将 20 入栈。\nstack = [3, 20] index -\u0026gt; inorder[8] = 4 我们遍历 7，8 和 4。同理可得它们都是上一个节点（栈顶节点）的右儿子，所以它们会依次入栈。\nstack = [3, 20, 7, 8, 4] index -\u0026gt; inorder[8] = 4 我们遍历 5，这时情况就不一样了。我们发现 index 恰好指向当前的栈顶节点 4，也就是说 4 没有右儿子，那么 5 必须为栈中某个节点的左儿子。那么如何找到这个节点呢？栈中的节点的顺序和它们在反向前序遍历中出现的顺序是一致的，而且每一个节点的左儿子都还没有被遍历过，那么这些节点的顺序和它们在反向中序遍历中出现的顺序一定是相反的。\n这是因为栈中的任意两个相邻的节点，前者都是后者的某个祖先。并且我们知道，栈中的任意一个节点的左儿子还没有被遍历过，说明后者一定是前者右儿子的子树中的节点，那么后者就先于前者出现在反向中序遍历中。\n因此我们可以把 index 不断向左移动，并与栈顶节点进行比较。如果 index 对应的元素恰好等于栈顶节点，那么说明我们在反向中序遍历中找到了栈顶节点，所以将 index 减少 1 并弹出栈顶节点，直到 index 对应的元素不等于栈顶节点。按照这样的过程，我们弹出的最后一个节点 x 就是 5 的双亲节点，这是因为 5 出现在了 x 与 x 在栈中的下一个节点的反向中序遍历之间，因此 5 就是 x 的左儿子。\n回到我们的例子，我们会依次从栈顶弹出 4，8 和 7，并且将 index 向左移动了三次。我们将 5 作为最后弹出的节点 7 的左儿子，并将 5 入栈。\nstack = [3, 20, 5] index -\u0026gt; inorder[5] = 5 我们遍历 9。同理，index 恰好指向当前栈顶节点 5，那么我们会依次从栈顶弹出 5，20 和 3，并且将 index 向左移动了三次。我们将 9 作为最后弹出的节点 3 的左儿子，并将 9 入栈。\nstack = [9] index -\u0026gt; inorder[2] = 10 我们遍历 10，将 10 作为栈顶节点 9 的右儿子，并将 10 入栈。\nstack = [9, 10] index -\u0026gt; inorder[2] = 10 我们遍历 15。index 恰好指向当前栈顶节点 10，那么我们会依次从栈顶弹出 10 和 9，并且将 index 向左移动了两次。我们将 15 作为最后弹出的节点 9 的左儿子，并将 15 入栈。\nstack = [15] index -\u0026gt; inorder[0] = 15 此时遍历结束，我们就构造出了正确的二叉树。\n算法\n我们归纳出上述例子中的算法流程：\n我们用一个栈和一个指针辅助进行二叉树的构造。初始时栈中存放了根节点（后序遍历的最后一个节点），指针指向中序遍历的最后一个节点； 我们依次枚举后序遍历中除了第一个节点以外的每个节点。如果 index 恰好指向栈顶节点，那么我们不断地弹出栈顶节点并向左移动 index，并将当前节点作为最后一个弹出的节点的左儿子；如果 index 和栈顶节点不同，我们将当前节点作为栈顶节点的右儿子； 无论是哪一种情况，我们最后都将当前的节点入栈。 最后得到的二叉树即为答案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public TreeNode buildTree(int[] inorder, int[] postorder) { if (postorder == null || postorder.length == 0) { return null; } TreeNode root = new TreeNode(postorder[postorder.length - 1]); Deque\u0026lt;TreeNode\u0026gt; stack = new LinkedList\u0026lt;TreeNode\u0026gt;(); stack.push(root); int inorderIndex = inorder.length - 1; for (int i = postorder.length - 2; i \u0026gt;= 0; i--) { int postorderVal = postorder[i]; TreeNode node = stack.peek(); if (node.val != inorder[inorderIndex]) { node.right = new TreeNode(postorderVal); stack.push(node.right); } else { while (!stack.isEmpty() \u0026amp;\u0026amp; stack.peek().val == inorder[inorderIndex]) { node = stack.pop(); inorderIndex--; } node.left = new TreeNode(postorderVal); stack.push(node.left); } } return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)，我们需要使用 O(h)（其中 h 是树的高度）的空间存储栈。这里 h \u0026lt; n，所以（在最坏情况下）总空间复杂度为 O(n)。 day43 145. 二叉树的后序遍历 题目\n给你一棵二叉树的根节点 root ，返回其节点值的 后序遍历 。\n示例 1：\n1 2 输入：root = [1,null,2,3] 输出：[3,2,1] 示例 2：\n1 2 输入：root = [] 输出：[] 示例 3：\n1 2 输入：root = [1] 输出：[1] 提示：\n树中节点的数目在范围 [0, 100] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 **进阶：**递归算法很简单，你可以通过迭代算法完成吗？\n解法\n方法一：递归\n思路与算法\n首先我们需要了解什么是二叉树的后序遍历：按照访问左子树——右子树——根节点的方式遍历这棵树，而在访问左子树或者右子树的时候，我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。\n定义 postorder(root) 表示当前遍历到 root 节点的答案。按照定义，我们只要递归调用 postorder(root-\u0026gt;left) 来遍历 root 节点的左子树，然后递归调用 postorder(root-\u0026gt;right) 来遍历 root 节点的右子树，最后将 root 节点的值加入答案即可，递归终止的条件为碰到空节点。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public List\u0026lt;Integer\u0026gt; postorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); postorder(root, res); return res; } public void postorder(TreeNode root, List\u0026lt;Integer\u0026gt; res) { if (root == null) { return; } postorder(root.left, res); postorder(root.right, res); res.add(root.val); } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为递归过程中栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法二：迭代\n思路与算法\n我们也可以用迭代的方式实现方法一的递归函数，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其余的实现与细节都相同，具体可以参考下面的代码。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public List\u0026lt;Integer\u0026gt; postorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); if (root == null) { return res; } Deque\u0026lt;TreeNode\u0026gt; stack = new LinkedList\u0026lt;TreeNode\u0026gt;(); TreeNode prev = null; while (root != null || !stack.isEmpty()) { while (root != null) { stack.push(root); root = root.left; } root = stack.pop(); if (root.right == null || root.right == prev) { res.add(root.val); prev = root; root = null; } else { stack.push(root); root = root.right; } } return res; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为迭代过程中显式栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法三：Morris 遍历\n思路与算法\n有一种巧妙的方法可以在线性时间内，只占用常数空间来实现后序遍历。这种方法由 J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出，因此被称为 Morris 遍历。\nMorris 遍历的核心思想是利用树的大量空闲指针，实现空间开销的极限缩减。其后序遍历规则总结如下：\n新建临时节点，令该节点为 root； 如果当前节点的左子节点为空，则遍历当前节点的右子节点； 如果当前节点的左子节点不为空，在当前节点的左子树中找到当前节点在中序遍历下的前驱节点； 如果前驱节点的右子节点为空，将前驱节点的右子节点设置为当前节点，当前节点更新为当前节点的左子节点。 如果前驱节点的右子节点为当前节点，将它的右子节点重新设为空。倒序输出从当前节点的左子节点到该前驱节点这条路径上的所有节点。当前节点更新为当前节点的右子节点。 重复步骤 2 和步骤 3，直到遍历结束。 这样我们利用 Morris 遍历的方法，后序遍历该二叉搜索树，即可实现线性时间与常数空间的遍历。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class Solution { public List\u0026lt;Integer\u0026gt; postorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); if (root == null) { return res; } TreeNode p1 = root, p2 = null; while (p1 != null) { p2 = p1.left; if (p2 != null) { while (p2.right != null \u0026amp;\u0026amp; p2.right != p1) { p2 = p2.right; } if (p2.right == null) { p2.right = p1; p1 = p1.left; continue; } else { p2.right = null; addPath(res, p1.left); } } p1 = p1.right; } addPath(res, root); return res; } public void addPath(List\u0026lt;Integer\u0026gt; res, TreeNode node) { int count = 0; while (node != null) { ++count; res.add(node.val); node = node.right; } int left = res.size() - count, right = res.size() - 1; while (left \u0026lt; right) { int temp = res.get(left); res.set(left, res.get(right)); res.set(right, temp); left++; right--; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树的节点数。没有左子树的节点只被访问一次，有左子树的节点被访问两次。 空间复杂度：O(1)。只操作已经存在的指针（树的空闲指针），因此只需要常数的额外空间。 day44 102. 二叉树的层序遍历 题目\n给你二叉树的根节点 root ，返回其节点值的 层序遍历 。 （即逐层地，从左到右访问所有节点）。\n示例 1：\n1 2 输入：root = [3,9,20,null,null,15,7] 输出：[[3],[9,20],[15,7]] 示例 2：\n1 2 输入：root = [1] 输出：[[1]] 示例 3：\n1 2 输入：root = [] 输出：[] 提示：\n树中节点数目在范围 [0, 2000] 内 -1000 \u0026lt;= Node.val \u0026lt;= 1000 解法\n解法一：深度优先\n采用递归方法来找出层序序列。思路：我们知道，遍历递归地遍历二叉树是往深处走，我们只需要在往深处走的时候，将相同深度的节点放到一起即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { if (root==null) return new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls =new ArrayList\u0026lt;\u0026gt;(); doSome(root,1,lls); return lls; } // 递归层序遍历的秘诀：深度相同的节点放到同一个list public void doSome(TreeNode node,int depth,List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls){ // 当前深度下没有对应的“层list”，就创建 if(lls.size()\u0026lt;depth) { lls.add(new ArrayList\u0026lt;Integer\u0026gt;()); } //在同一深度，把节点加上去 lls.get(depth-1).add(node.val); if(node.left!=null) doSome(node.left,depth+1,lls); if(node.right!=null) doSome(node.right,depth+1,lls); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func levelOrder(root *TreeNode) [][]int { ans := [][]int{} if root==nil { return ans } var dfs func(root *TreeNode,depth int) dfs = func(root *TreeNode,depth int){ if root==nil{ return } if len(ans)\u0026lt;depth+1{ ans=append(ans,[]int{}) } ans[depth]=append(ans[depth],root.Val) dfs(root.Left,depth+1) dfs(root.Right,depth+1) return } dfs(root,0) return ans } 解法二：广度优先搜索\n解题思路\n本文将会讲解为什么这道题适合用广度优先搜索（BFS），以及 BFS 适用于什么样的场景。\nDFS（深度优先搜索）和 BFS（广度优先搜索）就像孪生兄弟，提到一个总是想起另一个。然而在实际使用中，我们用 DFS 的时候远远多于 BFS。那么，是不是 BFS 就没有什么用呢？\n如果我们使用 DFS/BFS 只是为了遍历一棵树、一张图上的所有结点的话，那么 DFS 和 BFS 的能力没什么差别，我们当然更倾向于更方便写、空间复杂度更低的 DFS 遍历。不过，某些使用场景是 DFS 做不到的，只能使用 BFS 遍历。这就是本文要介绍的两个场景：「层序遍历」、「最短路径」。\n本文包括以下内容：\nDFS 与 BFS 的特点比较 BFS 的适用场景 如何用 BFS 进行层序遍历 如何用 BFS 求解最短路径问题 DFS 与 BFS\n让我们先看看在二叉树上进行 DFS 遍历和 BFS 遍历的代码比较。\nDFS 遍历使用递归：\n1 2 3 4 5 6 7 void dfs(TreeNode root) { if (root == null) { return; } dfs(root.left); dfs(root.right); } BFS 遍历使用队列数据结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 void bfs(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); queue.add(root); while (!queue.isEmpty()) { TreeNode node = queue.poll(); // Java 的 pop 写作 poll() if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } } 只是比较两段代码的话，最直观的感受就是：DFS 遍历的代码比 BFS 简洁太多了！这是因为递归的方式隐含地使用了系统的 栈，我们不需要自己维护一个数据结构。如果只是简单地将二叉树遍历一遍，那么 DFS 显然是更方便的选择。\n虽然 DFS 与 BFS 都是将二叉树的所有结点遍历了一遍，但它们遍历结点的顺序不同。\n这个遍历顺序也是 BFS 能够用来解「层序遍历」、「最短路径」问题的根本原因。下面，我们结合几道例题来讲讲 BFS 是如何求解层序遍历和最短路径问题的。\nBFS 的应用一：层序遍历\nBFS 的层序遍历应用就是本题了：\nLeetCode 102. Binary Tree Level Order Traversal 二叉树的层序遍历（Medium）\n给定一个二叉树，返回其按层序遍历得到的节点值。层序遍历即逐层地、从左到右访问所有结点。\n什么是层序遍历呢？简单来说，层序遍历就是把二叉树分层，然后每一层从左到右遍历：\n乍一看来，这个遍历顺序和 BFS 是一样的，我们可以直接用 BFS 得出层序遍历结果。然而，层序遍历要求的输入结果和 BFS 是不同的。层序遍历要求我们区分每一层，也就是返回一个二维数组。而 BFS 的遍历结果是一个一维数组，无法区分每一层。\n那么，怎么给 BFS 遍历的结果分层呢？我们首先来观察一下 BFS 遍历的过程中，结点进队列和出队列的过程：\n截取 BFS 遍历过程中的某个时刻：\n可以看到，此时队列中的结点是 3、4、5，分别来自第 1 层和第 2 层。这个时候，第 1 层的结点还没出完，第 2 层的结点就进来了，而且两层的结点在队列中紧挨在一起，我们无法区分队列中的结点来自哪一层。\n因此，我们需要稍微修改一下代码，在每一层遍历开始前，先记录队列中的结点数量 n（也就是这一层的结点数量），然后一口气处理完这一层的 n 个结点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 二叉树的层序遍历 void bfs(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); queue.add(root); while (!queue.isEmpty()) { int n = queue.size(); for (int i = 0; i \u0026lt; n; i++) { // 变量 i 无实际意义，只是为了循环 n 次 TreeNode node = queue.poll(); if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } } } 这样，我们就将 BFS 遍历改造成了层序遍历。在遍历的过程中，结点进队列和出队列的过程为：\n可以看到，在 while 循环的每一轮中，都是将当前层的所有结点出队列，再将下一层的所有结点入队列，这样就实现了层序遍历。\n最终我们得到的题解代码为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); if (root != null) { queue.add(root); } while (!queue.isEmpty()) { int n = queue.size(); List\u0026lt;Integer\u0026gt; level = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { TreeNode node = queue.poll(); level.add(node.val); if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } res.add(level); } return res; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func levelOrder(root *TreeNode) [][]int { ans := [][]int{} if root==nil { return ans } queue := []*TreeNode{root} for len(queue)!=0 { n:=len(queue) lay :=[]int{} for i:=0;i\u0026lt;n;i++{ node:=queue[i] if node==nil{ continue } lay=append(lay,node.Val) if node.Left!=nil{ queue=append(queue,node.Left) } if node.Right!=nil{ queue=append(queue,node.Right) } } queue=queue[n:] ans = append(ans,lay) } return ans } BFS 的应用二：最短路径\n在一棵树中，一个结点到另一个结点的路径是唯一的，但在图中，结点之间可能有多条路径，其中哪条路最近呢？这一类问题称为最短路径问题。最短路径问题也是 BFS 的典型应用，而且其方法与层序遍历关系密切。\n在二叉树中，BFS 可以实现一层一层的遍历。在图中同样如此。从源点出发，BFS 首先遍历到第一层结点，到源点的距离为 1，然后遍历到第二层结点，到源点的距离为 2…… 可以看到，用 BFS 的话，距离源点更近的点会先被遍历到，这样就能找到到某个点的最短路径了。\n小贴士：\n很多同学一看到「最短路径」，就条件反射地想到「Dijkstra 算法」。为什么 BFS 遍历也能找到最短路径呢？\n这是因为，Dijkstra 算法解决的是带权最短路径问题，而我们这里关注的是无权最短路径问题。也可以看成每条边的权重都是 1。这样的最短路径问题，用 BFS 求解就行了。\n在面试中，你可能更希望写 BFS 而不是 Dijkstra 。毕竟，敢保证自己能写对 Dijkstra 算法的人不多。\n最短路径问题属于图算法。由于图的表示和描述比较复杂，本文用比较简单的网格结构代替。网格结构是一种特殊的图，它的表示和遍历都比较简单，适合作为练习题。在 LeetCode 中，最短路径问题也以网格结构为主。\n最短路径例题讲解\nLeetCode 1162. As Far from Land as Possible 离开陆地的最远距离（Medium）\n你现在手里有一份大小为 n×n 的地图网格 grid，上面的每个单元格都标记为 0 或者 1，其中 0 代表海洋，1 代表陆地，请你找出一个海洋区域，这个海洋区域到离它最近的陆地区域的距离是最大的。\n我们这里说的距离是「曼哈顿距离」。(x_0, y_0) 和 (x_1, y_1) 这两个区域之间的距离是 |x_0 - x_1| + |y_0 - y_1| 。\n如果我们的地图上只有陆地或者海洋，请返回 -1。\n这道题就是一个在网格结构中求最短路径的问题。同时，它也是一个「岛屿问题」，即用网格中的 1 和 0 表示陆地和海洋，模拟出若干个岛屿。\n在上一篇文章中，我们介绍了网格结构的基本概念，以及网格结构中的 DFS 遍历。其中一些概念和技巧也可以用在 BFS 遍历中：\n格子 (r, c) 的相邻四个格子为：(r-1, c)、(r+1, c)、(r, c-1) 和 (r, c+1)； 使用函数 inArea 判断当前格子的坐标是否在网格范围内； 将遍历过的格子标记为 2，避免重复遍历。 对于网格结构的性质、网格结构的 DFS 遍历技巧不是很了解的同学，可以复习一下上一篇文章：LeetCode 例题精讲 | 12 岛屿问题：网格结构中的 DFS。\n上一篇文章讲过了网格结构 DFS 遍历，这篇文章正好讲解一下网格结构的 BFS 遍历。要解最短路径问题，我们首先要写出层序遍历的代码，仿照上面的二叉树层序遍历代码，类似地可以写出网格层序遍历：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // 网格结构的层序遍历 // 从格子 (i, j) 开始遍历 void bfs(int[][] grid, int i, int j) { Queue\u0026lt;int[]\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); queue.add(new int[]{r, c}); while (!queue.isEmpty()) { int n = queue.size(); // 这里是取一个“圈”内的所有节点，层层递进 for (int i = 0; i \u0026lt; n; i++) { int[] node = queue.poll(); int r = node[0]; int c = node[1]; // 广度优先搜索，往四面扩散遍历 if (r-1 \u0026gt;= 0 \u0026amp;\u0026amp; grid[r-1][c] == 0) { grid[r-1][c] = 2; queue.add(new int[]{r-1, c}); } if (r+1 \u0026lt; N \u0026amp;\u0026amp; grid[r+1][c] == 0) { grid[r+1][c] = 2; queue.add(new int[]{r+1, c}); } if (c-1 \u0026gt;= 0 \u0026amp;\u0026amp; grid[r][c-1] == 0) { grid[r][c-1] = 2; queue.add(new int[]{r, c-1}); } if (c+1 \u0026lt; N \u0026amp;\u0026amp; grid[r][c+1] == 0) { grid[r][c+1] = 2; queue.add(new int[]{r, c+1}); } } } } 以上的层序遍历代码有几个注意点：\n队列中的元素类型是 int[] 数组，每个数组的长度为 2，包含格子的行坐标和列坐标。 为了避免重复遍历，这里使用到了和 DFS 遍历一样的技巧：把已遍历的格子标记为 2。注意：我们在将格子放入队列之前就将其标记为 2。想一想，这是为什么？ 在将格子放入队列之前就检查其坐标是否在网格范围内，避免将「不存在」的格子放入队列。 这段网格遍历代码还有一些可以优化的地方。由于一个格子有四个相邻的格子，代码中判断了四遍格子坐标的合法性，代码稍微有点啰嗦。我们可以用一个 moves 数组存储相邻格子的四个方向：\n1 2 3 int[][] moves = { {-1, 0}, {1, 0}, {0, -1}, {0, 1}, }; 然后把四个 if 判断变成一个循环：\n1 2 3 4 5 6 7 8 for (int[][] move : moves) { int r2 = r + move[0]; int c2 = c + move[1]; if (inArea(grid, r2, c2) \u0026amp;\u0026amp; grid[r2][c2] == 0) { grid[r2][c2] = 2; queue.add(new int[]{r2, c2}); } } 写好了层序遍历的代码，接下来我们看看如何来解决本题中的最短路径问题。\n这道题要找的是距离陆地最远的海洋格子。假设网格中只有一个陆地格子，我们可以从这个陆地格子出发做层序遍历，直到所有格子都遍历完。最终遍历了几层，海洋格子的最远距离就是几。\n那么有多个陆地格子的时候怎么办呢？一种方法是将每个陆地格子都作为起点做一次层序遍历，但是这样的时间开销太大。\nBFS 完全可以以多个格子同时作为起点。我们可以把所有的陆地格子同时放入初始队列，然后开始层序遍历，这样遍历的效果如下图所示：\n这种遍历方法实际上叫做「多源 BFS」。多源 BFS 的定义不是今天讨论的重点，你只需要记住多源 BFS 很方便，只需要把多个源点同时放入初始队列即可。\n需要注意的是，虽然上面的图示用 1、2、3、4 表示层序遍历的层数，但是在代码中，我们不需要给每个遍历到的格子标记层数，只需要用一个 distance 变量记录当前的遍历的层数（也就是到陆地格子的距离）即可。\n最终，我们得到的题解代码为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 public int maxDistance(int[][] grid) { int N = grid.length; Queue\u0026lt;int[]\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); // 将所有的陆地格子加入队列 for (int i = 0; i \u0026lt; N; i++) { for (int j = 0; j \u0026lt; N; j++) { if (grid[i][j] == 1) { queue.add(new int[]{i, j}); } } } // 如果地图上只有陆地或者海洋，返回 -1 if (queue.isEmpty() || queue.size() == N * N) { return -1; } int[][] moves = { {-1, 0}, {1, 0}, {0, -1}, {0, 1}, }; int distance = -1; // 记录当前遍历的层数（距离） while (!queue.isEmpty()) { distance++; int n = queue.size(); for (int i = 0; i \u0026lt; n; i++) { int[] node = queue.poll(); int r = node[0]; int c = node[1]; for (int[] move : moves) { int r2 = r + move[0]; int c2 = c + move[1]; if (inArea(grid, r2, c2) \u0026amp;\u0026amp; grid[r2][c2] == 0) { grid[r2][c2] = 2; queue.add(new int[]{r2, c2}); } } } } return distance; } // 判断坐标 (r, c) 是否在网格中 boolean inArea(int[][] grid, int r, int c) { return 0 \u0026lt;= r \u0026amp;\u0026amp; r \u0026lt; grid.length \u0026amp;\u0026amp; 0 \u0026lt;= c \u0026amp;\u0026amp; c \u0026lt; grid[0].length; } 总结\n可以看到，「BFS 遍历」、「层序遍历」、「最短路径」实际上是递进的关系。在 BFS 遍历的基础上区分遍历的每一层，就得到了层序遍历。在层序遍历的基础上记录层数，就得到了最短路径。\nBFS 遍历是一类很值得反复体会和练习的题目。一方面，BFS 遍历是一个经典的基础算法，需要重点掌握。另一方面，我们需要能根据题意分析出题目是要求最短路径，知道是要做 BFS 遍历。\n本文讲解的只是两道非常典型的例题。LeetCode 中还有许多层序遍历和最短路径的题目\n层序遍历的一些变种题目：\nLeetCode 103. Binary Tree Zigzag Level Order Traversal 之字形层序遍历 LeetCode 199. Binary Tree Right Side View 找每一层的最右结点 LeetCode 515. Find Largest Value in Each Tree Row 计算每一层的最大值 LeetCode 637. Average of Levels in Binary Tree 计算每一层的平均值 对于最短路径问题，还有两道题目也是求网格结构中的最短路径，和我们讲解的距离岛屿的最远距离非常类似：\nLeetCode 542. 01 Matrix LeetCode 994. Rotting Oranges 还有一道在真正的图结构中求最短路径的问题：\nLeetCode 310. Minimum Height Trees 经过了本文的讲解，相信解决这些题目也不是难事。\nday45 107. 二叉树的层序遍历 II 题目\n给你二叉树的根节点 root ，返回其节点值 自底向上的层序遍历 。 （即按从叶子节点所在层到根节点所在的层，逐层从左向右遍历）\n示例 1：\n1 2 输入：root = [3,9,20,null,null,15,7] 输出：[[15,7],[9,20],[3]] 示例 2：\n1 2 输入：root = [1] 输出：[[1]] 示例 3：\n1 2 输入：root = [] 输出：[] 提示：\n树中节点数目在范围 [0, 2000] 内 -1000 \u0026lt;= Node.val \u0026lt;= 1000 解法\n解法一：无脑广度优先\n在前面一题的思路上，转换一下：只需要在添加当层遍历list时，插入头部即可。\n题解\n解法二：不优雅的深度优先遍历\n将上题设深度优先产生的序列反序即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { void level(TreeNode root, int index, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { // 当前行对应的列表不存在，加一个空列表 if(res.size() \u0026lt; index) { res.add(new ArrayList\u0026lt;Integer\u0026gt;()); } // 将当前节点的值加入当前行的 res 中 res.get(index-1).add(root.val); // 递归处理左子树 if(root.left != null) { level(root.left, index+1, res); } // 递归处理右子树 if(root.right != null) { level(root.right, index+1, res); } } public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrderBottom(TreeNode root) { if(root == null) { return new ArrayList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt;(); } List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; list = new ArrayList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt;(); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt;(); level(root, 1, list); for(int i = list.size()-1; i \u0026gt;= 0; i--){ res.add(list.get(i)); } return res; } } day46 199. 二叉树的右视图 题目\n给定一个二叉树的 根节点 root，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。\n示例 1:\n1 2 输入: [1,2,3,null,5,null,4] 输出: [1,3,4] 示例 2:\n1 2 输入: [1,null,3] 输出: [1,3] 示例 3:\n1 2 输入: [] 输出: [] 提示:\n二叉树的节点个数的范围是 [0,100] -100 \u0026lt;= Node.val \u0026lt;= 100 解法\nBFS\n思路： 利用 BFS 进行层次遍历，记录下每层的最后一个元素。 时间复杂度： O(N)，每个节点都入队出队了 1 次。 空间复杂度： O(N)，使用了额外的队列空间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public List\u0026lt;Integer\u0026gt; rightSideView(TreeNode root) { if(root == null) return new ArrayList\u0026lt;Integer\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; ls =new ArrayList\u0026lt;\u0026gt;(); queue.offer(root); while(!queue.isEmpty()){ int size = queue.size(); while(size\u0026gt;0){ TreeNode node = queue.poll(); if(size==1) ls.add(node.val);// 层序遍历是最右边的节点最后被遍历 if(node.left!=null) queue.offer(node.left); if(node.right!=null) queue.offer(node.right); size--; } } return ls; } } DFS （时间100%）\n思路： 我们按照 「根结点 -\u0026gt; 右子树 -\u0026gt; 左子树」 的顺序访问，就可以保证每层都是最先访问最右边的节点的。\n（与先序遍历 「根结点 -\u0026gt; 左子树 -\u0026gt; 右子树」 正好相反，先序遍历每层最先访问的是最左边的节点）\n时间复杂度： O(N)，每个节点都访问了 1 次。 空间复杂度： O(N)，因为这不是一棵平衡二叉树，二叉树的深度最少是 logN, 最坏的情况下会退化成一条链表，深度就是 N，因此递归时使用的栈空间是 O(N) 的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); public List\u0026lt;Integer\u0026gt; rightSideView(TreeNode root) { dfs(root, 0); // 从根节点开始访问，根节点深度是0 return res; } private void dfs(TreeNode root, int depth) { if (root == null) { return; } // 先访问 当前节点，再递归地访问 右子树 和 左子树。 if (depth == res.size()) { // 如果当前节点所在深度还没有出现在res里，说明在该深度下当前节点是第一个被访问的节点，因此将当前节点加入res中。 res.add(root.val); } depth++; dfs(root.right, depth); dfs(root.left, depth); } } day47 429. N 叉树的层序遍历 题目\n给定一个 N 叉树，返回其节点值的层序遍历。（即从左到右，逐层遍历）。\n树的序列化输入是用层序遍历，每组子节点都由 null 值分隔（参见示例）。\n示例 1：\n1 2 输入：root = [1,null,3,2,4,null,5,6] 输出：[[1],[3,2,4],[5,6]] 示例 2：\n1 2 输入：root = [1,null,2,3,4,5,null,null,6,7,null,8,null,9,10,null,null,11,null,12,null,13,null,null,14] 输出：[[1],[2,3,4,5],[6,7,8,9,10],[11,12,13],[14]] 提示：\n树的高度不会超过 1000 树的节点总数在 [0, 10^4] 之间 解法\n方法一：广度优先搜索\n思路与算法\n对于「层序遍历」的题目，我们一般使用广度优先搜索。在广度优先搜索的每一轮中，我们会遍历同一层的所有节点。\n具体地，我们首先把根节点 root 放入队列中，随后在广度优先搜索的每一轮中，我们首先记录下当前队列中包含的节点个数（记为 cnt），即表示上一层的节点个数。在这之后，我们从队列中依次取出节点，直到取出了上一层的全部 cnt 个节点为止。当取出节点 cur 时，我们将 cur 的值放入一个临时列表，再将 cur 的所有子节点全部放入队列中。\n当这一轮遍历完成后，临时列表中就存放了当前层所有节点的值。这样一来，当整个广度优先搜索完成后，我们就可以得到层序遍历的结果。\n细节\n需要特殊判断树为空的情况。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(Node root) { if (root == null) return new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls = new ArrayList\u0026lt;\u0026gt;(); lls.add(new ArrayList\u0026lt;\u0026gt;()); lls.get(0).add(root.val); Queue\u0026lt;Node\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(root.children); while (!queue.isEmpty()) { int size = queue.size(); List\u0026lt;Integer\u0026gt; ls = new ArrayList\u0026lt;\u0026gt;(); while (size \u0026gt; 0) { Node node = queue.poll(); ls.add(node.val); if (node.children != null) queue.addAll(node.children); size--; } lls.add(ls); } return lls; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是树中包含的节点个数。在广度优先搜索的过程中，我们需要遍历每一个节点恰好一次。 空间复杂度：O(n)，即为队列需要使用的空间。在最坏的情况下，树只有两层，且最后一层有 n-1 个节点，此时就需要 O(n) 的空间。 day48 637. 二叉树的层平均值 题目\n给定一个非空二叉树的根节点 root , 以数组的形式返回每一层节点的平均值。与实际答案相差 10-5 以内的答案可以被接受。\n示例 1：\n1 2 3 4 输入：root = [3,9,20,null,null,15,7] 输出：[3.00000,14.50000,11.00000] 解释：第 0 层的平均值为 3,第 1 层的平均值为 14.5,第 2 层的平均值为 11 。 因此返回 [3, 14.5, 11] 。 示例 2:\n1 2 输入：root = [3,9,20,15,7] 输出：[3.00000,14.50000,11.00000] 提示：\n树中节点数量在 [1, 104] 范围内 -231 \u0026lt;= Node.val \u0026lt;= 231 - 1 解法\n方法一：深度优先搜索\n使用深度优先搜索计算二叉树的层平均值，需要维护两个数组，counts 用于存储二叉树的每一层的节点数，sums 用于存储二叉树的每一层的节点值之和。搜索过程中需要记录当前节点所在层，如果访问到的节点在第 i 层，则将 counts[i] 的值加 1，并将该节点的值加到 sums[i]。\n遍历结束之后，第 i 层的平均值即为 sums[i]/counts[i]。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Solution { public List\u0026lt;Double\u0026gt; averageOfLevels(TreeNode root) { List\u0026lt;Integer\u0026gt; counts = new ArrayList\u0026lt;Integer\u0026gt;(); List\u0026lt;Double\u0026gt; sums = new ArrayList\u0026lt;Double\u0026gt;(); dfs(root, 0, counts, sums); List\u0026lt;Double\u0026gt; averages = new ArrayList\u0026lt;Double\u0026gt;(); int size = sums.size(); for (int i = 0; i \u0026lt; size; i++) { averages.add(sums.get(i) / counts.get(i)); } return averages; } public void dfs(TreeNode root, int level, List\u0026lt;Integer\u0026gt; counts, List\u0026lt;Double\u0026gt; sums) { if (root == null) { return; } if (level \u0026lt; sums.size()) { sums.set(level, sums.get(level) + root.val); counts.set(level, counts.get(level) + 1); } else { sums.add(1.0 * root.val); counts.add(1); } dfs(root.left, level + 1, counts, sums); dfs(root.right, level + 1, counts, sums); } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树中的节点个数。 深度优先搜索需要对每个节点访问一次，对于每个节点，维护两个数组的时间复杂度都是 O(1)，因此深度优先搜索的时间复杂度是 O(n)。 遍历结束之后计算每层的平均值的时间复杂度是 O(h)，其中 h 是二叉树的高度，任何情况下都满足 h≤n。 因此总时间复杂度是 O(n)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度取决于两个数组的大小和递归调用的层数，两个数组的大小都等于二叉树的高度，递归调用的层数不会超过二叉树的高度，最坏情况下，二叉树的高度等于节点个数。 方法二：广度优先搜索\n也可以使用广度优先搜索计算二叉树的层平均值。从根节点开始搜索，每一轮遍历同一层的全部节点，计算该层的节点数以及该层的节点值之和，然后计算该层的平均值。\n如何确保每一轮遍历的是同一层的全部节点呢？我们可以借鉴层次遍历的做法，广度优先搜索使用队列存储待访问节点，只要确保在每一轮遍历时，队列中的节点是同一层的全部节点即可。具体做法如下：\n初始时，将根节点加入队列； 每一轮遍历时，将队列中的节点全部取出，计算这些节点的数量以及它们的节点值之和，并计算这些节点的平均值，然后将这些节点的全部非空子节点加入队列，重复上述操作直到队列为空，遍历结束。 由于初始时队列中只有根节点，满足队列中的节点是同一层的全部节点，每一轮遍历时都会将队列中的当前层节点全部取出，并将下一层的全部节点加入队列，因此可以确保每一轮遍历的是同一层的全部节点。\n具体实现方面，可以在每一轮遍历之前获得队列中的节点数量 size，遍历时只遍历 size 个节点，即可满足每一轮遍历的是同一层的全部节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public List\u0026lt;Double\u0026gt; averageOfLevels(TreeNode root) { List\u0026lt;Double\u0026gt; averages = new ArrayList\u0026lt;Double\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;TreeNode\u0026gt;(); queue.offer(root); while (!queue.isEmpty()) { double sum = 0; int size = queue.size(); for (int i = 0; i \u0026lt; size; i++) { TreeNode node = queue.poll(); sum += node.val; TreeNode left = node.left, right = node.right; if (left != null) { queue.offer(left); } if (right != null) { queue.offer(right); } } averages.add(sum / size); } return averages; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树中的节点个数。 广度优先搜索需要对每个节点访问一次，时间复杂度是 O(n)。 需要对二叉树的每一层计算平均值，时间复杂度是 O(h)，其中 h 是二叉树的高度，任何情况下都满足 h≤n。 因此总时间复杂度是 O(n)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度取决于队列开销，队列中的节点个数不会超过 n。 day49 98. 验证二叉搜索树 题目\n给你一个二叉树的根节点 root ，判断其是否是一个有效的二叉搜索树。\n有效 二叉搜索树定义如下：\n节点的左子树只包含 小于 当前节点的数。 节点的右子树只包含 大于 当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 示例 1：\n1 2 输入：root = [2,1,3] 输出：true 示例 2：\n1 2 3 输入：root = [5,1,4,null,null,3,6] 输出：false 解释：根节点的值是 5 ，但是右子节点的值是 4 。 提示：\n树中节点数目范围在[1, 104] 内 -2^31^ \u0026lt;= Node.val \u0026lt;= 2^31^ - 1 解法\n方法一: 递归\n思路和算法\n要解决这道题首先我们要了解二叉搜索树有什么性质可以给我们利用，由题目给出的信息我们可以知道：如果该二叉树的左子树不为空，则左子树上所有节点的值均小于它的根节点的值； 若它的右子树不空，则右子树上所有节点的值均大于它的根节点的值；它的左右子树也为二叉搜索树。\n这启示我们设计一个递归函数 helper(root, lower, upper) 来递归判断，函数表示考虑以 root 为根的子树，判断子树中所有节点的值是否都在 (l,r) 的范围内（注意是开区间）。如果 root 节点的值 val 不在 (l,r) 的范围内说明不满足条件直接返回，否则我们要继续递归调用检查它的左右子树是否满足，如果都满足才说明这是一棵二叉搜索树。\n那么根据二叉搜索树的性质，在递归调用左子树时，我们需要把上界 upper 改为 root.val，即调用 helper(root.left, lower, root.val)，因为左子树里所有节点的值均小于它的根节点的值。同理递归调用右子树时，我们需要把下界 lower 改为 root.val，即调用 helper(root.right, root.val, upper)。\n函数递归调用的入口为 helper(root, -inf, +inf)， inf 表示一个无穷大的值。\n下图展示了算法如何应用在示例 2 上：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public boolean isValidBST(TreeNode root) { return isValidBST(root, Long.MIN_VALUE, Long.MAX_VALUE); } // 主要是递归过程中传递了左右子节点的值用于比较 public boolean isValidBST(TreeNode node, long lower, long upper) { if (node == null) { return true; } if (node.val \u0026lt;= lower || node.val \u0026gt;= upper) { return false; } return isValidBST(node.left, lower, node.val) \u0026amp;\u0026amp; isValidBST(node.right, node.val, upper); } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树的节点个数。在递归调用的时候二叉树的每个节点最多被访问一次，因此时间复杂度为 O(n)。 空间复杂度：O(n)，其中 n 为二叉树的节点个数。递归函数在递归过程中需要为每一层递归函数分配栈空间，所以这里需要额外的空间且该空间取决于递归的深度，即二叉树的高度。最坏情况下二叉树为一条链，树的高度为 n ，递归最深达到 n 层，故最坏情况下空间复杂度为 O(n) 。 方法二：中序遍历+类似于单调栈\n思路和算法\n基于方法一中提及的性质，我们可以进一步知道二叉搜索树「中序遍历」得到的值构成的序列一定是升序的，这启示我们在中序遍历的时候实时检查当前节点的值是否大于前一个中序遍历到的节点的值即可。如果均大于说明这个序列是升序的，整棵树是二叉搜索树，否则不是，下面的代码我们使用栈来模拟中序遍历的过程。\n可能有读者不知道中序遍历是什么，我们这里简单提及。中序遍历是二叉树的一种遍历方式，它先遍历左子树，再遍历根节点，最后遍历右子树。而我们二叉搜索树保证了左子树的节点的值均小于根节点的值，根节点的值均小于右子树的值，因此中序遍历以后得到的序列一定是升序序列。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public boolean isValidBST(TreeNode root) { Deque\u0026lt;TreeNode\u0026gt; stack = new LinkedList\u0026lt;TreeNode\u0026gt;(); double inorder = -Double.MAX_VALUE; while (!stack.isEmpty() || root != null) { while (root != null) { stack.push(root); root = root.left; } root = stack.pop(); // 如果中序遍历得到的节点的值小于等于前一个 inorder，说明不是二叉搜索树 if (root.val \u0026lt;= inorder) { return false; } inorder = root.val; root = root.right; } return true; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树的节点个数。二叉树的每个节点最多被访问一次，因此时间复杂度为 O(n) 。 空间复杂度：O(n)，其中 n 为二叉树的节点个数。栈最多存储 n 个节点，因此需要额外的 O(n) 的空间。 day50 108. 将有序数组转换为二叉搜索树 题目\n给你一个整数数组 nums ，其中元素已经按 升序 排列，请你将其转换为一棵 高度平衡 二叉搜索树。\n高度平衡 二叉树是一棵满足「每个节点的左右两个子树的高度差的绝对值不超过 1 」的二叉树。\n示例 1：\n1 2 3 输入：nums = [-10,-3,0,5,9] 输出：[0,-3,9,-10,null,5] 解释：[0,-10,5,null,-3,null,9] 也将被视为正确答案： 示例 2：\n1 2 3 输入：nums = [1,3] 输出：[3,1] 解释：[1,null,3] 和 [3,1] 都是高度平衡二叉搜索树。 提示：\n1 \u0026lt;= nums.length \u0026lt;= 104 -104 \u0026lt;= nums[i] \u0026lt;= 104 nums 按 严格递增 顺序排列 解法\n前言\n二叉搜索树的中序遍历是升序序列，题目给定的数组是按照升序排序的有序数组，因此可以确保数组是二叉搜索树的中序遍历序列。\n**给定二叉搜索树的中序遍历，是否可以唯一地确定二叉搜索树？答案是否定的。**如果没有要求二叉搜索树的高度平衡，则任何一个数字都可以作为二叉搜索树的根节点，因此可能的二叉搜索树有多个。\n如果增加一个限制条件，即要求二叉搜索树的高度平衡，是否可以唯一地确定二叉搜索树？答案仍然是否定的。\n直观地看，我们可以选择中间数字作为二叉搜索树的根节点，这样分给左右子树的数字个数相同或只相差 11，可以使得树保持平衡。如果数组长度是奇数，则根节点的选择是唯一的，如果数组长度是偶数，则可以选择中间位置左边的数字作为根节点或者选择中间位置右边的数字作为根节点，选择不同的数字作为根节点则创建的平衡二叉搜索树也是不同的。\n确定平衡二叉搜索树的根节点之后，其余的数字分别位于平衡二叉搜索树的左子树和右子树中，左子树和右子树分别也是平衡二叉搜索树，因此可以通过递归的方式创建平衡二叉搜索树。\n当然，这只是我们直观的想法，为什么这么建树一定能保证是「平衡」的呢？这里可以参考「1382. 将二叉搜索树变平衡」，这两道题的构造方法完全相同，这种方法是正确的，1382 题解中给出了这个方法的正确性证明：1382 官方题解，感兴趣的同学可以戳进去参考。\n递归的基准情形是平衡二叉搜索树不包含任何数字，此时平衡二叉搜索树为空。\n在给定中序遍历序列数组的情况下，每一个子树中的数字在数组中一定是连续的，因此可以通过数组下标范围确定子树包含的数字，下标范围记为 [left,right]。对于整个中序遍历序列，下标范围从 left=0 到 right*=nums.length−1。当 left\u0026gt;*right 时，平衡二叉搜索树为空。\n以下三种方法中，方法一总是选择中间位置左边的数字作为根节点，方法二总是选择中间位置右边的数字作为根节点，方法三是方法一和方法二的结合，选择任意一个中间位置数字作为根节点。\n方法一：中序遍历，总是选择中间位置左边的数字作为根节点\n选择中间位置左边的数字作为根节点，则根节点的下标为 mid*=(*left+right)/2，此处的除法为整数除法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public TreeNode sortedArrayToBST(int[] nums) { return helper(nums, 0, nums.length - 1); } public TreeNode helper(int[] nums, int left, int right) { if (left \u0026gt; right) { return null; } // 总是选择中间位置左边的数字作为根节点 int mid = (left + right) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = helper(nums, left, mid - 1); root.right = helper(nums, mid + 1, right); return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是数组的长度。每个数字只访问一次。 空间复杂度：O(log n)，其中 n 是数组的长度。空间复杂度不考虑返回值，因此空间复杂度主要取决于递归栈的深度，递归栈的深度是 O(log n)。 方法二：中序遍历，总是选择中间位置右边的数字作为根节点\n选择中间位置右边的数字作为根节点，则根节点的下标为 mid=(left+right+1)/2，此处的除法为整数除法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public TreeNode sortedArrayToBST(int[] nums) { return helper(nums, 0, nums.length - 1); } public TreeNode helper(int[] nums, int left, int right) { if (left \u0026gt; right) { return null; } // 总是选择中间位置右边的数字作为根节点 int mid = (left + right + 1) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = helper(nums, left, mid - 1); root.right = helper(nums, mid + 1, right); return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是数组的长度。每个数字只访问一次。 空间复杂度：O(log n)，其中 nn 是数组的长度。空间复杂度不考虑返回值，因此空间复杂度主要取决于递归栈的深度，递归栈的深度是 O(log n)。 方法三：中序遍历，选择任意一个中间位置数字作为根节点\n选择任意一个中间位置数字作为根节点，则根节点的下标为 mid=(left+right)/2 和 mid=(left+right+1)/2 两者中随机选择一个，此处的除法为整数除法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { Random rand = new Random(); public TreeNode sortedArrayToBST(int[] nums) { return helper(nums, 0, nums.length - 1); } public TreeNode helper(int[] nums, int left, int right) { if (left \u0026gt; right) { return null; } // 选择任意一个中间位置数字作为根节点 int mid = (left + right + rand.nextInt(2)) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = helper(nums, left, mid - 1); root.right = helper(nums, mid + 1, right); return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是数组的长度。每个数字只访问一次。 空间复杂度：O(log n)，其中 n 是数组的长度。空间复杂度不考虑返回值，因此空间复杂度主要取决于递归栈的深度，递归栈的深度是 O(log n)。 day51 235. 二叉搜索树的最近公共祖先 题目\n给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5]\n示例 1:\n1 2 3 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 输出: 6 解释: 节点 2 和节点 8 的最近公共祖先是 6。 示例 2:\n1 2 3 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 4 输出: 2 解释: 节点 2 和节点 4 的最近公共祖先是 2, 因为根据定义最近公共祖先节点可以为节点本身。 说明:\n所有节点的值都是唯一的。 p、q 为不同节点且均存在于给定的二叉搜索树中。 解法\n方法一：两次遍历\n思路与算法\n注意到题目中给出的是一棵「二叉搜索树」，因此我们可以快速地找出树中的某个节点以及从根节点到该节点的路径，例如我们需要找到节点 p：\n我们从根节点开始遍历； 如果当前节点就是 p，那么成功地找到了节点； 如果当前节点的值大于 p 的值，说明 p 应该在当前节点的左子树，因此将当前节点移动到它的左子节点； 如果当前节点的值小于 p 的值，说明 p 应该在当前节点的右子树，因此将当前节点移动到它的右子节点。 对于节点 q 同理。在寻找节点的过程中，我们可以顺便记录经过的节点，这样就得到了从根节点到被寻找节点的路径。\n当我们分别得到了从根节点到 p 和 q 的路径之后，我们就可以很方便地找到它们的最近公共祖先了。显然，p 和 q 的最近公共祖先就是从根节点到它们路径上的「分岔点」，也就是最后一个相同的节点。因此，如果我们设从根节点到 p 的路径为数组 path_p，从根节点到 q 的路径为数组 path_q，那么只要找出最大的编号 i，其满足path_p[i]=path_q[i]\n那么对应的节点就是「分岔点」，即 p 和 q 的最近公共祖先就是 path_p[i]（或 path_q[i]）。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { List\u0026lt;TreeNode\u0026gt; path_p = getPath(root, p); List\u0026lt;TreeNode\u0026gt; path_q = getPath(root, q); TreeNode ancestor = null; for (int i = 0; i \u0026lt; path_p.size() \u0026amp;\u0026amp; i \u0026lt; path_q.size(); ++i) { if (path_p.get(i) == path_q.get(i)) { ancestor = path_p.get(i); } else { break; } } return ancestor; } public List\u0026lt;TreeNode\u0026gt; getPath(TreeNode root, TreeNode target) { List\u0026lt;TreeNode\u0026gt; path = new ArrayList\u0026lt;TreeNode\u0026gt;(); TreeNode node = root; while (node != target) { path.add(node); if (target.val \u0026lt; node.val) { node = node.left; } else { node = node.right; } } path.add(node); return path; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是给定的二叉搜索树中的节点个数。上述代码需要的时间与节点 p 和 q 在树中的深度线性相关，而在最坏的情况下，树呈现链式结构，p 和 q 一个是树的唯一叶子结点，一个是该叶子结点的父节点，此时时间复杂度为Θ(n)。 空间复杂度：O(n)，我们需要存储根节点到 p 和 q 的路径。和上面的分析方法相同，在最坏的情况下，路径的长度为 Θ(n)，因此需要 Θ(n) 的空间。 方法二：一次遍历\n思路与算法\n在方法一中，我们对从根节点开始，通过遍历找出到达节点 p 和 q 的路径，一共需要两次遍历。我们也可以考虑将这两个节点放在一起遍历。\n整体的遍历过程与方法一中的类似：\n我们从根节点开始遍历； 如果当前节点的值大于 p 和 q 的值，说明 p 和 q 应该在当前节点的左子树，因此将当前节点移动到它的左子节点； 如果当前节点的值小于 p 和 q 的值，说明 p 和 q 应该在当前节点的右子树，因此将当前节点移动到它的右子节点； 如果当前节点的值不满足上述两条要求，那么说明当前节点就是「分岔点」。此时，p 和 q 要么在当前节点的不同的子树中，要么其中一个就是当前节点。 可以发现，如果我们将这两个节点放在一起遍历，我们就省去了存储路径需要的空间。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { TreeNode ancestor = root; while (true) { if (p.val \u0026lt; ancestor.val \u0026amp;\u0026amp; q.val \u0026lt; ancestor.val) { ancestor = ancestor.left; } else if (p.val \u0026gt; ancestor.val \u0026amp;\u0026amp; q.val \u0026gt; ancestor.val) { ancestor = ancestor.right; } else { break; } } return ancestor; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是给定的二叉搜索树中的节点个数。分析思路与方法一相同。 空间复杂度：O(1)。 day51 450. 删除二叉搜索树中的节点 题目\n给定一个二叉搜索树的根节点 root 和一个值 key，删除二叉搜索树中的 key 对应的节点，并保证二叉搜索树的性质不变。返回二叉搜索树（有可能被更新）的根节点的引用。\n一般来说，删除节点可分为两个步骤：\n首先找到需要删除的节点； 如果找到了，删除它。 示例 1:\n1 2 3 4 5 输入：root = [5,3,6,2,4,null,7], key = 3 输出：[5,4,6,2,null,null,7] 解释：给定需要删除的节点值是 3，所以我们首先找到 3 这个节点，然后删除它。 一个正确的答案是 [5,4,6,2,null,null,7], 如下图所示。 另一个正确答案是 [5,2,6,null,4,null,7]。 示例 2:\n1 2 3 输入: root = [5,3,6,2,4,null,7], key = 0 输出: [5,3,6,2,4,null,7] 解释: 二叉树不包含值为 0 的节点 示例 3:\n1 2 输入: root = [], key = 0 输出: [] 提示:\n节点数的范围 [0, 104]. -105 \u0026lt;= Node.val \u0026lt;= 105 节点值唯一 root 是合法的二叉搜索树 -105 \u0026lt;= key \u0026lt;= 105 进阶： 要求算法时间复杂度为 O(h)，h 为树的高度。\n解法\n方法一：递归\n思路\n二叉搜索树有以下性质：\n左子树的所有节点（如果有）的值均小于当前节点的值； 右子树的所有节点（如果有）的值均大于当前节点的值； 左子树和右子树均为二叉搜索树。 二叉搜索树的题目往往可以用递归来解决。此题要求删除二叉树的节点，函数 deleteNode 的输入是二叉树的根节点 root 和一个整数 key，输出是删除值为 key 的节点后的二叉树，并保持二叉树的有序性。可以按照以下情况分类讨论：\nroot 为空，代表未搜索到值为 key 的节点，返回空。 root.val*\u0026gt;*key，表示值为 key 的节点可能存在于 root 的左子树中，需要递归地在 root.left 调用 deleteNode，并返回 root。 root.val*\u0026lt;*key，表示值为 key 的节点可能存在于 root 的右子树中，需要递归地在 root.right 调用 deleteNode，并返回root。 root.val*=*key，root即为要删除的节点。此时要做的是删除root，并将它的子树合并成一棵子树，保持有序性，并返回根节点。根据root的子树情况分成以下情况讨论： root 为叶子节点，没有子树。此时可以直接将它删除，即返回空。 root 只有左子树，没有右子树。此时可以将它的左子树作为新的子树，返回它的左子节点。 root 只有右子树，没有左子树。此时可以将它的右子树作为新的子树，返回它的右子节点。 root 有左右子树，这时可以将 root 的后继节点（比 root 大的最小节点，即它的右子树中的最小节点，记为 successor）作为新的根节点替代 root，并将 successor 从 root 的右子树中删除，使得在保持有序性的情况下合并左右子树。 简单证明，successor 位于 root 的右子树中，因此大于 root 的所有左子节点；successor 是 root 的右子树中的最小节点，因此小于 root 的右子树中的其他节点。以上两点保持了新子树的有序性。 在代码实现上，我们可以先寻找 successor，再删除它。successor 是 root 的右子树中的最小节点，可以先找到 root 的右子节点，再不停地往左子节点寻找，直到找到一个不存在左子节点的节点，这个节点即为 successor。然后递归地在 root.right 调用 deleteNode 来删除 successor。因为 successor 没有左子节点，因此这一步递归调用不会再次步入这一种情况。然后将 successor 更新为新的 root 并返回。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Solution { public TreeNode deleteNode(TreeNode root, int key) { if (root == null) { return null; } if (root.val \u0026gt; key) { root.left = deleteNode(root.left, key); return root; } if (root.val \u0026lt; key) { root.right = deleteNode(root.right, key); return root; } if (root.val == key) { if (root.left == null \u0026amp;\u0026amp; root.right == null) { return null; } if (root.right == null) { return root.left; } if (root.left == null) { return root.right; } TreeNode successor = root.right; while (successor.left != null) { successor = successor.left; } root.right = deleteNode(root.right, successor.val); successor.right = root.right; successor.left = root.left; return successor; } return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为 root 的节点个数。最差情况下，寻找和删除 successor 各需要遍历一次树。 空间复杂度：O(n)，其中 n 为 root 的节点个数。递归的深度最深为 O(n)。 方法二：迭代\n思路\n方法一的递归深度最多为 n，而大部分是由寻找值为 key 的节点贡献的，而寻找节点这一部分可以用迭代来优化。寻找并删除 successor 时，也可以用一个变量保存它的父节点，从而可以节省一步递归操作。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class Solution { public TreeNode deleteNode(TreeNode root, int key) { TreeNode cur = root, curParent = null; while (cur != null \u0026amp;\u0026amp; cur.val != key) { curParent = cur; if (cur.val \u0026gt; key) { cur = cur.left; } else { cur = cur.right; } } if (cur == null) { return root; } if (cur.left == null \u0026amp;\u0026amp; cur.right == null) { cur = null; } else if (cur.right == null) { cur = cur.left; } else if (cur.left == null) { cur = cur.right; } else { TreeNode successor = cur.right, successorParent = cur; while (successor.left != null) { successorParent = successor; successor = successor.left; } if (successorParent.val == cur.val) { successorParent.right = successor.right; } else { successorParent.left = successor.right; } successor.right = cur.right; successor.left = cur.left; cur = successor; } if (curParent == null) { return cur; } else { if (curParent.left != null \u0026amp;\u0026amp; curParent.left.val == key) { curParent.left = cur; } else { curParent.right = cur; } return root; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 为 root 的节点个数。最差情况下，需要遍历一次树。 空间复杂度：O(1)。使用的空间为常数。 day52 501. 二叉搜索树中的众数 题目\n给你一个含重复值的二叉搜索树（BST）的根节点 root ，找出并返回 BST 中的所有 众数（即，出现频率最高的元素）。\n如果树中有不止一个众数，可以按 任意顺序 返回。\n假定 BST 满足如下定义：\n结点左子树中所含节点的值 小于等于 当前节点的值 结点右子树中所含节点的值 大于等于 当前节点的值 左子树和右子树都是二叉搜索树 示例 1：\n1 2 输入：root = [1,null,2,2] 输出：[2] 示例 2：\n1 2 输入：root = [0] 输出：[0] 提示：\n树中节点的数目在范围 [1, 104] 内 -105 \u0026lt;= Node.val \u0026lt;= 105 **进阶：**你可以不使用额外的空间吗？（假设由递归产生的隐式调用栈的开销不被计算在内）\n解法\n方法一：当作普通二叉树\n先便利二叉树，然后值作为键，重复次数作为值放到一个map，之后统计这个map即可\n方法二：中序遍历\n思路与算法\n首先我们一定能想到一个最朴素的做法：因为这棵树的中序遍历是一个有序的序列，所以我们可以先获得这棵树的中序遍历，然后从扫描这个中序遍历序列，然后用一个哈希表来统计每个数字出现的个数，这样就可以找到出现次数最多的数字。但是这样做的空间复杂度显然不是 O(1) 的，原因是哈希表和保存中序遍历序列的空间代价都是 O(n)。\n首先，我们考虑在寻找出现次数最多的数时，不使用哈希表。 这个优化是基于二叉搜索树中序遍历的性质：一棵二叉搜索树的中序遍历序列是一个非递减的有序序列。例如：\n1 2 3 4 5 1 / \\ 0 2 / \\ / -1 0 2 这样一颗二叉搜索树的中序遍历序列是 {−1,0,0,1,2,2}。我们可以发现重复出现的数字一定是一个连续出现的，例如这里的 0 和 2，它们都重复出现了，并且所有的 0 都集中在一个连续的段内，所有的 2 也集中在一个连续的段内。我们可以顺序扫描中序遍历序列，用 base 记录当前的数字，用 count 记录当前数字重复的次数，用 maxCount 来维护已经扫描过的数当中出现最多的那个数字的出现次数，用 answer 数组记录出现的众数。每次扫描到一个新的元素：\n首先更新base和count: 如果该元素和 base 相等，那么 count 自增 1； 否则将 base 更新为当前数字，count 复位为 1。 然后更新maxCount： 如果 count = maxCount，那么说明当前的这个数字（base）出现的次数等于当前众数出现的次数，将 base 加入 answer 数组； 如果 count*\u0026gt;*maxCount，那么说明当前的这个数字（base）出现的次数大于当前众数出现的次数，因此，我们需要将 maxCount 更新为 count，清空 answer 数组后将 base 加入 answer 数组。 我们可以把这个过程写成一个 update 函数。这样我们在寻找出现次数最多的数字的时候就可以省去一个哈希表带来的空间消耗。\n然后，我们考虑不存储这个中序遍历序列。 如果我们在递归进行中序遍历的过程中，访问当了某个点的时候直接使用上面的 update 函数，就可以省去中序遍历序列的空间，代码如下。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Solution { List\u0026lt;Integer\u0026gt; answer = new ArrayList\u0026lt;Integer\u0026gt;(); int base, count, maxCount; public int[] findMode(TreeNode root) { dfs(root); int[] mode = new int[answer.size()]; for (int i = 0; i \u0026lt; answer.size(); ++i) { mode[i] = answer.get(i); } return mode; } public void dfs(TreeNode o) { if (o == null) { return; } dfs(o.left); update(o.val); dfs(o.right); } public void update(int x) { if (x == base) { ++count; } else { count = 1; base = x; } if (count == maxCount) { answer.add(base); } if (count \u0026gt; maxCount) { maxCount = count; answer.clear(); answer.add(base); } } } 复杂度分析\n时间复杂度：O(n)。即遍历这棵树的复杂度。 空间复杂度：O(n)。即递归的栈空间的空间代价。 方法三：Morris 中序遍历\n思路与算法\n接着上面的思路，我们用 Morris 中序遍历的方法把中序遍历的空间复杂度优化到 O(1)。 我们在中序遍历的时候，一定先遍历左子树，然后遍历当前节点，最后遍历右子树。在常规方法中，我们用递归回溯或者是栈来保证遍历完左子树可以再回到当前节点，但这需要我们付出额外的空间代价。我们需要用一种巧妙地方法可以在 O(1) 的空间下，遍历完左子树可以再回到当前节点。我们希望当前的节点在遍历完当前点的前驱之后被遍历，我们可以考虑修改它的前驱节点的 right 指针。当前节点的前驱节点的 right 指针可能本来就指向当前节点（前驱是当前节点的父节点），也可能是当前节点左子树最右下的节点。如果是后者，我们希望遍历完这个前驱节点之后再回到当前节点，可以将它的 right 指针指向当前节点。\nMorris 中序遍历的一个重要步骤就是寻找当前节点的前驱节点，并且 Morris 中序遍历寻找下一个点始终是通过转移到 right 指针指向的位置来完成的。\n如果当前节点没有左子树，则遍历这个点，然后跳转到当前节点的右子树。 如果当前节点有左子树，那么它的前驱节点一定在左子树上，我们可以在左子树上一直向右行走，找到当前点的前驱节点。 如果前驱节点没有右子树，就将前驱节点的 right 指针指向当前节点。这一步是为了在遍历完前驱节点后能找到前驱节点的后继，也就是当前节点。 如果前驱节点的右子树为当前节点，说明前驱节点已经被遍历过并被修改了 right 指针，这个时候我们重新将前驱的右孩子设置为空，遍历当前的点，然后跳转到当前节点的右子树。 因此我们可以得到这样的代码框架：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 TreeNode *cur = root, *pre = nullptr; while (cur) { if (!cur-\u0026gt;left) { // ...遍历 cur cur = cur-\u0026gt;right; continue; } pre = cur-\u0026gt;left; while (pre-\u0026gt;right \u0026amp;\u0026amp; pre-\u0026gt;right != cur) { pre = pre-\u0026gt;right; } if (!pre-\u0026gt;right) { pre-\u0026gt;right = cur; cur = cur-\u0026gt;left; } else { pre-\u0026gt;right = nullptr; // ...遍历 cur cur = cur-\u0026gt;right; } } 最后我们将 ...遍历 cur 替换成之前的 update 函数即可。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Solution { int base, count, maxCount; List\u0026lt;Integer\u0026gt; answer = new ArrayList\u0026lt;Integer\u0026gt;(); public int[] findMode(TreeNode root) { TreeNode cur = root, pre = null; while (cur != null) { if (cur.left == null) { update(cur.val); cur = cur.right; continue; } pre = cur.left; while (pre.right != null \u0026amp;\u0026amp; pre.right != cur) { pre = pre.right; } if (pre.right == null) { pre.right = cur; cur = cur.left; } else { pre.right = null; update(cur.val); cur = cur.right; } } int[] mode = new int[answer.size()]; for (int i = 0; i \u0026lt; answer.size(); ++i) { mode[i] = answer.get(i); } return mode; } public void update(int x) { if (x == base) { ++count; } else { count = 1; base = x; } if (count == maxCount) { answer.add(base); } if (count \u0026gt; maxCount) { maxCount = count; answer.clear(); answer.add(base); } } } 复杂度分析\n时间复杂度：O(n)。每个点被访问的次数不会超过两次，故这里的时间复杂度是 O(n)。 空间复杂度：O(1)。使用临时空间的大小和输入规模无关。 day53 530. 二叉搜索树的最小绝对差 题目\n给你一个二叉搜索树的根节点 root ，返回 树中任意两不同节点值之间的最小差值 。\n差值是一个正数，其数值等于两值之差的绝对值。\n示例 1：\n1 2 输入：root = [4,2,6,1,3] 输出：1 示例 2：\n1 2 输入：root = [1,0,48,null,null,12,49] 输出：1 提示：\n树中节点的数目范围是 [2, 104] 0 \u0026lt;= Node.val \u0026lt;= 105 **注意：**本题与 783 https://leetcode-cn.com/problems/minimum-distance-between-bst-nodes/ 相同\n解法\n方法一：中序遍历\n思路与算法\n考虑对升序数组 aa 求任意两个元素之差的绝对值的最小值，答案一定为相邻两个元素之差的最小值，即 $$ \\textit{ans}=\\min_{i=0}^{n-2}\\left{a[i+1]-a[i]\\right} $$ 其中 n 为数组 a 的长度。其他任意间隔距离大于等于 2 的下标对 (i,j) 的元素之差一定大于下标对 (i,i+1) 的元素之差，故不需要再被考虑。\n回到本题，本题要求二叉搜索树任意两节点差的绝对值的最小值，而我们知道二叉搜索树有个性质为二叉搜索树中序遍历得到的值序列是递增有序的，因此我们只要得到中序遍历后的值序列即能用上文提及的方法来解决。\n朴素的方法是经过一次中序遍历将值保存在一个数组中再进行遍历求解，我们也可以在中序遍历的过程中用 pre 变量保存前驱节点的值，这样即能边遍历边更新答案，不再需要显式创建数组来保存，需要注意的是 pre 的初始值需要设置成任意负数标记开头，下文代码中设置为 -1−1。\n二叉树的中序遍历有多种方式，包括递归、栈、Morris 遍历等，读者可选择自己最擅长的来实现。下文代码提供最普遍的递归方法来实现，其他遍历方法的介绍可以详细看「94. 二叉树的中序遍历的官方题解」，这里不再赘述。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { int pre; int ans; public int getMinimumDifference(TreeNode root) { ans = Integer.MAX_VALUE; pre = -1; dfs(root); return ans; } public void dfs(TreeNode root) { if (root == null) { return; } dfs(root.left); if (pre == -1) { pre = root.val; } else { ans = Math.min(ans, root.val - pre); pre = root.val; } dfs(root.right); } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉搜索树节点的个数。每个节点在中序遍历中都会被访问一次且只会被访问一次，因此总时间复杂度为 O(n)。 空间复杂度：O(n)。递归函数的空间复杂度取决于递归的栈深度，而栈深度在二叉搜索树为一条链的情况下会达到 O(n) 级别。 day54 538. 把二叉搜索树转换为累加树 题目\n给出二叉 搜索 树的根节点，该树的节点值各不相同，请你将其转换为累加树（Greater Sum Tree），使每个节点 node 的新值等于原树中大于或等于 node.val 的值之和。\n提醒一下，二叉搜索树满足下列约束条件：\n节点的左子树仅包含键 小于 节点键的节点。 节点的右子树仅包含键 大于 节点键的节点。 左右子树也必须是二叉搜索树。 **注意：**本题和 1038: https://leetcode-cn.com/problems/binary-search-tree-to-greater-sum-tree/ 相同\n示例 1：\n1 2 输入：[4,1,6,0,2,5,7,null,null,null,3,null,null,null,8] 输出：[30,36,21,36,35,26,15,null,null,null,33,null,null,null,8] 示例 2：\n1 2 输入：root = [0,null,1] 输出：[1,null,1] 示例 3：\n1 2 输入：root = [1,0,2] 输出：[3,3,2] 示例 4：\n1 2 输入：root = [3,2,4,1] 输出：[7,9,4,10] 提示：\n树中的节点数介于 0 和 104 之间。 每个节点的值介于 -104 和 104 之间。 树中的所有值 互不相同 。 给定的树为二叉搜索树。 解法\n前言\n二叉搜索树是一棵空树，或者是具有下列性质的二叉树：\n若它的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若它的右子树不空，则右子树上所有节点的值均大于它的根节点的值； 它的左、右子树也分别为二叉搜索树。 由这样的性质我们可以发现，二叉搜索树的中序遍历是一个单调递增的有序序列。如果我们反序地中序遍历该二叉搜索树，即可得到一个单调递减的有序序列。\n方法一：反序中序遍历\n思路及算法\n本题中要求我们将每个节点的值修改为原来的节点值加上所有大于它的节点值之和。这样我们只需要反序中序遍历该二叉搜索树，记录过程中的节点值之和，并不断更新当前遍历到的节点的节点值，即可得到题目要求的累加树。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { int sum = 0; public TreeNode convertBST(TreeNode root) { if (root != null) { convertBST(root.right); sum += root.val; root.val = sum; convertBST(root.left); } return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为递归过程中栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法二：Morris 遍历\n思路及算法\n有一种巧妙的方法可以在线性时间内，只占用常数空间来实现中序遍历。这种方法由 J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出，因此被称为 Morris 遍历。\nMorris 遍历的核心思想是利用树的大量空闲指针，实现空间开销的极限缩减。其反序中序遍历规则总结如下：\n如果当前节点的右子节点为空，处理当前节点，并遍历当前节点的左子节点； 如果当前节点的右子节点不为空，找到当前节点右子树的最左节点（该节点为当前节点中序遍历的前驱节点）； 如果最左节点的左指针为空，将最左节点的左指针指向当前节点，遍历当前节点的右子节点； 如果最左节点的左指针不为空，将最左节点的左指针重新置为空（恢复树的原状），处理当前节点，并将当前节点置为其左节点； 重复步骤 1 和步骤 2，直到遍历结束。 这样我们利用 Morris 遍历的方法，反序中序遍历该二叉搜索树，即可实现线性时间与常数空间的遍历。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Solution { public TreeNode convertBST(TreeNode root) { int sum = 0; TreeNode node = root; while (node != null) { if (node.right == null) { sum += node.val; node.val = sum; node = node.left; } else { TreeNode succ = getSuccessor(node); if (succ.left == null) { succ.left = node; node = node.right; } else { succ.left = null; sum += node.val; node.val = sum; node = node.left; } } } return root; } public TreeNode getSuccessor(TreeNode node) { TreeNode succ = node.right; while (succ.left != null \u0026amp;\u0026amp; succ.left != node) { succ = succ.left; } return succ; } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。没有左子树的节点只被访问一次，有左子树的节点被访问两次。 空间复杂度：O(1)。只操作已经存在的指针（树的空闲指针），因此只需要常数的额外空间。 day55 669. 修剪二叉搜索树 题目\n给你二叉搜索树的根节点 root ，同时给定最小边界low 和最大边界 high。通过修剪二叉搜索树，使得所有节点的值在[low, high]中。修剪树 不应该 改变保留在树中的元素的相对结构 (即，如果没有被移除，原有的父代子代关系都应当保留)。 可以证明，存在 唯一的答案 。\n所以结果应当返回修剪好的二叉搜索树的新的根节点。注意，根节点可能会根据给定的边界发生改变。\n示例 1：\n1 2 输入：root = [1,0,2], low = 1, high = 2 输出：[1,null,2] 示例 2：\n1 2 输入：root = [3,0,4,null,2,null,null,1], low = 1, high = 3 输出：[3,2,null,1] 提示：\n树中节点数在范围 [1, 104] 内 0 \u0026lt;= Node.val \u0026lt;= 104 树中每个节点的值都是 唯一 的 题目数据保证输入是一棵有效的二叉搜索树 0 \u0026lt;= low \u0026lt;= high \u0026lt;= 104 解法\n方法一：递归\n对根结点 root 进行深度优先遍历。对于当前访问的结点，如果结点为空结点，直接返回空结点；如果结点的值小于 low，那么说明该结点及它的左子树都不符合要求，我们返回对它的右结点进行修剪后的结果；如果结点的值大于 high，那么说明该结点及它的右子树都不符合要求，我们返回对它的左子树进行修剪后的结果；如果结点的值位于区间 [low,high]，我们将结点的左结点设为对它的左子树修剪后的结果，右结点设为对它的右子树进行修剪后的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public TreeNode trimBST(TreeNode root, int low, int high) { if (root == null) { return null; } if (root.val \u0026lt; low) { return trimBST(root.right, low, high); } else if (root.val \u0026gt; high) { return trimBST(root.left, low, high); } else { root.left = trimBST(root.left, low, high); root.right = trimBST(root.right, low, high); return root; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树的结点数目。 空间复杂度：O(n)。递归栈最坏情况下需要 O(n) 的空间。 方法二：迭代\n如果一个结点 node 符合要求，即它的值位于区间 [low, high]，那么它的左子树与右子树应该如何修剪？\n我们先讨论左子树的修剪：\nnode 的左结点为空结点：不需要修剪 node 的左结点非空： 如果它的左结点 left 的值小于 low，那么 left 以及 left 的左子树都不符合要求，我们将 node 的左结点设为 left 的右结点，然后再重新对 node 的左子树进行修剪。 如果它的左结点 left 的值大于等于 low，又因为 node 的值已经符合要求，所以 left 的右子树一定符合要求。基于此，我们只需要对 left 的左子树进行修剪。我们令 node 等于 left ，然后再重新对 node 的左子树进行修剪。 以上过程可以迭代处理。对于右子树的修剪同理。\n我们对根结点进行判断，如果根结点不符合要求，我们将根结点设为对应的左结点或右结点，直到根结点符合要求，然后将根结点作为符合要求的结点，依次修剪它的左子树与右子树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Solution { public TreeNode trimBST(TreeNode root, int low, int high) { while (root != null \u0026amp;\u0026amp; (root.val \u0026lt; low || root.val \u0026gt; high)) { if (root.val \u0026lt; low) { root = root.right; } else { root = root.left; } } if (root == null) { return null; } for (TreeNode node = root; node.left != null; ) { if (node.left.val \u0026lt; low) { node.left = node.left.right; } else { node = node.left; } } for (TreeNode node = root; node.right != null; ) { if (node.right.val \u0026gt; high) { node.right = node.right.left; } else { node = node.right; } } return root; } } 复杂度分析\n时间复杂度：O(n)，其中 n 为二叉树的结点数目。最多访问 n 个结点。 空间复杂度：O(1)。 day56 701. 二叉搜索树中的插入操作 题目\n给定二叉搜索树（BST）的根节点 root 和要插入树中的值 value ，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据 保证 ，新值和原始二叉搜索树中的任意节点值都不同。\n注意，可能存在多种有效的插入方式，只要树在插入后仍保持为二叉搜索树即可。 你可以返回 任意有效的结果 。\n示例 1：\n1 2 3 输入：root = [4,2,7,1,3], val = 5 输出：[4,2,7,1,3,5] 解释：另一个满足题目要求可以通过的树是： 示例 2：\n1 2 输入：root = [40,20,60,10,30,50,70], val = 25 输出：[40,20,60,10,30,50,70,null,null,25] 示例 3：\n1 2 输入：root = [4,2,7,1,3,null,null,null,null,null,null], val = 5 输出：[4,2,7,1,3,5] 提示：\n树中的节点数将在 [0, 104]的范围内。 -108 \u0026lt;= Node.val \u0026lt;= 108 所有值 Node.val 是 独一无二 的。 -108 \u0026lt;= val \u0026lt;= 108 保证 val 在原始BST中不存在。 解法\n方法一：模拟\n思路与算法\n首先回顾二叉搜索树的性质：对于任意节点 root 而言，左子树（如果存在）上所有节点的值均小于 root.val，右子树（如果存在）上所有节点的值均大于 root.val，且它们都是二叉搜索树。\n因此，当将 val 插入到以 root 为根的子树上时，根据 val 与 root.val 的大小关系，就可以确定要将 val 插入到哪个子树中。\n如果该子树不为空，则问题转化成了将 val 插入到对应子树上。 否则，在此处新建一个以 val 为值的节点，并链接到其父节点 root 上。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Solution { public TreeNode insertIntoBST(TreeNode root, int val) { if (root == null) { return new TreeNode(val); } TreeNode pos = root; while (pos != null) { if (val \u0026lt; pos.val) { if (pos.left == null) { pos.left = new TreeNode(val); break; } else { pos = pos.left; } } else { if (pos.right == null) { pos.right = new TreeNode(val); break; } else { pos = pos.right; } } } return root; } } 复杂度分析\n时间复杂度：O(N)，其中 N 为树中节点的数目。最坏情况下，我们需要将值插入到树的最深的叶子结点上，而叶子节点最深为 O(N)。 空间复杂度：O(1)。我们只使用了常数大小的空间。 方法二：递归、\n我们知道二叉搜索树插入新的节点时，如果还要满足BST性质，那么还有一种简单得思路：直接遍历到何时得叶子节点，插入到叶子节点末尾即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { TreeNode parent = null; int flag = 0; // 二叉排序树插入节点，一定可以插入到叶子节点 public TreeNode insertIntoBST(TreeNode root, int val) { if(root==null) { if(flag == -1){ return new TreeNode(val); }else{ return new TreeNode(val); } } if(val\u0026gt;root.val){ parent = root; flag = 1; root.right = insertIntoBST(root.right,val); } else if(val\u0026lt;root.val){ parent = root; flag = -1; root.left = insertIntoBST(root.left,val); } return root; } } day57 110. 平衡二叉树 题目\n给定一个二叉树，判断它是否是高度平衡的二叉树。\n本题中，一棵高度平衡二叉树定义为：\n一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1 。\n示例 1：\n1 2 输入：root = [3,9,20,null,null,15,7] 输出：true 示例 2：\n1 2 输入：root = [1,2,2,3,3,null,null,4,4] 输出：false 示例 3：\n1 2 输入：root = [] 输出：true 提示：\n树中的节点数在范围 [0, 5000] 内 -104 \u0026lt;= Node.val \u0026lt;= 104 解法\n前言\n这道题中的平衡二叉树的定义是：二叉树的每个节点的左右子树的高度差的绝对值不超过 1，则二叉树是平衡二叉树。根据定义，一棵二叉树是平衡二叉树，当且仅当其所有子树也都是平衡二叉树，因此可以使用递归的方式判断二叉树是不是平衡二叉树，递归的顺序可以是自顶向下或者自底向上。\n方法一：自顶向下的递归\n定义函数 height，用于计算二叉树中的任意一个节点 p 的高度： $$ \\texttt{height}(p) = \\begin{cases} 0 \u0026amp; p \\text{ 是空节点}\\ \\max(\\texttt{height}(p.\\textit{left}), \\texttt{height}(p.\\textit{right}))+1 \u0026amp; p \\text{ 是非空节点} \\end{cases} $$ 有了计算节点高度的函数，即可判断二叉树是否平衡。具体做法类似于二叉树的前序遍历，即对于当前遍历到的节点，首先计算左右子树的高度，如果左右子树的高度差是否不超过 1，再分别递归地遍历左右子节点，并判断左子树和右子树是否平衡。这是一个自顶向下的递归的过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public boolean isBalanced(TreeNode root) { if (root == null) { return true; } else { return Math.abs(height(root.left) - height(root.right)) \u0026lt;= 1 \u0026amp;\u0026amp; isBalanced(root.left) \u0026amp;\u0026amp; isBalanced(root.right); } } public int height(TreeNode root) { if (root == null) { return 0; } else { return Math.max(height(root.left), height(root.right)) + 1; } } } 复杂度分析\n时间复杂度：O(n^2)，其中 n 是二叉树中的节点个数。 最坏情况下，二叉树是满二叉树，需要遍历二叉树中的所有节点，时间复杂度是 O(n)。 对于节点 p，如果它的高度是 d，则 height(p) 最多会被调用 d 次（即遍历到它的每一个祖先节点时）。对于平均的情况，一棵树的高度 h 满足 O(h)=O(logn)，因为 d≤h，所以总时间复杂度为 O(nlog n)。对于最坏的情况，二叉树形成链式结构，高度为 O(n)，此时总时间复杂度为 O(n^2)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度主要取决于递归调用的层数，递归调用的层数不会超过 n。 方法二：自底向上的递归\n方法一由于是自顶向下递归，因此对于同一个节点，函数 height 会被重复调用，导致时间复杂度较高。如果使用自底向上的做法，则对于每个节点，函数 height 只会被调用一次。\n自底向上递归的做法类似于后序遍历，对于当前遍历到的节点，先递归地判断其左右子树是否平衡，再判断以当前节点为根的子树是否平衡。如果一棵子树是平衡的，则返回其高度（高度一定是非负整数），否则返回 -1。如果存在一棵子树不平衡，则整个二叉树一定不平衡。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public boolean isBalanced(TreeNode root) { return height(root) \u0026gt;= 0; } public int height(TreeNode root) { if (root == null) { return 0; } int leftHeight = height(root.left); int rightHeight = height(root.right); if (leftHeight == -1 || rightHeight == -1 || Math.abs(leftHeight - rightHeight) \u0026gt; 1) { return -1; } else { return Math.max(leftHeight, rightHeight) + 1; } } } 复杂度分析\n时间复杂度：O(n)，其中 n 是二叉树中的节点个数。使用自底向上的递归，每个节点的计算高度和判断是否平衡都只需要处理一次，最坏情况下需要遍历二叉树中的所有节点，因此时间复杂度是 O(n)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度主要取决于递归调用的层数，递归调用的层数不会超过 n。 day58 1382. 将二叉搜索树变平衡 题目\n给你一棵二叉搜索树，请你返回一棵 平衡后 的二叉搜索树，新生成的树应该与原来的树有着相同的节点值。如果有多种构造方法，请你返回任意一种。\n如果一棵二叉搜索树中，每个节点的两棵子树高度差不超过 1 ，我们就称这棵二叉搜索树是 平衡的 。\n示例 1：\n1 2 3 输入：root = [1,null,2,null,3,null,4,null,null] 输出：[2,1,3,null,null,null,4] 解释：这不是唯一的正确答案，[3,1,4,null,2,null,null] 也是一个可行的构造方案。 示例 2：\n1 2 输入: root = [2,1,3] 输出: [2,1,3] 提示：\n树节点的数目在 [1, 104] 范围内。 1 \u0026lt;= Node.val \u0026lt;= 105 解法\n贪心构造 day59 28. 找出字符串中第一个匹配项的下标 题目\n给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串的第一个匹配项的下标（下标从 0 开始）。如果 needle 不是 haystack 的一部分，则返回 -1 。\n示例 1：\n1 2 3 4 输入：haystack = \u0026#34;sadbutsad\u0026#34;, needle = \u0026#34;sad\u0026#34; 输出：0 解释：\u0026#34;sad\u0026#34; 在下标 0 和 6 处匹配。 第一个匹配项的下标是 0 ，所以返回 0 。 示例 2：\n1 2 3 输入：haystack = \u0026#34;leetcode\u0026#34;, needle = \u0026#34;leeto\u0026#34; 输出：-1 解释：\u0026#34;leeto\u0026#34; 没有在 \u0026#34;leetcode\u0026#34; 中出现，所以返回 -1 。 提示：\n1 \u0026lt;= haystack.length, needle.length \u0026lt;= 104 haystack 和 needle 仅由小写英文字符组成 解法\nKMP\nday60 459. 重复的子字符串 题目\n给定一个非空的字符串 s ，检查是否可以通过由它的一个子串重复多次构成。\n示例 1:\n1 2 3 输入: s = \u0026#34;abab\u0026#34; 输出: true 解释: 可由子串 \u0026#34;ab\u0026#34; 重复两次构成。 示例 2:\n1 2 输入: s = \u0026#34;aba\u0026#34; 输出: false 示例 3:\n1 2 3 输入: s = \u0026#34;abcabcabcabc\u0026#34; 输出: true 解释: 可由子串 \u0026#34;abc\u0026#34; 重复四次构成。 (或子串 \u0026#34;abcabc\u0026#34; 重复两次构成。) 提示：\n1 \u0026lt;= s.length \u0026lt;= 104 s 由小写英文字母组成 解法\nhttps://leetcode.cn/problems/repeated-substring-pattern/solution/zhong-fu-de-zi-zi-fu-chuan-by-leetcode-solution/\nday61 50. Pow(x, n) 题目\n实现 pow(x, n) ，即计算 x 的整数 n 次幂函数（即，xn ）。\n示例 1：\n1 2 输入：x = 2.00000, n = 10 输出：1024.00000 示例 2：\n1 2 输入：x = 2.10000, n = 3 输出：9.26100 示例 3：\n1 2 3 输入：x = 2.00000, n = -2 输出：0.25000 解释：2-2 = 1/22 = 1/4 = 0.25 提示：\n-100.0 \u0026lt; x \u0026lt; 100.0 -231 \u0026lt;= n \u0026lt;= 231-1 -104 \u0026lt;= xn \u0026lt;= 104 解法\n使用暴力迭代和暴力递归均会爆栈或超时\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // 递归求解爆栈 public double myPow(double x, int n) { if(n==0) return 1.0; else if(n\u0026gt;0) return myPow(x,n-1)*x; else return myPow(x,n+1)*(1.0/x); } // 迭代超时 public double myPow(double x, int n) { if(n==0) return 1.0; else if(n\u0026gt;0) { double r=x; for(int i=0;i\u0026lt;n-1;i++){ r=r*x; } return r; }else { double r=(1.0/x); for(int i=0;i\u0026lt;-n-1;i++){ r =r*(1.0/x); } return r; } } 方法一：快速幂+递归\n方法二：快速幂+迭代\nhttps://leetcode.cn/problems/powx-n/solution/powx-n-by-leetcode-solution/\nday62 33. 搜索旋转排序数组 题目\n整数数组 nums 按升序排列，数组中的值 互不相同 。\n在传递给函数之前，nums 在预先未知的某个下标 k（0 \u0026lt;= k \u0026lt; nums.length）上进行了 旋转，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2] 。\n给你 旋转后 的数组 nums 和一个整数 target ，如果 nums 中存在这个目标值 target ，则返回它的下标，否则返回 -1 。\n你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。\n示例 1：\n1 2 输入：nums = [4,5,6,7,0,1,2], target = 0 输出：4 示例 2：\n1 2 输入：nums = [4,5,6,7,0,1,2], target = 3 输出：-1 示例 3：\n1 2 输入：nums = [1], target = 0 输出：-1 提示：\n1 \u0026lt;= nums.length \u0026lt;= 5000 -104 \u0026lt;= nums[i] \u0026lt;= 104 nums 中的每个值都 独一无二 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -104 \u0026lt;= target \u0026lt;= 104 解法\nhttps://leetcode.cn/problems/search-in-rotated-sorted-array/solution/sou-suo-xuan-zhuan-pai-xu-shu-zu-by-leetcode-solut/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func search(nums []int, target int) int { l:=0 r:=len(nums)-1 mid :=0 for l\u0026lt;=r { mid = (r-l)/2+l if nums[mid] == target { return mid } if nums[mid]\u0026lt;nums[0]{// 右边有序 if target\u0026lt;=nums[r]\u0026amp;\u0026amp;target\u0026gt;nums[mid]{ l = mid + 1 }else{ r = mid - 1 } }else{// 左边有序 if target\u0026gt;=nums[l]\u0026amp;\u0026amp;target\u0026lt;nums[mid]{ r = mid - 1 }else{ l = mid + 1 } } } return -1 } day63 34. 在排序数组中查找元素的第一个和最后一个位置 题目\n给你一个按照非递减顺序排列的整数数组 nums，和一个目标值 target。请你找出给定目标值在数组中的开始位置和结束位置。\n如果数组中不存在目标值 target，返回 [-1, -1]。\n你必须设计并实现时间复杂度为 O(log n) 的算法解决此问题。\n示例 1：\n1 2 输入：nums = [5,7,7,8,8,10], target = 8 输出：[3,4] 示例 2：\n1 2 输入：nums = [5,7,7,8,8,10], target = 6 输出：[-1,-1] 示例 3：\n1 2 输入：nums = [], target = 0 输出：[-1,-1] 提示：\n0 \u0026lt;= nums.length \u0026lt;= 105 -109 \u0026lt;= nums[i] \u0026lt;= 109 nums 是一个非递减数组 -109 \u0026lt;= target \u0026lt;= 109 解法\n官方题解\n二分+暴力\n先二分找数，找到之后再两边扩展。\n问题：如果查找的数组重复度很高，而且刚好是target的重复，算法逐渐退化到O(n).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func searchRange(nums []int, target int) []int { ans:=[]int{-1,-1} idx:=-1 l:=0 r:=len(nums)-1 mid:=0 for l\u0026lt;=r{ mid = (r-l)/2+l if nums[mid]==target{ idx = mid break }else if nums[mid]\u0026gt;target{ r = mid - 1 }else{ l = mid + 1 } } if idx!=-1{ l=idx r=idx for l\u0026gt;=0\u0026amp;\u0026amp;nums[l]==target { l-- } l++ for r\u0026lt;len(nums)\u0026amp;\u0026amp;nums[r]==target { r++ } r-- ans[0] = l ans[1] = r } return ans } 二分+二分\n针对问题二再进行优化：二分都是找到索引最小的那个值。第一次二分找target，第二次二分找target+1的索引再-1\n1 2 3 4 5 6 7 8 func searchRange(nums []int, target int) []int { leftmost := sort.SearchInts(nums, target) if leftmost == len(nums) || nums[leftmost] != target { return []int{-1, -1} } rightmost := sort.SearchInts(nums, target + 1) - 1 return []int{leftmost, rightmost} } day64 81. 搜索旋转排序数组 II 题目\n已知存在一个按非降序排列的整数数组 nums ，数组中的值不必互不相同。\n在传递给函数之前，nums 在预先未知的某个下标 k（0 \u0026lt;= k \u0026lt; nums.length）上进行了 旋转 ，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,4,4,5,6,6,7] 在下标 5 处经旋转后可能变为 [4,5,6,6,7,0,1,2,4,4] 。\n给你 旋转后 的数组 nums 和一个整数 target ，请你编写一个函数来判断给定的目标值是否存在于数组中。如果 nums 中存在这个目标值 target ，则返回 true ，否则返回 false 。\n你必须尽可能减少整个操作步骤。\n示例 1：\n1 2 输入：nums = [2,5,6,0,0,1,2], target = 0 输出：true 示例 2：\n1 2 输入：nums = [2,5,6,0,0,1,2], target = 3 输出：false 提示：\n1 \u0026lt;= nums.length \u0026lt;= 5000 -104 \u0026lt;= nums[i] \u0026lt;= 104 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -104 \u0026lt;= target \u0026lt;= 104 进阶：\n这是 搜索旋转排序数组 的延伸题目，本题中的 nums 可能包含重复元素。 这会影响到程序的时间复杂度吗？会有怎样的影响，为什么？ 解法\nhttps://leetcode.cn/problems/search-in-rotated-sorted-array-ii/solution/sou-suo-xuan-zhuan-pai-xu-shu-zu-ii-by-l-0nmp/\nday65 153. 寻找旋转排序数组中的最小值 题目\n已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,2,4,5,6,7] 在变化后可能得到：\n若旋转 4 次，则可以得到 [4,5,6,7,0,1,2] 若旋转 7 次，则可以得到 [0,1,2,4,5,6,7] 注意，数组 [a[0], a[1], a[2], ..., a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], ..., a[n-2]] 。\n给你一个元素值 互不相同 的数组 nums ，它原来是一个升序排列的数组，并按上述情形进行了多次旋转。请你找出并返回数组中的 最小元素 。\n你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。\n示例 1：\n1 2 3 输入：nums = [3,4,5,1,2] 输出：1 解释：原数组为 [1,2,3,4,5] ，旋转 3 次得到输入数组。 示例 2：\n1 2 3 输入：nums = [4,5,6,7,0,1,2] 输出：0 解释：原数组为 [0,1,2,4,5,6,7] ，旋转 4 次得到输入数组。 示例 3：\n1 2 3 输入：nums = [11,13,15,17] 输出：11 解释：原数组为 [11,13,15,17] ，旋转 4 次得到输入数组。 提示：\nn == nums.length 1 \u0026lt;= n \u0026lt;= 5000 -5000 \u0026lt;= nums[i] \u0026lt;= 5000 nums 中的所有整数 互不相同 nums 原来是一个升序排序的数组，并进行了 1 至 n 次旋转 解法\nLeecode官方题解\nday66 11. 盛最多水的容器 题目\n给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。\n找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。\n返回容器可以储存的最大水量。\n**说明：**你不能倾斜容器。\n示例 1：\n1 2 3 输入：[1,8,6,2,5,4,8,3,7] 输出：49 解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例 2：\n1 2 输入：height = [1,1] 输出：1 提示：\nn == height.length 2 \u0026lt;= n \u0026lt;= 105 0 \u0026lt;= height[i] \u0026lt;= 104 解法\n双指针解法\nday67 45. 跳跃游戏 II 题目\n给你一个非负整数数组 nums ，你最初位于数组的第一个位置。\n数组中的每个元素代表你在该位置可以跳跃的最大长度。\n你的目标是使用最少的跳跃次数到达数组的最后一个位置。\n假设你总是可以到达数组的最后一个位置。\n示例 1:\n1 2 3 4 输入: nums = [2,3,1,1,4] 输出: 2 解释: 跳到最后一个位置的最小跳跃数是 2。 从下标为 0 跳到下标为 1 的位置，跳 1 步，然后跳 3 步到达数组的最后一个位置。 示例 2:\n1 2 输入: nums = [2,3,0,1,4] 输出: 2 提示:\n1 \u0026lt;= nums.length \u0026lt;= 104 0 \u0026lt;= nums[i] \u0026lt;= 1000 解法\n两种贪心策略解决：一种是反向查找出发位置；另一种是正向查找可达到的最大位置。\nLeecode官方题解\nday68 53. 最大子数组和 题目\n给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。\n子数组 是数组中的一个连续部分。\n示例 1：\n1 2 3 输入：nums = [-2,1,-3,4,-1,2,1,-5,4] 输出：6 解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 示例 2：\n1 2 输入：nums = [1] 输出：1 示例 3：\n1 2 输入：nums = [5,4,-1,7,8] 输出：23 提示：\n1 \u0026lt;= nums.length \u0026lt;= 105 -104 \u0026lt;= nums[i] \u0026lt;= 104 **进阶：**如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的 分治法 求解。\n解法\n官方题解：贪心、动态规划、分治\n贪心：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public int maxSubArray(int[] nums) { int maxSum = Integer.MIN_VALUE;//最大和 int thisSum = 0;//当前和 int len = nums.length; for(int i = 0; i \u0026lt; len; i++) { thisSum += nums[i]; if(maxSum \u0026lt; thisSum) { maxSum = thisSum; } //如果当前和小于0则归零，因为对于后面的元素来说这些是减小的。于是归零，意即从此处算开始最大和 if(thisSum \u0026lt; 0) { thisSum = 0; } } return maxSum; } day69 55. 跳跃游戏 题目\n给定一个非负整数数组 nums ，你最初位于数组的 第一个下标 。\n数组中的每个元素代表你在该位置可以跳跃的最大长度。\n判断你是否能够到达最后一个下标。\n示例 1：\n1 2 3 输入：nums = [2,3,1,1,4] 输出：true 解释：可以先跳 1 步，从下标 0 到达下标 1, 然后再从下标 1 跳 3 步到达最后一个下标。 示例 2：\n1 2 3 输入：nums = [3,2,1,0,4] 输出：false 解释：无论怎样，总会到达下标为 3 的位置。但该下标的最大跳跃长度是 0 ， 所以永远不可能到达最后一个下标。 提示：\n1 \u0026lt;= nums.length \u0026lt;= 3 * 104 0 \u0026lt;= nums[i] \u0026lt;= 105 解法\n解法一：贪心\n官方题解\n解法一：反向遍历\n不断地从最后一个位置开始往前找能到达当前位置的索引，然后再往前找。直到找到第一个位置可以到达为止\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func canJump(nums []int) bool { if len(nums)\u0026lt;=1{ return true } if nums[0] == 0 { return false } idx:=len(nums)-1 for i:=len(nums)-2;i\u0026gt;=0;i--{ if i+nums[i]\u0026gt;=idx{ // fmt.Println(idx,nums[idx]) idx = i } } return idx == 0 } day70 56. 合并区间 题目\n以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。请你合并所有重叠的区间，并返回 一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间 。\n示例 1：\n1 2 3 输入：intervals = [[1,3],[2,6],[8,10],[15,18]] 输出：[[1,6],[8,10],[15,18]] 解释：区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2：\n1 2 3 输入：intervals = [[1,4],[4,5]] 输出：[[1,5]] 解释：区间 [1,4] 和 [4,5] 可被视为重叠区间。 提示：\n1 \u0026lt;= intervals.length \u0026lt;= 104 intervals[i].length == 2 0 \u0026lt;= starti \u0026lt;= endi \u0026lt;= 104 解法\n总的来说，就是找规律\n官方题解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func merge(intervals [][]int) [][]int { sort.Slice(intervals,func(i,j int) bool{ return intervals[i][0]\u0026lt;intervals[j][0] }) merged := [][]int{{intervals[0][0],intervals[0][1]}} for i:=1;i\u0026lt;len(intervals);i++{ if intervals[i][0]\u0026gt;merged[len(merged)-1][1]{ merged = append(merged,intervals[i]) }else if merged[len(merged)-1][1]\u0026lt;intervals[i][1] { merged[len(merged)-1][1] = intervals[i][1] } } return merged } day71 121. 买卖股票的最佳时机 题目\n给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。\n你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。\n返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。\n示例 1：\n1 2 3 4 输入：[7,1,5,3,6,4] 输出：5 解释：在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 示例 2：\n1 2 3 输入：prices = [7,6,4,3,1] 输出：0 解释：在这种情况下, 没有交易完成, 所以最大利润为 0。 提示：\n1 \u0026lt;= prices.length \u0026lt;= 105 0 \u0026lt;= prices[i] \u0026lt;= 104 解法\n暴力\\动态规划(记录历史最小值)\n官方题解\nday72 122. 买卖股票的最佳时机 II 题目\n给你一个整数数组 prices ，其中 prices[i] 表示某支股票第 i 天的价格。\n在每一天，你可以决定是否购买和/或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。\n返回 你能获得的 最大 利润 。\n示例 1：\n1 2 3 4 5 输入：prices = [7,1,5,3,6,4] 输出：7 解释：在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5 - 1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6 - 3 = 3 。 总利润为 4 + 3 = 7 。 示例 2：\n1 2 3 4 输入：prices = [1,2,3,4,5] 输出：4 解释：在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5 - 1 = 4 。 总利润为 4 。 示例 3：\n1 2 3 输入：prices = [7,6,4,3,1] 输出：0 解释：在这种情况下, 交易无法获得正利润，所以不参与交易可以获得最大利润，最大利润为 0 。 题解\n贪心\\动态规划\n官方题解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // dp class Solution { public int maxProfit(int[] prices) { // dp[i][0]表示第i天不持有股票的最大利润，dp[i][1]表示第i天持有股票的最大利润 int[][] dp = new int[prices.length][2]; // 初始化 dp[0][0] = 0; dp[0][1] = -prices[0]; // 状态转移 for(int i=1;i\u0026lt;prices.length;i++){ // 不持有股票的利润，应该是前一天不持有股票和持有股票但是今天要卖出的最大利润 dp[i][0] = Math.max(dp[i-1][0],dp[i-1][1]+prices[i]); // 持有股票的利润，应该是前一天不持有股票，今天买入的和前一天持有股票的最大利润 dp[i][1] = Math.max(dp[i-1][0]-prices[i],dp[i-1][1]); } return dp[prices.length-1][0]; } } // dp 优化 class Solution { public int maxProfit(int[] prices) { // 初始化 int dp0 = 0; int dp1 = -prices[0]; // 状态转移 for(int i=1;i\u0026lt;prices.length;i++){ // 不持有股票的利润，应该是前一天不持有股票和持有股票但是今天要卖出的最大利润 dp0 = Math.max(dp0,dp1+prices[i]); // 持有股票的利润，应该是前一天不持有股票，今天买入的和前一天持有股票的最大利润 dp1 = Math.max(dp0-prices[i],dp1); } return dp0; } } 例如：7,1,2,5,4,6,4 找局部递增序列，局部递增序列最小值买入，最大值卖出 先找最小值，再找较大值，遇到小的值就结束，找下一个最小值 由于递增序列内的数字序列：5-2+2-1 = 5-1 所以，我们可以再遇到一个正的利润就可以直接把这个利润加上即可，当然，也可以找出递增区间的最值的差值就是利润\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public int maxProfit(int[] prices) { int min = Integer.MAX_VALUE;// 最小值 int w = 0;// 利润 for(int i=0;i\u0026lt;prices.length;i++){ if(prices[i]-min\u0026gt;0){ w += prices[i]-min; min = Integer.MAX_VALUE; // 重置 } if(min\u0026gt;prices[i]){ min = prices[i]; } } return w; } } day73 134. 加油站 题目\n在一条环路上有 n 个加油站，其中第 i 个加油站有汽油 gas[i] 升。\n你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。\n给定两个整数数组 gas 和 cost ，如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1 。如果存在解，则 保证 它是 唯一 的。\n示例 1:\n1 2 3 4 5 6 7 8 9 10 输入: gas = [1,2,3,4,5], cost = [3,4,5,1,2] 输出: 3 解释: 从 3 号加油站(索引为 3 处)出发，可获得 4 升汽油。此时油箱有 = 0 + 4 = 4 升汽油 开往 4 号加油站，此时油箱有 4 - 1 + 5 = 8 升汽油 开往 0 号加油站，此时油箱有 8 - 2 + 1 = 7 升汽油 开往 1 号加油站，此时油箱有 7 - 3 + 2 = 6 升汽油 开往 2 号加油站，此时油箱有 6 - 4 + 3 = 5 升汽油 开往 3 号加油站，你需要消耗 5 升汽油，正好足够你返回到 3 号加油站。 因此，3 可为起始索引。 示例 2:\n1 2 3 4 5 6 7 8 9 输入: gas = [2,3,4], cost = [3,4,3] 输出: -1 解释: 你不能从 0 号或 1 号加油站出发，因为没有足够的汽油可以让你行驶到下一个加油站。 我们从 2 号加油站出发，可以获得 4 升汽油。 此时油箱有 = 0 + 4 = 4 升汽油 开往 0 号加油站，此时油箱有 4 - 3 + 2 = 3 升汽油 开往 1 号加油站，此时油箱有 3 - 3 + 3 = 3 升汽油 你无法返回 2 号加油站，因为返程需要消耗 4 升汽油，但是你的油箱只有 3 升汽油。 因此，无论怎样，你都不可能绕环路行驶一周。 提示:\ngas.length == n cost.length == n 1 \u0026lt;= n \u0026lt;= 105 0 \u0026lt;= gas[i], cost[i] \u0026lt;= 104 解法\n数学证明，降低时间复杂度\nday74 135. 分发糖果 题目\nn 个孩子站成一排。给你一个整数数组 ratings 表示每个孩子的评分。\n你需要按照以下要求，给这些孩子分发糖果：\n每个孩子至少分配到 1 个糖果。 相邻两个孩子评分更高的孩子会获得更多的糖果。 请你给每个孩子分发糖果，计算并返回需要准备的 最少糖果数目 。\n示例 1：\n1 2 3 输入：ratings = [1,0,2] 输出：5 解释：你可以分别给第一个、第二个、第三个孩子分发 2、1、2 颗糖果。 示例 2：\n1 2 3 4 输入：ratings = [1,2,2] 输出：4 解释：你可以分别给第一个、第二个、第三个孩子分发 1、2、1 颗糖果。 第三个孩子只得到 1 颗糖果，这满足题面中的两个条件。 提示：\nn == ratings.length 1 \u0026lt;= n \u0026lt;= 2 * 104 0 \u0026lt;= ratings[i] \u0026lt;= 2 * 104 解法\n方法一：两次遍历\n我们可以将「相邻的孩子中，评分高的孩子必须获得更多的糖果」这句话拆分为两个规则，分别处理。\n左规则：当 $\\textit{ratings}[i - 1] \u0026lt; \\textit{ratings}[i]$ 时，i 号学生的糖果数量将比 i - 1 号孩子的糖果数量多。 右规则：当 $\\textit{ratings}[i] \u0026gt; \\textit{ratings}[i + 1]$ 时，i 号学生的糖果数量将比 i + 1 号孩子的糖果数量多。 我们遍历该数组两次，处理出每一个学生分别满足左规则或右规则时，最少需要被分得的糖果数量。每个人最终分得的糖果数量即为这两个数量的最大值。\n具体地，以左规则为例：我们从左到右遍历该数组，假设当前遍历到位置 i，如果有 $\\textit{ratings}[i - 1] \u0026lt; \\textit{ratings}[i]$ 那么 i 号学生的糖果数量将比 i - 1 号孩子的糖果数量多，我们令 $\\textit{left}[i] = \\textit{left}[i - 1] + 1$ 即可，否则我们令 $\\textit{left}[i] = 1$。\n在实际代码中，我们先计算出左规则 $\\textit{left}$ 数组，在计算右规则的时候只需要用单个变量记录当前位置的右规则，同时计算答案即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 // 形式一 class Solution { public int candy(int[] ratings) { // 每人先发一个糖果 int sum = ratings.length; // 挨个比较相邻评分的大小，分高者糖果数+1 int[] matrix = new int[ratings.length]; for(int i=1;i\u0026lt;ratings.length;i++){ if(ratings[i]\u0026gt;ratings[i-1]\u0026amp;\u0026amp;matrix[i]\u0026lt;=matrix[i-1]){ matrix[i]=matrix[i-1]+1; } } // System.out.println(Arrays.toString(matrix)); for(int i=ratings.length-2;i\u0026gt;=0;i--){ if(ratings[i]\u0026gt;ratings[i+1]\u0026amp;\u0026amp;matrix[i]\u0026lt;=matrix[i+1]){ matrix[i]=matrix[i+1]+1; } } // System.out.println(Arrays.toString(matrix)); for(int i=0;i\u0026lt;matrix.length;i++){sum+=matrix[i];} return sum; } } // 形式二 class Solution { public int candy(int[] ratings) { int n = ratings.length; int[] left = new int[n]; for (int i = 0; i \u0026lt; n; i++) { if (i \u0026gt; 0 \u0026amp;\u0026amp; ratings[i] \u0026gt; ratings[i - 1]) { left[i] = left[i - 1] + 1; } else { left[i] = 1; } } int right = 0, ret = 0; for (int i = n - 1; i \u0026gt;= 0; i--) { if (i \u0026lt; n - 1 \u0026amp;\u0026amp; ratings[i] \u0026gt; ratings[i + 1]) { right++; } else { right = 1; } ret += Math.max(left[i], right); } return ret; } } 方法二：常数空间遍历\nhttps://leetcode.cn/problems/candy/solution/fen-fa-tang-guo-by-leetcode-solution-f01p/\nday75 376. 摆动序列 题目\n如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为 **摆动序列 。**第一个差（如果存在的话）可能是正数或负数。仅有一个元素或者含两个不等元素的序列也视作摆动序列。\n例如， [1, 7, 4, 9, 2, 5] 是一个 摆动序列 ，因为差值 (6, -3, 5, -7, 3) 是正负交替出现的。 相反，[1, 4, 7, 2, 5] 和 [1, 7, 4, 5, 5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 子序列 可以通过从原始序列中删除一些（也可以不删除）元素来获得，剩下的元素保持其原始顺序。\n给你一个整数数组 nums ，返回 nums 中作为 摆动序列 的 最长子序列的长度 。\n示例 1：\n1 2 3 输入：nums = [1,7,4,9,2,5] 输出：6 解释：整个序列均为摆动序列，各元素之间的差值为 (6, -3, 5, -7, 3) 。 示例 2：\n1 2 3 4 输入：nums = [1,17,5,10,13,15,10,5,16,8] 输出：7 解释：这个序列包含几个长度为 7 摆动序列。 其中一个是 [1, 17, 10, 13, 10, 16, 8] ，各元素之间的差值为 (16, -7, 3, -3, 6, -8) 。 示例 3：\n1 2 输入：nums = [1,2,3,4,5,6,7,8,9] 输出：2 提示：\n1 \u0026lt;= nums.length \u0026lt;= 1000 0 \u0026lt;= nums[i] \u0026lt;= 1000 **进阶：**你能否用 O(n) 时间复杂度完成此题?\n解法\n动态规划\\贪心算法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public int wiggleMaxLength(int[] nums) { // 顺序遍历，“峰”才算上：一旦遇到非递增数据或者非递减数据，就算做最长子序列的一个元素 if(nums.length==1){ return 1; }else if(nums.length==2\u0026amp;\u0026amp;nums[0]!=nums[1]){ return 2; } int ans = 1; int state = 0; //表示先前的状态：0表示前面的两个数差为0，1表示差为正，-1表示差为负 // 只需计数“峰”的数量即可 for(int i=1;i\u0026lt;nums.length;i++){ if(nums[i]\u0026lt;nums[i-1]\u0026amp;\u0026amp;state==1){ ans++; state = -1; }else if(nums[i]\u0026gt;nums[i-1]\u0026amp;\u0026amp;state==-1){ ans++; state = 1; }else if(nums[i]\u0026gt;nums[i-1]\u0026amp;\u0026amp;state==0){ ans++; state = 1; }else if(nums[i]\u0026lt;nums[i-1]\u0026amp;\u0026amp;state==0){ ans++; state = -1; } } return ans; } } day76 409. 最长回文串 题目\n给定一个包含大写字母和小写字母的字符串 s ，返回 通过这些字母构造成的 最长的回文串 。\n在构造过程中，请注意 区分大小写 。比如 \u0026quot;Aa\u0026quot; 不能当做一个回文字符串。\n示例 1:\n1 2 3 4 输入:s = \u0026#34;abccccdd\u0026#34; 输出:7 解释: 我们可以构造的最长的回文串是\u0026#34;dccaccd\u0026#34;, 它的长度是 7。 示例 2:\n1 2 输入:s = \u0026#34;a\u0026#34; 输入:1 示例 3：\n1 2 输入:s = \u0026#34;aaaaaccc\u0026#34; 输入:7 提示:\n1 \u0026lt;= s.length \u0026lt;= 2000 s 只由小写 和/或 大写英文字母组成 解法\n解法一：数学规律\n题目要求仅仅只是需要根据给出的字符序列构造出最长回文串序列，回文串的长度如果是偶数那么前一半字符序列是后一半字符序列的“镜像”；如果是奇数，那么中间会有一个字符作为分隔，左右两边是“镜像”关系。针对“镜像”字符序列，也就是说，一个字符重复n次，如果n是偶数，那么可以将这些字符一半放在左边，一半放在右边；如果是奇数，那么可以去掉一个字符，然后将一半放左边，一半放右边。如果给的字符序列里存在奇数个重复字符，都可以作为“镜像”序列的地中情况；如果都是偶数，那么只能构造第一个“镜像”字符序列。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class Solution { public int longestPalindrome(String s) { if(s.length()==1) return 1; // 先字典排序 char[] chs = s.toCharArray(); Arrays.sort(chs); // System.out.println(Arrays.toString(chs)); char pre = chs[0]; int count = 1; int[] counts = new int[chs.length]; int countIdx = 0; for(int i=1;i\u0026lt;chs.length;i++){ if(chs[i]==pre){ count++; }else{ // 暂存 counts[countIdx] = count; countIdx++; count=1; pre = chs[i]; } } counts[countIdx]=count; // System.out.println(Arrays.toString(counts)); int Len = 1; boolean flag = false; for(int i=0;i\u0026lt;counts.length\u0026amp;\u0026amp;counts[i]!=0;i++){ if(counts[i]%2==0){ Len+=counts[i]; }else{ if(counts[i]\u0026gt;1){ Len+=counts[i]-1; } flag=true; } } return flag?Len:Len-1; } } 解法二：官方解法\nhttps://leetcode.cn/problems/longest-palindrome/solution/zui-chang-hui-wen-chuan-by-leetcode-solution/\nday77 435. 无重叠区间 题目\n给定一个区间的集合 intervals ，其中 intervals[i] = [starti, endi] 。返回 需要移除区间的最小数量，使剩余区间互不重叠 。\n示例 1:\n1 2 3 输入: intervals = [[1,2],[2,3],[3,4],[1,3]] 输出: 1 解释: 移除 [1,3] 后，剩下的区间没有重叠。 示例 2:\n1 2 3 输入: intervals = [ [1,2], [1,2], [1,2] ] 输出: 2 解释: 你需要移除两个 [1,2] 来使剩下的区间没有重叠。 示例 3:\n1 2 3 输入: intervals = [ [1,2], [2,3] ] 输出: 0 解释: 你不需要移除任何区间，因为它们已经是无重叠的了。 提示:\n1 \u0026lt;= intervals.length \u0026lt;= 105 intervals[i].length == 2 -5 * 104 \u0026lt;= starti \u0026lt; endi \u0026lt;= 5 * 104 解法\n动态规划\\贪心\nday78 452. 用最少数量的箭引爆气球 题目\n有一些球形气球贴在一堵用 XY 平面表示的墙面上。墙面上的气球记录在整数数组 points ，其中points[i] = [xstart, xend] 表示水平直径在 xstart 和 xend之间的气球。你不知道气球的确切 y 坐标。\n一支弓箭可以沿着 x 轴从不同点 完全垂直 地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 x``start，x``end， 且满足 xstart ≤ x ≤ x``end，则该气球会被 引爆 。可以射出的弓箭的数量 没有限制 。 弓箭一旦被射出之后，可以无限地前进。\n给你一个数组 points ，返回引爆所有气球所必须射出的 最小 弓箭数 。\n示例 1：\n1 2 3 4 5 输入：points = [[10,16],[2,8],[1,6],[7,12]] 输出：2 解释：气球可以用2支箭来爆破: -在x = 6处射出箭，击破气球[2,8]和[1,6]。 -在x = 11处发射箭，击破气球[10,16]和[7,12]。 示例 2：\n1 2 3 输入：points = [[1,2],[3,4],[5,6],[7,8]] 输出：4 解释：每个气球需要射出一支箭，总共需要4支箭。 示例 3：\n1 2 3 4 5 输入：points = [[1,2],[2,3],[3,4],[4,5]] 输出：2 解释：气球可以用2支箭来爆破: - 在x = 2处发射箭，击破气球[1,2]和[2,3]。 - 在x = 4处射出箭，击破气球[3,4]和[4,5]。 提示:\n1 \u0026lt;= points.length \u0026lt;= 105 points[i].length == 2 -231 \u0026lt;= xstart \u0026lt; xend \u0026lt;= 231 - 1 解法\n注意给出的数据有大数，所以涉及到运算，不能直接向加减乘除等。\n解法一：暴力\n问题可以看成是：我们删除一些重叠的区间，之后再统计这些没有删除的区间\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Solution { public int findMinArrowShots(int[][] points) { if(points.length==1) return 1; // 按照右端点排序 Arrays.sort(points, new Comparator\u0026lt;int[]\u0026gt;() { public int compare(int[] point1, int[] point2) { if (point1[1] \u0026gt; point2[1]) { return 1; } else if (point1[1] \u0026lt; point2[1]) { return -1; } else { return 0; } } }); // 记录是否删除区间 boolean[] isDel = new boolean[points.length]; int ans = 0; // 基准区间的左右值 for(int i=0;i\u0026lt;points.length;i++){ if(!isDel[i]){ int pivotLeft = points[i][0]; int pivotRight = points[i][1]; for(int j=i+1;j\u0026lt;points.length;j++){ // 当前区间还在 if(!isDel[j]){ if(points[j][0]\u0026gt;pivotRight){} else{ isDel[j] = true; } } } } } for(int i=0;i\u0026lt;isDel.length;i++){ if(!isDel[i]){ ans++; } } return ans; } } 解法二：排序+贪心\nday79 455. 分发饼干 题目\n假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。\n对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] \u0026gt;= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。\n示例 1:\n1 2 3 4 5 6 输入: g = [1,2,3], s = [1,1] 输出: 1 解释: 你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。 虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。 所以你应该输出1。 示例 2:\n1 2 3 4 5 6 输入: g = [1,2], s = [1,2,3] 输出: 2 解释: 你有两个孩子和三块小饼干，2个孩子的胃口值分别是1,2。 你拥有的饼干数量和尺寸都足以让所有孩子满足。 所以你应该输出2. 提示：\n1 \u0026lt;= g.length \u0026lt;= 3 * 104 0 \u0026lt;= s.length \u0026lt;= 3 * 104 1 \u0026lt;= g[i], s[j] \u0026lt;= 231 - 1 解法\n排序+双指针+贪心\nday80 605. 种花问题 题目\n假设有一个很长的花坛，一部分地块种植了花，另一部分却没有。可是，花不能种植在相邻的地块上，它们会争夺水源，两者都会死去。\n给你一个整数数组 flowerbed 表示花坛，由若干 0 和 1 组成，其中 0 表示没种植花，1 表示种植了花。另有一个数 n ，能否在不打破种植规则的情况下种入 n 朵花？能则返回 true ，不能则返回 false。\n示例 1：\n1 2 输入：flowerbed = [1,0,0,0,1], n = 1 输出：true 示例 2：\n1 2 输入：flowerbed = [1,0,0,0,1], n = 2 输出：false 提示：\n1 \u0026lt;= flowerbed.length \u0026lt;= 2 * 104 flowerbed[i] 为 0 或 1 flowerbed 中不存在相邻的两朵花 0 \u0026lt;= n \u0026lt;= flowerbed.length 解法\n解法一：模拟\n触雷首尾位置需要特殊处理，其余位置就是比较前一个和后一个是否都没种花，如果都没种花，再看当前位置是否种花，如果没，就种花，否则就需要迭代下一个花坛位置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Solution { public boolean canPlaceFlowers(int[] flowerbed, int n) { if(flowerbed.length==1){ if(flowerbed[0]==1) return n==0; else return n\u0026lt;=1; } for(int i=0;i\u0026lt;flowerbed.length;i++){ // 当碰到收尾时，需要收敛比较 if(i==0){ if(flowerbed[i]!=flowerbed[i+1]){}else { if(n==0) break; flowerbed[i]=1; n--; } }else if(i==flowerbed.length-1){ if(flowerbed[i]!=flowerbed[i-1]){}else { if(n==0) break; flowerbed[i]=1; n--; } }else{ if(flowerbed[i]!=flowerbed[i-1]||flowerbed[i]!=flowerbed[i+1]){} else { if(n==0) break; flowerbed[i]=1; n--; } } } // System.out.println(Arrays.toString(flowerbed)); return n==0; } } 解法二：贪心\nhttps://leetcode.cn/problems/can-place-flowers/solution/chong-hua-wen-ti-by-leetcode-solution-sojr/\nday81 738. 单调递增的数字 题目\n当且仅当每个相邻位数上的数字 x 和 y 满足 x \u0026lt;= y 时，我们称这个整数是单调递增的。\n给定一个整数 n ，返回 小于或等于 n 的最大数字，且数字呈 单调递增 。\n示例 1:\n1 2 输入: n = 10 输出: 9 示例 2:\n1 2 输入: n = 1234 输出: 1234 示例 3:\n1 2 输入: n = 332 输出: 299 提示:\n0 \u0026lt;= n \u0026lt;= 109 解法\n解法一：数学规律\n我们只需要找到单调递增的最后一个数位上的数字，如果该数字是最后一个，那么整体都满足单调递增特性直接返回；如果不是，那么当前数位和前面的数位组成的数字减去1，再把后面的数位全部变成数字9，之后前面和后面凭借在一起返回即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class Solution { public int monotoneIncreasingDigits(int n) { // 取位数 int Len = 0; int tmp = n; while(tmp!=0){ tmp=tmp/10; Len++; } // System.out.println(\u0026#34;位数\u0026#34;+Len); // 如果是一位直接返回结果 if(Len==1) return n; // 其他位数，循环处理 tmp = n; // 保证单调递增 int preMax = 0; int curMax = 0; int index = 0; while(index\u0026lt;Len){ // 取出当前位 curMax = tmp/(int)Math.pow(10,Len-index-1)%10; // System.out.println(\u0026#34;preMax=\u0026#34;+preMax+\u0026#34;curMax=\u0026#34;+curMax); if(curMax\u0026lt;preMax){ // 往前回溯分割点 int i = index-1; while(i\u0026gt;=0){ if(tmp/(int)Math.pow(10,Len-i-1)%10!=preMax){ break; } i--; } i++; // System.out.println(\u0026#34;回溯次数=\u0026#34;+i); // 将该位及以前的数据减去（该位数字+1），再和后面等位数的数字9拼接 // int lastCast = tmp/(int)Math.pow(10,Len-i-1); // System.out.println(\u0026#34;lastCast=\u0026#34;+lastCast); int front = tmp/(int)Math.pow(10,Len-i-1)-1; int back = 0; int j=Len-i-1; while(j\u0026gt;0){ back=back*10+9; j--; } // System.out.println(\u0026#34;front=\u0026#34;+front+\u0026#34;back=\u0026#34;+back); return front*((int)Math.pow(10,Len-i-1))+back; } preMax = curMax; index++; } return n; } } 解法二：贪心\nhttps://leetcode.cn/problems/monotone-increasing-digits/solution/dan-diao-di-zeng-de-shu-zi-by-leetcode-s-5908/\nday82 1005. K 次取反后最大化的数组和 题目\n给你一个整数数组 nums 和一个整数 k ，按以下方法修改该数组：\n选择某个下标 i 并将 nums[i] 替换为 -nums[i] 。 重复这个过程恰好 k 次。可以多次选择同一个下标 i 。\n以这种方式修改数组后，返回数组 可能的最大和 。\n示例 1：\n1 2 3 输入：nums = [4,2,3], k = 1 输出：5 解释：选择下标 1 ，nums 变为 [4,-2,3] 。 示例 2：\n1 2 3 输入：nums = [3,-1,0,2], k = 3 输出：6 解释：选择下标 (1, 2, 2) ，nums 变为 [3,1,0,2] 。 示例 3：\n1 2 3 输入：nums = [2,-3,-1,5,-4], k = 2 输出：13 解释：选择下标 (1, 4) ，nums 变为 [2,3,-1,5,4] 。 提示：\n1 \u0026lt;= nums.length \u0026lt;= 104 -100 \u0026lt;= nums[i] \u0026lt;= 100 1 \u0026lt;= k \u0026lt;= 104 解法\n解法一：寻找数学规律\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 // 代码一 class Solution { public int largestSumAfterKNegations(int[] nums, int k) { Arrays.sort(nums); for(int i=0;i\u0026lt;nums.length;i++){ if(k\u0026lt;=0) break; if(nums[i]\u0026lt;0) { nums[i]*=-1; k--; } } // 可以通过找最值，省去第二次排序懒得写了 Arrays.sort(nums); int sum = 0; // System.out.println(Arrays.toString(nums)); // 找和 for(int i=0;i\u0026lt;nums.length;i++){ sum+=nums[i]; } return k%2==1?sum-2*nums[0]:sum; } } // 官方解法 class Solution { public int largestSumAfterKNegations(int[] nums, int k) { Map\u0026lt;Integer, Integer\u0026gt; freq = new HashMap\u0026lt;Integer, Integer\u0026gt;(); for (int num : nums) { freq.put(num, freq.getOrDefault(num, 0) + 1); } int ans = Arrays.stream(nums).sum(); for (int i = -100; i \u0026lt; 0; ++i) { if (freq.containsKey(i)) { int ops = Math.min(k, freq.get(i)); ans += (-i) * ops * 2; freq.put(i, freq.get(i) - ops); freq.put(-i, freq.getOrDefault(-i, 0) + ops); k -= ops; if (k == 0) { break; } } } if (k \u0026gt; 0 \u0026amp;\u0026amp; k % 2 == 1 \u0026amp;\u0026amp; !freq.containsKey(0)) { for (int i = 1; i \u0026lt;= 100; ++i) { if (freq.containsKey(i)) { ans -= i * 2; break; } } } return ans; } } day83 1013. 将数组分成和相等的三个部分 题目\n给你一个整数数组 arr，只有可以将其划分为三个和相等的 非空 部分时才返回 true，否则返回 false。\n形式上，如果可以找出索引 i + 1 \u0026lt; j 且满足 (arr[0] + arr[1] + ... + arr[i] == arr[i + 1] + arr[i + 2] + ... + arr[j - 1] == arr[j] + arr[j + 1] + ... + arr[arr.length - 1]) 就可以将数组三等分。\n示例 1：\n1 2 3 输入：arr = [0,2,1,-6,6,-7,9,1,2,0,1] 输出：true 解释：0 + 2 + 1 = -6 + 6 - 7 + 9 + 1 = 2 + 0 + 1 示例 2：\n1 2 输入：arr = [0,2,1,-6,6,7,9,-1,2,0,1] 输出：false 示例 3：\n1 2 3 输入：arr = [3,3,6,5,-2,2,5,1,-9,4] 输出：true 解释：3 + 3 = 6 = 5 - 2 + 2 + 5 + 1 - 9 + 4 提示：\n3 \u0026lt;= arr.length \u0026lt;= 5 * 104 -104 \u0026lt;= arr[i] \u0026lt;= 104 解法\n解法一：暴力法\n题目要求求出是否存在三个相等的划分。显而易见，我们可以暴力。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Solution { public boolean canThreePartsEqualSum(int[] arr) { // 先放第一个挡板 for(int i=0;i\u0026lt;arr.length-2;i++){ // 继续放第二个挡板 for(int j=i+1;j\u0026lt;arr.length-1;j++){ int firstPart = sum(0,i,arr); // System.out.println(firstPart+\u0026#34;-\u0026#34;+sum(i+1,j,arr)+\u0026#34;-\u0026#34;+sum(j+1,arr.length-1,arr)); if(firstPart==sum(i+1,j,arr)){ if(firstPart==sum(j+1,arr.length-1,arr)){ return true; } } } } return false; } public int sum(int l,int r,int[] arr){ int sum=0; for(;l\u0026lt;=r;l++){ sum+=arr[l]; } return sum; } } 遗憾的是，暴力算法的时间复杂度为O(n^3^)。问题出现在，每次求和都需要重新计算和。可以使用前缀和的方式优化一下算法\n解法二：前缀和优化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public boolean canThreePartsEqualSum(int[] arr) { int sum = 0; for(int i=0;i\u0026lt;arr.length;i++){ sum+=arr[i]; } int firstSum = 0; int secondSum = 0; // 先放第一个挡板 for(int i=0;i\u0026lt;arr.length-2;i++){ firstSum +=arr[i]; // 继续放第二个挡板 for(int j=i+1;j\u0026lt;arr.length-1;j++){ secondSum += arr[j]; // System.out.println(firstSum+\u0026#34;-\u0026#34;+secondSum+\u0026#34;-\u0026#34;+(sum-firstSum-secondSum)); if(firstSum==secondSum){ if(firstSum==sum-firstSum-secondSum){ return true; } } } secondSum=0; } return false; } } 解法三：贪心\n发现数学规律，寻找切分点\nday84 392. 判断子序列 题目\n给定字符串 s 和 t ，判断 s 是否为 t 的子序列。\n字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，\u0026quot;ace\u0026quot;是\u0026quot;abcde\u0026quot;的一个子序列，而\u0026quot;aec\u0026quot;不是）。\n进阶：\n如果有大量输入的 S，称作 S1, S2, \u0026hellip; , Sk 其中 k \u0026gt;= 10亿，你需要依次检查它们是否为 T 的子序列。在这种情况下，你会怎样改变代码？\n致谢：\n特别感谢 @pbrother 添加此问题并且创建所有测试用例。\n示例 1：\n1 2 输入：s = \u0026#34;abc\u0026#34;, t = \u0026#34;ahbgdc\u0026#34; 输出：true 示例 2：\n1 2 输入：s = \u0026#34;axc\u0026#34;, t = \u0026#34;ahbgdc\u0026#34; 输出：false 提示：\n0 \u0026lt;= s.length \u0026lt;= 100 0 \u0026lt;= t.length \u0026lt;= 10^4 两个字符串都只由小写字符组成。 解法\n解法一：双指针\nhttps://leetcode.cn/problems/is-subsequence/solution/pan-duan-zi-xu-lie-by-leetcode-solution/\n解法二：动态规划\nhttps://leetcode.cn/problems/is-subsequence/solution/shi-pin-jiang-jie-dong-tai-gui-hua-qiu-jie-is-subs/\n比官方题解稍微好理解一点\nday85 62. 不同路径 题目\n一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。\n机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。\n问总共有多少条不同的路径？\n示例 1：\n1 2 输入：m = 3, n = 7 输出：28 示例 2：\n1 2 3 4 5 6 7 输入：m = 3, n = 2 输出：3 解释： 从左上角开始，总共有 3 条路径可以到达右下角。 1. 向右 -\u0026gt; 向下 -\u0026gt; 向下 2. 向下 -\u0026gt; 向下 -\u0026gt; 向右 3. 向下 -\u0026gt; 向右 -\u0026gt; 向下 示例 3：\n1 2 输入：m = 7, n = 3 输出：28 示例 4：\n1 2 输入：m = 3, n = 3 输出：6 提示：\n1 \u0026lt;= m, n \u0026lt;= 100 题目数据保证答案小于等于 2 * 109 解法\n动态规划/组合数学\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public int uniquePaths(int m, int n) { // 定义状态：dp[i][j]表示移动到i行j列位置有多少条路径 long[][] dp = new long[m][n]; // 初始状态：第一行和第一列都只有一条路径 for(int i=0;i\u0026lt;m;i++){ dp[i][0] = 1; } for(int i=0;i\u0026lt;n;i++){ dp[0][i] = 1; } // 状态转移方程：dp[i][j]显然是由dp[i-1][j]或dp[i][j-1]转移而来，那么结果应该取二者之和 for(int i=1;i\u0026lt;m;i++){ for(int j=1;j\u0026lt;n;j++){ dp[i][j] = dp[i-1][j]+dp[i][j-1]; } } return (int)dp[m-1][n-1]; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func uniquePaths(m int, n int) int { dp := make([][]int,m) for i:=0;i\u0026lt;m;i++{ dp[i] = make([]int,n) dp[i][0] = 1 } for j:=0;j\u0026lt;n;j++{ dp[0][j] = 1 } // 状态转移 for i:=1;i\u0026lt;m;i++{ for j:=1;j\u0026lt;n;j++{ dp[i][j] = dp[i-1][j]+dp[i][j-1] } } return dp[m-1][n-1] } day86 63. 不同路径 II 题目\n一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。\n机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish”）。\n现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？\n网格中的障碍物和空位置分别用 1 和 0 来表示。\n示例 1：\n1 2 3 4 5 6 输入：obstacleGrid = [[0,0,0],[0,1,0],[0,0,0]] 输出：2 解释：3x3 网格的正中间有一个障碍物。 从左上角到右下角一共有 2 条不同的路径： 1. 向右 -\u0026gt; 向右 -\u0026gt; 向下 -\u0026gt; 向下 2. 向下 -\u0026gt; 向下 -\u0026gt; 向右 -\u0026gt; 向右 示例 2：\n1 2 输入：obstacleGrid = [[0,1],[0,0]] 输出：1 提示：\nm == obstacleGrid.length n == obstacleGrid[i].length 1 \u0026lt;= m, n \u0026lt;= 100 obstacleGrid[i][j] 为 0 或 1 解法\n解法一：动态规划\n思路和上一道题一样，都满足动态规划的几个条件\n官方动态规划题解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Solution { public int uniquePathsWithObstacles(int[][] obstacleGrid) { int rowNum = obstacleGrid.length; int spanNum = obstacleGrid[0].length; int[][] dp = new int[rowNum][spanNum]; // 初始化dp for(int i=0;i\u0026lt;rowNum;i++){ if(obstacleGrid[i][0]==1){ break; } dp[i][0] = 1; } for(int i=0;i\u0026lt;spanNum;i++){ if(obstacleGrid[0][i]==1){ break; } dp[0][i] = 1; } // System.out.println(rowNum+\u0026#34;-\u0026#34;+spanNum); // 状态转移 for(int i=1;i\u0026lt;rowNum;i++){ for(int j=1;j\u0026lt;spanNum;j++){ if(obstacleGrid[i][j]==0){ dp[i][j]=dp[i-1][j]+dp[i][j-1]; // System.out.println(dp[i][j]+\u0026#34;=\u0026#34;+dp[i-1][j]+\u0026#34;+\u0026#34;+dp[i][j-1]); } } } //最后结果 return dp[rowNum-1][spanNum-1]; } } day87 64. 最小路径和 题目\n给定一个包含非负整数的 *m* x *n* 网格 grid ，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。\n**说明：**每次只能向下或者向右移动一步。\n示例 1：\n1 2 3 输入：grid = [[1,3,1],[1,5,1],[4,2,1]] 输出：7 解释：因为路径 1→3→1→1→1 的总和最小。 示例 2：\n1 2 输入：grid = [[1,2,3],[4,5,6]] 输出：12 提示：\nm == grid.length n == grid[i].length 1 \u0026lt;= m, n \u0026lt;= 200 0 \u0026lt;= grid[i][j] \u0026lt;= 100 解法\n解法一：动态规划\nhttps://leetcode.cn/problems/minimum-path-sum/solution/zui-xiao-lu-jing-he-by-leetcode-solution/\n这题和不同的路径的状态转移方程有点类似\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func minPathSum(grid [][]int) int { m:=len(grid) n:=len(grid[0]) dp := make([][]int,m) s:=0 for i:=0;i\u0026lt;m;i++{ dp[i] = make([]int,n) dp[i][0] = s + grid[i][0] s = dp[i][0] } s=0 for j:=0;j\u0026lt;n;j++{ dp[0][j] =s + grid[0][j] s = dp[0][j] } for i:=1;i\u0026lt;m;i++{ for j:=1;j\u0026lt;n;j++{ if dp[i-1][j]\u0026gt;dp[i][j-1]{ dp[i][j] = dp[i][j-1]+grid[i][j] }else{ dp[i][j] = dp[i-1][j]+grid[i][j] } } } // fmt.Println(dp) return dp[m-1][n-1] } day88 70. 爬楼梯 题目\n假设你正在爬楼梯。需要 n 阶你才能到达楼顶。\n每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？\n示例 1：\n1 2 3 4 5 输入：n = 2 输出：2 解释：有两种方法可以爬到楼顶。 1. 1 阶 + 1 阶 2. 2 阶 示例 2：\n1 2 3 4 5 6 输入：n = 3 输出：3 解释：有三种方法可以爬到楼顶。 1. 1 阶 + 1 阶 + 1 阶 2. 1 阶 + 2 阶 3. 2 阶 + 1 阶 提示：\n1 \u0026lt;= n \u0026lt;= 45 解法\n官方题解：动态规划/矩阵快速幂/通项公式/\ndp应该是最简单的\nhttps://leetcode.cn/problems/climbing-stairs/solution/pa-lou-ti-by-leetcode-solution/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public int climbStairs(int n) { if(n\u0026lt;=2) return n; // 状态表示：dp[i]表示爬第i层楼梯的方法总数 int[] dp = new int[n+1]; // 状态初始化 //dp[0] = 1;// 没有意义，为了凑数,也可以置为1；也可以当作非法状态看待 dp[1] = 1;// 当爬第一层时，只有一种方法：爬一个台阶 // 状态转移方程：dp[i]是由dp[i-1]或dp[i-2]转移过来，那么显然方法总数就是前两者之和; dp[2] = 2; for(int i=3;i\u0026lt;=n;i++){ dp[i] = dp[i-1] + dp[i-2]; } return dp[n]; } } 还可以将这个问题看成是“物品的重量分别是1和2，背包的容量是n”的完全背包问题。\n二维DP解法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public int climbStairs(int n) { if(n\u0026lt;=2) return n; // 状态表示：dp[i][j]表示取前i物品，装下容量j的方法数，注意此处要排列数 int[][] dp = new int[3][n+1]; int[] w = new int[]{1,2}; // 状态初始化：取前i个物品装容量为0，只有一种方法，那就是什么都不取 dp[0][0]=1; // 状态转移 // 二维dp的顺序无所谓 for(int i=1;i\u0026lt;=2;i++){ for(int j=0;j\u0026lt;=n;j++){ if(j\u0026gt;=w[i-1]) { // 每次可以放的时候，都是取前0-i个数的和，这样才是排列数。 // 可以画出dp转移表验证一下。 //（其实这是画表时候找到的规律。感觉是一种数学定律） for(int k=0;k\u0026lt;i;k++){ dp[i][j] += dp[i][j-w[k]]; } } else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[2][n]; } } 一维dp解法，这里的一维dp不再是优化二维得来的了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public int climbStairs(int n) { if(n\u0026lt;=2) return n; // 状态表示：dp[j]表示装下容量j的方法数，注意此处要排列数 int[] dp = new int[n+1]; int[] w = new int[]{1,2}; // 状态初始化：装容量为0，只有一种方法，那就是什么都不取 dp[0]=1; // 状态转移 for(int j=0;j\u0026lt;=n;j++){ for(int i=1;i\u0026lt;=2;i++){ if(j\u0026gt;=w[i-1]) { dp[j] += dp[j-w[i-1]]; } } } // System.out.println(Arrays.toString(dp)); return dp[n]; } } day89 72. 编辑距离 题目\n给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数 。\n你可以对一个单词进行如下三种操作：\n插入一个字符 删除一个字符 替换一个字符 示例 1：\n1 2 3 4 5 6 输入：word1 = \u0026#34;horse\u0026#34;, word2 = \u0026#34;ros\u0026#34; 输出：3 解释： horse -\u0026gt; rorse (将 \u0026#39;h\u0026#39; 替换为 \u0026#39;r\u0026#39;) rorse -\u0026gt; rose (删除 \u0026#39;r\u0026#39;) rose -\u0026gt; ros (删除 \u0026#39;e\u0026#39;) 示例 2：\n1 2 3 4 5 6 7 8 输入：word1 = \u0026#34;intention\u0026#34;, word2 = \u0026#34;execution\u0026#34; 输出：5 解释： intention -\u0026gt; inention (删除 \u0026#39;t\u0026#39;) inention -\u0026gt; enention (将 \u0026#39;i\u0026#39; 替换为 \u0026#39;e\u0026#39;) enention -\u0026gt; exention (将 \u0026#39;n\u0026#39; 替换为 \u0026#39;x\u0026#39;) exention -\u0026gt; exection (将 \u0026#39;n\u0026#39; 替换为 \u0026#39;c\u0026#39;) exection -\u0026gt; execution (插入 \u0026#39;u\u0026#39;) 提示：\n0 \u0026lt;= word1.length, word2.length \u0026lt;= 500 word1 和 word2 由小写英文字母组成 解法\n方法一：动态规划找编辑距离\n好理解的图表解法\n官方题解\nday90 分解质因数 题目\n问题描述\n求出区间[a,b]中所有整数的质因数分解。\n输入格式\n输入两个整数a，b。\n输出格式\n每行输出一个数的分解，形如k=a1a2a3\u0026hellip;(a1\u0026lt;=a2\u0026lt;=a3\u0026hellip;，k也是从小到大的)(具体可看样例)\n样例输入\n1 3 10 样例输出\n1 2 3 4 5 6 7 8 3=3 4=2*2 5=5 6=2*3 7=7 8=2*2*2 9=3*3 10=2*5 提示\n先筛出所有素数，然后再分解。\n数据规模和约定\n2\u0026lt;=a\u0026lt;=b\u0026lt;=10000\n解法\n思路：这道题的暴力解法思路显而易见：对区间内的每一个数都进行质因数分解。但是这样做显然做了很多的重复计算，我们可以利用空间换时间的原理，将第一次计算出来的结果保存好，这样第二次再计算时，直接拿出来用即可，这样就避免了大量的重复计算\n代码演示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import java.util.*; public class Main{ public static void main(String args[]){ Scanner sc = new Scanner(System.in); int a = sc.nextInt(); int b = sc.nextInt(); String[] dict = new String[b+1]; dict[2] = \u0026#34;2*\u0026#34;; dict[3] = \u0026#34;3*\u0026#34;; //\tSystem.out.println(\u0026#34;2=2\u0026#34;); for(int i=2;i\u0026lt;=b;i++) { int tmp = i; boolean flag = true; // 算质因数 for(int n=2;n\u0026lt;i\u0026amp;\u0026amp;tmp\u0026gt;=1;n++) { // 找到的第一个最小因子n，就是质因数。再拆分剩余因子 if(tmp%n==0) { flag = false; // 将第一个因子拼凑在tmp位置上 if(dict[n]!=null) { dict[tmp]=dict[n]; }else { dict[tmp] = n+\u0026#34;*\u0026#34;; } // 剩余因子的记忆化搜索，降低时间复杂度 if(dict[tmp/n]!=null) { dict[tmp]=dict[tmp]+dict[tmp/n]; break; } // 迭代剩余因子的分解，需要重新开始 tmp = tmp/n; n=2; } } // 如果i为素数，那么i就没有因子，所以需要直接初始化dict if(flag) { dict[i] = i+\u0026#34;*\u0026#34;; } if(i\u0026gt;=a) { if(dict[i].length()\u0026gt;1\u0026amp;\u0026amp;dict[i].charAt(dict[i].length()-1)==\u0026#39;*\u0026#39;) { System.out.println(i+\u0026#34;=\u0026#34;+dict[i].substring(0, dict[i].length()-1)); }else System.out.println(i+\u0026#34;=\u0026#34;+dict[i]); } } //System.out.print(Arrays.toString(dict)); sc.close(); } day91 最大分解 题目\n问题描述\n给出一个正整数n，求一个和最大的序列a0，a1，a2，……，ap，满足n=a0\u0026gt;a1\u0026gt;a2\u0026gt;……\u0026gt;ap且ai+1是ai的约数，输出a1+a2+……+ap的最大值\n输入格式\n输入仅一行，包含一个正整数n\n输出格式\n一个正整数，表示最大的序列和，即a1+a2+……+ap的最大值\n样例输入\n10\n样例输出\n6\n数据规模和约定\n1\u0026lt;n\u0026lt;=10^6\n样例说明\np=2 a0=10，a1=5，a2=1，6=5+1\n解法\n主要采用dfs+贪心优化\n思路：这个题目本身就是不断得分解得到约数，显而易见应该使用递归编程求解；我们再思考题目：题目的意思是让我们找出约数序列和得最大值，可以验证这种贪心策略：每次都取最大约数，这样得到的结果总是当前最优，这样每一步的最优解的和自然是最终的最大值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner sc =new Scanner(System.in); System.out.print(dfs(sc.nextInt())); sc.close(); } // dfs+贪心优化 public static int dfs(int num) { // 判断当前数是否是素数，是的话就停止当前分支递归 if(num\u0026lt;=3) return 1; boolean flag = false; for(int i=2;i\u0026lt;num;i++) { if(num%i==0) { flag =true; // 找到第一个约数i，也就是最小得那个约数i;就去递归最大得那个约数 // System.out.println(num+\u0026#34;|\u0026#34;+num/i+\u0026#34;|\u0026#34;+i); return dfs(num/i)+num/i; } } // 素数 if(!flag) { return 1; } return 1; } } day92 46. 全排列 题目\n给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。\n示例 1：\n1 2 输入：nums = [1,2,3] 输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] 示例 2：\n1 2 输入：nums = [0,1] 输出：[[0,1],[1,0]] 示例 3：\n1 2 输入：nums = [1] 输出：[[1]] 提示：\n1 \u0026lt;= nums.length \u0026lt;= 6 -10 \u0026lt;= nums[i] \u0026lt;= 10 nums 中的所有整数 互不相同 解法\n解法一：dfs+回溯\n这个问题可以看作有 n 个排列成一行的空格，我们需要从左往右依此填入题目给定的 n 个数，每个数只能使用一次。那么很直接的可以想到一种穷举的算法，即从左往右每一个位置都依此尝试填入一个数，看能不能填完这 n 个空格，在程序中我们可以用「回溯法」来模拟这个过程。\nDFS+回溯\n解法二：dfs+回溯+哈希去重\n全排列可以从【组合数】里筛选出来，利用哈希表去重。同时，需要注意深度拷贝的问题\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { HashSet\u0026lt;Integer\u0026gt; hm; public Solution(){ hm = new HashSet\u0026lt;\u0026gt;(); } public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls = new ArrayList\u0026lt;\u0026gt;(); dfs(lls,new ArrayList\u0026lt;\u0026gt;(),nums,nums.length,0); return lls; } public void dfs(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls,List\u0026lt;Integer\u0026gt; ls,int[] nums,int len,int step){ if(step==len){ // 核心关键: ls是引用类型,所以此处要深拷贝出来 lls.add(new ArrayList\u0026lt;\u0026gt;(ls)); return; } for(int i=0;i\u0026lt;len;i++){ if (hm.contains(nums[i])){ continue; } ls.add(nums[i]); hm.add(nums[i]); dfs(lls,ls,nums,len,step+1); hm.remove(nums[i]); ls.remove(ls.size()-1); } } } // 全排列不去重 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func permute(nums []int) [][]int { ans := [][]int{} used := make([]bool,len(nums)) var dfs func(nums []int,item []int,step int) dfs = func(nums []int,item []int,step int){ if step==len(nums){ newItem:=make([]int,len(item)) copy(newItem,item) ans = append(ans,newItem) return } for i:=0;i\u0026lt;len(nums);i++{ if used[i] { continue } item = append(item,nums[i]) used[i] = true dfs(nums,item,step+1) used[i] = false item = item[:len(item)-1] } } dfs(nums,[]int{},0) return ans } day93 马虎的算式 题目\n小明是个急性子，上小学的时候经常把老师写在黑板上的题目抄错了。有一次，老师出的题目是：36 x 495 = ? 他却给抄成了：396 x 45 = ?但结果却很戏剧性，他的答案竟然是对的！！因为 36 * 495 = 396 * 45 = 17820 类似这样的巧合情况可能还有很多，比如：27 * 594 = 297 * 54 假设 a b c d e 代表1~9不同的5个数字（注意是各不相同的数字，且不含0）能满足形如： ab * cde = adb * ce 这样的算式一共有多少种呢？请你利用计算机的优势寻找所有的可能，并回答不同算式的种类数。满足乘法交换律的算式计为不同的种类，所以答案肯定是个偶数。\n解法\n解法一：暴力枚举\n题目的意思就是在1~9里找到五个不同的数字组合，满足题目的要求。直接五个for循环嵌套解决\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class Main { public static void main(String[] args){ int a,b,c,d,e; int count=0;//记录满足条件的个数 for(a=1;a\u0026lt;=9;a++) for(b=1;b\u0026lt;=9;b++) for(c=1;c\u0026lt;=9;c++) for(d=1;d\u0026lt;=9;d++) for(e=1;e\u0026lt;=9;e++){//abcde代表1-9各不同的5个数字\tif(a!=b\u0026amp;\u0026amp;a!=c\u0026amp;\u0026amp;a!=d\u0026amp;\u0026amp;a!=e\u0026amp;\u0026amp;b!=c\u0026amp;\u0026amp;b!=d\u0026amp;\u0026amp;b!=e \u0026amp;\u0026amp;c!=d\u0026amp;\u0026amp;c!=e\u0026amp;\u0026amp;d!=e){ int sum1=(a*10+b)*(c*100+d*10+e); int sum2=(a*100+d*10+b)*(c*10+e); if(sum1==sum2) { count++; } } } System.out.println(count); } } 解法二：优雅的DFS+回溯\n我们可以通过dfs搜索解决。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import java.util.Arrays; public class Main { static int[] nums = {1,2,3,4,5,6,7,8,9}; static boolean[] vs = new boolean[9]; static int count = 0; public static void main(String[] args) { dfs(new int[5],0); System.out.print(count); } // DFS枚举: 找出五个数的所有组合 public static void dfs(int[] zuhe,int step) { if(step\u0026gt;4) { // 处理五个数 int a = zuhe[0]*10+zuhe[1]; int b = zuhe[2]*100+zuhe[3]*10+zuhe[4]; int c = zuhe[0]*100+zuhe[3]*10+zuhe[1]; int d = zuhe[2]*10+zuhe[4]; if(a*b==c*d) { // System.out.println(Arrays.toString(zuhe)); count++; } return; } for(int i=0;i\u0026lt;nums.length;i++) { // 取出没有被拿的数，保证每位数字都不一样 if(!vs[i]) { vs[i]=true; zuhe[step]=nums[i]; dfs(zuhe,step+1); vs[i]=false;// 回溯 zuhe[step]=0; // 回溯 } }\t} } day94 振兴中华 题目\n小明参加了学校的趣味运动会，其中的一个项目是：跳格子。地上画着一些格子，每个格子里写一个字，如下所示：（也可参见p1.jpg）\n从我做起振 我做起振兴 做起振兴中 起振兴中华\n比赛时，先站在左上角的写着“从”字的格子里，可以横向或纵向跳到相邻的格子里，但不能跳到对角的格子或其它位置。一直要跳到“华”字结束。 要求跳过的路线刚好构成“从我做起振兴中华”这句话。 请你帮助小明算一算他一共有多少种可能的跳跃路线呢？ 解法\n解法一：DFS\n思路：只要每次都是往右或往左走，就一定满足题目要求。所以，就是求这样走的路径有多少。显然，可以通过DFS直接模拟这个过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class Main { static String[][] matrix = { {\u0026#34;从\u0026#34;,\u0026#34;我\u0026#34;,\u0026#34;做,\u0026#34;,\u0026#34;起\u0026#34;,\u0026#34;振\u0026#34;}, {\u0026#34;我\u0026#34;,\u0026#34;做,\u0026#34;,\u0026#34;起\u0026#34;,\u0026#34;振\u0026#34;,\u0026#34;兴\u0026#34;}, {\u0026#34;做,\u0026#34;,\u0026#34;起\u0026#34;,\u0026#34;振\u0026#34;,\u0026#34;兴\u0026#34;,\u0026#34;中\u0026#34;}, {\u0026#34;起\u0026#34;,\u0026#34;振\u0026#34;,\u0026#34;兴\u0026#34;,\u0026#34;中\u0026#34;,\u0026#34;华\u0026#34;} }; public static void main(String[] args) { System.out.print(dfs(0,0)); } public static int dfs(int i,int j) { if(i==3||j==4) { return 1; } // 下一步走,且回溯前面往下走的步数，换一种往右走 return dfs(i+1,j)+dfs(i,j+1); } } day95 123. 买卖股票的最佳时机 III 题目\n给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。\n设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。\n**注意：**你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。\n示例 1:\n1 2 3 4 输入：prices = [3,3,5,0,0,3,1,4] 输出：6 解释：在第 4 天（股票价格 = 0）的时候买入，在第 6 天（股票价格 = 3）的时候卖出，这笔交易所能获得利润 = 3-0 = 3 。 随后，在第 7 天（股票价格 = 1）的时候买入，在第 8 天 （股票价格 = 4）的时候卖出，这笔交易所能获得利润 = 4-1 = 3 。 示例 2：\n1 2 3 4 5 输入：prices = [1,2,3,4,5] 输出：4 解释：在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。 因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3：\n1 2 3 输入：prices = [7,6,4,3,1] 输出：0 解释：在这个情况下, 没有交易完成, 所以最大利润为 0。 示例 4：\n1 2 输入：prices = [1] 输出：0 提示：\n1 \u0026lt;= prices.length \u0026lt;= 105 0 \u0026lt;= prices[i] \u0026lt;= 105 解法\n动态规划\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public int maxProfit(int[] prices) { // 状态定义：dp[i][0]表示没有操作 // dp[i][1]表示前i天第一次持有股票的最大利润 // dp[i][2]表示前i天第一次卖出股票的最大利润 // dp[i][3]表示前i天第二次持有股票的最大利润 // dp[i][4]表示前i天第二次卖出股票的最大利润 int[][] dp = new int[prices.length+1][5]; // 初始化 dp[0][0] = 0; dp[0][2] =0; dp[0][1] = -prices[0]; dp[0][3] = -prices[0]; for(int i=1;i\u0026lt;=prices.length;i++){ dp[i][1] = Math.max(dp[i-1][0]-prices[i-1],dp[i-1][1]); dp[i][2] = Math.max(dp[i-1][1]+prices[i-1],dp[i-1][2]); dp[i][3] = Math.max(dp[i-1][2]-prices[i-1],dp[i-1][3]); dp[i][4] = Math.max(dp[i-1][3]+prices[i-1],dp[i-1][4]); } return dp[prices.length][4]; } } day96 152. 乘积最大子数组 题目\n给你一个整数数组 nums ，请你找出数组中乘积最大的非空连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。\n测试用例的答案是一个 32-位 整数。\n子数组 是数组的连续子序列。\n示例 1:\n1 2 3 输入: nums = [2,3,-2,4] 输出: 6 解释: 子数组 [2,3] 有最大乘积 6。 示例 2:\n1 2 3 输入: nums = [-2,0,-1] 输出: 0 解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 提示:\n1 \u0026lt;= nums.length \u0026lt;= 2 * 104 -10 \u0026lt;= nums[i] \u0026lt;= 10 nums 的任何前缀或后缀的乘积都 保证 是一个 32-位 整数 解法\n方法一：数学规律\n依题意不难看出：我们可以顺序连乘，直到遇到0时，我们需要重新连乘。每次连乘都要找到最大的乘积。我们需要正向和反向两个操作才能把所有组合全部遍历一遍。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public int maxProduct(int[] nums) { int ans = 1; int max = Integer.MIN_VALUE; if(nums.length==1) return nums[0]; // 正向求最大 for(int i=0;i\u0026lt;nums.length;i++){ if(nums[i]==0){ ans=1; continue; } ans*=nums[i]; if(ans\u0026gt;max){ max = ans; } } // 反向求最大 ans = 1; for(int i=nums.length-1;i\u0026gt;=0;i--){ if(nums[i]==0){ ans=1; continue; } ans*=nums[i]; if(ans\u0026gt;max){ max = ans; } } return max\u0026gt;0?max:0; } } 方法二：动态规划\nday97 416. 分割等和子集 题目\n给你一个 只包含正整数 的 非空 数组 nums 。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。\n示例 1：\n1 2 3 输入：nums = [1,5,11,5] 输出：true 解释：数组可以分割成 [1, 5, 5] 和 [11] 。 示例 2：\n1 2 3 输入：nums = [1,2,3,5] 输出：false 解释：数组不能分割成两个元素和相等的子集。 提示：\n1 \u0026lt;= nums.length \u0026lt;= 200 1 \u0026lt;= nums[i] \u0026lt;= 100 解法\n每个元素只可以被选择1次，就是01背包问题 背包容量为sum/2，也就是在nums里挑选出刚好装满背包可能性\n01背包的应用\n二维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public boolean canPartition(int[] nums) { if(nums.length\u0026lt;=1) return false; int sum = 0; for(int v:nums) sum+=v; if(sum%2!=0) return false; int bag = sum/2;// 背包容量 // 定义状态：dp[i][j]表示放入前i个物品时，是否可以凑出容量j boolean[][] dp = new boolean[nums.length+1][bag+1]; // 初始化 dp[0][0] = true; for(int i=1;i\u0026lt;=nums.length;i++){ for(int j=0;j\u0026lt;=bag;j++){ if(j\u0026gt;=nums[i-1]) dp[i][j] = dp[i-1][j]||dp[i-1][j-nums[i-1]]; else dp[i][j] = dp[i-1][j]; } } return dp[nums.length][bag]; } } 滚动数组优化二维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public boolean canPartition(int[] nums) { if(nums.length\u0026lt;=1) return false; int sum = 0; for(int v:nums) sum+=v; if(sum%2!=0) return false; int bag = sum/2;// 背包容量 // 定义状态：dp[j]表示是否可以凑出容量j boolean[] dp = new boolean[bag+1]; // 初始化 dp[0] = true; // 注意一维dp为了避免状态转移过程中状态的覆盖。需要先遍历物品再遍历背包； // 而且由于物品只放入一次，所以背包需要倒序遍历，否则就会放入多次物品，那这就不是01背包了，而是完全背包 for(int i=1;i\u0026lt;=nums.length;i++){ for(int j=bag;j\u0026gt;=0;j--){ if(j\u0026gt;=nums[i-1]) dp[j] = dp[j]||dp[j-nums[i-1]]; } } return dp[bag]; } } 背包倒序遍历的解释：\n需要倒序遍历背包容量。如果是顺序放入，那么根据dp转移方程： 第j个状态可能是由前面的第j-w[i]个状态转移过来。 如果顺序遍历，那么就会先算出前面的值，算出前面的值过后，后面dp转移的时候又可能会用到， 那么也就是说，一个物品被放入了多次。这就不是01背包问题了，而是完全背包\nday98 474. 一和零 题目\n给你一个二进制字符串数组 strs 和两个整数 m 和 n 。\n请你找出并返回 strs 的最大子集的长度，该子集中 最多 有 m 个 0 和 n 个 1 。\n如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。\n示例 1：\n1 2 3 4 输入：strs = [\u0026#34;10\u0026#34;, \u0026#34;0001\u0026#34;, \u0026#34;111001\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;0\u0026#34;], m = 5, n = 3 输出：4 解释：最多有 5 个 0 和 3 个 1 的最大子集是 {\u0026#34;10\u0026#34;,\u0026#34;0001\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;} ，因此答案是 4 。 其他满足题意但较小的子集包括 {\u0026#34;0001\u0026#34;,\u0026#34;1\u0026#34;} 和 {\u0026#34;10\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;} 。{\u0026#34;111001\u0026#34;} 不满足题意，因为它含 4 个 1 ，大于 n 的值 3 。 示例 2：\n1 2 3 输入：strs = [\u0026#34;10\u0026#34;, \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;], m = 1, n = 1 输出：2 解释：最大的子集是 {\u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;} ，所以答案是 2 。 提示：\n1 \u0026lt;= strs.length \u0026lt;= 600 1 \u0026lt;= strs[i].length \u0026lt;= 100 strs[i] 仅由 '0' 和 '1' 组成 1 \u0026lt;= m, n \u0026lt;= 100 解法\n解法一：01背包的变种\nhttps://leetcode.cn/problems/ones-and-zeroes/solution/yi-he-ling-by-leetcode-solution-u2z2/\n二维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Solution { public int findMaxForm(String[] strs, int m, int n) { // 统计1的个数 int[] ones = new int[strs.length]; int count = 0; for(int i=0;i\u0026lt;strs.length;i++){ for(int j=0;j\u0026lt;strs[i].length();j++){ if(strs[i].charAt(j)==\u0026#39;1\u0026#39;) count++; } ones[i] = count; count = 0; } // 定义dp: 表示选择的字符串个数 int[][][] dp = new int[strs.length+1][m+1][n+1]; // 初始化状态：当i==0时，表示取零个串，当然dp[0][][] = 0 // 状态转移: 完全就是01背包问题，只不过背包的维度增加了。 for(int i=1;i\u0026lt;=strs.length;i++){ for(int j=m;j\u0026gt;=0;j--){ for(int k=n;k\u0026gt;=0;k--){ int a = strs[i-1].length()-ones[i-1]; int b = ones[i-1]; if(j\u0026gt;=a\u0026amp;\u0026amp;k\u0026gt;=b){ dp[i][j][k] = Math.max(dp[i-1][j][k],dp[i-1][j-a][k-b]+1); }else{ dp[i][j][k] = dp[i-1][j][k]; } } } } return dp[strs.length][m][n]; } } 滚动数组优化为一维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public int findMaxForm(String[] strs, int m, int n) { // 统计1的个数 int[] ones = new int[strs.length]; int count = 0; for(int i=0;i\u0026lt;strs.length;i++){ for(int j=0;j\u0026lt;strs[i].length();j++){ if(strs[i].charAt(j)==\u0026#39;1\u0026#39;) count++; } ones[i] = count; count = 0; } // 定义dp: 表示选择的字符串个数 int[][] dp = new int[m+1][n+1]; // 初始化状态：为0 // 状态转移 for(int i=1;i\u0026lt;=strs.length;i++){ for(int j=m;j\u0026gt;=0;j--){// 倒序 for(int k=n;k\u0026gt;=0;k--){// 倒序 int a = strs[i-1].length()-ones[i-1]; int b = ones[i-1]; if(j\u0026gt;=a\u0026amp;\u0026amp;k\u0026gt;=b){ dp[j][k] = Math.max(dp[j][k],dp[j-a][k-b]+1); } } } } return dp[m][n]; } } day99 494. 目标和 题目\n给你一个整数数组 nums 和一个整数 target 。\n向数组中的每个整数前添加 '+' 或 '-' ，然后串联起所有整数，可以构造一个 表达式 ：\n例如，nums = [2, 1] ，可以在 2 之前添加 '+' ，在 1 之前添加 '-' ，然后串联起来得到表达式 \u0026quot;+2-1\u0026quot; 。 返回可以通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目。\n示例 1：\n1 2 3 4 5 6 7 8 输入：nums = [1,1,1,1,1], target = 3 输出：5 解释：一共有 5 种方法让最终目标和为 3 。 -1 + 1 + 1 + 1 + 1 = 3 +1 - 1 + 1 + 1 + 1 = 3 +1 + 1 - 1 + 1 + 1 = 3 +1 + 1 + 1 - 1 + 1 = 3 +1 + 1 + 1 + 1 - 1 = 3 示例 2：\n1 2 输入：nums = [1], target = 1 输出：1 提示：\n1 \u0026lt;= nums.length \u0026lt;= 20 0 \u0026lt;= nums[i] \u0026lt;= 1000 0 \u0026lt;= sum(nums[i]) \u0026lt;= 1000 -1000 \u0026lt;= target \u0026lt;= 1000 解法\n方法一：DFS\n暴力DFS搜索所有解空间即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public int findTargetSumWays(int[] nums, int target) { return dfs(nums,target,0,0); } public int dfs(int[] nums, int target,int i,int next){ if(i==nums.length){ if(next==target) return 1; return 0; } return dfs(nums,target,i+1,next+nums[i])+dfs(nums,target,i+1,next-nums[i]); } } 方法二：DP\nhttps://leetcode.cn/problems/target-sum/solution/mu-biao-he-by-leetcode-solution-o0cp/\n将nums分成两堆数字（s1,s2），使得这两堆数字的差值等于target s1-s2=target s1+s2=sum 2*s1 = sum+target s1 = (sum+target)/2 s2 = (sum-target)/2 也就是说需要分出一堆数字为和为s1或s2的堆 将问题转化为为了背包问题 这里的话还是选择填充容量为s2的背包 因为选择s1的背包的话，如果要求的target为负数的话，数组的索引初始化会溢出\n二维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { public int findTargetSumWays(int[] nums, int target) { if(nums.length\u0026lt;=1) return nums[0]==Math.abs(target)?1:0; int sum = 0; for(int v:nums) sum+=v; if((sum-target)%2!=0||target\u0026gt;sum) return 0; int bag = Math.abs((sum-target))/2; // 状态表示：dp[i][j]表示取第i个数后j容量背包的方法总数 int[][] dp = new int[nums.length+1][bag+1]; // 初始化 dp[0][0] = 1; // 状态转移 for(int i=1;i\u0026lt;=nums.length;i++){ for(int j=0;j\u0026lt;=bag;j++){ if(j\u0026gt;=nums[i-1]){ dp[i][j] = dp[i-1][j] + dp[i-1][j-nums[i-1]]; }else if(j\u0026lt;nums[i-1]){ // 放不下，方法数不变 dp[i][j] = dp[i-1][j]; } } } return dp[nums.length][bag]; } } 滚动数组优化为一维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public int findTargetSumWays(int[] nums, int target) { if(nums.length\u0026lt;=1) return nums[0]==Math.abs(target)?1:0; int sum = 0; for(int v:nums) sum+=v; if((sum-target)%2!=0||target\u0026gt;sum) return 0; int bag = Math.abs((sum-target))/2; // 状态表示：dp[i][j]表示取第i个数后j容量背包的方法总数 int[] dp = new int[bag+1]; // 初始化 dp[0] = 1; // 状态转移 for(int i=1;i\u0026lt;=nums.length;i++){ for(int j=bag;j\u0026gt;=0;j--){// 倒序避免状态的覆盖，从而造成01背包，避免完全背包 if(j\u0026gt;=nums[i-1]){ dp[j] = dp[j] + dp[j-nums[i-1]]; } } } return dp[bag]; } } day100 1049. 最后一块石头的重量 II 题目\n有一堆石头，用整数数组 stones 表示。其中 stones[i] 表示第 i 块石头的重量。\n每一回合，从中选出任意两块石头，然后将它们一起粉碎。假设石头的重量分别为 x 和 y，且 x \u0026lt;= y。那么粉碎的可能结果如下：\n如果 x == y，那么两块石头都会被完全粉碎； 如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。 最后，最多只会剩下一块 石头。返回此石头 最小的可能重量 。如果没有石头剩下，就返回 0。\n示例 1：\n1 2 3 4 5 6 7 输入：stones = [2,7,4,1,8,1] 输出：1 解释： 组合 2 和 4，得到 2，所以数组转化为 [2,7,1,8,1]， 组合 7 和 8，得到 1，所以数组转化为 [2,1,1,1]， 组合 2 和 1，得到 1，所以数组转化为 [1,1,1]， 组合 1 和 1，得到 0，所以数组转化为 [1]，这就是最优值。 示例 2：\n1 2 输入：stones = [31,26,33,21,40] 输出：5 提示：\n1 \u0026lt;= stones.length \u0026lt;= 30 1 \u0026lt;= stones[i] \u0026lt;= 100 解法\n方法一：动态规划\nhttps://leetcode.cn/problems/last-stone-weight-ii/solution/zui-hou-yi-kuai-shi-tou-de-zhong-liang-i-95p9/\n二维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public int lastStoneWeightII(int[] stones) { if(stones.length\u0026lt;=1) return stones[0]; int sum = 0; for(int v:stones) sum+=v; int bag = sum/2;// 直接向下取整 // 状态表示：dp[i][j]表示选取前i个石头，容量为j时，能放入的最大重量 int[][] dp = new int[stones.length][bag+1]; // 初始化 //dp[i][0] = 0; // dp[0][j] = stones[0]; 初始化i=0时的情况，装得下就可以取第0个石头 for(int j=0;j\u0026lt;=bag;j++){ if(j\u0026gt;=stones[0]){ dp[0][j] = stones[0]; } } for(int i=1;i\u0026lt;stones.length;i++){ for(int j=1;j\u0026lt;=bag;j++){ if(j\u0026gt;=stones[i]) dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-stones[i]]+stones[i]); else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return sum-2*dp[stones.length-1][bag]; } } // 将石头分成总重量最接近的两堆，这样碰撞后可以得到最小的堆 // 也就是要在一堆石头里找到可以最接近sum的half堆 滚动数组优化的DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public int lastStoneWeightII(int[] stones) { if(stones.length\u0026lt;=1) return stones[0]; int sum = 0; for(int v:stones) sum+=v; int bag = sum/2;// 直接向下取整 // 状态表示：dp[j]表示选取容量为j时，能放入的最大重量 int[] dp = new int[bag+1]; // 初始化 dp[0] = 0; // 初始化从0开始，因为第一行还没被填写 for(int i=0;i\u0026lt;stones.length;i++){ for(int j=bag;j\u0026gt;=stones[i];j--){// 倒序 if(j\u0026gt;=stones[i]) dp[j] = Math.max(dp[j],dp[j-stones[i]]+stones[i]); } } // System.out.println(Arrays.deepToString(dp)); return sum-2*dp[bag]; } } day101 139. 单词拆分 题目\n给你一个字符串 s 和一个字符串列表 wordDict 作为字典。请你判断是否可以利用字典中出现的单词拼接出 s 。\n**注意：**不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。\n示例 1：\n1 2 3 输入: s = \u0026#34;leetcode\u0026#34;, wordDict = [\u0026#34;leet\u0026#34;, \u0026#34;code\u0026#34;] 输出: true 解释: 返回 true 因为 \u0026#34;leetcode\u0026#34; 可以由 \u0026#34;leet\u0026#34; 和 \u0026#34;code\u0026#34; 拼接成。 示例 2：\n1 2 3 4 输入: s = \u0026#34;applepenapple\u0026#34;, wordDict = [\u0026#34;apple\u0026#34;, \u0026#34;pen\u0026#34;] 输出: true 解释: 返回 true 因为 \u0026#34;applepenapple\u0026#34; 可以由 \u0026#34;apple\u0026#34; \u0026#34;pen\u0026#34; \u0026#34;apple\u0026#34; 拼接成。 注意，你可以重复使用字典中的单词。 示例 3：\n1 2 输入: s = \u0026#34;catsandog\u0026#34;, wordDict = [\u0026#34;cats\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;sand\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;cat\u0026#34;] 输出: false 提示：\n1 \u0026lt;= s.length \u0026lt;= 300 1 \u0026lt;= wordDict.length \u0026lt;= 1000 1 \u0026lt;= wordDict[i].length \u0026lt;= 20 s 和 wordDict[i] 仅有小写英文字母组成 wordDict 中的所有字符串 互不相同 解法\n解法一：DP\nhttps://leetcode.cn/problems/word-break/solution/dan-ci-chai-fen-by-leetcode-solution/\nday102 279. 完全平方数 题目\n给你一个整数 n ，返回 和为 n 的完全平方数的最少数量 。\n完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。\n示例 1：\n1 2 3 输入：n = 12 输出：3 解释：12 = 4 + 4 + 4 示例 2：\n1 2 3 输入：n = 13 输出：2 解释：13 = 4 + 9 提示：\n1 \u0026lt;= n \u0026lt;= 104 解法\n解法一：DP\nhttps://leetcode.cn/problems/perfect-squares/solution/wan-quan-ping-fang-shu-by-leetcode-solut-t99c/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public int numSquares(int n) { // dp[i][j]表示选取前i个完全平方数，和为j时的所选完全平方数最少数量 int num = 1; for(;num\u0026lt;n;num++){ if(num*num\u0026gt;=n) break; } int[][] dp = new int[num+1][n+1]; // 初始化：毫无疑问，求得最小值，为了避免初始状态的覆盖，需要初始化为n+1，因为最多的情况就是n个1 for(int[] one:dp) Arrays.fill(one,n+1); // 开始 dp[0][0] = 0; for(int i=1;i\u0026lt;=num;i++){ for(int j=0;j\u0026lt;=n;j++){ if(j\u0026gt;=i*i) dp[i][j] = Math.min(dp[i-1][j],dp[i][j-i*i]+1); else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[num][n]; } } 优化DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public int numSquares(int n) { // dp[j]表示所选完全平方数和为j时的最少数量 int num = 1; for(;num\u0026lt;n;num++){ if(num*num\u0026gt;=n) break; } int[] dp = new int[n+1]; // 初始化：毫无疑问，求得最小值，为了避免初始状态的覆盖，需要初始化为n+1，因为最多的情况就是n个1 Arrays.fill(dp,n+1); // 开始 dp[0] = 0; for(int i=1;i\u0026lt;=num;i++){ for(int j=0;j\u0026lt;=n;j++){ if(j\u0026gt;=i*i) dp[j] = Math.min(dp[j],dp[j-i*i]+1); } } // System.out.println(Arrays.toString(dp)); return dp[n]; } } day103 322. 零钱兑换 题目\n给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。\n计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。\n你可以认为每种硬币的数量是无限的。\n示例 1：\n1 2 3 输入：coins = [1, 2, 5], amount = 11 输出：3 解释：11 = 5 + 5 + 1 示例 2：\n1 2 输入：coins = [2], amount = 3 输出：-1 示例 3：\n1 2 输入：coins = [1], amount = 0 输出：0 提示：\n1 \u0026lt;= coins.length \u0026lt;= 12 1 \u0026lt;= coins[i] \u0026lt;= 231 - 1 0 \u0026lt;= amount \u0026lt;= 104 解法\n方法一：记忆化搜索\n方法二：DP\nhttps://leetcode.cn/problems/coin-change/solution/322-ling-qian-dui-huan-by-leetcode-solution/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Solution { public int coinChange(int[] coins, int amount) { // 状态定义：dp[i][j]表示取前i种面额的硬币，容量为j，所需的最少硬币个数 int[][] dp = new int[coins.length + 1][amount + 1]; // 初始状态：因为状态方程是求最小值，所以这里需要初始化一个较大的数即可 // 不然结果全为0.当然初始化也有讲究，不能初始化一个特别大的数如Integer.MAX_VALUE， // 太大的数在状态转移过程中会溢出。我们知道硬币的面值最小是1，那么最多的硬币数量就是面值，所以初始化为面值+1 for (int[] ints : dp) { Arrays.fill(ints,amount+1); } // 注意因为状态转移取得是最小值，所以这里状态初始化，需要初始化为大于amount的数字才行。不然小了。就直接取那个小的数了。显然不正确，因为会被覆盖了 dp[0][0]=0; // 状态转移：如果当前硬币面额可以取出，既可以取也可以不取，取两种方式的最优解 for (int i = 1; i \u0026lt;= coins.length; i++) { for (int j = 0; j \u0026lt;= amount; j++) { if (coins[i - 1] \u0026lt;= j) dp[i][j] = Math.min(dp[i][j - coins[i - 1]] + 1, dp[i - 1][j]); else dp[i][j] = dp[i - 1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[coins.length][amount]\u0026gt;amount?-1:dp[coins.length][amount]; } } 优化DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int coinChange(int[] coins, int amount) { // dp[j]表示凑出金额j的最少硬币个数 int[] dp = new int[amount+1]; // 初始化 Arrays.fill(dp,amount+1); dp[0]=0; // 状态转移 for(int i=1;i\u0026lt;=coins.length;i++){ for(int j=0;j\u0026lt;=amount;j++){ if(j\u0026gt;=coins[i-1]) dp[j] = Math.min(dp[j],dp[j-coins[i-1]]+1); } } // System.out.println(Arrays.deepToString(dp)); return dp[amount]\u0026gt;amount?-1:dp[amount]; } } day104 377. 组合总和 Ⅳ 题目\n给你一个由 不同 整数组成的数组 nums ，和一个目标整数 target 。请你从 nums 中找出并返回总和为 target 的元素组合的个数。\n题目数据保证答案符合 32 位整数范围。\n示例 1：\n1 2 3 4 5 6 7 8 9 10 11 12 输入：nums = [1,2,3], target = 4 输出：7 解释： 所有可能的组合为： (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) 请注意，顺序不同的序列被视作不同的组合。 示例 2：\n1 2 输入：nums = [9], target = 3 输出：0 提示：\n1 \u0026lt;= nums.length \u0026lt;= 200 1 \u0026lt;= nums[i] \u0026lt;= 1000 nums 中的所有元素 互不相同 1 \u0026lt;= target \u0026lt;= 1000 **进阶：**如果给定的数组中含有负数会发生什么？问题会产生何种变化？如果允许负数出现，需要向题目中添加哪些限制条件？\n解法\n方法一：记忆化搜索\n方法二：DP\nhttps://leetcode.cn/problems/combination-sum-iv/solution/zu-he-zong-he-iv-by-leetcode-solution-q8zv/\nday105 509. 斐波那契数 斐波那契数 （通常用 F(n) 表示）形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是：\n1 2 F(0) = 0，F(1) = 1 F(n) = F(n - 1) + F(n - 2)，其中 n \u0026gt; 1 给定 n ，请计算 F(n) 。\n示例 1：\n1 2 3 输入：n = 2 输出：1 解释：F(2) = F(1) + F(0) = 1 + 0 = 1 示例 2：\n1 2 3 输入：n = 3 输出：2 解释：F(3) = F(2) + F(1) = 1 + 1 = 2 示例 3：\n1 2 3 输入：n = 4 输出：3 解释：F(4) = F(3) + F(2) = 2 + 1 = 3 提示：\n0 \u0026lt;= n \u0026lt;= 30 解法\n方法一：DFS暴搜\n斐波那契数的定义很符合递归函数，因此比较容易写出递归函数写法\n1 2 3 4 5 6 class Solution { public int fib(int n) { if(n\u0026lt;=1) return n; return fib(n-1)+fib(n-2); } } 方法二：DFS+记忆化搜索\n方法一还可以改进一下。我们很容易知道：斐波那契数是不断往两个分支递归，这样会出现一些数字的重复计算。我们可以在递归的过程中将这些数据记录下来，下次要用时，直接拿出来用即可，避免多余的递归。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public int fib(int n) { int[] history = new int[n]; return dfs(n,history,0); } public int dfs(int n,int[] history,int i){ if(n\u0026lt;=1) return n; // 有记录，则直接拿 if(history[i]!=0) return history[i]; // 没有记录，需要计算 else { history[i] = fib(n-1)+fib(n-2); return history[i]; } } } 方法三：DP\n这道题也容易想到动态规划的解法，因为题目的定义看着就很动态规划，状态转移方程已经很显然地给出了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public int fib(int n) { if(n\u0026lt;=1) return n; // 状态表示：dp[i]表示第i个斐波那契数 int[] dp = new int[n+1]; // 初始化dp dp[0] = 0; dp[1] = 1; for(int i=2;i\u0026lt;=n;i++){ dp[i] = dp[i-1] + dp[i-2]; } return dp[n]; } } // 由于当前状态只与前两个状态有关，所以可以利用滚动数组地思想优化为常数空间复杂度 day106 746. 使用最小花费爬楼梯 题目\n给你一个整数数组 cost ，其中 cost[i] 是从楼梯第 i 个台阶向上爬需要支付的费用。一旦你支付此费用，即可选择向上爬一个或者两个台阶。\n你可以选择从下标为 0 或下标为 1 的台阶开始爬楼梯。\n请你计算并返回达到楼梯顶部的最低花费。\n示例 1：\n1 2 3 4 5 输入：cost = [10,15,20] 输出：15 解释：你将从下标为 1 的台阶开始。 - 支付 15 ，向上爬两个台阶，到达楼梯顶部。 总花费为 15 。 示例 2：\n1 2 3 4 5 6 7 8 9 10 输入：cost = [1,100,1,1,1,100,1,1,100,1] 输出：6 解释：你将从下标为 0 的台阶开始。 - 支付 1 ，向上爬两个台阶，到达下标为 2 的台阶。 - 支付 1 ，向上爬两个台阶，到达下标为 4 的台阶。 - 支付 1 ，向上爬两个台阶，到达下标为 6 的台阶。 - 支付 1 ，向上爬一个台阶，到达下标为 7 的台阶。 - 支付 1 ，向上爬两个台阶，到达下标为 9 的台阶。 - 支付 1 ，向上爬一个台阶，到达楼梯顶部。 总花费为 6 。 提示：\n2 \u0026lt;= cost.length \u0026lt;= 1000 0 \u0026lt;= cost[i] \u0026lt;= 999 解法\n方法一：DP\nhttps://leetcode.cn/problems/min-cost-climbing-stairs/solution/shi-yong-zui-xiao-hua-fei-pa-lou-ti-by-l-ncf8/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public int minCostClimbingStairs(int[] cost) { // 状态定义：dp[i]表示爬到第i阶台阶的最低花费 int[] dp = new int[cost.length+1]; // 状态初始化：题目说可以直接不花费任何费用到达1或0 dp[0] = 0; dp[1] = 0; // 状态转移方程：dp[i]状态是由dp[i-1]或dp[i-2]转移过来，选择费用最低的方案即可 for(int i=2;i\u0026lt;dp.length;i++){ dp[i] = Math.min(dp[i-1]+cost[i-1],dp[i-2]+cost[i-2]); } // System.out.println(Arrays.toString(dp)); return dp[cost.length]; } } day107 17. 电话号码的字母组合 题目\n给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。答案可以按 任意顺序 返回。\n给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。\n示例 1：\n1 2 输入：digits = \u0026#34;23\u0026#34; 输出：[\u0026#34;ad\u0026#34;,\u0026#34;ae\u0026#34;,\u0026#34;af\u0026#34;,\u0026#34;bd\u0026#34;,\u0026#34;be\u0026#34;,\u0026#34;bf\u0026#34;,\u0026#34;cd\u0026#34;,\u0026#34;ce\u0026#34;,\u0026#34;cf\u0026#34;] 示例 2：\n1 2 输入：digits = \u0026#34;\u0026#34; 输出：[] 示例 3：\n1 2 输入：digits = \u0026#34;2\u0026#34; 输出：[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;] 提示：\n0 \u0026lt;= digits.length \u0026lt;= 4 digits[i] 是范围 ['2', '9'] 的一个数字。 解法\n本题很容易想到DFS暴力搜索的方法，再加上题目给的数据集很小，所以DFS暴力搜索可以解决。依据题目输入digits字符串，找到其中每个数字字符匹配的一个字符之后再组合即是答案，于是，就可以不断地遍历digits，去搜索所有的字符情况，当然需要注意回溯。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { String[] ss; public List\u0026lt;String\u0026gt; letterCombinations(String digits) { ss = new String[]{\u0026#34;\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;abc\u0026#34;,\u0026#34;def\u0026#34;,\u0026#34;ghi\u0026#34;,\u0026#34;jkl\u0026#34;,\u0026#34;mno\u0026#34;,\u0026#34;pqrs\u0026#34;,\u0026#34;tuv\u0026#34;,\u0026#34;wxyz\u0026#34;}; List\u0026lt;String\u0026gt; ls = new ArrayList\u0026lt;\u0026gt;(); if(digits.length()==0) { return ls; } dfs(digits,digits.length(),\u0026#34;\u0026#34;,ls); return ls; } public void dfs(String digits,int step,String ans,List\u0026lt;String\u0026gt; ls){ // 结束条件 if(step==0){ // System.out.println(ans); ls.add(ans); return; } // 开始放第sIdx个位置 int sIdx = Integer.parseInt(Character.toString(digits.charAt(digits.length()-step)),10); for(int j=0;j\u0026lt;ss[sIdx].length();j++){ // 先取第一个 ans+= ss[sIdx].charAt(j); dfs(digits,step-1,ans,ls); // 回溯到上一层 ans = ans.substring(0,ans.length()-1); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func letterCombinations(digits string) []string { if len(digits) == 0 { return []string{} } table:=[]string{\u0026#34;\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;abc\u0026#34;,\u0026#34;def\u0026#34;,\u0026#34;ghi\u0026#34;,\u0026#34;jkl\u0026#34;,\u0026#34;mno\u0026#34;,\u0026#34;pqrs\u0026#34;,\u0026#34;tuv\u0026#34;,\u0026#34;wxyz\u0026#34;} return dfs(table,digits,0,\u0026#34;\u0026#34;) } func dfs(table []string, digits string, step int, cur string) []string { if step == len(digits){ return []string{cur} } ans := []string{} // 依次取号 didx,_:=strconv.Atoi(string(digits[step])) // 遍历号对应所有字符 for i:=0;i\u0026lt;len(table[didx]);i++{ cur+=string(table[didx][i]) ans = append(ans,dfs(table,digits,step+1,cur)...) cur = string(cur[:len(cur)-1]) } return ans } day108 22. 括号生成 题目\n数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。\n示例 1：\n1 2 输入：n = 3 输出：[\u0026#34;((()))\u0026#34;,\u0026#34;(()())\u0026#34;,\u0026#34;(())()\u0026#34;,\u0026#34;()(())\u0026#34;,\u0026#34;()()()\u0026#34;] 示例 2：\n1 2 输入：n = 1 输出：[\u0026#34;()\u0026#34;] 提示：\n1 \u0026lt;= n \u0026lt;= 8 解法\n官方解法：暴力与回溯\n我这里的解法是暴力回溯。其实这个问题也是属于组合问题，但是组合是有条件限制的。必须满足括号匹配原则的组合才能追加到最后的结果里。那么我们怎样判断呢？\n可以通过栈这种数据结构来实现括号匹配：当栈空的时候就往里面加括号，如果非空，就看栈顶的括号与即将加入的括号是否配对，配对就pop掉栈顶的括号，否则就加入到栈中。最后如果栈空了，就表明括号匹配，否则不能追加到结果里。\n当然，其实这个过程可以优化一下：没必要等到所有括号处理完毕再判断，其实中间过程就可以提前判断出来，终止继续深度递归了，可以在一定程度优化时间复杂度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // 先dfs放下所有组合，然后用栈来判断dfs的过程是否合理 class Solution { String[] table; public Solution(){ table = new String[]{\u0026#34;(\u0026#34;,\u0026#34;)\u0026#34;}; } public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); Stack\u0026lt;String\u0026gt; st = new Stack\u0026lt;\u0026gt;(); dfs(list,0,n,\u0026#34;\u0026#34;,st); return list; } public void dfs(List\u0026lt;String\u0026gt; list,int step,int n,String ans,Stack\u0026lt;String\u0026gt; st){ if(step==2*n){ if(st.empty()) list.add(ans); return; } for(int i=0;i\u0026lt;2;i++){ // 在放括号之前，需要保证括号配对 // 下面的变量用来记录当前深度的值，方便回溯 boolean op=false; String tmp =\u0026#34;\u0026#34;; if(st.empty()){ st.push(table[i]); }else{ if(table[i]==\u0026#34;)\u0026#34;\u0026amp;\u0026amp;st.peek()==\u0026#34;(\u0026#34;){ tmp = st.pop(); op = true; }else{ st.push(table[i]); } } ans+=table[i]; dfs(list,step+1,n,ans,st); // 注意：这里借助栈来判断是否括号配对的同时也是dfs，所以需要注意回溯 ans = ans.substring(0,ans.length()-1); if(!op){ st.pop(); }else{ st.push(tmp); } } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 func generateParenthesis(n int) []string { stack :=[]byte{} return dfs(stack,\u0026#34;\u0026#34;,0,n) } func dfs(stack []byte,cur string,step int,n int) []string{ if step==2*n { if len(stack)==0{ return []string{cur} }else{ return []string{} } } ans := []string{} for i:=0;i\u0026lt;2;i++{ next:=byte(\u0026#39;(\u0026#39;) if i==1{ next = \u0026#39;)\u0026#39; } flag:=false last :=byte(\u0026#39; \u0026#39;) if len(stack)!=0\u0026amp;\u0026amp;next==\u0026#39;)\u0026#39;\u0026amp;\u0026amp;stack[len(stack)-1]==\u0026#39;(\u0026#39;{ last = stack[len(stack)-1] stack = stack[:len(stack)-1] }else{ stack = append(stack,next) flag = true } ans = append(ans,dfs(stack,cur+string(next),step+1,n)...) // 回溯 栈 if flag{ stack = stack[:len(stack)-1] }else{ stack = append(stack,last) } } return ans } day109 39. 组合总和 题目\n给你一个 无重复元素 的整数数组 candidates 和一个目标整数 target ，找出 candidates 中可以使数字和为目标数 target 的 所有 不同组合 ，并以列表形式返回。你可以按 任意顺序 返回这些组合。\ncandidates 中的 同一个 数字可以 无限制重复被选取 。如果至少一个数字的被选数量不同，则两种组合是不同的。\n对于给定的输入，保证和为 target 的不同组合数少于 150 个。\n示例 1：\n1 2 3 4 5 6 输入：candidates = [2,3,6,7], target = 7 输出：[[2,2,3],[7]] 解释： 2 和 3 可以形成一组候选，2 + 2 + 3 = 7 。注意 2 可以使用多次。 7 也是一个候选， 7 = 7 。 仅有这两种组合。 示例 2：\n1 2 输入: candidates = [2,3,5], target = 8 输出: [[2,2,2,2],[2,3,3],[3,5]] 示例 3：\n1 2 输入: candidates = [2], target = 1 输出: [] 提示：\n1 \u0026lt;= candidates.length \u0026lt;= 30 2 \u0026lt;= candidates[i] \u0026lt;= 40 candidates 的所有元素 互不相同 1 \u0026lt;= target \u0026lt;= 40 解法\n题解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 import java.util.ArrayDeque; import java.util.ArrayList; import java.util.Deque; import java.util.List; public class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { int len = candidates.length; List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (len == 0) { return res; } Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(candidates, 0, len, target, path, res); return res; } /** * @param candidates 候选数组 * @param begin 搜索起点 * @param len 冗余变量，是 candidates 里的属性，可以不传 * @param target 每减去一个元素，目标值变小 * @param path 从根结点到叶子结点的路径，是一个栈 * @param res 结果集列表 */ private void dfs(int[] candidates, int begin, int len, int target, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { // target 为负数和 0 的时候不再产生新的孩子结点 if (target \u0026lt; 0) { return; } if (target == 0) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } // 重点理解这里从 begin 开始搜索的语意 for (int i = begin; i \u0026lt; len; i++) { path.addLast(candidates[i]); // 注意：由于每一个元素可以重复使用，下一轮搜索的起点依然是i，这里非常容易弄错 dfs(candidates, i, len, target - candidates[i], path, res); // 状态重置 path.removeLast(); } } } // 剪枝优化版 import java.util.ArrayDeque; import java.util.ArrayList; import java.util.Arrays; import java.util.Deque; import java.util.List; public class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { int len = candidates.length; List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (len == 0) { return res; } // 排序是剪枝的前提 Arrays.sort(candidates); Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(candidates, 0, len, target, path, res); return res; } private void dfs(int[] candidates, int begin, int len, int target, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { // 由于进入更深层的时候，小于 0 的部分被剪枝，因此递归终止条件值只判断等于 0 的情况 if (target == 0) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } for (int i = begin; i \u0026lt; len; i++) { // 重点理解这里剪枝，前提是候选数组已经有序 // 已经为负数了，没有必要再搜索下去了，提前结束 if (target - candidates[i] \u0026lt; 0) { break; } path.addLast(candidates[i]); dfs(candidates, i, len, target - candidates[i], path, res); path.removeLast(); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func combinationSum(candidates []int, target int) [][]int { ans := [][]int{} var dfs func(candidates, subs []int, target, step int) dfs = func(candidates, subs []int, target, step int) { if target \u0026lt; 0 { return } tmp := make([]int, len(subs)) if target == 0 { copy(tmp, subs) ans = append(ans, tmp) return } for i := step; i \u0026lt; len(candidates); i++ { subs = append(subs, candidates[i]) dfs(candidates, subs, target-candidates[i], i) subs = subs[:len(subs)-1] } return } dfs(candidates, []int{}, target, 0) return ans } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // 剪枝优化版 func combinationSum(candidates []int, target int) [][]int { ans := [][]int{} var dfs func(candidates, subs []int, target, step int) dfs = func(candidates, subs []int, target, step int) { tmp := make([]int, len(subs)) if target == 0 { copy(tmp, subs) ans = append(ans, tmp) return } for i := step; i \u0026lt; len(candidates); i++ { if target-candidates[i]\u0026lt;0{ break } subs = append(subs, candidates[i]) dfs(candidates, subs, target-candidates[i], i) subs = subs[:len(subs)-1] } return } sort.Ints(candidates) dfs(candidates, []int{}, target, 0) return ans } day110 78. 子集 题目\n给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（幂集）。\n解集 不能 包含重复的子集。你可以按 任意顺序 返回解集。\n示例 1：\n1 2 输入：nums = [1,2,3] 输出：[[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]] 示例 2：\n1 2 输入：nums = [0] 输出：[[],[0]] 提示：\n1 \u0026lt;= nums.length \u0026lt;= 10 -10 \u0026lt;= nums[i] \u0026lt;= 10 nums 中的所有元素 互不相同 解法\nDFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; ls = new ArrayList\u0026lt;\u0026gt;(); dfs(lls,ls,nums,0); return lls; } public void dfs(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; lls,List\u0026lt;Integer\u0026gt; ls,int[] nums,int step){ if(step==nums.length){ lls.add(new ArrayList\u0026lt;\u0026gt;(ls)); return; } // System.out.println(ls.toString()); ls.add(ls.size(),nums[step]); // System.out.println(\u0026#34;递归前：\u0026#34;+ls.toString()); // 取下一个数 dfs(lls,ls,nums,step+1); // System.out.println(\u0026#34;递归后：\u0026#34;+ls.toString()); // 回溯 ls.remove(ls.size()-1); // 直接取下一个数，不取当前数 dfs(lls,ls,nums,step+1); } } day111 343. 整数拆分 题目\n给定一个正整数 n ，将其拆分为 k 个 正整数 的和（ k \u0026gt;= 2 ），并使这些整数的乘积最大化。\n返回 你可以获得的最大乘积 。\n示例 1:\n1 2 3 输入: n = 2 输出: 1 解释: 2 = 1 + 1, 1 × 1 = 1。 示例 2:\n1 2 3 输入: n = 10 输出: 36 解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36。 提示:\n2 \u0026lt;= n \u0026lt;= 58 解法\n代码随想录\u0026ndash;dp\n官方题解\nday112 96. 不同的二叉搜索树 题目\n给你一个整数 n ，求恰由 n 个节点组成且节点值从 1 到 n 互不相同的 二叉搜索树 有多少种？返回满足题意的二叉搜索树的种数。\n示例 1：\n1 2 输入：n = 3 输出：5 示例 2：\n1 2 输入：n = 1 输出：1 提示：\n1 \u0026lt;= n \u0026lt;= 19 解法\n相比官方更容易看懂得解法\n官方题解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func numTrees(n int) int { if n==0 || n==1{ return 1 } if c,ok:=m[n];ok{ return c } count:=0 for i:=1;i\u0026lt;=n;i++{ l:=numTrees(i-1) r:=numTrees(n-i) count += l*r } m[n]=count return count } // 记忆化 var m = map[int]int{} day113 518. 零钱兑换 II 题目\n给你一个整数数组 coins 表示不同面额的硬币，另给一个整数 amount 表示总金额。\n请你计算并返回可以凑成总金额的硬币组合数。如果任何硬币组合都无法凑出总金额，返回 0 。\n假设每一种面额的硬币有无限个。\n题目数据保证结果符合 32 位带符号整数。\n示例 1：\n1 2 3 4 5 6 7 输入：amount = 5, coins = [1, 2, 5] 输出：4 解释：有四种方式可以凑成总金额： 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 示例 2：\n1 2 3 输入：amount = 3, coins = [2] 输出：0 解释：只用面额 2 的硬币不能凑成总金额 3 。 示例 3：\n1 2 输入：amount = 10, coins = [10] 输出：1 提示：\n1 \u0026lt;= coins.length \u0026lt;= 300 1 \u0026lt;= coins[i] \u0026lt;= 5000 coins 中的所有值 互不相同 0 \u0026lt;= amount \u0026lt;= 5000 解法\n二维DP\n典型的完全背包问题，需要注意：这里的背包容量需要正序遍历，因为状态转移方程是不断地去拿左上角的值，我们要确保这个值肯定要被算过（这样才符合完全背包的定义）。所以需要正序遍历背包容量。\n状态转移方程也有不同\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int change(int amount, int[] coins) { // 状态定义：dp[i][j]表示取1-i种硬币时，可以凑出面额为j的方法有多少种 int[][] dp = new int[coins.length+1][amount+1]; // 初始化：没有拿硬币时，可以凑出0的面额，方法有1种，就是什么都不做 dp[0][0] = 1; // 状态转移 for(int i=1;i\u0026lt;=coins.length;i++){ for(int j=0;j\u0026lt;=amount;j++){ if(j\u0026gt;=coins[i-1]) dp[i][j] = dp[i-1][j] + dp[i][j-coins[i-1]]; else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[coins.length][amount]; } } 滚动数组优化为一维DP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public int change(int amount, int[] coins) { // 状态定义：dp[i]可以凑出面额为i的方法有多少种 int[] dp = new int[amount+1]; // 初始化：没有拿硬币时，可以凑出0的面额，方法有1种，就是什么都不做 dp[0] = 1; // 状态转移 for(int i=1;i\u0026lt;=coins.length;i++){ for(int j=coins[i-1];j\u0026lt;=amount;j++){// 枚举背包容量，当然最小肯定就是cons[i-1]了 if(j\u0026gt;=coins[i-1]) dp[j] = dp[j] + dp[j-coins[i-1]]; } } // System.out.println(Arrays.roString(dp)); return dp[amount]; } } day114 多重背包 题目\n有N种物品和一个容量为V 的背包。第i种物品最多有Mi件可用，每件耗费的空间是Ci ，价值是Wi 。求解将哪些物品装入背包可使这些物品的耗费的空间 总和不超过背包容量，且价值总和最大。\n解法\n多重背包和01背包是非常像的， 为什么和01背包像呢？\n每件物品最多有Mi件可用，把Mi件摊开，其实就是一个01背包问题了。\n例如：\n背包最大重量为10。\n物品为：\n重量 价值 数量 物品0 1 15 2 物品1 3 20 3 物品2 4 30 2 问背包能背的物品最大价值是多少？\n和如下情况有区别么？\n重量 价值 数量 物品0 1 15 1 物品0 1 15 1 物品1 3 20 1 物品1 3 20 1 物品1 3 20 1 物品2 4 30 1 物品2 4 30 1 毫无区别，这就转成了一个01背包问题了，且每个物品只用一次。\n时间复杂度：O(m × n × k)，m：物品种类个数，n背包容量，k单类物品数量 也有另一种实现方式，就是把每种商品遍历的个数放在01背包里面在遍历一遍。\n毫无区别，这就转成了一个01背包问题了，且每个物品只用一次。\n时间复杂度：O(m × n × k)，m：物品种类个数，n背包容量，k单类物品数量 从代码里可以看出是01背包里面在加一个for循环遍历一个每种商品的数量。 和01背包还是如出一辙的。\n当然还有那种二进制优化的方法，其实就是把每种物品的数量，打包成一个个独立的包。\n和以上在循环遍历上有所不同，因为是分拆为各个包最后可以组成一个完整背包，具体原理我就不做过多解释了，大家了解一下就行，面试的话基本不会考完这个深度了，感兴趣可以自己深入研究一波。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public void testMultiPack1(){ // 版本一：改变物品数量为01背包格式 List\u0026lt;Integer\u0026gt; weight = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(1, 3, 4)); List\u0026lt;Integer\u0026gt; value = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(15, 20, 30)); List\u0026lt;Integer\u0026gt; nums = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(2, 3, 2)); int bagWeight = 10; for (int i = 0; i \u0026lt; nums.size(); i++) { while (nums.get(i) \u0026gt; 1) { // 把物品展开为i weight.add(weight.get(i)); value.add(value.get(i)); nums.set(i, nums.get(i) - 1); } } int[] dp = new int[bagWeight + 1]; for(int i = 0; i \u0026lt; weight.size(); i++) { // 遍历物品 for(int j = bagWeight; j \u0026gt;= weight.get(i); j--) { // 遍历背包容量 dp[j] = Math.max(dp[j], dp[j - weight.get(i)] + value.get(i)); } System.out.println(Arrays.toString(dp)); } } public void testMultiPack2(){ // 版本二：改变遍历个数 int[] weight = new int[] {1, 3, 4}; int[] value = new int[] {15, 20, 30}; int[] nums = new int[] {2, 3, 2}; int bagWeight = 10; int[] dp = new int[bagWeight + 1]; for(int i = 0; i \u0026lt; weight.length; i++) { // 遍历物品 for(int j = bagWeight; j \u0026gt;= weight[i]; j--) { // 遍历背包容量 // 以上为01背包，然后加一个遍历个数 for (int k = 1; k \u0026lt;= nums[i] \u0026amp;\u0026amp; (j - k * weight[i]) \u0026gt;= 0; k++) { // 遍历个数 dp[j] = Math.max(dp[j], dp[j - k * weight[i]] + k * value[i]); } System.out.println(Arrays.toString(dp)); } } } day115 198. 打家劫舍 题目\n你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。\n给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。\n示例 1：\n1 2 3 4 输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2：\n1 2 3 4 输入：[2,7,9,3,1] 输出：12 解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 提示：\n1 \u0026lt;= nums.length \u0026lt;= 100 0 \u0026lt;= nums[i] \u0026lt;= 400 解法\ndp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public int rob(int[] nums) { // dp[i]表示偷窃前i个房屋，得到的最高金额为多少 int[] dp = new int[nums.length+1]; // 初始化 dp[0] = 0; dp[1] = nums[0]; for(int i=2;i\u0026lt;=nums.length;i++){ dp[i] = Math.max(dp[i-1],dp[i-2]+nums[i-1]); } //System.out.println(Arrays.toString(dp)); return dp[nums.length]; } } day116 213. 打家劫舍 II 题目\n你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。\n给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额。\n示例 1：\n1 2 3 输入：nums = [2,3,2] 输出：3 解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 示例 2：\n1 2 3 4 输入：nums = [1,2,3,1] 输出：4 解释：你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 3：\n1 2 输入：nums = [1,2,3] 输出：3 提示：\n1 \u0026lt;= nums.length \u0026lt;= 100 0 \u0026lt;= nums[i] \u0026lt;= 1000 解法\ndp\n这道题目和198.打家劫舍 (opens new window)是差不多的，唯一区别就是成环了。\n对于一个数组，成环的话主要有如下三种情况：\n情况一：考虑不包含首尾元素 情况二：考虑包含首元素，不包含尾元素 情况三：考虑包含尾元素，不包含首元素 注意我这里用的是\u0026quot;考虑\u0026quot;，例如情况三，虽然是考虑包含尾元素，但不一定要选尾部元素！ 对于情况三，取nums[1] 和 nums[3]就是最大的。\n而情况二 和 情况三 都包含了情况一了，所以只考虑情况二和情况三就可以了。\n分析到这里，本题其实比较简单了。 剩下的和198.打家劫舍 (opens new window)就是一样的了。\nday117 337. 打家劫舍 III 题目\n小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为 root 。\n除了 root 之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果 两个直接相连的房子在同一天晚上被打劫 ，房屋将自动报警。\n给定二叉树的 root 。返回 在不触动警报的情况下 ，小偷能够盗取的最高金额 。\n示例 1:\n1 2 3 输入: root = [3,2,3,null,3,null,1] 输出: 7 解释: 小偷一晚能够盗取的最高金额 3 + 3 + 1 = 7 示例 2:\n1 2 3 输入: root = [3,4,5,1,3,null,1] 输出: 9 解释: 小偷一晚能够盗取的最高金额 4 + 5 = 9 提示：\n树的节点数在 [1, 104] 范围内 0 \u0026lt;= Node.val \u0026lt;= 104 解法\n树形dp\nday118 188. 买卖股票的最佳时机 IV 题目\n给定一个整数数组 prices ，它的第 i 个元素 prices[i] 是一支给定的股票在第 i 天的价格，和一个整型 k 。\n设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。也就是说，你最多可以买 k 次，卖 k 次。\n**注意：**你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。\n示例 1：\n1 2 3 输入：k = 2, prices = [2,4,1] 输出：2 解释：在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2 。 示例 2：\n1 2 3 4 输入：k = 2, prices = [3,2,6,5,0,3] 输出：7 解释：在第 2 天 (股票价格 = 2) 的时候买入，在第 3 天 (股票价格 = 6) 的时候卖出, 这笔交易所能获得利润 = 6-2 = 4 。 随后，在第 5 天 (股票价格 = 0) 的时候买入，在第 6 天 (股票价格 = 3) 的时候卖出, 这笔交易所能获得利润 = 3-0 = 3 。 提示：\n0 \u0026lt;= k \u0026lt;= 100 0 \u0026lt;= prices.length \u0026lt;= 1000 0 \u0026lt;= prices[i] \u0026lt;= 1000 解法\ndp\n思路是\u0026quot;买卖股票最大利润Ⅲ\u0026quot;的变形思维。在原来基础上，由5个状态变为2*k+1个状态，遵循先买后卖原则，找到最大利润\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int maxProfit(int k, int[] prices) { int[][] dp = new int[prices.length+1][2*k+1]; // 初始化 dp[0][0] = 0; dp[0][2] =0; for(int i=0;i\u0026lt;2*k+1;i++){ if(i%2==1) dp[0][i] = -prices[0]; } for(int i=1;i\u0026lt;=prices.length;i++){ for(int j=0;j\u0026lt;2*k;j++){ dp[i][1+j] = Math.max(dp[i-1][j]+(int)Math.pow(-1,j+1)*prices[i-1],dp[i-1][1+j]); } } return dp[prices.length][2*k]; } } day119 300. 最长递增子序列 题目\n给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。\n子序列 是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。\n示例 1：\n1 2 3 输入：nums = [10,9,2,5,3,7,101,18] 输出：4 解释：最长递增子序列是 [2,3,7,101]，因此长度为 4 。 示例 2：\n1 2 输入：nums = [0,1,0,3,2,3] 输出：4 示例 3：\n1 2 输入：nums = [7,7,7,7,7,7,7] 输出：1 提示：\n1 \u0026lt;= nums.length \u0026lt;= 2500 -104 \u0026lt;= nums[i] \u0026lt;= 104 进阶：\n你能将算法的时间复杂度降低到 O(n log(n)) 吗? 解法\n官方题解\n比较容易想的应该是【贪心+二分搜索】\nday120 309. 最佳买卖股票时机含冷冻期 题目\n给定一个整数数组prices，其中第 prices[i] 表示第 *i* 天的股票价格 。\n设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）:\n卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 **注意：**你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。\n示例 1:\n1 2 3 输入: prices = [1,2,3,0,2] 输出: 3 解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] 示例 2:\n1 2 输入: prices = [1] 输出: 0 提示：\n1 \u0026lt;= prices.length \u0026lt;= 5000 0 \u0026lt;= prices[i] \u0026lt;= 1000 解法\ndp\nday121 516. 最长回文子序列 题目\n给你一个字符串 s ，找出其中最长的回文子序列，并返回该序列的长度。\n子序列定义为：不改变剩余字符顺序的情况下，删除某些字符或者不删除任何字符形成的一个序列。\n示例 1：\n1 2 3 输入：s = \u0026#34;bbbab\u0026#34; 输出：4 解释：一个可能的最长回文子序列为 \u0026#34;bbbb\u0026#34; 。 示例 2：\n1 2 3 输入：s = \u0026#34;cbbd\u0026#34; 输出：2 解释：一个可能的最长回文子序列为 \u0026#34;bb\u0026#34; 。 提示：\n1 \u0026lt;= s.length \u0026lt;= 1000 s 仅由小写英文字母组成 解法\ndp\n这题的难点主要在于定义状态和找到状态转移方程。\nday122 583. 两个字符串的删除操作 题目\n给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。\n一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。\n例如，\u0026quot;ace\u0026quot; 是 \u0026quot;abcde\u0026quot; 的子序列，但 \u0026quot;aec\u0026quot; 不是 \u0026quot;abcde\u0026quot; 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。\n示例 1：\n1 2 3 输入：text1 = \u0026#34;abcde\u0026#34;, text2 = \u0026#34;ace\u0026#34; 输出：3 解释：最长公共子序列是 \u0026#34;ace\u0026#34; ，它的长度为 3 。 示例 2：\n1 2 3 输入：text1 = \u0026#34;abc\u0026#34;, text2 = \u0026#34;abc\u0026#34; 输出：3 解释：最长公共子序列是 \u0026#34;abc\u0026#34; ，它的长度为 3 。 示例 3：\n1 2 3 输入：text1 = \u0026#34;abc\u0026#34;, text2 = \u0026#34;def\u0026#34; 输出：0 解释：两个字符串没有公共子序列，返回 0 。 提示：\n1 \u0026lt;= text1.length, text2.length \u0026lt;= 1000 text1 和 text2 仅由小写英文字符组成。 题解\n可以将问题转化为求解【最长公共子序列问题】.\ndp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public int minDistance(String word1, String word2) { int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i \u0026lt;= m; i++) { char c1 = word1.charAt(i - 1); for (int j = 1; j \u0026lt;= n; j++) { char c2 = word2.charAt(j - 1); if (c1 == c2) { dp[i][j] = dp[i - 1][j - 1] + 1; } else { dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); } } } return word1.length()+word2.length()-2*dp[m][n]; } } day123 1143. 最长公共子序列 题目\n给定两个单词 word1 和 word2 ，返回使得 word1 和 word2 相同所需的最小步数。\n每步 可以删除任意一个字符串中的一个字符。\n示例 1：\n1 2 3 输入: word1 = \u0026#34;sea\u0026#34;, word2 = \u0026#34;eat\u0026#34; 输出: 2 解释: 第一步将 \u0026#34;sea\u0026#34; 变为 \u0026#34;ea\u0026#34; ，第二步将 \u0026#34;eat \u0026#34;变为 \u0026#34;ea\u0026#34; 示例 2:\n1 2 输入：word1 = \u0026#34;leetcode\u0026#34;, word2 = \u0026#34;etco\u0026#34; 输出：4 提示：\n1 \u0026lt;= word1.length, word2.length \u0026lt;= 500 word1 和 word2 只包含小写英文字母 题解\ndp\nday124 647. 回文子串 题目\n给你一个字符串 s ，请你统计并返回这个字符串中 回文子串 的数目。\n回文字符串 是正着读和倒过来读一样的字符串。\n子字符串 是字符串中的由连续字符组成的一个序列。\n具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。\n示例 1：\n1 2 3 输入：s = \u0026#34;abc\u0026#34; 输出：3 解释：三个回文子串: \u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34; 示例 2：\n1 2 3 输入：s = \u0026#34;aaa\u0026#34; 输出：6 解释：6个回文子串: \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aaa\u0026#34; 提示：\n1 \u0026lt;= s.length \u0026lt;= 1000 s 由小写英文字母组成 解法\ndp+中心拓展\nday125 674. 最长连续递增序列 问题\n给定一个未经排序的整数数组，找到最长且 连续递增的子序列，并返回该序列的长度。\n连续递增的子序列 可以由两个下标 l 和 r（l \u0026lt; r）确定，如果对于每个 l \u0026lt;= i \u0026lt; r，都有 nums[i] \u0026lt; nums[i + 1] ，那么子序列 [nums[l], nums[l + 1], ..., nums[r - 1], nums[r]] 就是连续递增子序列。\n示例 1：\n1 2 3 4 输入：nums = [1,3,5,4,7] 输出：3 解释：最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为 5 和 7 在原数组里被 4 隔开。 示例 2：\n1 2 3 输入：nums = [2,2,2,2,2] 输出：1 解释：最长连续递增序列是 [2], 长度为1。 提示：\n1 \u0026lt;= nums.length \u0026lt;= 104 -109 \u0026lt;= nums[i] \u0026lt;= 109 解法\n解法一：双指针\n不断地寻找连续递增子序列的长度，取最长。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public int findLengthOfLCIS(int[] nums) { if(nums.length\u0026lt;=1) return nums.length; int ans = 1; int tmp = 1; for(int l=0,r=1;l\u0026lt;nums.length\u0026amp;\u0026amp;r\u0026lt;nums.length;){ if(nums[r]\u0026gt;nums[l]){ l++;r++; tmp++; }else{ tmp = Math.max(tmp,ans); ans = tmp; tmp = 1; l = r; r++; } } // 额外检验递增序列 return tmp\u0026gt;ans?tmp:ans; } } 解法二：贪心\n下面的算法其实和双指针本质上是一个东西。\n贪心\nday126 714. 买卖股票的最佳时机含手续费 题目\n给定一个整数数组 prices，其中 prices[i]表示第 i 天的股票价格 ；整数 fee 代表了交易股票的手续费用。\n你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。\n返回获得利润的最大值。\n**注意：**这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。\n示例 1：\n1 2 3 4 5 6 7 8 输入：prices = [1, 3, 2, 8, 4, 9], fee = 2 输出：8 解释：能够达到的最大利润: 在此处买入 prices[0] = 1 在此处卖出 prices[3] = 8 在此处买入 prices[4] = 4 在此处卖出 prices[5] = 9 总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8 示例 2：\n1 2 输入：prices = [1,3,7,5,10,3], fee = 3 输出：6 提示：\n1 \u0026lt;= prices.length \u0026lt;= 5 * 104 1 \u0026lt;= prices[i] \u0026lt; 5 * 104 0 \u0026lt;= fee \u0026lt; 5 * 104 解法\n解法一：dp\n本题只是在【188. 买卖股票的最佳时机 IV】的基础上再附加手续费。所以，我们直接在原来的动态规划转移方程上再加上“如果是买入股票，无需处理；如果是卖出股票，需增加手续费”。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public int maxProfit(int[] prices, int fee) { // dp[i][j]表示第i天（从1开始），第j个状态的最大利润 int[][] dp = new int[prices.length+1][2*prices.length+1]; // 初始化 for(int i=0;i\u0026lt;2*prices.length+1;i++){ if(i%2==1) dp[0][i] = - prices[0]; } int s = 0; int fuhao = 0; for(int i=1;i\u0026lt;=prices.length;i++){ for(int j=0;j\u0026lt;2*prices.length;j++){ fuhao = (int)Math.pow(-1,j+1); if (fuhao==1 ){ s = fee; }else { s = 0; } dp[i][j+1] = Math.max(dp[i-1][j]+fuhao*prices[i-1] - s ,dp[i-1][j+1]); } } return dp[prices.length][2*prices.length]; } } 上面的解法中，定义的状态太多，有较高的空间复杂度。leetcode提交中，显示“内存超出限制”。\n解法二：优化dp\n优化dp\nday127 208. 实现 Trie (前缀树) 题目\nTrie（发音类似 \u0026ldquo;try\u0026rdquo;）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。\n请你实现 Trie 类：\nTrie() 初始化前缀树对象。 void insert(String word) 向前缀树中插入字符串 word 。 boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。 boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 输入 [\u0026#34;Trie\u0026#34;, \u0026#34;insert\u0026#34;, \u0026#34;search\u0026#34;, \u0026#34;search\u0026#34;, \u0026#34;startsWith\u0026#34;, \u0026#34;insert\u0026#34;, \u0026#34;search\u0026#34;] [[], [\u0026#34;apple\u0026#34;], [\u0026#34;apple\u0026#34;], [\u0026#34;app\u0026#34;], [\u0026#34;app\u0026#34;], [\u0026#34;app\u0026#34;], [\u0026#34;app\u0026#34;]] 输出 [null, null, true, false, true, null, true] 解释 Trie trie = new Trie(); trie.insert(\u0026#34;apple\u0026#34;); trie.search(\u0026#34;apple\u0026#34;); // 返回 True trie.search(\u0026#34;app\u0026#34;); // 返回 False trie.startsWith(\u0026#34;app\u0026#34;); // 返回 True trie.insert(\u0026#34;app\u0026#34;); trie.search(\u0026#34;app\u0026#34;); // 返回 True 提示：\n1 \u0026lt;= word.length, prefix.length \u0026lt;= 2000 word 和 prefix 仅由小写英文字母组成 insert、search 和 startsWith 调用次数 总计 不超过 3 * 104 次 解法\n解法一：模拟\n就是不断地去匹配寻找Trie树的下一个节点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class Trie { boolean isWord; // 单词是否以当前字符结尾 Trie[] nextNode; public Trie() { nextNode = new Trie[26]; } public void insert(String word) { Trie root = this; for(int i=0;i\u0026lt;word.length();i++){ int idx = word.charAt(i)-\u0026#39;a\u0026#39;; if(root.nextNode[idx]==null){ Trie n = new Trie(); root.nextNode[idx] = n; } root = root.nextNode[idx]; } root.isWord = true; return; } public boolean search(String word) { Trie root = this; for(int i=0;i\u0026lt;word.length();i++){ int idx = word.charAt(i)-\u0026#39;a\u0026#39;; if(root.nextNode[idx]==null){ return false; } root = root.nextNode[idx]; } return root.isWord; } public boolean startsWith(String prefix) { Trie root = this; for(int i=0;i\u0026lt;prefix.length();i++){ int idx = prefix.charAt(i)-\u0026#39;a\u0026#39;; if(root.nextNode[idx]==null){ return false; } root = root.nextNode[idx]; } return true; } } /** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * boolean param_2 = obj.search(word); * boolean param_3 = obj.startsWith(prefix); */ day128 718. 最长重复子数组 题目\n给两个整数数组 nums1 和 nums2 ，返回 两个数组中 公共的 、长度最长的子数组的长度 。\n示例 1：\n1 2 3 输入：nums1 = [1,2,3,2,1], nums2 = [3,2,1,4,7] 输出：3 解释：长度最长的公共子数组是 [3,2,1] 。 示例 2：\n1 2 输入：nums1 = [0,0,0,0,0], nums2 = [0,0,0,0,0] 输出：5 提示：\n1 \u0026lt;= nums1.length, nums2.length \u0026lt;= 1000 0 \u0026lt;= nums1[i], nums2[i] \u0026lt;= 100 代码\n解法一：暴力算法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public int findLength(int[] nums1, int[] nums2) { int maxLen = 0; for(int i=0;i\u0026lt;nums1.length;i++){ for(int j=0;j\u0026lt;nums2.length;j++){ int k=0; while(i+k\u0026lt;nums1.length\u0026amp;\u0026amp;j+k\u0026lt;nums2.length\u0026amp;\u0026amp;nums1[i+k]==nums2[j+k]){ k+=1; } if(maxLen\u0026lt;k) maxLen = k; } } return maxLen; } } 解法二：动态规划\n思路及算法\n暴力解法的过程中，我们发现最坏情况下对于任意 i 与 j ，A[i] 与 B[j] 比较了 min⁡(i+1,j+1) 次。这也是导致了该暴力解法时间复杂度过高的根本原因。\n不妨设 A 数组为 [1, 2, 3]，B 两数组为为 [1, 2, 4] ，那么在暴力解法中 A[2] 与 B[2] 被比较了三次。这三次比较分别是我们计算 A[0:] 与 B[0:] 最长公共前缀、 A[1:] 与 B[1:] 最长公共前缀以及 A[2:] 与 B[2:] 最长公共前缀时产生的。\n我们希望优化这一过程，使得任意一对 A[i] 和 B[j] 都只被比较一次。这样我们自然而然想到利用这一次的比较结果。如果 A[i] == B[j]，那么我们知道 A[i:] 与 B[j:] 的最长公共前缀为 A[i + 1:] 与 B[j + 1:] 的最长公共前缀的长度加一，否则我们知道 A[i:] 与 B[j:] 的最长公共前缀为零。\n这样我们就可以提出动态规划的解法：令 dp[i][j] 表示 A[i:] 和 B[j:] 的最长公共前缀，那么答案即为所有 dp[i][j] 中的最大值。如果 A[i] == B[j]，那么 dp[i][j] = dp[i + 1][j + 1] + 1，否则 dp[i][j] = 0。\n这里借用了 Python 表示数组的方法，A[i:] 表示数组 A 中索引 i 到数组末尾的范围对应的子数组。\n考虑到这里 dp[i][j] 的值从 dp[i + 1][j + 1] 转移得到，所以我们需要倒过来，首先计算 dp[len(A) - 1][len(B) - 1]，最后计算 dp[0][0]。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int findLength(int[] nums1, int[] nums2) { // dp[i][j]表示nums1[i:]和nums2[j:]的最长公共前缀长度 int[][] dp = new int[nums1.length+1][nums2.length+1]; int max =0; for(int i=nums1.length-1;i\u0026gt;=0;i--){ for(int j= nums2.length-1;j\u0026gt;=0;j--){ if(nums1[i]==nums2[j]){ dp[i][j] = dp[i+1][j+1]+1; } max = Math.max(dp[i][j],max); } } return max; } } day129 1035. 不相交的线 题目\n在两条独立的水平线上按给定的顺序写下 nums1 和 nums2 中的整数。\n现在，可以绘制一些连接两个数字 nums1[i] 和 nums2[j] 的直线，这些直线需要同时满足满足：\nnums1[i] == nums2[j] 且绘制的直线不与任何其他连线（非水平线）相交。 请注意，连线即使在端点也不能相交：每个数字只能属于一条连线。\n以这种方法绘制线条，并返回可以绘制的最大连线数。\n示例 1：\n1 2 3 4 输入：nums1 = [1,4,2], nums2 = [1,2,4] 输出：2 解释：可以画出两条不交叉的线，如上图所示。 但无法画出第三条不相交的直线，因为从 nums1[1]=4 到 nums2[2]=4 的直线将与从 nums1[2]=2 到 nums2[1]=2 的直线相交。 示例 2：\n1 2 输入：nums1 = [2,5,1,2,5], nums2 = [10,5,2,1,5,2] 输出：3 示例 3：\n1 2 输入：nums1 = [1,3,7,1,7,5], nums2 = [1,9,2,5,1] 输出：2 提示：\n1 \u0026lt;= nums1.length, nums2.length \u0026lt;= 500 1 \u0026lt;= nums1[i], nums2[j] \u0026lt;= 2000 解法\n解法一：动态规划\n其实这个问题本质上就是求解最长公共子序列问题\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public int maxUncrossedLines(int[] nums1, int[] nums2) { // dp[i][j]表示nums1[:i]与nums2[:j]中最长公共子序列的长度 int[][] dp = new int[nums1.length+1][nums2.length+1]; // 初始状态 dp[0][0] = 0; for(int i=1;i\u0026lt;=nums1.length;i++){ for(int j=1;j\u0026lt;=nums2.length;j++){ if(nums1[i-1]==nums2[j-1]){ dp[i][j] = dp[i-1][j-1]+1; }else{ dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]); } } } return dp[nums1.length][nums2.length]; } } day130 1646. 获取生成数组中的最大值 题目\n给你一个整数 n 。按下述规则生成一个长度为 n + 1 的数组 nums ：\nnums[0] = 0 nums[1] = 1 当 2 \u0026lt;= 2 * i \u0026lt;= n 时，nums[2 * i] = nums[i] 当 2 \u0026lt;= 2 * i + 1 \u0026lt;= n 时，nums[2 * i + 1] = nums[i] + nums[i + 1] 返回生成数组 nums 中的 最大 值。\n示例 1：\n1 2 3 4 5 6 7 8 9 10 11 12 输入：n = 7 输出：3 解释：根据规则： nums[0] = 0 nums[1] = 1 nums[(1 * 2) = 2] = nums[1] = 1 nums[(1 * 2) + 1 = 3] = nums[1] + nums[2] = 1 + 1 = 2 nums[(2 * 2) = 4] = nums[2] = 1 nums[(2 * 2) + 1 = 5] = nums[2] + nums[3] = 1 + 2 = 3 nums[(3 * 2) = 6] = nums[3] = 2 nums[(3 * 2) + 1 = 7] = nums[3] + nums[4] = 2 + 1 = 3 因此，nums = [0,1,1,2,1,3,2,3]，最大值 3 示例 2：\n1 2 3 输入：n = 2 输出：1 解释：根据规则，nums[0]、nums[1] 和 nums[2] 之中的最大值是 1 示例 3：\n1 2 3 输入：n = 3 输出：2 解释：根据规则，nums[0]、nums[1]、nums[2] 和 nums[3] 之中的最大值是 2 提示：\n0 \u0026lt;= n \u0026lt;= 100 解法\n解法一：模拟\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public int getMaximumGenerated(int n) { if(n\u0026lt;=0) return 0; int max = 1; int[] nums = new int[n+1]; nums[1] = 1; for(int i=2;i\u0026lt;=n;i++){ if(i%2==0){ nums[i] = nums[i/2]; }else{ nums[i] = nums[(i-1)/2] + nums[(i+1)/2]; } max = Math.max(max,nums[i]); } // System.out.println(Arrays.toString(nums)); return max; } } day131 79. 单词搜索 题目\n解法\n解法一：二进制法和dfs法\n官方解法\n解法二：暴力dfs\n思路比较简单，对每一个位置进行四个方向的dfs，找到即可结束dfs了。\n结束条件：当前i处字符相等，且是word最后一个字符，可以return true了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Solution { public boolean exist(char[][] board, String word) { boolean ok =false; boolean[][] v = new boolean[board.length][board[0].length]; for(int i=0;i\u0026lt;board.length;i++){ for(int j=0;j\u0026lt;board[0].length;j++){ ok = dfs(board,word,i,j,0,v); if(ok) return true; // System.out.println(ok); } } return false; } public boolean dfs(char[][] board,String word,int nextI,int nextJ,int i,boolean[][] v){ if(word.charAt(i)!=board[nextI][nextJ]){ return false; }else if (i == word.length() - 1) { return true; } // System.out.println(board[nextI][nextJ]); v[nextI][nextJ] = true; boolean ok = false; // 四个方向dfs if(ok_(board,nextI+1,nextJ)\u0026amp;\u0026amp;!v[nextI+1][nextJ]){// 右 ok = ok||dfs(board,word,nextI+1,nextJ,i+1,v); } if(ok_(board,nextI-1,nextJ)\u0026amp;\u0026amp;!v[nextI-1][nextJ]){// 左 ok = ok||dfs(board,word,nextI-1,nextJ,i+1,v); } if(ok_(board,nextI,nextJ+1)\u0026amp;\u0026amp;!v[nextI][nextJ+1]){// 下 ok = ok||dfs(board,word,nextI,nextJ+1,i+1,v); } if(ok_(board,nextI,nextJ-1)\u0026amp;\u0026amp;!v[nextI][nextJ-1]){// 上 ok = ok||dfs(board,word,nextI,nextJ-1,i+1,v); } v[nextI][nextJ] = false; return ok; } public boolean ok_(char[][] board,int newi,int newj){ return newi \u0026gt;= 0 \u0026amp;\u0026amp; newi \u0026lt; board.length \u0026amp;\u0026amp; newj \u0026gt;= 0 \u0026amp;\u0026amp; newj \u0026lt; board[0].length; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 func exist(board [][]byte, word string) bool { var dfs func(step,row,col int) v := make([][]bool,len(board)) m:=len(board) n:=len(board[0]) for i:=0;i\u0026lt;len(board);i++{ v[i]=make([]bool,len(board[0])) } check:=func(i,j int)bool{ return i\u0026gt;=0\u0026amp;\u0026amp;i\u0026lt;m\u0026amp;\u0026amp;j\u0026gt;=0\u0026amp;\u0026amp;j\u0026lt;n } ans:=false dfs = func(step,row,col int){ if board[row][col]!=word[step]{ return } if step==len(word)-1{ ans = true return } // fmt.Println(string(board[row][col]),string(word[step])) v[row][col]=true dir:=[][]int{{-1,0},{1,0},{0,1},{0,-1}} for i:=0;i\u0026lt;4;i++{ newrow:=row+dir[i][0] newcol:=col+dir[i][1] if check(newrow,newcol)\u0026amp;\u0026amp;!v[newrow][newcol]{ dfs(step+1,newrow,newcol) } } v[row][col]=false } for i:=0;i\u0026lt;m;i++{ for j:=0;j\u0026lt;n;j++{ if word[0]==board[i][j] { // 开始dfs dfs(0,i,j) if ans { return ans } } } } return false } day132 36. 有效的数独 题目\n请你判断一个 9 x 9 的数独是否有效。只需要 根据以下规则 ，验证已经填入的数字是否有效即可。\n数字 1-9 在每一行只能出现一次。 数字 1-9 在每一列只能出现一次。 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图） 注意：\n一个有效的数独（部分已被填充）不一定是可解的。 只需要根据以上规则，验证已经填入的数字是否有效即可。 空白格用 '.' 表示。 示例 1：\n1 2 3 4 5 6 7 8 9 10 11 输入：board = [[\u0026#34;5\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;3\u0026#34;] ,[\u0026#34;4\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;1\u0026#34;] ,[\u0026#34;7\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;5\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;9\u0026#34;]] 输出：true 示例 2：\n1 2 3 4 5 6 7 8 9 10 11 12 输入：board = [[\u0026#34;8\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;3\u0026#34;] ,[\u0026#34;4\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;1\u0026#34;] ,[\u0026#34;7\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;5\u0026#34;] ,[\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;9\u0026#34;]] 输出：false 解释：除了第一行的第一个数字从 5 改为 8 以外，空格内其他数字均与 示例1 相同。 但由于位于左上角的 3x3 宫内有两个 8 存在, 因此这个数独是无效的。 提示：\nboard.length == 9 board[i].length == 9 board[i][j] 是一位数字（1-9）或者 '.' 解法\n数组哈希法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public boolean isValidSudoku(char[][] board) { int[][] rows = new int[9][9]; int[][] columns = new int[9][9]; int[][][] subboxes = new int[3][3][9]; for (int i = 0; i \u0026lt; 9; i++) { for (int j = 0; j \u0026lt; 9; j++) { char c = board[i][j]; if (c != \u0026#39;.\u0026#39;) { int index = c - \u0026#39;0\u0026#39; - 1; rows[i][index]++; columns[j][index]++; subboxes[i / 3][j / 3][index]++; if (rows[i][index] \u0026gt; 1 || columns[j][index] \u0026gt; 1 || subboxes[i / 3][j / 3][index] \u0026gt; 1) { return false; } } } } return true; } } day133 37. 解数独 题目\n编写一个程序，通过填充空格来解决数独问题。\n数独的解法需 遵循如下规则：\n数字 1-9 在每一行只能出现一次。 数字 1-9 在每一列只能出现一次。 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图） 数独部分空格内已填入了数字，空白格用 '.' 表示。\n示例 1：\n1 2 3 输入：board = [[\u0026#34;5\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;],[\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;],[\u0026#34;.\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;],[\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;3\u0026#34;],[\u0026#34;4\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;1\u0026#34;],[\u0026#34;7\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;],[\u0026#34;.\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;],[\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;5\u0026#34;],[\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;9\u0026#34;]] 输出：[[\u0026#34;5\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;],[\u0026#34;6\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;8\u0026#34;],[\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;7\u0026#34;],[\u0026#34;8\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;3\u0026#34;],[\u0026#34;4\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;1\u0026#34;],[\u0026#34;7\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;6\u0026#34;],[\u0026#34;9\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;4\u0026#34;],[\u0026#34;2\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;9\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;3\u0026#34;,\u0026#34;5\u0026#34;],[\u0026#34;3\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;5\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;8\u0026#34;,\u0026#34;6\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;7\u0026#34;,\u0026#34;9\u0026#34;]] 解释：输入的数独如上图所示，唯一有效的解决方案如下所示： 提示：\nboard.length == 9 board[i].length == 9 board[i][j] 是一位数字或者 '.' 题目数据 保证 输入数独仅有一个解 解法\n解法一：dfs+回溯\n其实题目的解法很明显，dfs暴搜，主要是回溯的过程和以往的题目不太一样：填数独并不是挨着格子填的，所以回溯的时候也不是挨着格子回溯，先记录这些空格子的位置，回溯的时候按照记录来回溯即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class Solution { private boolean[][] line = new boolean[9][9]; private boolean[][] column = new boolean[9][9]; private boolean[][][] block = new boolean[3][3][9]; private boolean valid = false; private List\u0026lt;int[]\u0026gt; spaces = new ArrayList\u0026lt;int[]\u0026gt;(); public void solveSudoku(char[][] board) { for (int i = 0; i \u0026lt; 9; ++i) { for (int j = 0; j \u0026lt; 9; ++j) { if (board[i][j] == \u0026#39;.\u0026#39;) { spaces.add(new int[]{i, j}); } else { int digit = board[i][j] - \u0026#39;0\u0026#39; - 1; line[i][digit] = column[j][digit] = block[i / 3][j / 3][digit] = true; } } } dfs(board, 0); } public void dfs(char[][] board, int pos) { if (pos == spaces.size()) { valid = true; return; } int[] space = spaces.get(pos); int i = space[0], j = space[1]; for (int digit = 0; digit \u0026lt; 9 \u0026amp;\u0026amp; !valid; ++digit) { if (!line[i][digit] \u0026amp;\u0026amp; !column[j][digit] \u0026amp;\u0026amp; !block[i / 3][j / 3][digit]) { line[i][digit] = column[j][digit] = block[i / 3][j / 3][digit] = true; board[i][j] = (char) (digit + \u0026#39;0\u0026#39; + 1); dfs(board, pos + 1); line[i][digit] = column[j][digit] = block[i / 3][j / 3][digit] = false; } } } } 其他解法\nhttps://leetcode.cn/problems/sudoku-solver/solutions/414120/jie-shu-du-by-leetcode-solution/\nday134 51. N 皇后 题目\n按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或同一斜线上的棋子。\nn 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。\n给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。\n每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 'Q' 和 '.' 分别代表了皇后和空位。\n示例 1：\n1 2 3 输入：n = 4 输出：[[\u0026#34;.Q..\u0026#34;,\u0026#34;...Q\u0026#34;,\u0026#34;Q...\u0026#34;,\u0026#34;..Q.\u0026#34;],[\u0026#34;..Q.\u0026#34;,\u0026#34;Q...\u0026#34;,\u0026#34;...Q\u0026#34;,\u0026#34;.Q..\u0026#34;]] 解释：如上图所示，4 皇后问题存在两个不同的解法。 示例 2：\n1 2 输入：n = 1 输出：[[\u0026#34;Q\u0026#34;]] 提示：\n1 \u0026lt;= n \u0026lt;= 9 解法\n解法一：dfs+回溯\nhttps://leetcode.cn/problems/n-queens/solutions/398929/nhuang-hou-by-leetcode-solution/\nday135 52. N 皇后 II 题目\nn 皇后问题 研究的是如何将 n 个皇后放置在 n × n 的棋盘上，并且使皇后彼此之间不能相互攻击。\n给你一个整数 n ，返回 n 皇后问题 不同的解决方案的数量。\n示例 1：\n1 2 3 输入：n = 4 输出：2 解释：如上图所示，4 皇后问题存在两个不同的解法。 示例 2：\n1 2 输入：n = 1 输出：1 提示：\n1 \u0026lt;= n \u0026lt;= 9 解法\n解法一：dfs+回溯\nhttps://leetcode.cn/problems/n-queens-ii/solutions/449388/nhuang-hou-ii-by-leetcode-solution/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Solution { int total = 0; public int totalNQueens(int n) { int[] queens = new int[n]; Arrays.fill(queens,-1); Set\u0026lt;Integer\u0026gt; column = new HashSet\u0026lt;\u0026gt;(); Set\u0026lt;Integer\u0026gt; l = new HashSet\u0026lt;\u0026gt;(); Set\u0026lt;Integer\u0026gt; r = new HashSet\u0026lt;\u0026gt;(); dfs(n,0,column,l,r); return total; } public void dfs(int n,int k,Set\u0026lt;Integer\u0026gt; column,Set\u0026lt;Integer\u0026gt; l,Set\u0026lt;Integer\u0026gt; r){ if(k==n){ total++; return; } for(int i=0;i\u0026lt;n;i++){ if(column.contains(i)) continue; if(l.contains(k-i)) continue; if(r.contains(k+i)) continue; column.add(i); l.add(k-i); r.add(k+i); dfs(n,k+1,column,l,r); column.remove(i); l.remove(k-i); r.remove(k+i); } } } day136 191. 位1的个数 题目\n编写一个函数，输入是一个无符号整数（以二进制串的形式），返回其二进制表达式中数字位数为 \u0026lsquo;1\u0026rsquo; 的个数（也被称为汉明重量）。\n提示：\n请注意，在某些语言（如 Java）中，没有无符号整数类型。在这种情况下，输入和输出都将被指定为有符号整数类型，并且不应影响您的实现，因为无论整数是有符号的还是无符号的，其内部的二进制表示形式都是相同的。 在 Java 中，编译器使用二进制补码记法来表示有符号整数。因此，在 示例 3 中，输入表示有符号整数 -3。 示例 1：\n1 2 3 输入：n = 00000000000000000000000000001011 输出：3 解释：输入的二进制串 00000000000000000000000000001011 中，共有三位为 \u0026#39;1\u0026#39;。 示例 2：\n1 2 3 输入：n = 00000000000000000000000010000000 输出：1 解释：输入的二进制串 00000000000000000000000010000000 中，共有一位为 \u0026#39;1\u0026#39;。 示例 3：\n1 2 3 输入：n = 11111111111111111111111111111101 输出：31 解释：输入的二进制串 11111111111111111111111111111101 中，共有 31 位为 \u0026#39;1\u0026#39;。 提示：\n输入必须是长度为 32 的 二进制串 。 进阶：\n如果多次调用这个函数，你将如何优化你的算法？ 解法\n解法一：位运算\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class Solution { // you need to treat n as an unsigned value public int hammingWeight(int n) { int cnt = 0; while(n!=0){ if(n\u0026lt;0) { // 取最高位 if(((n\u0026gt;\u0026gt;31)\u0026amp;1)==1) cnt++; n\u0026lt;\u0026lt;=1; }else { // 取最低位 if((n\u0026amp;1)==1) cnt++; n\u0026gt;\u0026gt;=1; } } return cnt; } } day137 231. 2 的幂 题目\n给你一个整数 n，请你判断该整数是否是 2 的幂次方。如果是，返回 true ；否则，返回 false 。\n如果存在一个整数 x 使得 n == 2x ，则认为 n 是 2 的幂次方。\n示例 1：\n1 2 3 输入：n = 1 输出：true 解释：20 = 1 示例 2：\n1 2 3 输入：n = 16 输出：true 解释：24 = 16 示例 3：\n1 2 输入：n = 3 输出：false 示例 4：\n1 2 输入：n = 4 输出：true 示例 5：\n1 2 输入：n = 5 输出：false 提示：\n-231 \u0026lt;= n \u0026lt;= 231 - 1 **进阶：**你能够不使用循环/递归解决此问题吗？\n解法\n解法一：位运算\n32个bit位，最多只有一个bit位为1才是2的幂，否则不是。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public boolean isPowerOfTwo(int n) { boolean ok =false; if(n\u0026lt;=0) return false; int cnt =0; for(int i=0;i\u0026lt;32;i++){ if((n\u0026amp;(1\u0026lt;\u0026lt;i))!=0) { cnt++; if(cnt\u0026gt;1) return false; } } return true; } } 进阶：O(1)实现\nhttps://leetcode.cn/problems/power-of-two/solutions/796201/2de-mi-by-leetcode-solution-rny3/\n1 2 3 4 5 6 7 8 9 10 11 12 13 // n \u0026amp; (n - 1)移除最低位的1，这样还剩1的话，说明不满足 class Solution { public boolean isPowerOfTwo(int n) { return n \u0026gt; 0 \u0026amp;\u0026amp; (n \u0026amp; (n - 1)) == 0; } } // n \u0026amp; -n 直接获取最低位1. 并且n\u0026gt;0，n \u0026amp; -n = n。所以n为2的幂 class Solution { public boolean isPowerOfTwo(int n) { return n \u0026gt; 0 \u0026amp;\u0026amp; (n \u0026amp; -n) == n; } } day138 200. 岛屿数量 题目\n解法\n解法一：并查集\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class Solution { public int numIslands(char[][] grid) { int m = grid.length,n = grid[0].length; int x = 0; for(int i=0;i\u0026lt;m;i++){ for(int j=0;j\u0026lt;n;j++){ if(grid[i][j]==\u0026#39;0\u0026#39;) x++; } } Uf uf = new Uf(n*m); for(int i=0;i\u0026lt;m;i++){ for(int j=0;j\u0026lt;n;j++){ if(grid[i][j]==\u0026#39;1\u0026#39;) { needUf(grid,i,j,m,n,uf); } } } return uf.count - x; } // 判断当前网格是否需要union. 是岛屿且与前面的连通 public void needUf(char[][] grid,int i,int j,int row,int col,Uf uf){ int[][] dirs = new int[][]{{0,-1},{0,1},{1,0},{-1,0}}; for(int[] dir:dirs){ int newi = i+dir[0],newj = j+dir[1]; if(newi\u0026lt;0||newi\u0026gt;=row||newj\u0026lt;0||newj\u0026gt;=col) continue; if(grid[newi][newj] == \u0026#39;1\u0026#39;) { if(!uf.connected(newi*col+newj,i*col+j)){ uf.union(newi*col+newj,i*col+j); } } } return; } } class Uf{ int count; // 连通分量 int[] parents; // 存储每个节点的父节点 public Uf(int n){ count = n; parents = new int[n]; for(int i=0;i\u0026lt;n;i++){ parents[i] = i; } } // 将两个点连通 public void union(int p,int q){ int rootp = find(p); int rootq = find(q); if(rootp==rootq) return; parents[rootp] = rootq; count--; } public boolean connected(int p,int q){ return find(p)==find(q); } // 查找x的根节点 public int find(int x){ if(x!=parents[x]){ x = find(parents[x]); } return parents[x]; } } day139 547. 省份数量 题目\n有 n 个城市，其中一些彼此相连，另一些没有相连。如果城市 a 与城市 b 直接相连，且城市 b 与城市 c 直接相连，那么城市 a 与城市 c 间接相连。\n省份 是一组直接或间接相连的城市，组内不含其他没有相连的城市。\n给你一个 n x n 的矩阵 isConnected ，其中 isConnected[i][j] = 1 表示第 i 个城市和第 j 个城市直接相连，而 isConnected[i][j] = 0 表示二者不直接相连。\n返回矩阵中 省份 的数量。\n示例 1：\n1 2 输入：isConnected = [[1,1,0],[1,1,0],[0,0,1]] 输出：2 示例 2：\n1 2 输入：isConnected = [[1,0,0],[0,1,0],[0,0,1]] 输出：3 提示：\n1 \u0026lt;= n \u0026lt;= 200 n == isConnected.length n == isConnected[i].length isConnected[i][j] 为 1 或 0 isConnected[i][i] == 1 isConnected[i][j] == isConnected[j][i] 解法\n解法一：并查集\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Solution { public int findCircleNum(int[][] isConnected) { int n=isConnected.length; UF uf = new UF(n); for(int i=0;i\u0026lt;n;i++){ for(int j=i+1;j\u0026lt;n;j++){ // 剪枝：对称矩阵只需要对角线一半即可 if(isConnected[i][j]==1/*\u0026amp;\u0026amp;!uf.connected(i,j)*/){ uf.union(i,j); } } } return uf.cnt; } } class UF{ int cnt; // 连通分量个数 int[] parents; // 存储父节点 public UF(int n){ cnt = n; parents = new int[n]; for(int i=0;i\u0026lt;n;i++) parents[i] = i; } public void union(int a,int b){ int ra = find(a); int rb = find(b); if(ra==rb) return; // 已经连通 parents[ra] = rb; // 没连通，就将其中一个根节点拼上去 // parents[rb] = ra; cnt--; } // 判断a和b是否连通（是否含有相同根节点） public boolean connected(int a, int b){ return find(a)==find(b); } // 寻找根节点 public int find(int x){ if(x!=parents[x]){ parents[x] = find(parents[x]); } return parents[x]; } } day140 2. 两数相加 题目\n给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。\n请你将两个数相加，并以相同形式返回一个表示和的链表。\n你可以假设除了数字 0 之外，这两个数都不会以 0 开头。\n示例 1：\n1 2 3 输入：l1 = [2,4,3], l2 = [5,6,4] 输出：[7,0,8] 解释：342 + 465 = 807. 示例 2：\n1 2 输入：l1 = [0], l2 = [0] 输出：[0] 示例 3：\n1 2 输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] 输出：[8,9,9,9,0,0,0,1] 提示：\n每个链表中的节点数在范围 [1, 100] 内 0 \u0026lt;= Node.val \u0026lt;= 9 题目数据保证列表表示的数字不含前导零 解法\n解法一：模拟\n模拟数学上两数相加法则：低位相加大于10，减10得进位1，模10得本位。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { jw := 0 l3 := \u0026amp;ListNode{} tmp := l3 rs := 0 for l1!=nil\u0026amp;\u0026amp;l2!=nil { rs = l1.Val+l2.Val+jw; tmp.Val = rs%10 jw = rs/10 if(l1.Next!=nil||l2.Next!=nil){ tmp.Next = new(ListNode) tmp = tmp.Next } l1 = l1.Next l2 = l2.Next } for(l1!=nil){ rs = l1.Val+jw tmp.Val = rs%10 jw = rs/10 if(l1.Next!=nil){ tmp.Next = new(ListNode) tmp = tmp.Next } l1=l1.Next } for(l2!=nil){ rs = l2.Val+jw tmp.Val = rs%10 jw = rs/10 if(l2.Next!=nil) { tmp.Next = new(ListNode) tmp = tmp.Next } l2=l2.Next } if(jw!=0){ tmp.Next = new(ListNode) tmp.Next.Val = jw } return l3 } day141 3. 无重复字符的最长子串 题目\n给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。\n示例 1:\n1 2 3 输入: s = \u0026#34;abcabcbb\u0026#34; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026#34;abc\u0026#34;，所以其长度为 3。 示例 2:\n1 2 3 输入: s = \u0026#34;bbbbb\u0026#34; 输出: 1 解释: 因为无重复字符的最长子串是 \u0026#34;b\u0026#34;，所以其长度为 1。 示例 3:\n1 2 3 4 输入: s = \u0026#34;pwwkew\u0026#34; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026#34;wke\u0026#34;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，\u0026#34;pwke\u0026#34; 是一个子序列，不是子串。 提示：\n0 \u0026lt;= s.length \u0026lt;= 5 * 104 s 由英文字母、数字、符号和空格组成 解法\n解法一：滑动窗口\nhttps://leetcode.cn/problems/longest-substring-without-repeating-characters/solutions/227999/wu-zhong-fu-zi-fu-de-zui-chang-zi-chuan-by-leetc-2/\nday142 4. 寻找两个正序数组的中位数 题目\n给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。\n算法的时间复杂度应该为 O(log (m+n)) 。\n示例 1：\n1 2 3 输入：nums1 = [1,3], nums2 = [2] 输出：2.00000 解释：合并数组 = [1,2,3] ，中位数 2 示例 2：\n1 2 3 输入：nums1 = [1,2], nums2 = [3,4] 输出：2.50000 解释：合并数组 = [1,2,3,4] ，中位数 (2 + 3) / 2 = 2.5 提示：\nnums1.length == m nums2.length == n 0 \u0026lt;= m \u0026lt;= 1000 0 \u0026lt;= n \u0026lt;= 1000 1 \u0026lt;= m + n \u0026lt;= 2000 -106 \u0026lt;= nums1[i], nums2[i] \u0026lt;= 106 解法\n解法一：双指针\n这个问题最核心的就是合并两个有序数组。我们使用双指针算法：一个指针指向数组nums1首部，另一个指向数组nums2首部，然后比较大小，最小的放到新数组最前面，然后移动那个最小指针再和其他数组指针比较。如果某个数组元素提前结束，将这个数组的后面元素附加过来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func findMedianSortedArrays(nums1 []int, nums2 []int) float64 { // 合并 nums := make([]int,len(nums1)+len(nums2)) i1:=0 i2:=0 for i:=0;i\u0026lt;len(nums);i++ { if i1\u0026lt;len(nums1)\u0026amp;\u0026amp;i2\u0026lt;len(nums2) { if nums1[i1]\u0026lt;nums2[i2]{ nums[i] = nums1[i1] i1++ }else{ nums[i] = nums2[i2] i2++ } }else if i1\u0026gt;=len(nums1) \u0026amp;\u0026amp; i2 \u0026lt;len(nums2) { nums[i] = nums2[i2] i2++ }else if i1\u0026lt;len(nums1) \u0026amp;\u0026amp; i2 \u0026gt;=len(nums2){ nums[i] = nums1[i1] i1++ } } if len(nums)%2==1{ return float64(nums[len(nums)/2]) } return (0.0 + float64(nums[len(nums)/2]+nums[len(nums)/2-1])) / 2 } day143 5. 最长回文子串 题目\n给你一个字符串 s，找到 s 中最长的回文子串。\n如果字符串的反序与原始字符串相同，则该字符串称为回文字符串。\n示例 1：\n1 2 3 输入：s = \u0026#34;babad\u0026#34; 输出：\u0026#34;bab\u0026#34; 解释：\u0026#34;aba\u0026#34; 同样是符合题意的答案。 示例 2：\n1 2 输入：s = \u0026#34;cbbd\u0026#34; 输出：\u0026#34;bb\u0026#34; 提示：\n1 \u0026lt;= s.length \u0026lt;= 1000 s 仅由数字和英文字母组成 解法\n解法一：动态规划\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func longestPalindrome(s string) string { len:=len(s) if len\u0026lt;2 { return s } maxlen:=1 begin:=0 dp := make([][]bool, len) // 初始化：长度为1的是回文串；长度为2的子串，但是两个字符相等的也是子串 for i := 0; i \u0026lt; len; i++ { dp[i] = make([]bool, len) dp[i][i] = true if i\u0026lt;len-1\u0026amp;\u0026amp;s[i]==s[i+1]{ dp[i][i+1] = true } } // 状态转移：P(i,j)=P(i+1,j−1)∧(Si==Sj) for L:=2;L\u0026lt;=len;L++{ for i:=0;i\u0026lt;len;i++{ j:=i+L-1 if j\u0026gt;=len { break } if s[i] == s[j] \u0026amp;\u0026amp; j-i+1\u0026gt;2 { dp[i][j] = dp[i+1][j-1] } if dp[i][j] \u0026amp;\u0026amp; maxlen\u0026lt;j-i+1{ maxlen = j-i+1 begin = i } } } return s[begin:begin+maxlen] } day144 10. 正则表达式匹配 题目\n给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 '.' 和 '*' 的正则表达式匹配。\n'.' 匹配任意单个字符 '*' 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。\n示例 1：\n1 2 3 输入：s = \u0026#34;aa\u0026#34;, p = \u0026#34;a\u0026#34; 输出：false 解释：\u0026#34;a\u0026#34; 无法匹配 \u0026#34;aa\u0026#34; 整个字符串。 示例 2:\n1 2 3 输入：s = \u0026#34;aa\u0026#34;, p = \u0026#34;a*\u0026#34; 输出：true 解释：因为 \u0026#39;*\u0026#39; 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 \u0026#39;a\u0026#39;。因此，字符串 \u0026#34;aa\u0026#34; 可被视为 \u0026#39;a\u0026#39; 重复了一次。 示例 3：\n1 2 3 输入：s = \u0026#34;ab\u0026#34;, p = \u0026#34;.*\u0026#34; 输出：true 解释：\u0026#34;.*\u0026#34; 表示可匹配零个或多个（\u0026#39;*\u0026#39;）任意字符（\u0026#39;.\u0026#39;）。 提示：\n1 \u0026lt;= s.length \u0026lt;= 20 1 \u0026lt;= p.length \u0026lt;= 20 s 只包含从 a-z 的小写字母。 p 只包含从 a-z 的小写字母，以及字符 . 和 *。 保证每次出现字符 * 时，前面都匹配到有效的字符 解法\n解法一：动态规划\nhttps://leetcode.cn/problems/regular-expression-matching/solutions/296114/shou-hui-tu-jie-wo-tai-nan-liao-by-hyj8/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func isMatch(s string, p string) bool { m, n := len(s), len(p) // if m==0||n==0 {return false} // dp[i][j]表示s[:i]是否与p[:j]匹配 dp := make([][]bool, m+1) for i:=0;i\u0026lt;=m;i++{ dp[i] = make([]bool,n+1) } matched := func(i,j int) bool { return s[i]==p[j]||p[j]==\u0026#39;.\u0026#39; } dp[0][0] = true for j:=1;j\u0026lt;=n;j++{ if p[j-1]==\u0026#39;*\u0026#39; { dp[0][j] = dp[0][j-2] } } for i:=1;i\u0026lt;m+1;i++{ for j:=1;j\u0026lt;n+1;j++{ if matched(i-1,j-1){ dp[i][j] = dp[i-1][j-1] }else{ if p[j-1]==\u0026#39;*\u0026#39;{ if matched(i-1,j-2){// 三种情况 dp[i][j] = dp[i-1][j-1]||dp[i][j-2]||dp[i-1][j] }else{ dp[i][j] = dp[i][j-2] } } } } } return dp[m][n] } day145 21. 合并两个有序链表 题目\n将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。\n示例 1：\n1 2 输入：l1 = [1,2,4], l2 = [1,3,4] 输出：[1,1,2,3,4,4] 示例 2：\n1 2 输入：l1 = [], l2 = [] 输出：[] 示例 3：\n1 2 输入：l1 = [], l2 = [0] 输出：[0] 提示：\n两个链表的节点数目范围是 [0, 50] -100 \u0026lt;= Node.val \u0026lt;= 100 l1 和 l2 均按 非递减顺序 排列 解法\n解法一：双指针\n两个指针分别指向两个链表首部，依次比较首部大小，将小的从所在链表移除并移入到新的链表中，如果某个链表为空，则将另一个非空链表直接拿过来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func mergeTwoLists(list1 *ListNode, list2 *ListNode) *ListNode { dummpy:=\u0026amp;ListNode{} tmp:=dummpy for list1!=nil||list2!=nil { if list1!=nil\u0026amp;\u0026amp;list2!=nil{ if list1.Val\u0026gt;list2.Val { tmp.Next = list2 tmp = tmp.Next list2 = list2.Next }else{ tmp.Next = list1 tmp = tmp.Next list1 = list1.Next } }else if list1==nil\u0026amp;\u0026amp;list2!=nil { tmp.Next = list2 list2 = nil }else if list1!=nil\u0026amp;\u0026amp;list2==nil{ tmp.Next = list1 list1 = nil } } return dummpy.Next } day146 23. 合并 K 个升序链表 题目\n给你一个链表数组，每个链表都已经按升序排列。\n请你将所有链表合并到一个升序链表中，返回合并后的链表。\n示例 1：\n1 2 3 4 5 6 7 8 9 10 输入：lists = [[1,4,5],[1,3,4],[2,6]] 输出：[1,1,2,3,4,4,5,6] 解释：链表数组如下： [ 1-\u0026gt;4-\u0026gt;5, 1-\u0026gt;3-\u0026gt;4, 2-\u0026gt;6 ] 将它们合并到一个有序链表中得到。 1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4-\u0026gt;5-\u0026gt;6 示例 2：\n1 2 输入：lists = [] 输出：[] 示例 3：\n1 2 输入：lists = [[]] 输出：[] 提示：\nk == lists.length 0 \u0026lt;= k \u0026lt;= 10^4 0 \u0026lt;= lists[i].length \u0026lt;= 500 -10^4 \u0026lt;= lists[i][j] \u0026lt;= 10^4 lists[i] 按 升序 排列 lists[i].length 的总和不超过 10^4 解法\n解法一：顺序地两两合并\nk个链表合并可以抽象出两个链表进行合并后再和第三个链表进行两两合并，如此往复\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func mergeKLists(lists []*ListNode) *ListNode { if len(lists)==0{ return nil }else if len(lists)==1{ return lists[0] } var tmp *ListNode for i:=0;i\u0026lt;len(lists);i++ { tmp = mergeTwoList(tmp,lists[i]) } return tmp } func mergeTwoList(l1,l2 *ListNode) *ListNode{ l:=\u0026amp;ListNode{} head :=l for l1!=nil\u0026amp;\u0026amp;l2!=nil{ if l1.Val\u0026lt;l2.Val{ l.Next = l1 l1 = l1.Next }else{ l.Next = l2 l2 = l2.Next } l = l.Next } for l1!=nil { l.Next = l1 l1 = nil } for l2!=nil{ l.Next = l2 l2 = nil } return head.Next } 解法二：分治法\n不断将多个链表分成单个，然后两两配对合并\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func mergeKLists(lists []*ListNode) *ListNode { return split(lists,0,len(lists)-1) } func split(lists []*ListNode,l,r int) *ListNode { if l==r{ return lists[l] } if l\u0026gt;r{ return nil } mid :=(l+r)/2 l1:= split(lists,l,mid) l2:= split(lists,mid+1,r) return mergeTwoList(l1,l2) } func mergeTwoList(l1,l2 *ListNode) *ListNode{ l:=\u0026amp;ListNode{} head :=l for l1!=nil\u0026amp;\u0026amp;l2!=nil{ if l1.Val\u0026lt;l2.Val{ l.Next = l1 l1 = l1.Next }else{ l.Next = l2 l2 = l2.Next } l = l.Next } for l1!=nil { l.Next = l1 l1 = nil } for l2!=nil{ l.Next = l2 l2 = nil } return head.Next } day147 31. 下一个排列 题目\n整数数组的一个 排列 就是将其所有成员以序列或线性顺序排列。\n例如，arr = [1,2,3] ，以下这些都可以视作 arr 的排列：[1,2,3]、[1,3,2]、[3,1,2]、[2,3,1] 。 整数数组的 下一个排列 是指其整数的下一个字典序更大的排列。更正式地，如果数组的所有排列根据其字典顺序从小到大排列在一个容器中，那么数组的 下一个排列 就是在这个有序容器中排在它后面的那个排列。如果不存在下一个更大的排列，那么这个数组必须重排为字典序最小的排列（即，其元素按升序排列）。\n例如，arr = [1,2,3] 的下一个排列是 [1,3,2] 。 类似地，arr = [2,3,1] 的下一个排列是 [3,1,2] 。 而 arr = [3,2,1] 的下一个排列是 [1,2,3] ，因为 [3,2,1] 不存在一个字典序更大的排列。 给你一个整数数组 nums ，找出 nums 的下一个排列。\n必须** 原地 **修改，只允许使用额外常数空间。\n示例 1：\n1 2 输入：nums = [1,2,3] 输出：[1,3,2] 示例 2：\n1 2 输入：nums = [3,2,1] 输出：[1,2,3] 示例 3：\n1 2 输入：nums = [1,1,5] 输出：[1,5,1] 提示：\n1 \u0026lt;= nums.length \u0026lt;= 100 0 \u0026lt;= nums[i] \u0026lt;= 100 解法\n解法一：找数学规律\n推导过程：\n1 2 3 4 5 6 7 8 1,2,3-\u0026gt;1,3,2 1,3,2-\u0026gt;2,1,3 1,4,3,2-\u0026gt;2,1,3,4 2,6,5,4,3,1-\u0026gt;3,6,5,4,2,1-\u0026gt;3,1,2,4,5,6-\u0026gt; 2,6,7,5,4,1-\u0026gt;2,7,1,4,5,6 3,7,5,4,1-\u0026gt;4,7,5,3,1-\u0026gt;4,1,3,7,5 6,5,4,1-\u0026gt;1,4,5,6 1,3,2,4,5,6,9,10,11,15,14,19,16-\u0026gt;1,3,2,4,5,6,9,10,11,15,16,14,19 从右往左看，碰到降序序列(两个就够了)，说明不需要修改降序序列前的顺序，只需要调整降序序列后，需要从后面的序列里挑出第一个大于当前降序序列首部的数字或没有也可以，然后再把后面的升序列反转。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func nextPermutation(nums []int) { if len(nums) \u0026lt;= 1 { return } i := len(nums) - 2 j := len(nums) - 1 k := len(nums) - 1 // 找降序序列（从右往左） for i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026gt;= 0 { if nums[i] \u0026lt; nums[j] { break } i-- j-- } // i及其以后的序列是升序，i和j是降序（从右往左看） if i \u0026gt;= 0 { // 不是最后一个排列 // find: A[i]\u0026lt;A[k] for nums[i] \u0026gt;= nums[k] { k-- } // swap A[i], A[k] nums[i], nums[k] = nums[k], nums[i] } // 反转 for i,j:=j,len(nums)-1;i\u0026lt;j;i,j=i+1,j-1{ nums[i], nums[j] = nums[j], nums[i] } return } day148 32. 最长有效括号 题目\n给你一个只包含 '(' 和 ')' 的字符串，找出最长有效（格式正确且连续）括号子串的长度。\n示例 1：\n1 2 3 输入：s = \u0026#34;(()\u0026#34; 输出：2 解释：最长有效括号子串是 \u0026#34;()\u0026#34; 示例 2：\n1 2 3 输入：s = \u0026#34;)()())\u0026#34; 输出：4 解释：最长有效括号子串是 \u0026#34;()()\u0026#34; 示例 3：\n1 2 输入：s = \u0026#34;\u0026#34; 输出：0 提示：\n0 \u0026lt;= s.length \u0026lt;= 3 * 104 s[i] 为 '(' 或 ')' 解法\n方法一：栈\n这个与普通括号匹配有点不一样，求解子串长度，因此我们只需要在栈中存放下标即可。为了分开不同连续子串括号，我们需要预留)作为分界线。\nhttps://leetcode.cn/problems/longest-valid-parentheses/solutions/314827/shou-hua-tu-jie-zhan-de-xiang-xi-si-lu-by-hyj8/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func longestValidParentheses(s string) int { stack := []int{-1} max:=0 for i:=0;i\u0026lt;len(s);i++{ if s[i]==\u0026#39;( stack = append(stack,i) }else{ stack = stack[:len(stack)-1] if len(stack)==0 { stack = append(stack,i) }else { l := i-stack[len(stack)-1] if l\u0026gt;max{ max = l } } } } return max } 方法二：动态规划\nhttps://leetcode.cn/problems/longest-valid-parentheses/solutions/314683/zui-chang-you-xiao-gua-hao-by-leetcode-solution/\nday149 42. 接雨水 题目\n给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n示例 1：\n1 2 3 输入：height = [0,1,0,2,1,0,1,3,2,1,2,1] 输出：6 解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例 2：\n1 2 输入：height = [4,2,0,3,2,5] 输出：9 提示：\nn == height.length 1 \u0026lt;= n \u0026lt;= 2 * 104 0 \u0026lt;= height[i] \u0026lt;= 105 解法\n官方三解法\nhttps://leetcode.cn/problems/trapping-rain-water/solutions/692342/jie-yu-shui-by-leetcode-solution-tuvc/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func trap(height []int) int { n:=len(height) ans:=0 if n\u0026lt;=1 { return ans } leftdp := make([]int,n) rightdp := make([]int,n) max := func(a,b int) int{ if a\u0026gt;b{ return a } return b } min := func(a,b int) int{ if a\u0026lt;b{ return a } return b } // 初始化 leftdp[0] = height[0] rightdp[n-1] = height[n-1] // 记录状态 for i:=1;i\u0026lt;n;i++{ leftdp[i] = max(leftdp[i-1],height[i]) } for i:=n-2;i\u0026gt;=0;i--{ rightdp[i] = max(rightdp[i+1],height[i]) } // 状态转移 for i:=0;i\u0026lt;n;i++{ ans += min(leftdp[i],rightdp[i]) - height[i] } return ans } day150 48. 旋转图像 题目\n解法\n解法一：找数学规律\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func rotate(matrix [][]int) { reverseColFunc := func(matrix [][]int,j int){ n:=len(matrix) for i:=0;i\u0026lt;n/2;i++{ matrix[i][j],matrix[n-i-1][j] = matrix[n-i-1][j],matrix[i][j] } } for j:=0;j\u0026lt;len(matrix[0]);j++{ reverseColFunc(matrix,j) } n:=len(matrix) m:=len(matrix[0]) for i:=0;i\u0026lt;n;i++{ for j:=i+1;j\u0026lt;m;j++{ matrix[i][j],matrix[j][i] = matrix[j][i],matrix[i][j] } } return } // 先按照列，对每一个列都进行反转 // 再进行矩阵对称变换 反转还可以优化\n反转只需要O(n)\n1 2 3 4 5 6 7 8 9 10 11 12 13 func rotate(matrix [][]int) { n := len(matrix) // 水平翻转 for i := 0; i \u0026lt; n/2; i++ { matrix[i], matrix[n-1-i] = matrix[n-1-i], matrix[i] } // 主对角线翻转 for i := 0; i \u0026lt; n; i++ { for j := 0; j \u0026lt; i; j++ { matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j] } } } 其他解法\nhttps://leetcode.cn/problems/rotate-image/solutions/526980/xuan-zhuan-tu-xiang-by-leetcode-solution-vu3m/\nday151 49. 字母异位词分组 题目\n给你一个字符串数组，请你将 字母异位词 组合在一起。可以按任意顺序返回结果列表。\n字母异位词 是由重新排列源单词的所有字母得到的一个新单词。\n示例 1:\n1 2 输入: strs = [\u0026#34;eat\u0026#34;, \u0026#34;tea\u0026#34;, \u0026#34;tan\u0026#34;, \u0026#34;ate\u0026#34;, \u0026#34;nat\u0026#34;, \u0026#34;bat\u0026#34;] 输出: [[\u0026#34;bat\u0026#34;],[\u0026#34;nat\u0026#34;,\u0026#34;tan\u0026#34;],[\u0026#34;ate\u0026#34;,\u0026#34;eat\u0026#34;,\u0026#34;tea\u0026#34;]] 示例 2:\n1 2 输入: strs = [\u0026#34;\u0026#34;] 输出: [[\u0026#34;\u0026#34;]] 示例 3:\n1 2 输入: strs = [\u0026#34;a\u0026#34;] 输出: [[\u0026#34;a\u0026#34;]] 提示：\n1 \u0026lt;= strs.length \u0026lt;= 104 0 \u0026lt;= strs[i].length \u0026lt;= 100 strs[i] 仅包含小写字母 解法\n解法一：排序\n题目的核心在于如何判断两个字母是“字母异位词”？其实“字母异位词”排序后是相同字母，因此，我们可以借用哈希表来存储并去重：将排序后具有相同字符串的序列合并在一起即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func groupAnagrams(strs []string) [][]string { ans := [][]string{} m := make(map[string][]string,0) for i:=0;i\u0026lt;len(strs);i++{ b:=[]byte(strs[i]) sort.Slice(b, func(i, j int) bool { return b[i] \u0026lt; b[j] }) key:=string(b) m[key] = append(m[key],strs[i]) } for _,v:=range m{ ans = append(ans,v) } return ans } 解法二：计数\nhttps://leetcode.cn/problems/group-anagrams/solutions/520469/zi-mu-yi-wei-ci-fen-zu-by-leetcode-solut-gyoc/\nday152 53. 最大子数组和 题目\n给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。\n子数组 是数组中的一个连续部分。\n示例 1：\n1 2 3 输入：nums = [-2,1,-3,4,-1,2,1,-5,4] 输出：6 解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 示例 2：\n1 2 输入：nums = [1] 输出：1 示例 3：\n1 2 输入：nums = [5,4,-1,7,8] 输出：23 提示：\n1 \u0026lt;= nums.length \u0026lt;= 105 -104 \u0026lt;= nums[i] \u0026lt;= 104 **进阶：**如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的 分治法 求解。\n解法\n官方解法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func maxSubArray(nums []int) int { dp := make([]int,len(nums)) max := func(a,b int) int{ if a\u0026gt;b { return a } return b } dp[0] = nums[0] ans := nums[0] for i:=1;i\u0026lt;len(nums);i++{ dp[i] = max(dp[i-1]+nums[i],nums[i]) ans = max(dp[i],ans) } return ans } // dp[i]表示第i个数结尾的连续子数组的最大和 // dp[i] = max(nums[i],dp[i-1]+nums[i]) // 第i个数可以加入连续子序列，也可以不加入连续子序列，新开始一个新的子序列，主要依据是看谁最大 // 然后，找出所有连续子数组的最大和 day153 75. 颜色分类 题目\n解法\n解法一：排序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func sortColors(nums []int){ // 快速排序 // 分区 partition:=func(nums []int,start,end int)int{ pivot:=nums[end] left:=start right:=end for left\u0026lt;right{ for left\u0026lt;right\u0026amp;\u0026amp;nums[left]\u0026lt;=pivot{ left++ } for left\u0026lt;right\u0026amp;\u0026amp;nums[right]\u0026gt;=pivot{ right-- } // 交换 nums[left],nums[right] = nums[right],nums[left] } nums[left],nums[end] = nums[end],nums[left] return left } var qsort func(nums []int,left ,right int) qsort=func(nums []int,left,right int){ if left\u0026gt;=right{ return } p:=partition(nums,left,right) qsort(nums,left,p-1) qsort(nums,p+1,right) } qsort(nums,0,len(nums)-1) } 解法二：单指针\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func swapColors(colors []int, target int) (countTarget int) { for i, c := range colors { if c == target { colors[i], colors[countTarget] = colors[countTarget], colors[i] countTarget++ } } return } func sortColors(nums []int) { count0 := swapColors(nums, 0) // 把 0 排到前面 swapColors(nums[count0:], 1) // nums[:count0] 全部是 0 了，对剩下的 nums[count0:] 把 1 排到前面 } 解法三：双指针\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func sortColors(nums []int){ // 双指针 n0:=0 n1:=0 num:=0 for i:=0;i\u0026lt;len(nums);i++{ num=nums[i] nums[i]=2 if num\u0026lt;2{ nums[n1]=1 n1++ } if num\u0026lt;1{ nums[n0]=0 n0++ } } } day154 76. 最小覆盖子串\n题目\n给你一个字符串 s 、一个字符串 t 。返回 s 中涵盖 t 所有字符的最小子串。如果 s 中不存在涵盖 t 所有字符的子串，则返回空字符串 \u0026quot;\u0026quot; 。\n注意：\n对于 t 中重复字符，我们寻找的子字符串中该字符数量必须不少于 t 中该字符数量。 如果 s 中存在这样的子串，我们保证它是唯一的答案。 示例 1：\n1 2 3 输入：s = \u0026#34;ADOBECODEBANC\u0026#34;, t = \u0026#34;ABC\u0026#34; 输出：\u0026#34;BANC\u0026#34; 解释：最小覆盖子串 \u0026#34;BANC\u0026#34; 包含来自字符串 t 的 \u0026#39;A\u0026#39;、\u0026#39;B\u0026#39; 和 \u0026#39;C\u0026#39;。 示例 2：\n1 2 3 输入：s = \u0026#34;a\u0026#34;, t = \u0026#34;a\u0026#34; 输出：\u0026#34;a\u0026#34; 解释：整个字符串 s 是最小覆盖子串。 示例 3:\n1 2 3 4 输入: s = \u0026#34;a\u0026#34;, t = \u0026#34;aa\u0026#34; 输出: \u0026#34;\u0026#34; 解释: t 中两个字符 \u0026#39;a\u0026#39; 均应包含在 s 的子串中， 因此没有符合条件的子字符串，返回空字符串。 提示：\nm == s.length n == t.length 1 \u0026lt;= m, n \u0026lt;= 105 s 和 t 由英文字母组成 **进阶：**你能设计一个在 o(m+n) 时间内解决此问题的算法吗？\n解法\n解法一：滑动窗口+哈希表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 func minWindow(s string, t string) string { ori, cnt := map[byte]int{}, map[byte]int{} for i := 0; i \u0026lt; len(t); i++ { ori[t[i]]++ } sLen := len(s) len := math.MaxInt32 ansL, ansR := -1, -1 check := func() bool { for k, v := range ori { if cnt[k] \u0026lt; v { return false } } return true } for l, r := 0, 0; r \u0026lt; sLen; r++ { if r \u0026lt; sLen \u0026amp;\u0026amp; ori[s[r]] \u0026gt; 0 { cnt[s[r]]++ } // 查看窗口是否覆盖t，若覆盖再移动左指针缩小窗口，否则继续移动右指针扩大窗口 for check() \u0026amp;\u0026amp; l \u0026lt;= r { if (r - l + 1 \u0026lt; len) { len = r - l + 1 ansL, ansR = l, l + len } if _, ok := ori[s[l]]; ok { cnt[s[l]] -= 1 } l++ } } if ansL == -1 { return \u0026#34;\u0026#34; } return s[ansL:ansR] } ","permalink":"https://cold-bin.github.io/post/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/","tags":["算法题"],"title":"Leetcode每日一题"},{"categories":["redis"],"contents":"1.初识Redis Redis是一种键值型的NoSql数据库，这里有两个关键字：\n键值型\nNoSql\n其中键值型，是指Redis中存储的数据都是以key、value对的形式存储，而value的形式多种多样，可以是字符串、数值、甚至json：\n而NoSql则是相对于传统关系型数据库而言，有很大差异的一种数据库。\n1.1.认识NoSQL NoSql可以翻译做Not Only Sql（不仅仅是SQL），或者是No Sql（非Sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。\n1.1.1.结构化与非结构化 传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名、字段数据类型、字段约束等等信息，插入的数据必须遵守这些约束：\n而NoSql则对数据库格式没有严格约束，往往形式松散，自由。\n可以是键值型：\n也可以是文档型：\n甚至可以是图格式：\n1.1.2.关联和非关联 传统数据库的表与表之间往往存在关联，例如外键：\n而非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { id: 1, name: \u0026#34;张三\u0026#34;, orders: [ { id: 1, item: { id: 10, title: \u0026#34;荣耀6\u0026#34;, price: 4999 } }, { id: 2, item: { id: 20, title: \u0026#34;小米11\u0026#34;, price: 3999 } } ] } 此处要维护“张三”的订单与商品“荣耀”和“小米11”的关系，不得不冗余的将这两个商品保存在张三的订单文档中，不够优雅。还是建议用业务来维护关联关系。\n1.1.3.查询方式 传统关系型数据库会基于Sql语句做查询，语法有统一标准；\n而不同的非关系数据库查询语法差异极大，五花八门各种各样。\n1.1.4.事务 传统关系型数据库能满足事务ACID的原则。\n而非关系型数据库往往不支持事务，或者不能严格保证ACID的特性，只能实现基本的一致性。\n1.1.5.总结 除了上述四点以外，在存储方式、扩展性、查询性能上关系型与非关系型也都有着显著差异，总结如下：\n存储方式 关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些 扩展性 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦 1.2.认识Redis Redis诞生于2009年全称是Remote Dictionary Server 远程词典服务器，是一个基于内存的键值型NoSQL数据库。\n特征：\n键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性 低延迟，速度快（基于内存、IO多路复用、良好的编码）。 支持数据持久化 支持主从集群、分片集群 支持多语言客户端 作者：Antirez\nRedis的官方网站地址：https://redis.io/\n1.3.安装Redis 大多数企业都是基于Linux服务器来部署项目，而且Redis官方也没有提供Windows版本的安装包。因此课程中我们会基于Linux系统来安装Redis.\n此处选择的Linux版本为CentOS 7.\n1.3.1.依赖库 Redis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖：\n1 yum install -y gcc tcl 1.3.2.上传安装包并解压 然后将课前资料提供的Redis安装包上传到虚拟机的任意目录：\n例如，我放到了/usr/local/src 目录：\n解压缩：\n1 tar -xzf redis-6.2.6.tar.gz 解压后：\n进入redis目录：\n1 cd redis-6.2.6 运行编译命令：\n1 make \u0026amp;\u0026amp; make install 如果没有出错，应该就安装成功了。\n默认的安装路径是在 /usr/local/bin目录下：\n该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中：\nredis-cli：是redis提供的命令行客户端 redis-server：是redis的服务端启动脚本 redis-sentinel：是redis的哨兵启动脚本 1.3.3.启动 redis的启动方式有很多种，例如：\n默认启动 指定配置启动 开机自启 1.3.4.默认启动 安装完成后，在任意目录输入redis-server命令即可启动Redis：\n1 redis-server 如图：\n这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下CTRL + C则Redis停止。不推荐使用。\n1.3.5.指定配置启动 如果要让Redis以后台方式启动，则必须修改Redis配置文件，就在我们之前解压的redis安装包下（/usr/local/src/redis-6.2.6），名字叫redis.conf：\n我们先将这个配置文件备份一份：\n1 cp redis.conf redis.conf.bck 然后修改redis.conf文件中的一些配置：\n1 2 3 4 5 6 # 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0 bind 0.0.0.0 # 守护进程，修改为yes后即可后台运行 daemonize yes # 密码，设置后访问Redis必须输入密码 requirepass 123321 Redis的其它常见配置：\n1 2 3 4 5 6 7 8 9 10 # 监听的端口 port 6379 # 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录 dir . # 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15 databases 1 # 设置redis能够使用的最大内存 maxmemory 512mb # 日志文件，默认为空，不记录日志，可以指定日志文件名 logfile \u0026#34;redis.log\u0026#34; 启动Redis：\n1 2 3 4 # 进入redis安装目录 cd /usr/local/src/redis-6.2.6 # 启动 redis-server redis.conf 停止服务：\n1 2 3 # 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务， # 因为之前配置了密码，因此需要通过 -u 来指定密码 redis-cli -u 123321 shutdown 1.3.6.开机自启 我们也可以通过配置来实现开机自启。\n首先，新建一个系统服务文件：\n1 vi /etc/systemd/system/redis.service 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 [Unit] Description=redis-server After=network.target [Service] Type=forking ExecStart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf PrivateTmp=true [Install] WantedBy=multi-user.target 然后重载系统服务：\n1 systemctl daemon-reload 现在，我们可以用下面这组命令来操作redis了：\n1 2 3 4 5 6 7 8 # 启动 systemctl start redis # 停止 systemctl stop redis # 重启 systemctl restart redis # 查看状态 systemctl status redis 执行下面的命令，可以让redis开机自启：\n1 systemctl enable redis 1.4.Redis桌面客户端 安装完成Redis，我们就可以操作Redis，实现数据的CRUD了。这需要用到Redis客户端，包括：\n命令行客户端 图形化桌面客户端 编程客户端 1.4.1.Redis命令行客户端 Redis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：\n1 redis-cli [options] [commonds] 其中常见的options有：\n-h 127.0.0.1：指定要连接的redis节点的IP地址，默认是127.0.0.1 -p 6379：指定要连接的redis节点的端口，默认是6379 -a 123321：指定redis的访问密码 其中的commonds就是Redis的操作命令，例如：\nping：与redis服务端做心跳测试，服务端正常会返回pong 不指定commond时，会进入redis-cli的交互控制台：\n1.4.2.图形化桌面客户端 GitHub上的大神编写了Redis的图形化桌面客户端，地址：https://github.com/uglide/RedisDesktopManager\n不过该仓库提供的是RedisDesktopManager的源码，并未提供windows安装包。\n在下面这个仓库可以找到安装包：https://github.com/lework/RedisDesktopManager-Windows/releases\n1.4.3.安装 在课前资料中可以找到Redis的图形化桌面客户端：\n解压缩后，运行安装程序即可安装：\n安装完成后，在安装目录下找到rdm.exe文件：\n双击即可运行：\n1.4.4.建立连接 点击左上角的连接到Redis服务器按钮：\n在弹出的窗口中填写Redis服务信息：\n点击确定后，在左侧菜单会出现这个链接：\n点击即可建立连接了。\nRedis默认有16个仓库，编号从0至15. 通过配置文件可以设置仓库数量，但是不超过16，并且不能自定义仓库名称。\n如果是基于redis-cli连接Redis服务，可以通过select命令来选择数据库：\n1 2 # 选择 0号库 select 0 2.Redis常见命令与数据类型 Redis是典型的key-value数据库，key一般是字符串，而value包含很多不同的数据类型：\nRedis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令：\n不同类型的命令称为一个group，我们也可以通过help命令来查看各种不同group的命令：\n接下来，我们就学习常见的五种基本数据类型的相关命令。\n2.1.Redis通用命令 通用指令是部分数据类型的，都可以使用的指令，常见的有：\nKEYS：查看符合模板的所有key\nSCAN cursor [MATCH pattern] [COUNT count] [TYPE type]:\n增量迭代遍历，相较于KEYS命令更友好，因为KEYS会返回所有键数据，如果数据量太大会造成比较大的延时\nSORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern \u0026hellip;]] [ASC|DESC] [ALPHA] [STORE destination]：\n返回或存储 list, set 或sorted set 中的元素。默认是按照数值排序的，并且按照两个元素的双精度浮点数类型值进行比较。\nDEL：删除一个指定的key\nEXISTS：判断key是否存在\nEXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除\nTTL：查看一个KEY的剩余有效期\n通过help [command] 可以查看一个命令的具体用法，例如：\n1 2 3 4 5 6 7 # 查看keys命令的帮助信息： 127.0.0.1:6379\u0026gt; help keys KEYS pattern summary: Find all keys matching the given pattern since: 1.0.0 group: generic 2.1.1.Key结构 Redis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？\n例如，需要存储用户、商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？\n我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：\nRedis的key允许有多个单词形成层级结构，多个单词之间用\u0026rsquo;:\u0026lsquo;隔开，格式如下：\n1 项目名:业务名:类型:id 这个格式并非固定，也可以根据自己的需求来删除或添加词条。这样以来，我们就可以把不同类型的数据区分开了。从而避免了key的冲突问题。\n例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key：\nuser相关的key：heima:user:1\nproduct相关的key：heima:product:1\n如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储：\nKEY VALUE heima:user:1 {\u0026ldquo;id\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;Jack\u0026rdquo;, \u0026ldquo;age\u0026rdquo;: 21} heima:product:1 {\u0026ldquo;id\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;小米11\u0026rdquo;, \u0026ldquo;price\u0026rdquo;: 4999} 并且，在Redis的桌面客户端中，还会以相同前缀作为层级结构，让数据看起来层次分明，关系清晰：\n2.2.String类型 String类型，也就是字符串类型，是Redis中最简单的存储类型。\n其value是字符串，不过根据字符串的格式不同，又可以分为3类：\nstring：普通字符串 int：整数类型，可以做自增、自减操作 float：浮点类型，可以做自增、自减操作 不管是哪种格式，底层都是字节数组形式存储，只不过是编码方式不同。字符串类型的最大空间不能超过512m.\n2.2.1.String的常见命令 String的常见命令有：\nSET：添加或者修改已经存在的一个String类型的键值对 GET：根据key获取String类型的value MSET：批量添加多个String类型的键值对 MGET：根据多个key获取多个String类型的value INCR：让一个整型的key自增1 INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT：让一个浮点类型的数字自增并指定步长 SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX：添加一个String类型的键值对，并且指定有效期 2.3.Hash类型 Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。\nString结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便：\nHash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD：\nHash的常见命令有：\nHSET key field value：添加或者修改hash类型key的field的值 HGET key field：获取一个hash类型key的field的值 HDEL：删除一个或多个哈希表字段 HEXISTS：查看哈希表 key 中，指定的字段是否存在 HMSET：批量添加多个hash类型key的field的值 HMGET：批量获取多个hash类型key的field的值 HGETALL：获取一个hash类型的key中的所有的field和value HSCAN：迭代哈希表中的键值对（当然，一般hash不会存有巨量键值对，所以，HSCAN的意义不是很大） HKEYS：获取一个hash类型的key中的所有的field HLEN：获取哈希表中字段的数量 HSTRLEN：返回哈希表 key 中， 与给定域 field 相关联的值的字符串长度 HVALS：获取哈希表中所有值 HINCRBY:让一个hash类型key的字段值自增并指定步长 HINCRBYFLOAT：为哈希表 key 中的指定字段的浮点数值加上增量 increment HSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行 2.4.List类型 Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。\n特征也与LinkedList类似：\n有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。\nList的常见命令有：\nLINDEX: 返回列表 key 里索引 index 位置存储的元素。（支持负数倒序索引，-1表示倒数第一个，-2表示倒数第二个） LPUSH key element \u0026hellip; ：向列表左侧插入一个或多个元素 LPOP key：移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element \u0026hellip; ：向列表右侧插入一个或多个元素 RPOP key：移除并返回列表右侧的第一个元素 LRANGE key star end：返回一段角标范围内的所有元素 BLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil. 阻塞版本操作 2.5.Set类型 Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征：\n无序\n元素不可重复\n查找快\n支持交集、并集、差集等功能。这些功能很契合一些业务。\nSet的常见命令有：\nSADD key member \u0026hellip; ：向set中添加一个或多个元素 SREM key member \u0026hellip; : 移除set中的指定元素 SCARD key： 返回set中元素的个数 SISMEMBER key member：判断一个元素是否存在于set中 SMEMBERS：获取set中的所有元素 SINTER key1 key2 \u0026hellip; ：求key1与key2的交集 SDIFF：返回第一个集合和其他集合的差集 SDIFFSTORE：SDIFF过后再存储到destination SINTER：返回所有给定集合的成员交集。后缀-store就类似上面 SUINON：返回所有给定集合的并集。后缀-store就类似上面 SSCAN：迭代集合中的元素。当需要取出集合数据量巨大时，也可以考虑使用SSCAN迭代读取 例如两个集合：s1和s2:\n求交集：SINTER s1 s2\n求s1与s2的不同：SDIFF s1 s2\n练习：\n将下列数据用Redis的Set集合来存储： 张三的好友有：李四、王五、赵六 李四的好友有：王五、麻子、二狗 利用Set的命令实现下列功能： 计算张三的好友有几人 计算张三和李四有哪些共同好友 查询哪些人是张三的好友却不是李四的好友 查询张三和李四的好友总共有哪些人 判断李四是否是张三的好友 判断张三是否是李四的好友 将李四从张三的好友列表中移除 2.6.SortedSet类型 Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。\nSortedSet具备下列特性：\n可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。\nSortedSet的常见命令有：\nZADD key [NX|XX] [GT|LT] [CH] [INCR] score member [score member \u0026hellip;]：添加一个或多个元素到sorted set ，如果已经存在则更新其score值\nZADD 支持参数，参数位于 key 名字和第一个 score 参数之间:\nXX: 仅更新存在的成员，不添加新成员。 NX: 不更新存在的成员。只添加新成员。 LT: 更新新的分值比当前分值小的成员，不存在则新增。 GT: 更新新的分值比当前分值大的成员，不存在则新增。 CH: 返回变更成员的数量。变更的成员是指 新增成员 和 score值更新的成员，命令指明的和之前score值相同的成员不计在内。 注意: 在通常情况下，ZADD返回值只计算新添加成员的数量。 INCR: ZADD 使用该参数与 ZINCRBY 功能一样。一次只能操作一个score-element对。 注意: GT, LT 和 NX 三者互斥不能同时使用。\nZREM key member：删除sorted set中的一个指定元素\nZSCORE key member : 获取sorted set中的指定元素的score值\nZRANK key member：获取sorted set 中的指定元素的排名\nZCARD key：获取sorted set中的元素个数\nZCOUNT key min max：统计score值在给定范围内的所有元素的个数。注意时间复杂度达到了O(logN)\nZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值\nZRANGE key min max：按照score排序后，获取指定排名范围内的元素\nZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素\nZDIFF、ZINTER、ZUNION：求差集、交集、并集\n注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如：\n升序获取sorted set 中的指定元素的排名：ZRANK key member\n降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber\n练习题：\n将班级的下列学生得分存入Redis的SortedSet中：\nJack 85, Lucy 89, Rose 82, Tom 95, Jerry 78, Amy 92, Miles 76\n并实现下列功能：\n删除Tom同学 获取Amy同学的分数 获取Rose同学的排名 查询80分以下有几个学生 给Amy同学加2分 查出成绩前3名的同学 查出成绩80分以下的所有同学 2.7.HyperLogLog类型 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。\n在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 264 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\n但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\n什么是基数?\n比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。\n常用命令：\nPFADD key element [element \u0026hellip;]：添加指定元素到 HyperLogLog 中。 PFCOUNT key [key \u0026hellip;]：返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey \u0026hellip;]：将多个 HyperLogLog 合并为一个 HyperLogLog（并集） 2.8.GEO类型 Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，该功能在 Redis 3.2 版本新增。\nRedis GEO 操作方法有：\nGEOADD key longitude latitude member [longitude latitude member \u0026hellip;]：添加地理位置的坐标。\nGEOPOS key member [member \u0026hellip;]：获取地理位置的坐标。\nGEODIST key member1 member2 [m|km|ft|mi]：计算两个位置之间的距离。\n最后一个距离单位参数说明：\nm ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]：根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。\n参数说明：\nm ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺 WITHDIST: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 WITHCOORD: 将位置元素的经度和维度也一并返回。 WITHHASH: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 COUNT 限定返回的记录数。 ASC: 查找结果根据距离从近到远排序。 DESC: 查找结果根据从远到近排序。 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]：根据储存在位置集合里面的某个地点获取指定范围内的地理位置集合。\n参数说明：\nm ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 WITHDIST: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 WITHCOORD: 将位置元素的经度和维度也一并返回。 WITHHASH: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 COUNT 限定返回的记录数。 ASC: 查找结果根据距离从近到远排序。 DESC: 查找结果根据从远到近排序。 GEOHASH key member [member \u0026hellip;]：返回一个或多个位置对象的 geohash 值。\n2.9.Stream类型 Redis Stream 是 Redis 5.0 版本新增加的数据结构。\nStream 实际上是一个具有消息发布/订阅功能的组件，也就常说的消息队列。其实这种类似于 broker/consumer(生产者/消费者)的数据结构很常见，比如 RabbitMQ 消息中间件、Celery 消息中间件，以及 Kafka 分布式消息系统等，而 Redis Stream 正是借鉴了 Kafaka 系统。\n1) 优点\nStrean 除了拥有很高的性能和内存利用率外, 它最大的特点就是提供了消息的持久化存储，以及主从复制功能，从而解决了网络断开、Redis 宕机情况下，消息丢失的问题，即便是重启 Redis，存储的内容也会存在。\n2) 流程\nStream 消息队列主要由四部分组成，分别是：消息本身、生产者、消费者和消费组，对于前述三者很好理解，下面了解什么是消费组。\n一个 Stream 队列可以拥有多个消费组，每个消费组中又包含了多个消费者，组内消费者之间存在竞争关系。当某个消费者消费了一条消息时，同组消费者，都不会再次消费这条消息。被消费的消息 ID 会被放入等待处理的 Pending_ids 中。每消费完一条信息，消费组的游标就会向前移动一位，组内消费者就继续去争抢下消息。\nRedis Stream 的结构如下所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容：\n每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。\n上图解析：\nConsumer Group ：消费组，使用 XGROUP CREATE 命令创建，一个消费组有多个消费者(Consumer)。 last_delivered_id ：游标，每个消费组会有个游标 last_delivered_id ，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。 pending_ids ：消费者(Consumer)的状态变量，作用是维护消费者的未确认的 id。 pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack (Acknowledge character：确认字符）。 消息队列相关命令：\nXADD - 添加消息到末尾 XTRIM - 对流进行修剪，限制长度 XDEL - 删除消息 XLEN - 获取流包含的元素数量，即消息长度 XRANGE - 获取消息列表，会自动过滤已经删除的消息 XREVRANGE - 反向获取消息列表，ID 从大到小 XREAD - 以阻塞或非阻塞方式获取消息列表 消费者组相关命令：\nXGROUP CREATE - 创建消费者组 XREADGROUP GROUP - 读取消费者组中的消息 XACK - 将消息标记为\u0026quot;已处理\u0026quot; XGROUP SETID - 为消费者组设置新的最后递送消息ID XGROUP DELCONSUMER - 删除消费者 XGROUP DESTROY - 删除消费者组 XPENDING - 显示待处理消息的相关信息 XCLAIM - 转移消息的归属权 XINFO - 查看流和消费者组的相关信息； XINFO GROUPS - 打印消费者组的信息； XINFO STREAM - 打印流信息 2.10.bitmap位图操作 在平时开发过程中，经常会有一些 bool 类型数据需要存取。比如记录用户一年内签到的次数，签了是 1，没签是 0。如果使用 key-value 来存储，那么每个用户都要记录 365 次，当用户成百上亿时，需要的存储空间将非常巨大。为了解决这个问题，Redis 提供了位图结构。\n位图（bitmap）同样属于 string 数据类型。Redis 中一个字符串类型的值最多能存储 512 MB 的内容，每个字符串由多个字节组成，每个字节又由 8 个 Bit 位组成。位图结构正是使用“位”来实现存储的，它通过将比特位设置为 0 或 1来达到数据存取的目的，这大大增加了 value 存储数量，它存储上限为2^32 。\n位图本质上就是一个普通的字节串，也就是 bytes 数组。您可以使用getbit/setbit命令来处理这个位数组，位图的结构如下所示：\n位图适用于一些特定的应用场景，比如用户签到次数、或者登录次数等。上图是表示一位用户 10 天内来网站的签到次数，1 代表签到，0 代表未签到，这样可以很轻松地统计出用户的活跃程度。相比于直接使用字符串而言，位图中的每一条记录仅占用一个 bit 位，从而大大降低了内存空间使用率。\nRedis 官方也做了一个实验，他们模拟了一个拥有 1 亿 2 千 8 百万用户的系统，然后使用 Redis 的位图来统计“日均用户数量”，最终所用时间的约为 50ms，且仅仅占用 16 MB内存。\n2.10.1.位图应用原理 某网站要统计一个用户一年的签到记录，若用 sring 类型存储，则需要 365 个键值对。若使用位图存储，用户签到就存 1，否则存 0。最后会生成 11010101\u0026hellip; 这样的存储结果，其中每天的记录只占一位，一年就是 365 位，约为 46 个字节。如果只想统计用户签到的天数，那么统计 1 的个数即可。\n位图操作的优势，相比于字符串而言，它不仅效率高，而且还非常的节省空间。\nRedis 的位数组是自动扩展的，如果设置了某个偏移位置超出了现有的内容范围，位数组就会自动扩充。\n下面设置一个名为 a 的 key，我们对这个 key 进行位图操作，使得 a 的对应的 value 变为“he”。\n首先我们分别获取字符“h”和字符“e”的八位二进制码，如下所示：\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; bin(ord(\u0026#34;h\u0026#34;)) \u0026#39;0b1101000\u0026#39; \u0026gt;\u0026gt;\u0026gt; bin(ord(\u0026#34;e\u0026#34;)) \u0026#39;0b1100101\u0026#39; 接下来，只要对需值为 1 的位进行操作即可。如下图所示：\n把 h 和 e 的二进制码连接在一起，第一位的下标是 0，依次递增至 15，然后将数字为 1 的位置标记出来，得到 1/2/4/9/10/13/15，我们把这组数字称为位的“偏置数”，最后按照上述偏置数对字符 a 进行如下位图操作。注意，key 的初始二进制位全部为 0。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 C:\\Users\\Administrator\u0026gt;redis-cli 127.0.0.1:6379\u0026gt; SETBIT a 1 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT a 2 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT a 4 1 (integer) 0 127.0.0.1:6379\u0026gt; get hello \u0026#34;h\u0026#34; 127.0.0.1:6379\u0026gt; SETBIT a 9 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT a 10 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT a 13 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT a 15 1 (integer) 0 127.0.0.1:6379\u0026gt; get hello \u0026#34;he\u0026#34; 从上述示例可以得出，位图操作会自动对 key 进行扩容。\n如果对应位的字节是不可以被打印的，那么 Redis 会以该字符的十六进制数来表示它，如下所示：\n1 2 3 4 5 127.0.0.1:6379\u0026gt; SETBIT b 0 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT b 1 1 (integer) 0 127.0.0.1:6379\u0026gt; get b \u0026#34;\\xc0\u0026#34; 常用命令：\nSETBIT key offset value：用来设置或者清除某一位上的值，其返回值是原来位上存储的值。key 在初始状态下所有的位都为 0 GETBIT key offset：用来获取某一位上的值。当偏移量 offset 比字符串的长度大，或者当 key 不存在时，返回 0。 BITCOUNT key [start end]：统计指定位区间上，值为 1 的个数。通过指定的 start 和 end 参数，可以让计数只在特定的字节上进行。start 和 end 参数和 GETRANGE 命令的参数类似，都可以使用负数，比如 -1 表示倒数第一个位， -2 表示倒数第二个位。 3.Redis的Java客户端 在Redis官网中提供了各种语言的客户端，地址：https://redis.io/docs/clients/\n其中Java客户端也包含很多：\n标记为*的就是推荐使用的java客户端，包括：\nJedis和Lettuce：这两个主要是提供了Redis命令对应的API，方便我们操作Redis，而SpringDataRedis又对这两种做了抽象和封装，因此我们后期会直接以SpringDataRedis来学习。 Redisson：是在Redis基础上实现了分布式的可伸缩的java数据结构，例如Map、Queue等，而且支持跨进程的同步机制：Lock、Semaphore等待，比较适合用来实现特殊的功能需求。 3.1.Jedis客户端 Jedis的官网地址： https://github.com/redis/jedis\n3.1.1.快速入门 我们先来个快速入门：\n1）引入依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;!--jedis--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--单元测试--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.7.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 2）建立连接\n新建一个单元测试类，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\u0026#34;192.168.150.101\u0026#34;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\u0026#34;123321\u0026#34;); // 3.选择库 jedis.select(0); } 3）测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Test void testString() { // 存入数据 String result = jedis.set(\u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); System.out.println(\u0026#34;result = \u0026#34; + result); // 获取数据 String name = jedis.get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } @Test void testHash() { // 插入hash数据 jedis.hset(\u0026#34;user:1\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;Jack\u0026#34;); jedis.hset(\u0026#34;user:1\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;); // 获取 Map\u0026lt;String, String\u0026gt; map = jedis.hgetAll(\u0026#34;user:1\u0026#34;); System.out.println(map); } 4）释放资源\n1 2 3 4 5 6 @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } 3.1.2.连接池 Jedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用Jedis连接池代替Jedis的直连方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package com.heima.jedis.util; import redis.clients.jedis.*; public class JedisConnectionFactory { private static JedisPool jedisPool; static { // 配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); // 创建连接池对象，参数：连接池配置、服务端ip、服务端端口、超时时间、密码 jedisPool = new JedisPool(poolConfig, \u0026#34;192.168.150.101\u0026#34;, 6379, 1000, \u0026#34;123321\u0026#34;); } public static Jedis getJedis(){ return jedisPool.getResource(); } } 3.2.SpringDataRedis客户端 SpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis\n提供了对不同Redis客户端的整合（Lettuce和Jedis） 提供了RedisTemplate统一API来操作Redis 支持Redis的发布订阅模型 支持Redis哨兵和Redis集群 支持基于Lettuce的响应式编程 支持基于JDK、JSON、字符串、Spring对象的数据序列化及反序列化 支持基于Redis的JDKCollection实现 SpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中：\n3.2.1.快速入门 SpringBoot已经提供了对SpringDataRedis的支持，使用非常简单。\n首先，新建一个maven项目，然后按照下面步骤执行：\n1）引入依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.7\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.heima\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redis-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;redis-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--redis依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--common-pool--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--Jackson依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 2）配置Redis 1 2 3 4 5 6 7 8 9 10 11 spring: redis: host: 192.168.150.101 port: 6379 password: 123321 lettuce: pool: max-active: 8 max-idle: 8 min-idle: 0 max-wait: 100ms 3）注入RedisTemplate 因为有了SpringBoot的自动装配，我们可以拿来就用：\n1 2 3 4 5 6 @SpringBootTest class RedisStringTests { @Autowired private RedisTemplate redisTemplate; } 4）编写测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @SpringBootTest class RedisStringTests { @Autowired private RedisTemplate edisTemplate; @Test void testString() { // 写入一条String数据 redisTemplate.opsForValue().set(\u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); // 获取string数据 Object name = stringRedisTemplate.opsForValue().get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } } 3.2.2.自定义序列化 RedisTemplate可以接收任意Object作为值写入Redis：\n只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的：\n缺点：\n可读性差 内存占用较大 我们可以自定义RedisTemplate的序列化方式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory connectionFactory){ // 创建RedisTemplate对象 RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; } } 这里采用了JSON序列化来代替默认的JDK序列化方式。最终结果如图：\n整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。\n3.2.3.StringRedisTemplate 为了节省内存空间，我们可以不使用JSON序列化器来处理value，而是统一使用String序列化器，要求只能存储String类型的key和value。当需要存储Java对象时，手动完成对象的序列化和反序列化。\n因为存入和读取时的序列化及反序列化都是我们自己实现的，SpringDataRedis就不会将class信息写入Redis了。\n这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。\n省去了我们自定义RedisTemplate的序列化方式的步骤，而是直接使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Autowired private StringRedisTemplate stringRedisTemplate; // JSON序列化工具 private static final ObjectMapper mapper = new ObjectMapper(); @Test void testSaveUser() throws JsonProcessingException { // 创建对象 User user = new User(\u0026#34;虎哥\u0026#34;, 21); // 手动序列化 String json = mapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(\u0026#34;user:200\u0026#34;, json); // 获取数据 String jsonUser = stringRedisTemplate.opsForValue().get(\u0026#34;user:200\u0026#34;); // 手动反序列化 User user1 = mapper.readValue(jsonUser, User.class); System.out.println(\u0026#34;user1 = \u0026#34; + user1); } ","permalink":"https://cold-bin.github.io/post/redis%E5%9F%BA%E7%A1%80/","tags":["redis各大数据类型","bitmap","jedis客户端"],"title":"Redis基础"},{"categories":["汇编"],"contents":"[toc]\n多个段程序 程序中对段名的引用，将被编译器处理为一个表示段地址的数值。\n1 2 3 mov ax, data mov ds, ax mov bx, ds:[6] 在代码段中使用数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ;计算 8 个数据的和存到 ax 寄存器 assume cs:code code segment dw 0123h,0456h,0789h,0abch,0defh,0fedh,0cbah,0987h ;define word 定义8个字形数据 start:\tmov bx, 0 ;标号start mov ax, 0 mov cx, 8 s:\tadd ax, cs:[bx] add bx, 2 loop s mov ax, 4c00h int 21h code ends end start ;end除了通知编译器程序结束外，还可以通知编译器程序的入口在什么地方 ;用end指令指明了程序的入口在标号start处，也就是说，“mov bx，0”是程序的第一条指令。 在代码段中使用栈 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ;利用栈，将程序中定义的数据逆序存放。 assume cs:codesg codesg segment dw 0123h，0456h，0789h，0abch，0defh，0fedh，0cbah，0987h ; 0-15单元 dw 0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0 ; 16-47单元作为栈使用 start:\tmov ax, cs mov ss, ax mov sp, 30h ;将设置栈顶ss:sp指向栈底cs:30。 30h = 48d mov bx, 0 mov cx, 8 s:\tpush cs:[bx] add bx, 2 loop s ;以上将代码段0~15单元中的8个字型数据依次入栈 mov bx, 0 mov cx, 8 s0:\tpop cs:[bx]\tadd bx，2 loop s0 ;以上依次出栈8个字型数据到代码段0~15单元中 mov ax，4c00h int 21h codesg ends end start\t;指明程序的入口在start处 将数据、代码、栈放入不同的段 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 assume cs:code,ds:data,ss:stack data segment dw 0123h,0456h,0789h,0abch,0defh,0fedh,0cbah,0987h ;0-15单元 data ends stack segment dw 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 ;0-31单元 stack ends code segment start:\tmov ax, stack;将名称为“stack”的段的段地址送入ax mov ss, ax mov sp, 20h ;设置栈顶ss:sp指向stack:20。 20h = 32d mov ax, data ;将名称为“data”的段的段地址送入ax mov ds, ax ;ds指向data段 mov bx, 0 ;ds:bx指向data段中的第一个单元 mov cx, 8 s:\tpush [bx] add bx, 2 loop s ;以上将data段中的0~15单元中的8个字型数据依次入栈 mov bx, 0 mov cx, 8 s0:\tpop [bx] add bx, 2 loop s0 ;以上依次出栈8个字型数据到data段的0~15单元中 mov ax, 4c00h int 21h code ends end start ;“end start”说明了程序的入口，这个入口将被写入可执行文件的描述信息， ;可执行文件中的程序被加载入内存后，CPU的CS:IP被设置指向这个入口，从而开始执行程序中的第一条指令 ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/","tags":["汇编里的栈"],"title":"包含多个段的程序"},{"categories":["汇编"],"contents":"[bx] 1、[bx]是什么呢？\n和[0]类似，[0]表示内存单元，它的偏移地址是0；\n2、内存单元的描述\n我们要完整地描述一个内存单元，需要两种信息： （1）内存单元的地址； （2）内存单元的长度（类型）；\n用[0]表示一个内存单元时，0表示单元的偏移地址，段地址默认在 ds 中，单元的长度（类型）可以由具体指令中的其他操作对象（比如说寄存器）指出；\n[bx]同样也表示一个内存单元，它的偏移地址在bx中，比如下面的指令：\nmov ax,[bx] mov al,[bx] 3、 描述性符号“()”\n为了描述上的简洁，以后将使用一个描述性的符号 “() ”来表示一个寄存器或一个内存单元中的内容。比如：\n（1）ax 中的内容为0010H，我们可以这样来描述：(ax)=0010H； （2）2000:1000 处的内容为0010H，我们可以这样来描述：(21000H)=0010H； （3）对于 mov ax,[2] 的功能，我们可以这样来描述：(ax)=((ds)*16+2)； （4）对于 mov [2],ax 的功能，我们可以这样来描述：((ds)*16+2)=(ax)； （5）对于 add ax,2 的功能，我们可以这样来描述：(ax)=(ax)+2； （6）对于 add ax,bx 的功能，我们可以这样来描述：(ax)=(ax)+(bx)； （7）对于 push ax 的功能，我们可以这样来描述：(sp) = (sp)-2 ((ss)*16＋(sp))=(ax) （8）对于 pop ax 的功能，我们可以这样来描述：(ax)=((ss)*16+(sp)) (sp)=(sp)+2\n4、约定符号 idata 表示常量\n我们在 Debug 中写过类似的指令：mov ax,[0]，表示将 ds:0 处的数据送入 ax 中。指令中，在“[…]”里用一个常量0表示内存单元的偏移地址。以后，我们用 idata 表示常量。\n例如：\nmov ax,[idata] 就代表 mov ax,[1]、mov ax,[2]、mov ax,[3] 等； mov bx,idata就代表 mov bx,1、mov bx,2、mov bx,3 等； mov ds,idata就代表 mov ds,1、mov ds,2 等； 我们之前的用法都是非法指令；\n5、[bx]\n1 mov [bx],ax 功能：bx 中存放的数据作为一个偏移地址 EA，段地址 SA 默认在 ds 中，将 ax 中的数据送入内存 SA:EA 处，即：(ds*16 +(bx)) = (ax)；\n问题：程序和内存中的情况如下图所示，写出程序执行后，21000H~21007H 单元中的内容。 （1）先看一下程序的前三条指令：\n1 2 3 mov ax,2000H mov ds,ax mov bx,1000H 这三条指令执行后，ds=2000H,bx=1000H；\n（2）再看第四条指令：mov ax,[bx]，\n指令执行前： ds=2000H，bx=1000H，则 mov ax,[bx] 将把内存2000:1000处的字型数据送入 ax 中； 指令执行后: ax=00beH； （3）再看第五六两条指令：\n1 2 inc bx inc bx 指令执行前： bx=1000H； 指令执行后: bx=0002H； （4）再看第七条指令：mov [bx],ax，\n指令执行前： ds=2000H，bx=1002H，则 mov [bx],ax 将把 ax 中的数据送入内存2000:1002处； 指令执行后: 2000:1002单元的内容为BE，2000:1003单元的内容为00； 之后的操作类似，最终结果： Loop指令 这个指令和循环有关；\n1、指令的格式是：loop 标号，CPU 执行 loop 指令的时候，要进行两步操作：\n(cx)=(cx)-1; 判断 cx 中的值，若不为零，则转至标号处执行；程序若为零，则向下执行。 2、通常，loop 指令实现循环，cx 中存放循环的次数；\n问题：编程计算212；\n1 2 3 4 5 6 7 8 9 10 assume cs:code code segment mov ax,2 mov cx,11 s: add ax,ax loop s mov ax,4c00h int 21h code ends end 分析：\n（1）标号：在汇编语言中，标号代表一个地址，此程序中有一个标号 s 。它实际上标识了一个地址，这个地址处有一条指令：add ax,ax；\n（2）loop s：CPU 执行 loop s 的时候，要进行两步操作：\n(cx)=(cx)-1； 判断 cx 中的值，不为0则转至标号 s 所标识的地址处执行(这里的指令是 add ax,ax)，如果为0则执行下一条指令(下一条指令是 mov ax,4c00h)； 总结：\n（1）在 cx 中存放循环次数； （2）loop 指令中的标号所标识地址要在前面； （3）要循环执行的程序段，要写在标号和 loop 指令的中间；\n用 cx 和 loop 指令相配合实现循环功能的程序框架如下：\n1 2 3 4 mov cx,循环次数 s: 循环执行的程序段 loop s 3、在 Debug 中跟踪供 loop 指令实现的循环程序\n注意：在汇编程序中，数据不能以字母开头，如果要输入像 FFFFH 这样的数，则要在前面添加一个0；\n在 debug 程序中引入 G 命令和 P 命令：\nG命令\nG命令如果后面不带参数，则一直执行程序，直到程序结束； G命令后面如果带参数，则执行到 ip 为那个参数地址停止； P命令\nT命令相当于单步进入（step into）； P命令相当于单步通过（step over）； Debug 和汇编编译器 Masm 对指令的不同处理 1、在 debug 中，可以直接用指令 mov ax,[0] 将偏移地址为0号单元的内容赋值给 ax；\n2、但通过 masm 编译器，mov ax,[0] 会被编译成 mov ax,0；\n要写成这样才能实现：mov ax,ds:[0]；\n也可以写成这样：\n1 2 mov bx,0 mov ax,[bx] 或者\n1 mov ax,ds:[bx] loop 和 [bx] 的联合应用 计算 ffff:0~ffff:b 单元中的数据的和，结果存储在 dx 中：\n1、注意两个问题：\n12个8位数据加载一起，最后的结果可能会超出8位（越界），故要用16位寄存器存放结果； 将一个8位的数据加入到16位寄存器中，类型不匹配，8位的数据不能与16位相加； 2、【解决办法】\n把原来8位的数据，先通过通用寄存器 ax，将它们转化成16位的； 3、代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 assume cs:codesg codesg segment start: ;指定数据段 mov ax,0ffffh mov ds,ax ;初始化 mov ax,0 mov dx,0 mov bx,0 ;指定循环次数 12次 mov cx,0ch circ: ;把8位数据存入al中,即ax中存放的是[bx]转化之后的16位数据，前8位都是0 mov al,[bx] ;进行累加 add dx,ax ;bx自增，变化内存的偏移地址 inc bx loop circ ;程序返回 mov ax,4c00h int 21H codesg ends end start 段前缀 指令 mov ax,[bx] 中，内存单元的偏移地址由 bx 给出，而段地址默认在 ds 中；\n我们可以在访问内存单元的指令中，显式地给出内存单元的段地址所在的段寄存器，比如 mov ax,ds:[0]，mov ax,ds:[bx] 这里的 ds 就叫做【段前缀】；\n一段安全的空间 8086模式中，随意向一段内存空间写入内容是很危险的，因为这段空间中可能存放着【重要的系统数据或代码】；\n在一般的 PC 机中，DOS 方式下，DOS 和其他合法的程序一般都不会使用【0:200~0:2FF】的256个字节的空间。所以，我们使用这段空间是安全的；\n段前缀的使用 将内存 ffff:0~ffff:b 段元中的数据拷贝到 0:200~0:20b 单元中；\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 assume cs:code code segment mov bx,0 ; (bx)=0，偏移地址从0开始 mov cx,12 ; (cx)=12，循环12次 s: mov ax,offffh mov ds,ax ; (ds)=0ffffh mov dl,[bx] ; (dl)= ((ds)*16+(bx))，将ffff:bx中的数据送入dl mov ax,0020h mov ds,ax ; (ds)=0020h mov [bx],dl ; ((ds)*16+(bx))=(dl)，将中dl的数据送入0020:bx inc bx ; (bx)=(bx)+1 loop s mov ax,4c00h int 21h code ends end 由于上述效率不高，因此进行如下优化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 assume cs:code code segment mov ax,offffh mov ds,ax ; (ds)=0ffffh mov ax,0020h mov es,ax ; (es)=0020h mov bx,0 ; (bx)=0，此时ds:bx指向ffff:0，es:bx指向0020:0 mov cx,12 ; (cx)=12，循环12次 s: mov dl,[bx] ; (dl)= ((ds)*16+(bx))，将ffff:bx中的数据送入dl mov es:[bx],dl ; ((es)*16+(bx))=(dl)，将中dl的数据送入0020:bx inc bx ; (bx)=(bx)+1 loop s mov ax,4c00h int 21h code ends end ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/","tags":["汇编循环结构"],"title":"汇编之BX和loop指令"},{"categories":["汇编"],"contents":"汇编之第一个程序 一个源程序从写出到执行的过程 一个汇编语言程序从写出到最终执行的简要过程： 编写 -\u0026gt; 编译连接 -\u0026gt; 执行\n对源程序进行编译连接：\n使用汇编语言编译程序（MASM.EXE）对源程序文件中的源程序进行编译，产生目标文件【.obj文件】 再用连接程序（LINK.EXE）对目标文件进行连接，生成可在操作系统中直接运行的可执行文件【.EXE文件】 可执行文件包含两部分内容：\n程序（从源程序的汇编指令翻译过来的机器码）和数据（源程序中定义的数据）； 相关的描述信息（比如：程序有多大、要占多少内存空间等）； 执行可执行文件中的程序：\n在操作系统（如：MSDOS）中，执行可执行文件中的程序； 操作系统依照可执行文件中的描述信息，将可执行文件中的机器码和数据加载入内存，并进行相关的初始化（比如：设置 CS:IP 指向第一条要执行的指令），然后由 CPU 执行程序； 源程序的主要结构 源程序由“ 汇编指令+伪指令+宏指令 ”组成： 伪指令：编译器处理； 汇编指令：编译为机器码；\n伪指令：\n没有对应的机器码的指令，不能由 CPU 直接执行； 伪指令是由编译器来执行的指令，编译器根据伪指令来进行相关的编译工作； segment 和 ends【定义一个段】\nsegment 和 ends 是一对成对使用的伪指令；\n编写汇编程序【必须】使用到的指令；\nsegment 和 ends 的功能是定义一个段：\nsegment：说明一个段开始； ends：说明一个段结束； 一个段必须有一个名称来标识，使用格式为\n1 2 段名 segment 段名 ends 一个汇编程序由多个段组成： 这些段用来存放【代码，数据或当作栈空间】来使用，一个有意义的汇编程序至少要有一个段，这个段用来存放代码。\nend【真正的没了】\nend 是一个汇编程序的结束标记； 编译器在编译汇编程序的过程中，如果碰到了伪指令 end，就结束对源程序的编译； 如果程序写完了，要在结尾处加上伪指令 end，否则，编译器无法知道程序在何处结束； 【切记】不要把 end 和 ends 搞混了！ end：汇编程序的结束标记； ends：与 segment 成对出现，表示一个段结束； assume【寄存器和段的关联假设】\n它假设某一段寄存器和程序中的某一个用 segment…ends 定义的段相关联； 通过 assume 说明这种关联，在需要的情况下，编译程序可以将段寄存器和某一具体的段相联系； 程序和源程序\n我们将源程序文件中的所有内容称为【源程序】 将源程序中最终由计算机执行处理的指令或数据称为【程序】 程序最先以汇编指令的形式，存储在源程序中，然后经过编译、连接后转变为机器码，存储在可执行文件中； 标号，标号与段名称有所区别：\n一个标号指代了一个地址，即是段名称，类似指针。 段名称（codesg）放在 segment 的前面，作为一个段的名称，这个段的名称最终将被汇编、连接程序处理为一个段的段地址； 任务：编程运算 23；\n1 2 3 4 5 6 7 assume cs:abc abc segment mov ax,2 add ax,ax add ax,ax abc ends end DOS 中的程序运行：\nDOS 是一个单任务操作系统：\n1） 一个程序 P2 在可执行文件中，则必须有一个正在运行的程序 P1，将 P2 从可执行文件中加载入内存后，将 CPU 的控制权交给 P2，P2 才能得以运行。P2 开始运行后，P1 暂停运行。\n2） 而当 P2 运行完毕后，应该将 CPU 的控制权交还给使它得以运行的程序 P1，此后，P1 继续运行。\n一个程序结束后，将 CPU 的控制权交还给是他得以运行的程序，称这个过程为：程序返回；\n程序返回 应该在程序的末尾添加返回的程序段。\n1 2 mov ax，4c00H int 21H 【中断机制】是 DOS 最伟大的机制，Windows 系统上是【消息机制】，这两条指令所实现的功能就是程序返回；\n几个和结束相关的内容：\n段结束：伪指令 通知编译器一个段的结束【ends】\n程序结束：伪指令 通知编译器程序的结束【end】\n程序返回：汇编指令\n1 2 mov ax,4c00H int 21H 语法错误和逻辑错误：\n语法错误 程序在编译时被编译器发现的错误； 容易发现； 逻辑错误 在编写时不会表现出来的错误、在运行时会发生的错误； 不容易发现； 以简化的方式进行汇编和连接 汇编使用的程序：masm.exe 连接使用的程序：link.exe 简化方式进行汇编和连接的程序：ml.exe\nMASM下载链接，提取码：gd2c；\n跟之前 汇编（三）：DEBUG 中提到的操作一样，修改配置文件，自动挂载 MASM 目录，可以输入 dir 进行验证；\n编写一个 Hello World 程序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 .model small .data strs DB \u0026#39;hello world\u0026#39;,13,10,\u0026#39;$\u0026#39; .code start: mov ax,@data mov ds,ax mov dx,offset strs mov ah,09h int 21h mov ah,4ch int 21h end start 先复制到 txt 文本中，然后将后缀改成 asm，使用 masm 1.asm 命令进行汇编； 然后通过 link 1.obj 进行链接；\n最后执行所生成的 exe 文件； 汇编和连接的作用 连接的作用：\n当源程序很大时，可以将他们分成多个源程序文件夹编译，每个源程序编译成为目标文件后，再用连接程序将它们连接在一起，生成一个可执行文件； 程序中调用了某个库文件中的子程序，需要将这个库文件和该程序生成的目标文件连接到一起，生成一个可执行文件； 一个源程序编译后，得到了存有机器码的目标文件，目标文件中的有些内容还不能直接用来生成可执行文件，连接程序将这些内容处理为最终的可执行信息。所以在只有一个源程序文件，而又不需要调用某个库中的子程序的情况下，也必须用连接程序对目标文件进行处理，生成可执行文件； 可执行文件中的程序装入内存并运行的原理 在 DOS 中，可执行文件中的程序 P1 若要运行，必须有一个正在运行的程序 P2，将 P1 从可执行文件中加载入内存，将 CP U的控制权交给P1，P1 才能得以运行；\n当 P1 运行完毕后，应该将 CPU 的控制权交还给使他得以运行的程序；\n操作系统的外壳：\n操作系统是由多个功能模块组成的庞大、复杂的软件系统，任何通用的操作系统，都需要提供一个称为 shell（外壳）的程序，用户（操作人员）使用这个程序来操作计算机系统工作； DOS 中有一个程序 command.com，这个程序在 DOS 中称为命令解释器，也就是 DOS 系统的 shell； 执行可执行文件 1.exe 时， （1）什么程序将 CPU 的控制权交给了 1.exe？ （2）将程序 1.exe 加载入内存后，如何使程序得以运行？ （3）1.exe 程序运行结束后，返回到了哪里？\n在 DOS 中直接执行 1.exe 时，是正在运行的 cmd.exe 将 1.exe 中的程序加载入内存； cmd.exe 设置 CPU 的 CS:IP 指向程序的第一条指令（即，程序的入口），从而使程序得以运行； 程序运行结束后，返回 cmd.exe 中，CPU 继续运行 cmd.exe； 汇编程序从写出到执行的过程： EXE文件中的程序的加载过程 程序加载后，ds 中存放着程序所在内存区的段地址，这个内存区的偏移地址为 0 ，则程序所在的内存区的地址为：ds:0； 这个内存区的前256个字节中存放的是 PSP，dos 用来和程序进行通信。 从 256字节处向后的空间存放的是程序。 所以，我们从 ds 中可以得到 PSP 的段地址 SA，PSP 的偏移地址为 0，则物理地址为 SA×16+0。 因为 PSP 占256（100H）字节，所以程序的物理地址是：SA×16+0+256= SA×16+16×16=（SA+16）×16+0，可用段地址和偏移地址表示为：SA+10:0。 ","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/","tags":[],"title":"汇编之第一个程序"},{"categories":["汇编"],"contents":"寄存器 概述 一个典型的CPU主要由运算器，控制器，寄存器等器件构成，它们靠内部总线相连（内部总线实现CPU内部各机器件间的联系，外部总线实现CPU和主板上其他器件之间的联系）。\n本篇博文叙述CPU中的寄存器，程序员可以通过指令读写寄存器，从而实现对CPU的控制。不同的CPU其寄存器的个数与结构也不相同，以8086CPU为例，其有14各寄存器，每个寄存器有一个名称，且所有寄存器都是16位的，可以存放两个字节，分别为：AX、BX、CX、DX、SI、DI、SP、BP、IP、CS、SS、DS、ES、PSW，后面将一一进行介绍。\n通用寄存器 AX，BX，CX，DX为四个通用寄存器，用于存放一般性数据（所谓一般性数据指的是那些并非用于CPU控制的数据）。如前所述，8086CPU使用的是16位寄存器，为了兼容之前针对8位CPU编写的程序，每个通用寄存器都可以分为两个8位寄存器来独立使用，即AX可分为AH和AL，BX可分为BH和BL，CX可分为CH和CL,DX可分为DH和DL。如下图所示： 通过汇编语言使用通用寄存器：（汇编语言中不能直接相加两个数字，而是要通过寄存器来做加法）\n1 2 3 4 5 6 7 // 18 + 2 MOV AX 18 // 将18送入寄存器AX add ax 8 // 将AX中的数值加上8 // 或如下写法 MOV AX 18 MOV BX 2 add ax bx // 将AX和BX中的数值相加，结果保存在AX中 **【注】：**在写一条汇编指令或一个寄存器名称时不区分大小写。\n代码段寄存器与指令指针寄存器 要说明这两个寄存器，首先我们要理解一下几个概念。\n物理地址 每个内存单元都有一个编号，而这就是内存单元唯一的地址，也就是物理地址。\n16位结构的CPU 一个16位结构的CPU意味着该CPU具有以下特征：\n运算器一次最多可以处理16位的数据 寄存器的最大宽度位16位 寄存器与运算器之间的通路为16位 8086CPU给出的物理地址方法（分段结构） 8086CPU的物理总线的宽度为20，而CPU内部只能处理和传输16位地址，那么如果将地址从内部简单的发出，那么只能送出16位的地址，表现出的寻址能力只有64KB，因此，8080CPU添加了了一个地址加法器，将两个16位地址合并为一个20位的物理地址。其中两个16位地址分别位段地址和偏移地址，地址加法器的计算方法为：物理地址 = 段地址 * 16 + 偏移地址， 即物理地址 = 段地址 \u0026lt;\u0026lt; (20 - 16) + 偏移地址。如下图所示：\n代码段寄存器CS，指令指针寄存器IP 8086CPU含有4个段寄存器(存放段地址)：CS，DS，SS，ES。本节只说明CS。CS为代码段寄存器，IP为指令指针寄存器，8086CPU从CS*16 + IP单元开始读取一条指令并执行，即通过CS寄存器找到段地址，IP则记录的是偏移地址，通过二者可以找到要执行的指令代码。\n**其过程如下：**将段地址和偏移地址送至地址加法器，之后通过地址总线送至内存芯片，随后内存芯片将对应地址空间中的内容（指令）通过数据总线送达CPU，此时CPU将增加IP寄存器的值（增加值为所读取指令的长度），以便读入下一条指令。\n修改CS、IP的指令 mov指令可以用于修改大部分寄存器的值，但不可以用于修改CS、IP的内容。若想修改，可以使用如下语法：*jmp 段地址 ： 偏移地址*，例如：jmp 2AE3:3 执行后 cs = 2AE3H, IP = 0003H。若仅想修改IP的内容，可用形如：*jmp 某一合法寄存器* 来完成，比如 jmp ax或 jmp bx ，那么该语句执行后便会用寄存器中的值修改IP寄存器。\n数据段寄存器DS和偏移地址[address] 8086CPU中有一个DS寄存器，通常用于存放要访问数据的段地址，在配合偏移地址[address]来使用，则可以读取指定内存单元中的数据，如：\n1 2 3 4 5 // 读取10000H单元的内容 mov bx, 1000H mov ds, bx mov al, [0] // [adderss]表示偏移地址 mov [0] cx; 上述代码描述了mov指令可以完成的四种操作：1）將数据直接写入寄存器；2）将一个寄存器中的内容送至另一个寄存器；3）将一个内存单元中的内容送至寄存器；4）将寄存器中的数据送入内存单元。\n**【注】：**8086CPU不支持将数据直接送入段寄存器，因此修改DS寄存器需要从另一个寄存器移入。\n栈寄存器 出栈入栈指令 汇编语言的出栈入栈指令为PUSH和POP，其使用方式如下：\n1 2 3 push/pop 寄存器； push/pop 段寄存器 push/pop 内存单元 栈段寄存器SS与偏移寄存器SP 栈顶通过段寄存器ss与偏移寄存器SP指出。在任意时刻SS:SP总是指向栈顶元素，即最后入栈的数据。每次入栈时，push指令都将先移动栈顶指针再写入数据。当栈为空时，栈顶指针SS:SP指向比栈底地址高一个元素大小的位置（栈由高地址向低地址增长）。\n**【注】：**将一段地址当作栈段仅仅是编程时的一种安排，而CPU并不会区别的对待这段内存地址空间。\nDebug工具 Debug是供程序员使用的程序调试工具，可以用它检查内存中任何地方的字节以及修改任何地方的字节。\n-r命令 r命令用于查看和改变CPU寄存器的内容，使用如下图所示\n-d命令 可以通过\u0026quot;d 段地址：偏移地址\u0026quot;的格式查看从指定地址起的内存块中的内容。\n-e命令 改写内存单元的内容\n-a命令 以汇编指令的形式改写指定的内存单元。如下所示：\n-t命令 执行-t命令后，CPU将执行CS:IP指向的指令。如下图所示：\n小结 CPU访问内存时，必须向内存提供内存单元的物理地址。8086CPU在内部用段地址和偏移地址移位相加的方法形成最终的物理地址 结论：CPU可以使用不同的段地址和偏移地址形成同一个物理地址\n偏移地址16位，变化范围为0~FFFFH，仅用偏移地址来寻址最多可寻64kb个内存单元\n在8086PC机，存储单元的地址用两个元素来表述————段地址和偏移地址\n可以根据需要，将地址连续，起始地址为16的倍数的一组内存单元定义为一个段\n段地址在8086CPU的段寄存器中存放。当8086CPU要访问内存时，由段寄存器提供内存单元的段地址。8086CPU有四个段寄存器，其中CS用来存放指令的段地址\nCS存放指令的段地址，IP存放指令的偏移地址\n8086机中，任意时刻，CPU将CS:IP指向的内容当作指令来执行\n8086CPU的工作执行过程：\n从CS:IP指向的内存单元读取指令，读取的指令进入指令缓冲器 IP指向下一条指令 执行指令（转到步骤1，重复这个过程） 8086CPU提供转移指令修改CS:IP的值（jmp指令）\n字在内存里存储时，要用两个连续的地址单元来存储，字的低位字节存放在低地址单元中，高位字节存放在地址单元中\n用mov指令访问内存单元，可以在mov指令中只给出内存单元的偏移地址，此时，段地址默认在DS寄存器中\n[address]表示一个偏移地址为address的内存单元\n在内存和寄存器之间传送字型数据时，高地址单元和高8位寄存器，低地址单元和低8位寄存器相对应\nmov、add、sub是具有两个操作对象的指令。jmp是具有一个操作对象的指令\n在8086CPU提供了栈操作机制：SS段寄存器存放栈顶的段地址，在SP寄存器里存放栈顶的偏移地址。提供入栈和出栈的指令，它们根据SS:IP指示的地址，按照栈的方式访问内存单元\npush：①SP=SP-2;②向SS:IP指向的字单元中送入数据\npop：①从SS:IP指向的字单元中读取数据;②SP=SP+2\n任意时刻，SS:IP指向的栈顶元素\n8086CPU只记录栈顶，栈顶空间大小，是否栈溢出等需要我们自己管理\n可以用栈来保存需要恢复寄存器的内容\n我们可以将一段内存定义为一个段，用一个段地址指示段，用偏移地址访问段内的内存单元。\n我们可以用一个段存放数据，将他定义为“数据段”\n我们可以用一个段存放代码，将他定义为“代码段”\n我们可以用一个段当作栈，将他定义为“栈段”\n不管我们如何安排，CPU将内存中的某些内容当作代码，是因为CS:IP指向了那里；CPU将内存中的某些内容当作栈，是因为CS:IP指向了那里\n所以，一段内存既可以是代码的存储空间，又可以是数据的存储空间，还可以是栈空间，也可以是栈空间，也可以什么都不是。关键在于CPU寄存器的设置，即CS、IP、SS、SP、DS的指向。\n","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/","tags":[],"title":"汇编之寄存器"},{"categories":["汇编"],"contents":"汇编基础术语 什么是汇编语言？汇编语言就是直接工作硬件之上的编程语言。\n机器语言 机器语言就是机器指令的集合。电子计算机的机器指令是一串二进制数字，而且不同的微处理器有不同的机器指令集，也就是机器语言。\n早期的程序都是编写一大段01的二进制代码，也就是机器语言，显然不利于人工的记忆和操作。为了解决机器语言的晦涩难记的问题，人们想出了汇编语言。\n汇编语言 汇编语言的主体是汇编指令。汇编指令和机器指令的差别在于指令的表示方法上。当然，汇编语言是人来设计的，机器是看不懂的，需要经过汇编语言的编译器，将汇编代码翻译成机器指令集，然后机器才可以执行代码。\n汇编语言的组成：\n汇编指令：机器码的助记符，有对应的机器码（核心） 伪指令：没有对应的机器码，由编译器执行，计算机并不执行 其他符号：如+、-、*、/等，有编译器识别，没有对应的机器码 存储器 CPU是计算机的核心部件，它控制整个计算机的运作并进行计算。要想让一个CPU工作，就必须向它提供指令和数据。指令和数据就放在存储器里，当然也指内存。CPU是与内存打交道，磁盘不能直接与CPU进行数据交换，中间需要存取速度更快的内存。\n指令和数据 在内存或磁盘上，指令和数据没有任何区别，都是二进制信息。CPU在工作的时候将有些信息看成指令，有些看成数据。\n存储单元 存储器被划分为若干个存储单元，每个存储单元从0开始顺序编号，例如一个存储器有128个存储单元，编号从0~127.\n一般一个存储单元就是一个字节\nCPU对存储器的读写 存储器被划分为多个存储单元，存储单元从零开始顺序编号，这些编号可以看作是存储单元在存储器的地址。\nCPU要从内存中读写数据，首先要指定存储单元的地址，然后需要明确进行何种操作。\n也就是说，CPU对数据进行读写时，必须和外部器件进行3类信息的交互\n存储单元的地址信息（地址信息） 器件的选择，读或写的命令（控制信息） 读或写的数据（数据信息） 那么，CPU通过什么将地址、数据和控制信息传到存储器的呢？——总线。\n总线从物理上来讲，就是一根根导线的集合。根据传送信息的不同，总线从逻辑上又分为3类：地址总线、控制总线、数据总线。\nCPU读数据：\nCPU通过地址线将地址信息3发出 CPU通过控制线发出内存读命令，选中存储器芯片并通知它，将要从中读取数据 存储器将3号单元里的数据8通过数据线送入CPU CPU写数据：\nCPU通过地址线将地址信息3发出 CPU通过控制线发出内存写命令，选中存储器芯片并通知它，将要从中写入数据 CPU通过数据线将数据26送入内存的3号单元中 地址总线 CPU通过地址总线来指定存储单元，由此可推知，地址总线上能传递多少种不同的信息，CPU就可以对多少个存储单元进行寻址，假设一个CPU有10根地址总线，那么其寻址空间为0 ~1023，即2^10个存储单元。即地址总线的宽度决定了CPU的寻址范围。\n数据总线 CPU和其它器件之间的数据传递是通过数据总线进行传递的，数据总线的宽度决定了CPU和其它器件间数据传送速度。8根数据总线一次可以传送一个8位二进制数据，因为每一根总线只能传递一个高电平或低电平来代表0或1，即1根总线一次只能传递一位。\n控制总线 CPU对外部器件的控制是通过控制总线完成的，有多少根控制总线就意味着CPU对外部器件有多少种控制。即控制总线的宽度对外部器件的控制能力。\n主板 每台PC都有一个主板，每个主板上都存在一些核心器件：CPU，存储器，外围芯片组，扩展卡槽等。扩展卡槽上一般有RAM内存条和各类接口卡。这些器件通过总线相连。\n接口卡 CPU对于诸如：显示器，音箱，打印机这样的外设不能直接控制，直接控制这些设备工作的是插在扩展插槽上的接口卡，由于扩展插槽通过总线和CPU相连，所以接口卡也通过总线和CPU相连。之后CPU通过总线向接口卡发送命令，接口卡在根据CPU的命令控制外设进行工作。（例：比如一个音箱通过USB插槽连接到了电脑，而该插槽通过总线和CPU相连，此时当要播放声音时会将数据指令等通过数据总线发送到卡槽，即音箱内的接口卡也会收到，之后控制音箱工作）\n各类存储器芯片 存储器从读写属性上可以分为随机存储器（RAM）和只读存储器(ROM)，其中随机存储器（RAM）可读可写但必须带电存储，关机后存储内容会丢失。只读存储器只能读取，不能写入，关机后其内容不会丢失。\n而从功能和连接方式看又可以分为以下3类：\n随机存储器（内存）：用于存放供CPU使用的绝大部分程序和数据 装有BIOS的ROM：BIOS（基本输入/输出系统）是由主板和各类接口卡厂商提供的软件系统，可以通过BIOS使用硬件设备进行最基本的输入输出。在主板和某写接口卡上插有存储相应BIOS的ROM，如：主板上的ROM中存储着主板的BIOS（通常即系统BIOS），显卡上的ROM存储着显卡的BIOS，网卡上装有网卡的BIOS。 接口卡上的RAM：有的接口卡需要对大批量的输入输出数据进行进行暂存，因此需要RAM。最典型的就是显卡上的RAM,一般称为显存。显卡随时将显存中的数据输出到显示器上。 CPU的内存地址空间 假设一个CPU的地址总线宽度为10，那么其寻址范围为1024个内存单元，而这1024个内存单元就构成了这个CPU的内存地址空间。\n各类物理上独立的器件都是通过总线与CPU相连的，CPU操作它们时也是将它们当作内存来看待的，每个物理存储器占用CPU的内存地址空间中的一部分，当CPU在某个物理器件对应的地址空间中读写数据，实际上就是在对应的物理存储器中读写数据。\n","permalink":"https://cold-bin.github.io/post/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/","tags":[],"title":"汇编之概述"},{"categories":["数据库","mongodb"],"contents":"MongoDB背景 1、MongoDB是什么？ MongoDB是一款为web应用程序和互联网基础设施设计的数据库管理系统。没错MongoDB就是数据库，是NoSQL类型的数据库\n2、为什么要用MongoDB？ （1）MongoDB提出的是文档、集合的概念，使用BSON（类JSON）作为其数据模型结构，其结构是面向对象的而不是二维表，存储一个用户在MongoDB中是这样子的。\n1 2 3 4 { username:\u0026#39;123\u0026#39;, password:\u0026#39;123\u0026#39; ｝ 使用这样的数据模型，使得MongoDB能在生产环境中提供高读写的能力，吞吐量较于mysql等SQL数据库大大增强。\n（2）易伸缩，自动故障转移。易伸缩指的是提供了分片能力，能对数据集进行分片，数据的存储压力分摊给多台服务器。自动故障转移是副本集的概念，MongoDB能检测主节点是否存活，当失活时能自动提升从节点为主节点，达到故障转移。\n（3）数据模型因为是面向对象的，所以可以表示丰富的、有层级的数据结构，比如博客系统中能把“评论”直接怼到“文章“的文档中，而不必像myqsl一样创建三张表来描述这样的关系。\n3、主要特性 （1）文档数据类型\nSQL类型的数据库是正规化的，可以通过主键或者外键的约束保证数据的完整性与唯一性，所以SQL类型的数据库常用于对数据完整性较高的系统。MongoDB在这一方面是不如SQL类型的数据库，且MongoDB没有固定的Schema，正因为MongoDB少了一些这样的约束条件，可以让数据的存储数据结构更灵活，存储速度更加快。\n（2）即时查询能力\nMongoDB保留了关系型数据库即时查询的能力，保留了索引（底层是基于B tree）的能力。这一点汲取了关系型数据库的优点，相比于同类型的NoSQL redis 并没有上述的能力。\n（3）复制能力\nMongoDB自身提供了副本集能将数据分布在多台机器上实现冗余，目的是可以提供自动故障转移、扩展读能力。\n（4）速度与持久性\nMongoDB的驱动实现一个写入语义 fire and forget ，即通过驱动调用写入时，可以立即得到返回得到成功的结果（即使是报错），这样让写入的速度更加快，当然会有一定的不安全性，完全依赖网络。\nMongoDB提供了Journaling日志的概念，实际上像mysql的bin-log日志，当需要插入的时候会先往日志里面写入记录，再完成实际的数据操作，这样如果出现停电，进程突然中断的情况，可以保障数据不会错误，可以通过修复功能读取Journaling日志进行修复。\n（5）数据扩展\nMongoDB使用分片技术对数据进行扩展，MongoDB能自动分片、自动转移分片里面的数据块，让每一个服务器里面存储的数据都是一样大小。\n4、C/S服务模型 MongoDB核心服务器主要是通过mongod程序启动的，而且在启动时不需对MongoDB使用的内存进行配置，因为其设计哲学是内存管理最好是交给操作系统，缺少内存配置是MongoDB的设计亮点，另外，还可通过mongos路由服务器使用分片功能。\nMongoDB的主要客户端是可以交互的js shell 通过mongo启动，使用js shell能使用js直接与MongoDB进行交流，像使用sql语句查询mysql数据一样使用js语法查询MongoDB的数据，另外还提供了各种语言的驱动包，方便各种语言的接入。\n5、完善的命令行工具 mongodump和mongorestore,备份和恢复数据库的标准工具。输出BSON格式，迁移数据库。\nmongoexport和mongoimport，用来导入导出JSON、CSV和TSV数据，数据需要支持多格式时有用。mongoimport还能用与大数据集的初始导入，但是在导入前顺便还要注意一下，为了能充分利用好mongoDB通常需要对数据模型做一些调整。\nmongosniff,网络嗅探工具，用来观察发送到数据库的操作。基本就是把网络上传输的BSON转换为易于人们阅读的shell语句。\n因此，可以总结得到，MongoDB结合键值存储和关系数据库的最好特性。因为简单，所以数据极快，而且相对容易伸缩，还提供复杂查询机制的数据库。MongoDB需要跑在64位的服务器上面，且最好单独部署，因为是数据库，所以也需要对其进行热备、冷备处理。\n6、几个shell实操 因为本篇文章不是API手册，所有这里对shell的使用也是基础的介绍什么功能可以用什么语句，主要是为了展示使用MongoDB shell的方便性，如果需要知道具体的MongoDB shell语法可以查阅官方文档。\n1、切换数据库\n1 use dba 创建数据库并不是必须的操作，数据库与集合只有在第一次插入文档时才会被创建，与对数据的动态处理方式是一致的。简化并加速开发过程，而且有利于动态分配命名空间。如果担心数据库或集合被意外创建，可以开启严格模式\n2、插入语法\n1 2 1 db.users.insert({username:\u0026#34;smith\u0026#34;}) 2 db.users.save({username:\u0026#34;smith\u0026#34;}) 3、查找语法\n1 2 1 db.users.find() 2 db.users.count() 4、更新语法\n1 2 3 4 5 6 7 8 9 10 11 1 db.users.update({username:\u0026#34;smith\u0026#34;},{$set:{country:\u0026#34;Canada\u0026#34;}}) 2 //把用户名为smith的用户的国家改成Canada 3 4 db.users.update({username:\u0026#34;smith\u0026#34;},{$unset:{country:1}}) 5 //把用户名为smith的用户的国家字段给移除 6 7 db.users.update({username:\u0026#34;jones\u0026#34;},{$set:{favorites:{movies:[\u0026#34;casablance\u0026#34;,\u0026#34;rocky\u0026#34;]}}}) 8 //这里主要体现多值修改，在favorties字段中添加多个值 9 10 db.users.update({\u0026#34;favorites.movies\u0026#34;:\u0026#34;casablance\u0026#34;},{$addToSet:{favorites.movies:\u0026#34;the maltese\u0026#34;}},false,true) 11 //多项更新 5、删除语法\n1 2 3 1 db.foo.remove() //删除所有数据 2 db.foo.remove({favorties.cities:\u0026#34;cheyene\u0026#34;}) //根据条件进行删除 3 db.drop() //删除整个集合 6、索引相关语法\n1 2 3 4 1 db.numbers.ensureIndex({num:1}) 2 //创建一个升序索引 3 db.numbers.getIndexes() 4 //获取全部索引 7、基本管理语法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1 show dbs 2 //查询所有数据库 3 show collections 4 //显示所有表 5 db.stats() 6 //显示数据库状态信息 7 db.numbers.stats() 8 //显示集合表状态信息 9 db,shutdownServer() 10 //停止数据库 11 db.help() 12 //获取数据库操作命令 13 db.foo.help() 14 //获取表操作命令 15 tab 键 //能自动帮我们补全命令 MongoDB基础操作 ","permalink":"https://cold-bin.github.io/post/mongodb%E5%9F%BA%E7%A1%80%E7%AF%87/","tags":["mongodb"],"title":"MongoDB基础篇"},{"categories":["存储"],"contents":"一.数据处理类型 联机事务处理 OLTP(on-line transaction processing) 联机分析处理 OLAP(On-Line Analytical Processing) 区别：\nOLTP 是传统关系型数据库的主要应用，用来执行一些基本的、日常的事务处理，比如数据库记录的增、删、改、查等等 OLAP 则是分布式数据库的主要应用，它对实时性要求不高，但处理的数据量大，通常应用于复杂的动态报表系统上。 二.行式存储和列式存储 Row-based storage storesatable in a sequence of rows. Column-based storage storesatable in a sequence of columns. 传统的关系型数据库，如 Oracle、DB2、MySQL、SQL SERVER 等采用行式存储法(Row-based)，在基于行式存储的数据库中， 数据是按照行数据为基础逻辑存储单元进行存储的， 一行中的数据在存储介质中以连续存储形式存在。\n列式存储(Column-based)是相对于行式存储来说的，新兴的 Hbase、HP Vertica、EMC Greenplum 等分布式数据库均采用列式存储。在基于列式存储的数据库中， 数据是按照列为基础的逻辑存储单元进行存储的，一列中的数据在存储介质中以连续存储形式存在。\n从上图可以很清楚地看到，行式存储下一张表的数据都是放在一起的，但列式存储下都被分开保存了。所以它们就有了如下这些优缺点对比：\n1.在数据写入上的对比 1）行存储的写入是一次完成。如果这种写入建立在操作系统的文件系统上，可以保证写入过程的成功或者失败，数据的完整性因此可以确定。\n2）列存储由于需要把一行记录拆分成单列保存，写入次数明显比行存储多（意味着磁头调度次数多，而磁头调度是需要时间的，一般在1ms~10ms)，再加上磁头需要在盘片上移动和定位花费的时间，实际时间消耗会更大。所以，行存储在写入上占有很大的优势。\n3）还有数据修改,这实际也是一次写入过程。不同的是，数据修改是对磁盘上的记录做删除标记。行存储是在指定位置写入一次，列存储是将磁盘定位到多个列上分别写入，这个过程仍是行存储的列数倍。所以，数据修改也是以行存储占优。\n2.在数据读取上的对比 1）数据读取时，行存储通常将一行数据完全读出，如果只需要其中几列数据的情况，就会存在冗余列，出于缩短处理时间的考量，消除冗余列的过程通常是在内存中进行的。\n2）列存储每次读取的数据是集合的一段或者全部，不存在冗余性问题。\n3） 两种存储的数据分布。由于列存储的每一列数据类型是同质的，不存在二义性问题。比如说某列数据类型为整型(int)，那么它的数据集合一定是整型数据。这种情况使数据解析变得十分容易。相比之下，行存储则要复杂得多，因为在一行记录中保存了多种类型的数据，数据解析需要在多种数据类型之间频繁转换，这个操作很消耗CPU，增加了解析的时间。所以，列存储的解析过程更有利于分析大数据。\n4）从数据的压缩以及更性能的读取来对比\n图列分析：首先将Customes Name列及Material列做逻辑化索引标识，查询时分别匹配Materia=Refrigerator及Customes Name=Miller的数据，然后做交叉匹配\n3.优缺点 1）行存储的写入是一次性完成，消耗的时间比列存储少，并且能够保证数据的完整性，缺点是数据读取过程中会产生冗余数据，如果只有少量数据，此影响可以忽略;数量大可能会影响到数据的处理效率。\n2）列存储在写入效率、保证数据完整性上都不如行存储，它的优势是在读取过程，不会产生冗余数据，这对数据完整性要求不高的大数据处理领域，比如互联网，犹为重要。查询过程中，可针对各列的运算并发执行(SMP)，***在内存中聚合完整记录集，***可能降低查询响应时间;可在数据列中高效查找数据，无需维护索引(任何列都能作为索引)，查询过程中能够尽量减少无关IO，避免全表扫描;因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率;如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。\n4.使用场景 如果你大部分时间都是关注整张表的内容，而不是单独某几列，并且所关注的内容是不需要通过任何聚集运算的，那么推荐使用行式存储。原因是重构每一行数据（即解压缩过程）对于HANA来说，是一个不小的负担。列式存储的话，比如你比较关注的都是某几列的内容，或者有频繁聚集需要的，通过聚集之后进行数据分析的表。\n行式存储的适用场景：\n1、适合随机的增删改查操作;\n2、需要在行中选取所有属性的查询操作;\n3、需要频繁插入或更新的操作，其操作与索引和行的大小更为相关。\n列式存储的适用场景：\n一般来说，一个OLAP类型的查询可能需要访问几百万甚至几十亿个数据行，且该查询往往只关心少数几个数据列。例如，查询今年销量最高的前20个商品，这个查询只关心三个数据列：时间（date）、商品（item）以及销售量（sales amount）。商品的其他数据列，例如商品URL、商品描述、商品所属店铺，等等，对这个查询都是没有意义的。而列式数据库只需要读取存储着“时间、商品、销量”的数据列，而行式数据库需要读取所有的数据列。因此，列式数据库大大地提高了OLAP大数据量查询的效率。\n很多列式数据库还支持列族（column group，Bigtable系统中称为locality group），即将多个经常一起访问的数据列的各个值存放在一起。如果读取的数据列属于相同的列族，列式数据库可以从相同的地方一次性读取多个数据列的值，避免了多个数据列的合并。列族是一种行列混合存储模式，这种模式能够同时满足OLTP和OLAP的查询需求。\n实操中我们会发现，行式数据库在读取数据的时候，会存在一个固有的“缺陷”。比如，所选择查询的目标即使只涉及少数几项属性，但由于这些目标数据埋藏在各行数据单元中，而行单元往往又特别大，应用程序必须读取每一条完整的行记录，从而使得读取效率大大降低，对此，行式数据库给出的优化方案是加“索引”。\n在OLTP类型的应用中，通过索引机制或给表分区等手段，可以简化查询操作步骤，并提升查询效率。但针对海量数据背景的OLAP应用(例如分布式数据库、数据仓库等等)，行式存储的数据库就有些“力不从心”了，行式数据库建立索引和物化视图，需要花费大量时间和资源，因此还是得不偿失，无法从根本上解决查询性能和维护成本等问题，也不适用于数据仓库等应用场景，所以后来出现了基于列式存储的数据库。对于数据仓库和分布式数据库来说，大部分情况下它会从各个数据源汇总数据，然后进行分析和反馈，其操作大多是围绕同一列属性的数据进行的，而当查询某属性的数据记录时，列式数据库只需返回与列属性相关的值，在大数据量查询场景中，列式数据库可在内存中高效组装各列的值，最终形成关系记录集，因此可以显著减少IO消耗，并降低查询响应时间，非常适合数据仓库和分布式的应用。\n5.总结 1.传统行式数据库的特性如下：\n①数据是按行存储的。\n②没有索引的查询使用大量I/O。比如一般的数据库表都会建立索引，通过索引加快查询效率。\n③建立索引和物化视图需要花费大量的时间和资源。\n④面对查询需求，数据库必须被大量膨胀才能满足需求。\n2.列式数据库的特性如下：\n①数据按列存储，即每一列单独存放。\n②数据即索引。\n③只访问查询涉及的列，可以大量降低系统I/O。\n④每一列由一个线程来处理，即查询的并发处理性能高。\n⑤数据类型一致,数据特征相似,可以高效压缩。比如有增量压缩、前缀压缩算法都是基于列存储的类型定制的,所以可以大幅度提高压缩比,有利于存储和网络输出数据带宽的消耗。\n","permalink":"https://cold-bin.github.io/post/%E5%AD%98%E5%82%A8%E4%B9%8B%E8%A1%8C%E5%88%97%E5%AD%98%E5%82%A8/","tags":["行存储","列存储"],"title":"存储之行列存储"},{"categories":["摘要算法"],"contents":"[toc]\nHASH算法 摘要算法又称哈希算法、散列算法，它的作用是：它通过一个函数，把任意长度的数据转换为一个长度固定的数据串（通常用16进制的字符串表示）。\n要注意摘要算法不是加密算法，不能用于加密（因为无法通过摘要反推明文），只能用于防篡改，但是它的单向计算特性决定了可以在不存储明文口令的情况下验证用户口令。\n哈希函数的主要作用不是完成数据加密与解密工作，它是用来检验数据完整性的重要技术，运算结果具有不可逆性。\n通过哈希函数，可以为数据创建\u0026quot;数组指纹\u0026quot;(散列值/哈希值)，哈希值通常是一个短的随机字母和数字组成的字符串。消息认证流程如下：\n在上述认证流程中，信息收发双方在通信前已经商定好了具体的哈希算法，并且该算法是公开的。\n如果消息在传递过程中被篡改，则该消息不能与已经获得的数字指纹(哈希值)相匹配。\n哈希函数的一些特性 消息的长度不受限制； 对于给定的消息，其哈希值的计算是很容易的； 如果两个哈希值不想同，则这两个哈希值的原文数据也不想同，这个特性使得哈希函数具有确定性的结果； 哈希函数的运算过程是不可逆的，即函数的单向性。这也是单向函数命名的由来。 对于一个已知的消息和其哈希值，要找到另一个消息使其获得相同的哈希值是不可能的，即抗弱碰撞性，用来防止伪造。 哈希函数官方用于消息完整性验证，是数据签名的核心技术。\n哈希函数的常用算法有：MD(消息摘要算法)、SHA(安全散列算法)及Mac(消息认证码算法)\n算法 输出长度（位） 输出长度（字节） SM3 256 bits 32 bytes MD5 128 bits 16 bytes SHA-1 160 bits 20 bytes RipeMD-160 160 bits 20 bytes SHA-256 256 bits 32 bytes SHA-512 512 bits 64 bytes 举个栗子 Java标准库提供了常用的哈希算法，并且有一套统一的接口。我们以MD5算法为例，看看如何对输入计算哈希：\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Test public void run2(){ try { MessageDigest messageDigest = MessageDigest.getInstance(\u0026#34;MD5\u0026#34;); // 反复调用update输入数据: messageDigest.update(\u0026#34;Hello\u0026#34;.getBytes(StandardCharsets.UTF_8)); messageDigest.update(\u0026#34;World\u0026#34;.getBytes(StandardCharsets.UTF_8)); byte[] result = messageDigest.digest(); System.out.println(new BigInteger(1, result).toString(16)); } catch (NoSuchAlgorithmException e) { e.printStackTrace(); } } 运算结果：\n68e109f0f40ca72a15e05cc22786f8e6\n哈希算法的用途 因为相同的输入永远会得到相同的输出，因此，如果输入被修改了，得到的输出就会不同。\n文件防篡改 我们在网站上下载软件的时候，经常看到下载页显示的哈希：\n如何判断下载到本地的软件是原始的、未经篡改的文件？我们只需要自己计算一下本地文件的哈希值，再与官网公开的哈希值对比，如果相同，说明文件下载正确，否则，说明文件已被篡改。\n存储用户口令 哈希算法的另一个重要用途是存储用户口令。如果直接将用户的原始口令存放到数据库中，会产生极大的安全风险：\n数据库管理员能够看到用户明文口令； 数据库数据一旦泄漏，黑客即可获取用户明文口令。 不存储用户的原始口令，那么如何对用户进行认证？\n方法是存储用户口令的哈希，例如：MD5\n在用户输入原始口令后，系统计算用户输入的原始口令的MD5并与数据库存储的MD5对比，如果一致，说明口令正确，否则，口令错误。\n因此，数据库存储用户名和口令的表内容应该像下面这样：\nusername password bob f30aa7a662c728b7407c54ae6bfd27d1 alice 25d55ad283aa400af464c76d713c07ad tim bed128365216c019988915ed3add75fb 这样一来，数据库管理员看不到用户的原始口令。即使数据库泄漏，黑客也无法拿到用户的原始口令。想要拿到用户的原始口令，必须用暴力穷举的方法，一个口令一个口令地试，直到某个口令计算的MD5恰好等于指定值。\n使用哈希口令时，还要注意防止彩虹表攻击。\n什么是彩虹表呢？上面讲到了，如果只拿到MD5，从MD5反推明文口令，只能使用暴力穷举的方法。\n然而黑客并不笨，暴力穷举会消耗大量的算力和时间。但是，如果有一个预先计算好的常用口令和它们的MD5的对照表：\n常用口令 MD5 hello123 f30aa7a662c728b7407c54ae6bfd27d1 12345678 25d55ad283aa400af464c76d713c07ad passw0rd bed128365216c019988915ed3add75fb 19700101 570da6d5277a646f6552b8832012f5dc … … 20201231 6879c0ae9117b50074ce0a0d4c843060 这个表就是彩虹表。\n如果用户使用了常用口令，黑客从MD5一下就能反查到原始口令：\nbob的MD5：f30aa7a662c728b7407c54ae6bfd27d1，原始口令：hello123；\nalice的MD5：25d55ad283aa400af464c76d713c07ad，原始口令：12345678；\ntim的MD5：bed128365216c019988915ed3add75fb，原始口令：passw0rd。\n这就是为什么不要使用常用密码，以及不要使用生日作为密码的原因。\n如何抵御彩虹表攻击 即使用户使用了常用口令，我们也可以采取措施来抵御彩虹表攻击：方法是对每个口令额外添加随机数，这个方法称之为加盐（salt）（如果盐值是随机还需要在数据库里存盐值）\n1 digest = 摘要算法(salt+inputPassword) 经过加盐处理的数据库表，内容如下：\nusername salt password bob H1r0a a5022319ff4c56955e22a74abcc2c210 alice 7$p2w e5de688c99e961ed6e560b972dab8b6a tim z5Sk9 1eee304b92dc0d105904e7ab58fd2f64 加盐的目的在于使黑客的彩虹表失效，即使用户使用常用口令，也无法从MD5反推原始口令。\nSHA-1\nSHA-1也是一种哈希算法，它的输出是160 bits，即20字节。SHA-1是由美国国家安全局开发的，SHA算法实际上是一个系列，包括SHA-0（已废弃）、SHA-1、SHA-256、SHA-512等。\n","permalink":"https://cold-bin.github.io/post/%E5%B8%B8%E8%A7%81%E6%91%98%E8%A6%81%E7%AE%97%E6%B3%95/","tags":["哈希"],"title":"常见摘要算法"},{"categories":["加密算法"],"contents":"[toc]\n一、概述 在安全领域，利用密钥加密算法来对通信的过程进行加密是一种常见的安全手段。利用该手段能够保障数据安全通信的三个目标：\n1、数据的保密性，防止用户的数据被窃取或泄露 2、保证数据的完整性，防止用户传输的数据被篡改 3、通信双方的身份确认，确保数据来源与合法的用户\n而常见的密钥加密算法类型大体可以分为三类：对称加密、非对称加密、单向加密。下面我们来了解下相关的算法原理及其常见的算法。\n二、对称加密 在加密传输中最初是采用对称密钥方式，也就是加密和解密都用相同的密钥。\n对称加密过程如下： 1.对称加密算法采用单密钥加密，在通信过程中，数据发送方将原始数据分割成固定大小的块，经过密钥和加密算法逐个加密后，发送给接收方\n2.接收方收到加密后的报文后，结合解密算法使用相同密钥解密组合后得出原始数据。\n图示：\n优点 是效率高，算法简单，系统开销小，适合加密大量数据。\n缺点 安全性差 加解密算法是公开的，因此在这过程中，密钥的安全传递就成为了至关重要的事了。而密钥通常来说是通过双方协商，以物理的方式传递给对方，或者利用第三方平台传递给对方，一旦这过程出现了密钥泄露，不怀好意的人就能结合相应的算法拦截解密出其加密传输的内容。 扩展性差 每对通信用户之间都需要协商密钥，n个用户的团体就需要协商n*(n-1)/2个不同的密钥，不便于管理；而如果都使用相同密钥的话，密钥被泄漏的机率大大增加，加密也就失去了意义。\n常见的对称加密算法 DES：分组式加密算法，以64位为分组对数据加密，加解密使用同一个算法。 3DES：三重数据加密算法，对每个数据块应用三次DES加密算法。 AES：高级加密标准算法，是美国联邦政府采用的一种区块加密标准，用于替代原先的DES，目前已被广泛应用。 Blowfish：Blowfish算法是一个64位分组及可变密钥长度的对称密钥分组密码算法，可用来加密64比特长度的字符串。\n三、非对称加密 非对称加密算法采用公钥和私钥两种不同的密码来进行加解密。公钥和私钥是成对存在，公钥是从私钥中提取产生公开给所有人的，如果使用公钥对数据进行加密，那么只有对应的私钥（不能公开）才能解密，反之亦然。N 个用户通信，需要2N个密钥。\n用途 非对称密钥加密适合对密钥或身份信息等敏感信息加密，从而在安全性上满足用户的需求。\n非对称加密过程 1.甲使用乙的公钥并结合相应的非对称算法将明文加密后发送给乙，并将密文发送给乙。 2.乙收到密文后，结合自己的私钥和非对称算法解密得到明文，得到最初的明文。\n图示：\n优点 具有比对称密钥加/解密方式更高的安全性，因为加密和解密用的是不同密钥，而且无法从一个密钥推导出另一个密钥，且公钥加密的信息只能用同一方的私钥进行解密。\n缺点 1.非对称密钥加密的缺点是算法非常复杂，导致加密大量数据所用的时间较长，只适合对少量数据进行加密。而且由于在加密过程中会添加较多附加信息，使得加密后的报文比较长，容易造成数据分片，不利于网络传输。 2.无法确认公钥的来源合法性以及数据的完整性。如何确认我们接下来会说\n常见算法包括： RSA：RSA算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但那时想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥，可用于加密，也能用于签名。 DSA：数字签名算法，仅能用于签名，不能用于加解密。 DSS：数字签名标准，技能用于签名，也可以用于加解密。 ELGamal：利用离散对数的原理对数据进行加解密或数据签名，其速度是最慢的。\n四、单向加密 单向加密算法只能用于对数据的加密，无法被解密，其特点为定长输出、雪崩效应（少量消息位的变化会引起信息摘要的许多位变化）。\n用途 单向加密算法常用于提取数据指纹，验证数据的完整性、数字摘要、数字签名等等。\n单向加密过程 1.发送者将明文通过单向加密算法加密生成定长的密文串，然后传递给接收方。\n2.接收方将用于比对验证的明文使用相同的单向加密算法进行加密，得出加密后的密文串。\n3.将之与发送者发送过来的密文串进行对比，若发送前和发送后的密文串相一致，则说明传输过程中数据没有损坏；若不一致，说明传输过程中数据丢失了。\n图示：\n常见算法\nMD5、sha1、sha224等等\n五、密钥交换 密钥交换IKE（Internet Key Exchange）通常是指双方通过交换密钥来实现数据加密和解密\n常见的密钥交换方式有下面两种：\n1、公钥加密 将公钥加密后通过网络传输到对方进行解密，这种方式缺点在于具有很大的可能性被拦截破解，因此不常用\n2、Diffie-Hellman DH算法是一种密钥交换算法，其既不用于加密，也不产生数字签名。\nDH算法通过双方共有的参数、私有参数和算法信息来进行加密，然后双方将计算后的结果进行交换，交换完成后再和属于自己私有的参数进行特殊算法，经过双方计算后的结果是相同的，此结果即为密钥。\n如：\nA 有p和g两个参数，A还有一个属于自己的私有参数x； B 有p和g两个参数，A还有一个属于自己的私有参数y； A和B均使用相同的加密算法计算其对应的值：value_A=p^(x%g)，value_B=p^(y%g) 随后双方交换计算后的值，然后再分别使用自己的私有参数对去求次方，如： A拿到value_B值后，对其求x平方得value_B^x=p^(xy%g)； B拿到value_A值后，对其求y平方得value_A^y=p^(xy%g)； 最终得到的结果是一致的。\n安全性\n在整个过程中，第三方人员只能获取p、g两个值，AB双方交换的是计算后的结果，因此这种方式是很安全的。\n如何确认公钥的来源合法性？ 答案：使用公钥证书\n公钥基础设施（PKI） 公钥基础设施是一个包括硬件、软件、人员、策略和规程的集合\n用途 用于实现基于公钥密码机制的密钥和证书的生成、管理、存储、分发和撤销的功能\n组成 签证机构CA、注册机构RA、证书吊销列表CRL和证书存取库CB。\n公钥证书 公钥证书是以数字签名的方式声明，它将公钥的值绑定到持有对应私钥的个人、设备或服务身份。公钥证书的生成遵循X.509协议的规定，其内容包括：证书名称、证书版本、序列号、算法标识、颁发者、有效期、有效起始日期、有效终止日期、公钥 、证书签名等等的内容。\nCA(Certificate Authority)证书认证的流程 1.客户A准备好要传送的数字信息（明文）。**（准备明文） **\n2.客户A对数字信息进行哈希（hash）运算，得到一个信息摘要。**（准备摘要） **\n3.客户A用CA的私钥（SK）对信息摘要进行加密得到客户A的数字签名，并将其附在数字信息上。**（用私钥对数字信息进行数字签名）　**\n4.客户A随机产生一个加密密钥（DES密钥），并用此密钥对要发送的信息进行加密，形成密文。**（生成密文） **\n5.客户A用双方共有的公钥（PK）对刚才随机产生的加密密钥进行加密，将加密后的DES密钥连同密文一起传送给乙。**（非对称加密，用公钥对DES密钥进行加密）　**\n6.银行B收到客户A传送过来的密文和加过密的DES密钥，先用自己的私钥（SK）对加密的DES密钥进行解密，得到DES密钥。（用私钥对DES密钥解密）\n7.银行B然后用DES密钥对收到的密文进行解密，得到明文的数字信息，然后将DES密钥抛弃（即DES密钥作废）。**（解密文）　**\n8.银行B用双方共有的公钥（PK）对客户A的数字签名进行解密，得到信息摘要。银行B用相同的hash算法对收到的明文再进行一次hash运算，得到一个新的信息摘要。**（用公钥解密数字签名） **\n9.银行B将收到的信息摘要和新产生的信息摘要进行比较，如果一致，说明收到的信息没有被修改过。**（对比信息摘要和信息） **\n如何保证CA的公钥没有被篡改呢？ 答案是没法保证CA的公钥没有被篡改。通常操作系统和浏览器会预制一些CA证书在本地。所以发送方应该去那些通过认证的CA处申请数字证书。这样是有保障的。\n但是如果系统中被插入了恶意的CA证书，依然可以通过假冒的数字证书发送假冒的发送方公钥来验证假冒的正文信息。所以安全的前提是系统中不能被人插入非法的CA证书。\n六、AES和RSA的区别 AES 加密 AES 算法依次对每个 128 位数据块应用一系列数学变换。由于这种方法的计算要求较低，AES 可用于笔记本电脑和智能手机等消费类设备上进行数据加密，以及快速加密大量数据。\nAES 是一种对称算法，它使用相同的 128、192 或 256 位密钥进行加密和解密。128、192 或 256 位的密钥可以理解为分别对应16、24和32个字节的16进制字符串密钥，AES 系统的安全性会随密钥长度呈指数增长。\n即使使用 128 位密钥，通过对 2128 个可能的密钥值进行暴力枚举，来尝试破解AES加密后的数据的任务也是个非常计算密集型的任务。事实上，AES 从未被破解，并且根据当前的技术趋势，预计在未来几年内仍将保持安全。\nRSA 加密 RSA 以麻省理工学院的科学家（Rivest、Shamir 和 Adleman）的名字命名， 于1977 年首次公布。它是一种非对称算法，它使用公开的已知密钥进行加密，但需要另外一个不同的密钥进行解密，这个不同的密钥只有预期的接收者知道。\n网上不少例子说的是公钥用于加密，私钥用于解密，其实这个说法不对\n私钥和公钥是一对，都可以加解密，配对使用，只不过公钥可以公布出去，而私钥是持有者自己保留的。\n一般的用法是私钥加密用于签名防数据被篡改，公钥加密用于加密防敏感信息，防止泄露。\n私钥加密公钥解密，能证明“私钥拥有者”的唯一身份，用于签名。（例如，token的签名算法，使用RSA的私钥加密，防止篡改） 公钥加密私钥解密，确保发送的信息，只有\u0026quot;私钥拥有者\u0026quot;能够解密。 这块先卖个关子，理解不了的话后面给大家介绍开放平台API验签和加密流程的时候再给大家细讲。\nRSA 算法需要的计算量比 AES 高，但速度要慢得多。它比较适合用于加密少量数据。\nRSA和AES结合使用 AES 算法的一个主要问题是，作为一种对称算法，它要求加密方和解密方使用相同的密钥。这就产生了一个关键的密钥管理问题——如何将非常重要的密钥分发给分布在世界各地的授权接收者，而不会冒在传输途中某个地方考虑不周导致密钥泄露的巨大风险？答案是结合 AES 和 RSA 加密的优势。\n在包括互联网在内的许多现代通信环境中，大量交换的数据都通过快速 AES 算法进行加密。为了获得解密数据所需的密钥，授权接收者发布一个公钥，同时保留一个只有他们知道的相关私钥。然后，发送方使用该公钥对他们自己的 AES 密钥进行RSA加密传输给接收方，接收方使用私钥解密得到AES密钥，再用该密钥对数据进行解密。\n","permalink":"https://cold-bin.github.io/post/%E5%B8%B8%E8%A7%81%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E6%A6%82%E5%BF%B5/","tags":["aes","rsa"],"title":"常见加密算法原理及概念"},{"categories":[],"contents":"","permalink":"https://cold-bin.github.io/search/","tags":[],"title":"搜索结果展示"},{"categories":["golang"],"contents":"[toc]\n定义 顾名思义，锁就是可以锁住某些东西的东西。在计算机科学中，锁(lock)是一种同步机制，用于在有许多执行线程的环境中强制对资源的访问限制。锁旨在强制实施互斥排他、并发控制策略。\n所谓的锁，可以理解为内存中的一个整型数或者一个结构体对象，拥有两种状态：空闲状态和上锁状态。加锁时，判断锁是否空闲，如果空闲，修改为上锁状态，返回成功；如果已经上锁，则返回失败。解锁时，则把锁状态修改为空闲状态。\n过程：\n1 2 3 4 1. read to get state of lock 2. judge the state of lock a. if locked,failed and return b. else if unlocked,lock and return 由上面的锁的控制过程可以知道：我们可以利用这种结构来作为资源的并发监视：将并发资源与锁进行匹配，一旦某个线程对当前资源发起读写操作，为了保证数据并发安全，需要将当前资源进行上锁。\n应用场景及分类 互斥锁\n一次只能一个线程拥有互斥锁，其他线程只有等待。互斥锁是在抢锁失败的情况下主动放弃CPU进入睡眠状态直到锁的状态改变时再唤醒，互斥锁在加锁操作时涉及上下文的切换。\n自旋锁\n在任何时刻同样只能有一个线程访问对象（这点和互斥锁一样）。**但是当获取锁操作失败时，不会进入睡眠，而是会在原地“自旋”，直到锁被释放。**这样节省了线程从睡眠状态到被唤醒期间的消耗，在加锁时间短暂的环境下会极大的提高效率。但如果加锁时间过长，则会非常浪费CPU资源。一直让锁跑在CPU上轮询锁是否释放。因此，自旋锁比较适用于加锁时间较短的场景，如果加锁时间过长的话，可以考虑使用互斥锁，直接放弃CPU进入睡眠。\ngolang可以通过runtime提供的原子操作CompareAndSwapInt32(即CAS)来做一个简易的自旋锁。实现的效果：\n只有第一个来获取锁的能成功（即 Swap），其他的由于 old 对不上，所以 CAS 的返回值 swapped 为 false； 无法获取锁的 goroutine 进入自旋（也就是先休眠，过一段时间继续探测能否获取锁）。 1 2 3 4 5 6 7 8 for { // 如果num2这个变量旧值为10那么会被成功修改为0，如果不为10，那么不会被修改就会一直在CPU原地地自旋直到出现10或超时 if atomic.CompareAndSwapInt32(\u0026amp;num2, 10, 0) { fmt.Println(\u0026#34;The second number has gone to zero.\u0026#34;) break } time.Sleep(time.Millisecond * 500) } 读写锁\u0026ndash;共享-独占锁\n读写锁是特殊的自旋锁。特性是读共享，写互斥。一次只有一个线程可以占有写模式的读写锁, 但是可以有多个线程同时占有读模式的读写锁。\n悲观锁\n顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。\n但是悲观锁本身比较悲观，比较得浪费资源：悲观地认为每次操作都是写操作，每次操作都会上锁来保证写操作的数据同步。但是真实的业务场景不一定是写多读少，例如日志存档这种应用场景，几乎只有insert和select操作。所以，契合这种读多写少的业务场景，可以使用乐观锁的机制来保证数据的并发安全。\n只要不在内存真的生成一个互斥锁的内部结构，这样就要性能稍微好点，一旦发生锁的阻塞，可能会涉及上下文的切换和锁等待，比较耗费系统资源\n乐观锁\n顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号或时间戳等机制。乐观锁适用于多读少写的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，又或者说数据表的版本号，其实都是提供的乐观锁。\n但是针对写多读少的场景，乐观锁就显得捉襟见肘勒。假设一段时间内服务器收到了大量的请求，其中大部分请求都是DML，少部分请求是DQL。那么采用乐观锁的机制来实现并发安全机制，就会使得短时间内会有大量的DML失败返回，然后不断重试直到成功。在系统外部来看，这感觉像是一个事故。因此针对写多读少的场景应该考虑使用悲观锁来保证数据的并发安全。\n信号量\n是用于线程间同步和互斥的，当一个线程完成操作后就通过信号量通知其它线程，然后别的线程就可以继续进行某些操作。\nGo语言的惯用法就是将带缓冲 channel 用作计数信号量。带缓冲channel中的当前数据个数代表的是当前同时处于活动状态的goroutine的数量，而带缓冲channel的容量就代表了允许同时处于活动状态的goroutine的最大数量。\n向带缓冲 channel 的一个发送操作表示获取一个信号量，而从 channel 的一个接收操作则表示释放一个信号量。\n计数信号量经常被使用于限制最大并发数，例如：实现一个同一时间允许的最多5个goroutine工作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \u0026#34;log\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() { active := make(chan struct{}, 5) // 代表允许同时活跃G的数量 jobs := make(chan int, 10) // 代表G的最大数量 go func() { for i := 0; i \u0026lt; 8; i++ { jobs \u0026lt;- i + 1 } close(jobs) }() var wg sync.WaitGroup for j := range jobs { wg.Add(1) active \u0026lt;- struct{}{} go func(j int) { defer func() { \u0026lt;-active }() log.Printf(\u0026#34;handle job: %d\\n\u0026#34;, j) time.Sleep(2 * time.Second) wg.Done() }(j) } wg.Wait() } 不要通过共享内存进行通信。建议，通过通信来共享内存。（Do not communicate by sharing memory; instead, share memory by communicating）\t\u0026ndash; Go 语言并发\n应用实例 MySQL的并发事务访问记录的问题 以InnoDB引擎为例，并发事务访问相同记录的情况大致可以划分为3种：\n读-读情况 读-读情况，即并发事务相继读取相同的记录。读取操作本身不会对记录有任何影响，并不会引起什么问题，所以允许这种情况的发生。（读写锁）\n写-写情况 写-写情况，即并发事务相继对相同的记录做出改动。\n在这种情况下可能会发生脏写的问题，任何一种隔离级别都不允许这种问题的发生。所以在多个未提交事务相继对一条记录做改动时，需要让它们排队执行，这个排队的过程其实是通过锁来实现的。这个锁其实是一个内存中的结构 ，在事务执行前本来是没有锁的，也就是说一开始是没有锁结构和记录进行关联的，如图所示：\n当一个事务想对这条记录做改动时，首先会看看内存中有没有与这条记录关联的锁结构，当没有的时候就会在内存中生成一个锁结构与之关联。比如，事务 T1要对这条记录做改动，就需要生成一个锁结构与之关联：\n在锁结构里有很多信息，为了简化理解，只把两个比较重要的属性拿了出来：\ntrx信息：代表这个锁结构是哪个事务生成的。 is_waiting：代表当前事务是否在等待。 在事务T1改动了这条记录后，就生成了一个锁结构与该记录关联，因为之前没有别的事务为这条记录加锁，所以is_waiting属性就是false，我们把这个场景就称值为获取锁成功，或者加锁成功，然后就可以继续执行操作了。\n在事务T1提交之前，另一个事务T2也想对该记录做改动，那么先看看有没有锁结构与这条记录关联，发现有一个锁结构与之关联后，然后也生成了一个锁结构与这条记录关联，不过锁结构的is_waiting属性值为true，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，图示：\n在事务T1提交之后，就会把该事务生成的锁结构释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务T2还在等待获取锁，所以把事务T2对应的锁结构的is_waiting属性设置为false，然后把该事务对应的线程唤醒，让它继续执行，此时事务T2就算获取到锁了。效果就是这样。\n小结\n不加锁\n意思就是不需要在内存中生成对应的锁结构，可以直接执行操作。\n获取锁成功，或者加锁成功\n意思就是在内存中生成了对应的锁结构，而且锁结构的is_waiting属性为false，也就是事务可以继续执行操作。\n获取锁失败，或者加锁失败，或者没有获取到锁\n意思就是在内存中生成了对应的锁结构，不过锁结构的is_waiting属性为true，也就是事务需要等待，不可以继续执行操作。\n读-写或写-读情况 读-写或写-读，即一个事务进行读取操作，另一个进行改动操作。这种情况下可能发生脏读、不可重复读、幻读的问题。\n各个数据库厂商对SQL标准的支持都可能不一样。比如MySQL在REPEATABLE READ隔离级别上就已经解决了幻读问题（MVCC\u0026ndash;\u0026gt;快照读+next-key lock\u0026ndash;\u0026gt;当前读）。\n并发问题的解决方案 怎么解决脏读、不可重复读、幻读这些问题呢？其实有两种可选的解决方案：\n方案一：读操作利用多版本并发控制（MVCC），写操作进行加锁（next-key lock）。——MySQl8.0默认的REPEATABLE READ隔离级别\n所谓的MVCC，就是生成一个ReadView，通过ReadView找到符合条件的记录版本（历史版本由undo log版本单链表构建)。查询语句只能读到在生成ReadView之前已提交事务所做的更改，在生成ReadView之前未提交的事务或者之后才开启的事务所做的更改是看不到的。而写操作肯定针对的是最新版本的记录，读记录的历史版本和改动记录的最新版本本身并不冲突，也就是采用MVCC时，读-写操作并不冲突。\n在REPEATABLE READ隔离级别下，一个事务在执行过程中只有第一次执行SELECT操作才会生成一个ReadView，之后的SELECT操作都复用这个ReadView，这样也就避免了不可重复读和幻读的问题\n所谓next-key lock就是临键锁：会根据查询等语句的原义，将扫描到的间隙范围加上锁，这样就能防止一个事务执行过程中相同查询语句出现“幻影”记录。\n方案二：读、写操作都采用加锁的方式。\n如果我们的一些业务场景不允许读取记录的旧版本，而是每次都必须去读取记录的最新版本。\n比如，在银行存款的事务中，你需要先把账户的余额读出来，然后将其加上本次存款的数额，最后再写到数据库中。在将账户余额读取出来后，就不想让别的事务再访问该余额，直到本次存款事务执行完成，其他事务才可以访问账户的余额。这样在读取记录的时候就需要对其进行加锁操作，这样也就意味着读操作和写操作也像写-写操作那样排队执行。\n脏读的产生是因为当前事务读取了另一个未提交事务写的一条记录，如果另一个事务在写记录的时候就给这条记录加锁，那么当前事务就无法继续读取该记录了，所以也就不会有脏读问题的产生了。\n不可重复读的产生是因为当前事务先读取一条记录，另外一个事务对该记录做了改动之后并提交，当前事务再次读取时会获得不同的值，如果在当前事务读取记录时就给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。\n幻读问题的产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记录。采用加锁的方式解决幻读问题就有一些麻烦，因为当前事务在第一次读取记录时幻影记录并不存在，所以读取的时候加锁就有点尴尬（因为你并不知道给谁加锁)。\nMySQL最高隔离级别，事务之间串行化执行，就避免了所有那四大问题\n小结\n采用MVCC方式的话，读-写操作彼此并不冲突性能更高。 采用加锁方式的话，读-写操作彼此需要排队执行，影响性能。 一般情况下我们当然愿意采用MVCC来解决读-写操作并发执行的问题，但是业务在某些特殊情况下，要求必须采用加锁的方式执行。\n","permalink":"https://cold-bin.github.io/post/%E9%94%81/","tags":["并发安全","mysql隔离级别"],"title":"锁"},{"categories":["golang"],"contents":"[toc]\n原子操作 并发是业务开发中经常要面对的问题，很多时候我们会直接用一把 sync.Mutex 互斥锁来线性化处理，保证每一时刻进入临界区的 goroutine 只有一个。这样避免了并发，但性能也随着降低。\n所以，我们进而又有了 RWMutex 读写锁，保障了多个读请求的并发处理，对共享资源的写操作和读操作则区别看待，并消除了读操作之间的互斥。\nMutex 和 RWMutex 锁的实现本身还是基于 atomic 包提供的原子操作，辅之以自旋等处理。很多时候其实我们不太需要锁住资源这个语意，而是一个原子操作就ok。这篇文章我们来看一下 atomic 包提供的能力。\n定义 不会被线程调度机制打断的操作。\n\u0026ldquo;原子操作(atomic operation)是不需要synchronized\u0026rdquo;，这是多线程编程的老生常谈了。所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch（切换到另一个线程）。使用原子操作，就可以保证你的操作一定是原子的，也就是不会被同一时刻其他并发线程打断，一定会执行。\nGolang的原子操作 Go语言通过内置包sync/atomic提供了对原子操作的支持，其提供的原子操作有以下几大类：\n增减，操作的方法名方式为AddXXXType，保证对操作数进行原子的增减，支持的类型为int32、int64、uint32、uint64、uintptr，使用时以实际类型替换前面我说的XXXType就是对应的操作方法。 载入，保证了读取到操作数前没有其他任务对它进行变更，操作方法的命名方式为LoadXXXType，支持的类型除了基础类型外还支持Pointer，也就是支持载入任何类型的指针。 存储，有载入了就必然有存储操作，这类操作的方法名以Store开头，支持的类型跟载入操作支持的那些一样。 比较并交换，也就是CAS（Compare And Swap），像Go的很多并发原语实现就是依赖的CAS操作，同样是支持上面列的那些类型。 交换，这个简单粗暴一些，不比较直接交换，这个操作很少会用。 互斥锁跟原子操作的区别 平日里，在并发编程里，Go语言sync包里的同步原语Mutex是我们经常用来保证并发安全的，那么他跟atomic包里的这些操作有啥区别呢？在我看来他们在使用目的和底层实现上都不一样：\n使用目的：互斥锁是用来保护一段逻辑，原子操作用于对一个变量的更新保护。 底层实现：Mutex由操作系统的调度器实现（上下文切换开销较大），而atomic包中的原子操作则由底层硬件指令直接提供支持，这些指令在执行的过程中是不允许中断的，因此原子操作可以在lock-free的情况下保证并发安全，并且它的性能也能做到随CPU个数的增多而线性扩展。 对于一个变量更新的保护，原子操作通常会更有效率，并且更能利用计算机多核的优势。\n所以，如果在业务里只是针对单个变量并发运算的安全时，不应该考虑加锁而是考虑使用原子操作保证其原子性。貌似原子操作是乐观锁的实现，如果同一时刻有太多变量对同一个变量进行写操作，使用CAS机制时，最多同一时刻只能有一个操作成功，其余操作全部失败终止或者进入原地地“自旋”。\n使用互斥锁的并发计数器程序的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func mutexAdd() { var a int32 = 0 var wg sync.WaitGroup var mu sync.Mutex // 互斥锁 start := time.Now() for i := 0; i \u0026lt; 100000000; i++ { wg.Add(1) go func() { defer wg.Done() mu.Lock() a += 1 mu.Unlock() }() } wg.Wait() timeSpends := time.Now().Sub(start).Nanoseconds() fmt.Printf(\u0026#34;use mutex a is %d, spend time: %v\\n\u0026#34;, a, timeSpends) } 把Mutex改成用方法atomic.AddInt32(\u0026amp;a, 1)调用，在不加锁的情况下仍然能确保对变量递增的并发安全。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func AtomicAdd() { var a int32 = 0 var wg sync.WaitGroup start := time.Now() for i := 0; i \u0026lt; 1000000; i++ { wg.Add(1) go func() { defer wg.Done() atomic.AddInt32(\u0026amp;a, 1) }() } wg.Wait() timeSpends := time.Now().Sub(start).Nanoseconds() fmt.Printf(\u0026#34;use atomic a is %d, spend time: %v\\n\u0026#34;, atomic.LoadInt32(\u0026amp;a), timeSpends) } 可以在本地运行以上这两段代码，可以观察到计数器的结果都最后都是1000000，都是线程安全的。\n上面两种方式：第一种互斥锁实现的并发计数例子里，每一次开了大量携程来对a加1，为了保证并发安全加了互斥锁，虽然有效但是性能开销太大了，期间发生了很多次上下文切换；第二种使用原子操作来对a变量实现加1操作，这个过程是原子的，其他协程无法打断，只能等待指令执行成功之后，其他协程才能拿到这个a的地址又进行原子操作，这样循环直到所有协程执行完毕后程序结束。相比第一种加互斥锁保证并发安全的方式，第二种原子操作的方式是通过CPU指令集实现的，没有上下文切换的开销，只是执行一个CPU指令而已。\n比较并交换 该操作简称CAS (Compare And Swap)。这类操作的前缀为 CompareAndSwap :\n1 2 3 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 该操作在进行交换前首先确保被操作数的值未被更改，即仍然保存着参数 old 所记录的值，满足此前提条件下才进行交换操作。CAS的做法类似操作数据库时常见的乐观锁机制。\n需要注意的是：当有大量的goroutine 对变量进行读写操作时，可能导致CAS操作无法成功，这时可以利用for循环多次尝试。(乐观锁不适合写多读少的场景)\n上面我只列出了比较典型的int32和unsafe.Pointer类型的CAS方法，主要是想说除了读数值类型进行比较交换，还支持对指针进行比较交换。\nunsafe.Pointer提供了绕过Go语言指针类型限制的方法，unsafe指的并不是说不安全，而是说官方并不保证向后兼容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 定义一个struct类型P type P struct{ x, y, z int } // 执行类型P的指针 var pP *P func main() { // 定义一个执行unsafe.Pointer值的指针变量 var unsafe1 = (*unsafe.Pointer)(unsafe.Pointer(\u0026amp;pP)) // Old pointer var sy P // 为了演示效果先将unsafe1设置成Old Pointer px := atomic.SwapPointer( unsafe1, unsafe.Pointer(\u0026amp;sy)) // 执行CAS操作，交换成功，结果返回true y := atomic.CompareAndSwapPointer( unsafe1, unsafe.Pointer(\u0026amp;sy), px) fmt.Println(y) } 上面的示例并不是在并发环境下进行的CAS，只是为了演示效果，先把被操作数设置成了Old Pointer。\n其实Mutex的底层实现也是依赖原子操作中的CAS实现的，原子操作的atomic包相当于是sync包里的那些同步原语的实现依赖。\n比如互斥锁Mutex的结构里有一个state字段，其是表示锁状态的状态位。\n1 2 3 4 type Mutex struct { state int32 sema uint32 } 为了方便理解，我们在这里将它的状态定义为0和1，0代表目前该锁空闲，1代表已被加锁，以下是sync.Mutex中Lock方法的部分实现代码。\n1 2 3 4 5 6 7 8 9 10 11 func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026amp;m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } 在atomic.CompareAndSwapInt32(\u0026amp;m.state, 0, mutexLocked)中，m.state代表锁的状态，通过CAS方法，判断锁此时的状态是否空闲（m.state==0），是，则对其加锁（mutexLocked常量的值为1）。\natomic.Value保证任意值的读写安全 atomic包里提供了一套Store开头的方法，用来保证各种类型变量的并发写安全，避免其他操作读到了修改变量过程中的脏数据。\n1 2 3 4 5 6 7 func StoreInt32(addr *int32, val int32) func StoreInt64(addr *int64, val int64) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) ... 这些操作方法的定义与上面介绍的那些操作的方法类似，我就不再演示怎么使用这些方法了。\n值得一提的是如果你想要并发安全的设置一个结构体的多个字段，除了把结构体转换为指针，通过StorePointer设置外，还可以使用atomic包后来引入的atomic.Value，它在底层为我们完成了从具体指针类型到unsafe.Pointer之间的转换。\n有了atomic.Value后，它使得我们可以不依赖于不保证兼容性的unsafe.Pointer类型，同时又能将任意数据类型的读写操作封装成原子性操作（中间状态对外不可见）。\natomic.Value类型对外暴露了两个方法：\nv.Store(c) - 写操作，将原始的变量c存放到一个atomic.Value类型的v里。 c := v.Load() - 读操作，从线程安全的v中读取上一步存放的内容。 1.17 版本我看还增加了Swap和CompareAndSwap方法。\n简洁的接口使得它的使用也很简单，只需将需要做并发保护的变量读取和赋值操作用Load()和Store()代替就行了。\n由于Load()返回的是一个interface{}类型，所以在使用前我们记得要先转换成具体类型的值，再使用。下面是一个简单的例子演示atomic.Value的用法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 type Demo struct { A int B int } var rect atomic.Value func main() { wg := sync.WaitGroup{} var container *Demo = \u0026amp;Demo{} // 10 个协程并发更新 for i := 0; i \u0026lt; 100; i++ { wg.Add(1) go func() { defer wg.Done() container.A = i container.B = i + 1 rect.Store(container) }() } wg.Wait() _r := rect.Load().(*Demo) fmt.Printf(\u0026#34;rect.width=%d\\nrect.length=%d\\n\u0026#34;, _r.A, _r.B) } 你也可以试试，不用atomic.Value，看看在并发条件下，两个字段的值是不是能跟预期的一样变成10和15。\n总结 原子操作由底层硬件支持，而锁则由操作系统的调度器实现。\n锁应当用来保护一段逻辑\n对于一个变量更新的保护，原子操作通常会更有效率，并且更能利用计算机多核的优势\n如果要更新的是一个复合对象，则应当使用atomic.Value封装好的实现\n项目案例分析 在net\\http官方包里，http.ListenAndServe()里面的逻辑就是：来一个请求就开一个协程去处理它。在高并发的场景下，虽然go可以轻松开启数百万协程，每个协程初始被分配2kb的内存大小。来一个请求就开一个协程，这样的方式太过粗暴，耗费资源。我们可以对goroutine做池化处理：维护cap容量的goroutine工作，当执行的任务小于cap时，这时资源是充足的，可以让每个goroutine对应一个执行任务；当执行任务大于cap时，这时认为资源是不足的，我们需要复用goroutine，也就是会存在一个goroutine执行完当前任务时，就再执行下一个任务，达到复用的目的。\nPool Pool是对外暴露的方法集合的接口。内部是pool结构体来实现的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Pool interface { // Name returns the corresponding pool name. Name() string // SetCap sets the goroutine capacity of the pool. SetCap(cap int32) // Go executes f. Go(f func()) // CtxGo executes f and accepts the context. CtxGo(ctx context.Context, f func()) // SetPanicHandler sets the panic handler. SetPanicHandler(f func(context.Context, interface{})) // WorkerCount returns the number of running workers WorkerCount() int32 } 简单看看这几个方法\nName() string 方法是指定当前池的名字 SetCap(cap int32)用来动态扩容，可以看到源码里是并发安全的（原子的将新值加载到pool.cap的） Go(f func())将函数放到任务链表里，并判断是否需要新开worker（即协程）来分担f所在的任务链表执行 CtxGo(ctx context.Context, f func())同Go(f func() SetPanicHandler(f func(context.Context, interface{}))是自定义panic处理逻辑，如果不定义将会默认调用github.com/bytedance/gopkg/util/logger来处理 WorkerCount() int32动态获取当前运行的worker，也就是正在跑的goroutine数量 pool pool是内部包自己实现Pool接口的结构体。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type pool struct { // The name of the pool name string // capacity of the pool, the maximum number of goroutines that are actually working cap int32 // 也就是限制 worker 的数量 // Configuration information config *Config // gopool提供默认配置，当任务数大于1时，就新开一个goroutine // 真正放函数的地方，所有的函数都放在这里 // linked list of tasks taskHead *task // 头节点 taskTail *task // 指向最后一个节点，实现一个O(1)的复杂度添加 taskLock sync.Mutex // 保证并发安全 taskCount int32 // 任务数量 // Record the number of running workers workerCount int32 // 正在跑的worker // This method will be called when the worker panic panicHandler func(context.Context, interface{}) } name是当前池的名字。 cap是当前池的容量，即允许在跑的最大协程数 config标识当前每个worker应当被分配的任务函数个数，包里默认是1 taskXXXX是任务链表，taskLock是为了保证代码片段的并发安全，taskCount动态标识当前还未执行的任务函数 workerCount标识当前正在跑的协程数 panicHandler自定义panic处理 pool的方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 func (p *pool) Name() string { return p.name } // SetCap 通过CPU指令集保证是原子的方式存到指定地址，保证并发安全 func (p *pool) SetCap(cap int32) { // 将值原子地设置到p里 atomic.StoreInt32(\u0026amp;p.cap, cap) } func (p *pool) Go(f func()) { p.CtxGo(context.Background(), f) } func (p *pool) CtxGo(ctx context.Context, f func()) { // 池里取空的对象 t := taskPool.Get().(*task) // 置新 t.ctx = ctx t.f = f // 下面代码逻辑上锁 p.taskLock.Lock() // 将 task 放到任务链表里，这个过程由于是多个Goroutine操作并发操作，都在往任务链表里塞 task ， // 因此，需要将task入链表的代码锁住，防止并发问题 if p.taskHead == nil { // 链表为空 p.taskHead = t p.taskTail = t } else { // p.taskTail.next = t p.taskTail = t } p.taskLock.Unlock() // 然后原子更改任务状态，不需要加锁（避免上下文切换），这样可以获得更好的性能 atomic.AddInt32(\u0026amp;p.taskCount, 1) // The following two conditions are met: // 1. the number of tasks is greater than the threshold. // 2. The current number of workers is less than the upper limit p.cap. // or there are currently no workers. // 根据默认的配置：worker数大于等于 p.cap时就会，就不会新建 worker了，此时就会复用已有的worker来执行； // 在小于p.cap时就会每个task新建一个goroutine if (atomic.LoadInt32(\u0026amp;p.taskCount) \u0026gt;= p.config.ScaleThreshold \u0026amp;\u0026amp; p.WorkerCount() \u0026lt; atomic.LoadInt32(\u0026amp;p.cap)) || p.WorkerCount() == 0 { // 新开一个 worker p.incWorkerCount() // 池里拿 w := workerPool.Get().(*worker) // p放到worker的pool里 w.pool = p // 再开一个协程跑，取这个worker的任务并执行 w.run() } } // SetPanicHandler the func here will be called after the panic has been recovered. func (p *pool) SetPanicHandler(f func(context.Context, interface{})) { p.panicHandler = f } func (p *pool) WorkerCount() int32 { return atomic.LoadInt32(\u0026amp;p.workerCount) } func (p *pool) incWorkerCount() { atomic.AddInt32(\u0026amp;p.workerCount, 1) } func (p *pool) decWorkerCount() { atomic.AddInt32(\u0026amp;p.workerCount, -1) } worker worker结构体里有一个pool指针，在gopool的逻辑实现里，多个worker的pool指向同一个pool。意思是多个worker针对同一个pool里的任务链表处理\n1 2 3 type worker struct { pool *pool // 多个worker都指向这个pool } worker的方法 run 新开一个携程去轮询任务链表，并作panic的处理。值得注意的是：取任务链表里的任务时，必须上锁。因为此时有多个协程并发地取任务链表执行。panic处理也挺有意思，这里是将panic和任务函数调用放到一个匿名函数调用里，这样就可以使得panic地捕获一定是任务函数地panic，而不是捕获run方法里的panic\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 func (w *worker) run() { go func() { for { var t *task // 必须上锁，因为可能有多个 worker（也就是多个协程） 同时再对这个pool里的task链表做取出操作，并执行这个task w.pool.taskLock.Lock() // 不断拿出 task if w.pool.taskHead != nil { t = w.pool.taskHead w.pool.taskHead = w.pool.taskHead.next // 原子的减少任务数 atomic.AddInt32(\u0026amp;w.pool.taskCount, -1) } // 任务取完了 if t == nil { // if there\u0026#39;s no task to do, exit w.close() w.pool.taskLock.Unlock() // 回收 w.Recycle() return } // 释放时机到了 w.pool.taskLock.Unlock() // 匿名函数调用，退一个栈从而形成独立环境，不让当前的defer去捕获外面函数里的异常，只是捕获这个执行函数的panic // 确保这个defer一定是捕获的task的panic func() { // 处理panic defer func() { if r := recover(); r != nil { if w.pool.panicHandler != nil { w.pool.panicHandler(t.ctx, r) } else { msg := fmt.Sprintf(\u0026#34;GOPOOL: panic in pool: %s: %v: %s\u0026#34;, w.pool.name, r, debug.Stack()) logger.CtxErrorf(t.ctx, msg) } } }() // 真正地执行函数 t.f() }() // 回收task t.Recycle() } }() } 其他方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 协程数减一 func (w *worker) close() { w.pool.decWorkerCount() } func (w *worker) zero() { w.pool = nil } // Recycle 回收worker func (w *worker) Recycle() { w.zero() // 放回池里 workerPool.Put(w) } task\n1 2 3 4 5 6 type task struct { ctx context.Context f func() next *task } 1 2 3 4 5 6 7 8 9 10 11 12 13 func (t *task) zero() { t.ctx = nil t.f = nil t.next = nil } // Recycle 回收到 sync.Pool里 func (t *task) Recycle() { // task 置空 t.zero() // 放到池里 taskPool.Put(t) } ","permalink":"https://cold-bin.github.io/post/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/","tags":["协程池","原子操作"],"title":"并发安全之原子操作"},{"categories":["http"],"contents":"简介 超文本传输协议（英文：HyperText Transfer Protocol，缩写：HTTP）是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP是万维网的数据通信的基础。它是基于TCP协议的应用层传输协议，简单来说就是客户端和服务端进行数据传输的一种规则。\nTCP：大家感兴趣的话就去学习《计算机网络》，反正以后都会学，这里简单描述一下，是一种面向连接的、可靠的、基于字节流的传输层通信协议。\nURL URL(Uniform Resource Locator)是“统一资源定位符”的英文缩写，用于描述一个网络上的资源, 基本格式如下\n1 2 3 scheme://host[:port#]/path/.../[?query-string][#anchor] 例如 http://localhost:8080/login?username=admin\u0026amp;password=123456 scheme: 指定使用的协议（如：http，https，ftp(File Transfer Protocol，文件传输协议)等）\nhost: http服务器的域名或者ip地址\nport: 指定的端口（默认为80）\npath：访问资源的路径\nquery-string：发送给http服务器的数据\nanchor：锚（如 http://localhost:8080/index#top ,这里的锚就是top）。#是用来指导浏览器动作的，对服务器端完全无用。所以，HTTP请求中不包括#。比如说跳转到顶部，第多少行啥的。\n请求报文 我们先来看看 Request 包的结构，Request 包分为 3 部分，第一部分叫 Request line（请求行）, 第二部分叫 Request header（请求头）, 第三部分是 body（主体）。header 和 body 之间有个空行，请求包的例子所示:\n响应报文 Response 包中的第一行叫做状态行，由 HTTP 协议版本号， 状态码， 状态消息三部分组成。\n状态码用来告诉 HTTP 客户端，HTTP 服务器是否产生了预期的 Response。HTTP/1.1 协议中定义了 5 类状态码， 状态码由三位数字组成，第一个数字定义了响应的类别\n1XX 提示信息 - 表示请求已被成功接收，继续处理（这个常用于内部，比如：当我们发post请求的时候，实际上会发送两次，先发送header，服务器响应100 continue，浏览器再发送body））\n2XX 成功 - 表示请求已被成功接收，理解，接受（如：200 - 请求成功，204 - 服务器成功处理，但未返回内容 等）\n3XX 重定向 - 要完成请求必须进行更进一步的处理（如：301 - 永久移动，302 - 临时移动 ，304 - 未更新等）\n4XX 客户端错误 - 请求有语法错误或请求无法实现（如:404）\n5XX 服务器端错误 - 服务器未能实现合法的请求（如：500 - 服务器内部错误，503 - 由于超载或系统维护，服务器暂时的无法处理客户端的请求）\n点击查看更多\nHTTP缓存 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Caching\nHTTP头部 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers\nHTTP认证 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Authentication\n保持状态的方式 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Cookies\nCookie:\nHTTP 协议是无状态， Cookie 来保存状态信息。Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销。\n实现方式：写在请求头里面\n用途：\n会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）\n个性化设置（如用户自定义设置、主题等）\n浏览器行为跟踪（如跟踪分析用户行为，定制个性化广告（突然想起了之前看的一个新闻：谷歌将终止第三方Cookie采集隐私数据，广告商遇“灭顶之灾”？）等）\nSession:\nsession是一种记录客户端状态的机制，不同的是cookie保存在客户端浏览器中，而session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这就是session。客户端浏览器再次访问时只需要从该session中查找该客户的状态就可以了。\nsession的原理是第一次访问的时候，随机生成一个sid并生成一个sid对应的对象，然后把这个sid发送到浏览器端。下次客户端把sid带回服务器，从服务器上找到这个sid对应的对象就可以了。\n由于使用session时会给cookie存一个sessionId(即加密后的connect.id),所以session是依赖于cookie的。（一般是这样，但是你也可以写在query-string，bodyl里面也行）\n注意 application/x-www-form-urlencoded\n表单代码：\n1 2 3 4 5 \u0026lt;form action=\u0026#34;http://localhost:8888/task/\u0026#34; method=\u0026#34;POST\u0026#34;\u0026gt; First name: \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;firstName\u0026#34; value=\u0026#34;Mickey\u0026amp;\u0026#34;\u0026gt;\u0026lt;br\u0026gt; Last name: \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;lastName\u0026#34; value=\u0026#34;Mouse \u0026#34;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;提交\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; 通过测试发现可以正常访问接口，在Chrome的开发者工具中可以看出，表单上传编码格式为application/x-www-form-urlencoded(Request Headers中)，参数的格式为key=value\u0026amp;key=value。\n我们可以看出，服务器知道参数用符号\u0026amp;间隔，如果参数值中需要\u0026amp;，则必须对其进行编码。编码格式就是application/x-www-form-urlencoded（将键值对的参数用\u0026amp;连接起来，如果有空格，将空格转换为+加号；有特殊符号，将特殊符号转换为ASCII HEX值）。\n对于Get请求，是将参数转换?key=value\u0026amp;key=value格式，连接到url后；如果是post方法\n","permalink":"https://cold-bin.github.io/post/http%E5%8D%8F%E8%AE%AE%E7%9F%A5%E8%AF%86%E7%82%B9/","tags":[],"title":"HTTP协议知识点"},{"categories":["mysql"],"contents":"[toc]\n第17章_其他数据库日志 千万不要小看日志。很多看似奇怪的问题，答案往往就藏在日志里。很多情况下，只有通过查看日志才能发现问题的原因，真正解决问题。所以，一定要学会查看日志，养成检查日志的习惯，对提升你的数据库应用开发能力至关重要。\nMySQL8.0 官网日志地址：“ https://dev.mysql.com/doc/refman/8.0/en/server-logs.html ”\n1. MySQL支持的日志 1.1 日志类型 MySQL有不同类型的日志文件，用来存储不同类型的日志，分为 二进制日志 、 错误日志 、 通用查询日志 和 慢查询日志 ，这也是常用的4种。MySQL8又新增两种支持的日志：中继日志和数据定义语句日志。使用这些日志文件，可以查看MySQL内部发生的事情。\n这6类日志分别为：\n慢查询日志：记录所有执行时间超过long_query_time的所有查询，方便我们对查询进行优化。 通用查询日志：记录所有连接的起始时间和终止时间，以及连接发送给数据库服务器的所有指令， 对我们复原操作的实际场景、发现问题，甚至是对数据库操作的审计都有很大的帮助。 错误日志：记录MySQL服务的启动、运行或停止MySQL服务时出现的问题，方便我们了解服务器的状态，从而对服务器进行维护。 二进制日志：记录所有更改数据的语句，可以用于主从服务器之间的数据同步，以及服务器遇到故障时数据的无损失恢复。 中继日志：用于主从服务器架构中，从服务器用来存放主服务器二进制日志内容的一个中间文件。从服务器通过读取中继日志的内容，来同步主服务器上的操作。 数据定义语句日志：记录数据定义语句执行的元数据操作。 除二进制日志外，其他日志都是 文本文件 。默认情况下，所有日志创建于 MySQL数据目录 中。\n1.2 日志的弊端 日志功能会 降低MySQL数据库的性能 。例如，在查询非常频繁的MySQL数据库系统中，如果开启了通用查询日志和慢查询日志，MySQL数据库会花费很多时间记录日志。 日志会 占用大量的磁盘空间 。对于用户量非常大，操作非常频繁的数据库，日志文件需要的存储空间设置比数据库文件需要的存储空间还要大。 2. 慢查询日志(slow query log) 前面章节《第09章_性能分析工具的使用》已经详细讲述。\n3. 通用查询日志(general query log) 通用查询日志用来 记录用户的所有操作 ，包括启动和关闭MySQL服务、所有用户的连接开始时间和截止 时间、发给 MySQL 数据库服务器的所有 SQL 指令等。当我们的数据发生异常时，查看通用查询日志， 还原操作时的具体场景，可以帮助我们准确定位问题。\n3.1 问题场景 3.2 查看当前状态 1 2 3 4 5 6 7 8 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;%general%\u0026#39;; +------------------+------------------------------+ | Variable_name | Value | +------------------+------------------------------+ | general_log | OFF | #通用查询日志处于关闭状态 | general_log_file | /var/lib/mysql/atguigu01.log | #通用查询日志文件的名称是atguigu01.log +------------------+------------------------------+ 2 rows in set (0.03 sec) 3.3 启动日志 方式1：永久性方式\n修改my.cnf或者my.ini配置文件来设置。在[mysqld]组下加入log选项，并重启MySQL服务。格式如下：\n1 2 3 [mysqld] general_log=ON general_log_file=[path[filename]] #日志文件所在目录路径，filename为日志文件 如果不指定目录和文件名，通用查询日志将默认存储在MySQL数据目录中的hostname.log文件中， hostname表示主机名。\n方式2：临时性方式\n1 SET GLOBAL general_log=on; # 开启通用查询日志 1 SET GLOBAL general_log_file=’path/filename’; # 设置日志文件保存位置 对应的，关闭操作SQL命令如下：\n1 SET GLOBAL general_log=off; # 关闭通用查询日志 查看设置后情况：\n1 SHOW VARIABLES LIKE \u0026#39;general_log%\u0026#39;; 3.4 查看日志 通用查询日志是以 文本文件 的形式存储在文件系统中的，可以使用 文本编辑器 直接打开日志文件。每台 MySQL服务器的通用查询日志内容是不同的。\n在Windows操作系统中，使用文本文件查看器； 在Linux系统中，可以使用vi工具或者gedit工具查看； 在Mac OSX系统中，可以使用文本文件查看器或者vi等工具查看。 从 SHOW VARIABLES LIKE 'general_log%'; 结果中可以看到通用查询日志的位置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /usr/sbin/mysqld, Version: 8.0.26 (MySQL Community Server - GPL). started with: Tcp port: 3306 Unix socket: /var/lib/mysql/mysql.sock Time Id Command Argument 2022-01-04T07:44:58.052890Z 10 Query SHOW VARIABLES LIKE \u0026#39;%general%\u0026#39; 2022-01-04T07:45:15.666672Z 10 Query SHOW VARIABLES LIKE \u0026#39;general_log%\u0026#39; 2022-01-04T07:45:28.970765Z 10 Query select * from student 2022-01-04T07:47:38.706804Z 11 Connect root@localhost on using Socket 2022-01-04T07:47:38.707435Z 11 Query select @@version_comment limit 1 2022-01-04T07:48:21.384886Z 12 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:21.385253Z 12 Query SET NAMES utf8 2022-01-04T07:48:21.385640Z 12 Query USE `atguigu12` 2022-01-04T07:48:21.386179Z 12 Query SHOW FULL TABLES WHERE Table_Type != \u0026#39;VIEW\u0026#39; 2022-01-04T07:48:23.901778Z 13 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:23.902128Z 13 Query SET NAMES utf8 2022-01-04T07:48:23.905179Z 13 Query USE `atguigu` 2022-01-04T07:48:23.905825Z 13 Query SHOW FULL TABLES WHERE Table_Type != \u0026#39;VIEW\u0026#39; 2022-01-04T07:48:32.163833Z 14 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:32.164451Z 14 Query SET NAMES utf8 2022-01-04T07:48:32.164840Z 14 Query USE `atguigu` 2022-01-04T07:48:40.006687Z 14 Query select * from account 在通用查询日志里面，我们可以清楚地看到，什么时候开启了新的客户端登陆数据库，登录之后做了什么 SQL 操作，针对的是哪个数据表等信息。\n3.5 停止日志 方式1：永久性方式\n修改 my.cnf 或者 my.ini 文件，把[mysqld]组下的 general_log 值设置为 OFF 或者把general_log一项 注释掉。修改保存后，再重启MySQL服务 ，即可生效。\n举例1：\n1 2 [mysqld] general_log=OFF 举例2：\n1 2 [mysqld] #general_log=ON 方式2：临时性方式\n使用SET语句停止MySQL通用查询日志功能：\n1 SET GLOBAL general_log=off; 查询通用日志功能：\n1 SHOW VARIABLES LIKE \u0026#39;general_log%\u0026#39;; 3.6 删除\\刷新日志 如果数据的使用非常频繁，那么通用查询日志会占用服务器非常大的磁盘空间。数据管理员可以删除很长时间之前的查询日志，以保证MySQL服务器上的硬盘空间。\n手动删除文件\n1 SHOW VARIABLES LIKE \u0026#39;general_log%\u0026#39;; 可以看出，通用查询日志的目录默认为MySQL数据目录。在该目录下手动删除通用查询日志 atguigu01.log\n使用如下命令重新生成查询日志文件，具体命令如下。刷新MySQL数据目录，发现创建了新的日志文 件。前提一定要开启通用日志。\n1 mysqladmin -uroot -p flush-logs 如果希望备份旧的通用查询日志，就必须先将旧的日志文件复制出来或者改名，然后执行上面的mysqladmin命令。正确流程如下：\n1 2 3 cd mysql-data-directory # 输入自己的通用日志文件所在目录 mv mysql.general.log mysql.general.log.old # 指定旧的文件名 以及 新的文件名 mysqladmin -uroot -p flush-logs 4. 错误日志(error log) 4.1 启动日志 在MySQL数据库中，错误日志功能是 默认开启 的。而且，错误日志 无法被禁止 。\n默认情况下，错误日志存储在MySQL数据库的数据文件夹下，名称默认为 mysqld.log （Linux系统）或 hostname.err （mac系统）。如果需要制定文件名，则需要在my.cnf或者my.ini中做如下配置：\n1 2 [mysqld] log-error=[path/[filename]] #path为日志文件所在的目录路径，filename为日志文件名 修改配置项后，需要重启MySQL服务以生效。\n4.2 查看日志 MySQL错误日志是以文本文件形式存储的，可以使用文本编辑器直接查看。\n查询错误日志的存储路径：\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;log_err%\u0026#39;; +----------------------------+----------------------------------------+ | Variable_name | Value | +----------------------------+----------------------------------------+ | log_error | /var/log/mysqld.log | | log_error_services | log_filter_internal; log_sink_internal | | log_error_suppression_list | | | log_error_verbosity | 2 | +----------------------------+----------------------------------------+ 4 rows in set (0.01 sec) 执行结果中可以看到错误日志文件是mysqld.log，位于MySQL默认的数据目录下。\n4.3 删除\\刷新日志 对于很久以前的错误日志，数据库管理员查看这些错误日志的可能性不大，可以将这些错误日志删除， 以保证MySQL服务器上的 硬盘空间 。MySQL的错误日志是以文本文件的形式存储在文件系统中的，可以 直接删除 。\n第一步（方式1）：删除操作\n1 rm -f /var/lib/mysql/mysqld.log 在运行状态下删除错误日志文件后，MySQL并不会自动创建日志文件。\n第一步（方式2）：重命名文件\n1 mv /var/log/mysqld.log /var/log/mysqld.log.old 第二步：重建日志\n1 mysqladmin -uroot -p flush-logs 可能会报错\n1 2 3 4 [root@atguigu01 log]# mysqladmin -uroot -p flush-logs Enter password: mysqladmin: refresh failed; error: \u0026#39;Could not open file \u0026#39;/var/log/mysqld.log\u0026#39; for error logging.\u0026#39; 官网提示：\n补充操作：\n1 install -omysql -gmysql -m0644 /dev/null /var/log/mysqld.log 4.4 MySQL 8.0 新特性 小结：\n通常情况下，管理员不需要查看错误日志。但是，MySQL服务器发生异常时，管理员可以从错误日志中找到发生异常的时间、原因，然后根据这些信息来解决异常。\n5. 二进制日志(bin log) binlog可以说是MySQL中比较 重要 的日志了，在日常开发及运维过程中，经常会遇到。\nbinlog即binary log，二进制日志文件，也叫作变更日志（update log）。它记录了数据库所有执行的 DDL 和 DML 等数据库更新事件的语句，但是不包含没有修改任何数据的语句（如数据查询语句select、 show等）。\n它以事件形式记录并保存在二进制文件中。通过这些信息，我们可以再现数据更新操作的全过程。\n如果想要记录所有语句（例如，为了识别有问题的查询），需要使用通用查询日志。\nbinlog主要应用场景：\n5.1 查看默认情况 查看记录二进制日志是否开启：在MySQL8中默认情况下，二进制文件是开启的。\n1 2 3 4 5 6 7 8 9 10 11 12 mysql\u0026gt; show variables like \u0026#39;%log_bin%\u0026#39;; +---------------------------------+----------------------------------+ | Variable_name | Value | +---------------------------------+----------------------------------+ | log_bin | ON | | log_bin_basename | /var/lib/mysql/binlog | | log_bin_index | /var/lib/mysql/binlog.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | sql_log_bin | ON | +---------------------------------+----------------------------------+ 6 rows in set (0.00 sec) 5.2 日志参数设置 方式1：永久性方式\n修改MySQL的 my.cnf 或 my.ini 文件可以设置二进制日志的相关参数：\n1 2 3 4 5 [mysqld] #启用二进制日志 log-bin=atguigu-bin binlog_expire_logs_seconds=600 max_binlog_size=100M 重新启动MySQL服务，查询二进制日志的信息，执行结果：\n1 2 3 4 5 6 7 8 9 10 11 12 mysql\u0026gt; show variables like \u0026#39;%log_bin%\u0026#39;; +---------------------------------+----------------------------------+ | Variable_name | Value | +---------------------------------+----------------------------------+ | log_bin | ON | | log_bin_basename | /var/lib/mysql/atguigu-bin | | log_bin_index | /var/lib/mysql/atguigu-bin.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | sql_log_bin | ON | +---------------------------------+----------------------------------+ 6 rows in set (0.00 sec) 设置带文件夹的bin-log日志存放目录\n如果想改变日志文件的目录和名称，可以对my.cnf或my.ini中的log_bin参数修改如下：\n1 2 [mysqld] log-bin=\u0026#34;/var/lib/mysql/binlog/atguigu-bin\u0026#34; 注意：新建的文件夹需要使用mysql用户，使用下面的命令即可。\n1 chown -R -v mysql:mysql binlog 方式2：临时性方式\n如果不希望通过修改配置文件并重启的方式设置二进制日志的话，还可以使用如下指令，需要注意的是 在mysql8中只有 会话级别 的设置，没有了global级别的设置。\n1 2 3 4 5 6 7 8 # global 级别 mysql\u0026gt; set global sql_log_bin=0; ERROR 1228 (HY000): Variable \u0026#39;sql_log_bin\u0026#39; is a SESSION variable and can`t be used with SET GLOBAL # session级别 mysql\u0026gt; SET sql_log_bin=0; Query OK, 0 rows affected (0.01 秒) 5.3 查看日志 当MySQL创建二进制日志文件时，先创建一个以“filename”为名称、以“.index”为后缀的文件，再创建一个以“filename”为名称、以“.000001”为后缀的文件。\nMySQL服务 重新启动一次 ，以“.000001”为后缀的文件就会增加一个，并且后缀名按1递增。即日志文件的个数与MySQL服务启动的次数相同；如果日志长度超过了 max_binlog_size 的上限（默认是1GB），就会创建一个新的日志文件。\n查看当前的二进制日志文件列表及大小。指令如下：\n1 2 3 4 5 6 7 mysql\u0026gt; SHOW BINARY LOGS; +--------------------+-----------+-----------+ | Log_name | File_size | Encrypted | +--------------------+-----------+-----------+ | atguigu-bin.000001 | 156 | No | +--------------------+-----------+-----------+ 1 行于数据集 (0.02 秒) 所有对数据库的修改都会记录在binlog中。但binlog是二进制文件，无法直接查看，想要更直观的观测它就要借助mysqlbinlog命令工具了。指令如下：在查看执行，先执行一条SQL语句，如下\n1 update student set name=\u0026#39;张三_back\u0026#39; where id=1; 开始查看binlog\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 mysqlbinlog -v \u0026#34;/var/lib/mysql/binlog/atguigu-bin.000002\u0026#34; #220105 9:16:37 server id 1 end_log_pos 324 CRC32 0x6b31978b Query thread_id=10 exec_time=0 error_code=0 SET TIMESTAMP=1641345397/*!*/; SET @@session.pseudo_thread_id=10/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/; SET @@session.sql_mode=1168113696/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C utf8mb3 *//*!*/; SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collatio n_server=255/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; /*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/; BEGIN /*!*/; # at 324 #220105 9:16:37 server id 1 end_log_pos 391 CRC32 0x74f89890 Table_map: `atguigu14`.`student` mapped to number 85 # at 391 #220105 9:16:37 server id 1 end_log_pos 470 CRC32 0xc9920491 Update_rows: table id 85 flags: STMT_END_F BINLOG \u0026#39; dfHUYRMBAAAAQwAAAIcBAAAAAFUAAAAAAAEACWF0Z3VpZ3UxNAAHc3R1ZGVudAADAw8PBDwAHgAG AQEAAgEhkJj4dA== dfHUYR8BAAAATwAAANYBAAAAAFUAAAAAAAEAAgAD//8AAQAAAAblvKDkuIkG5LiA54+tAAEAAAAL 5byg5LiJX2JhY2sG5LiA54+tkQSSyQ== \u0026#39;/*!*/; ### UPDATE `atguigu`.`student` ### WHERE ### @1=1 ### @2=\u0026#39;张三\u0026#39; ### @3=\u0026#39;一班\u0026#39; ### SET ### @1=1 ### @2=\u0026#39;张三_back\u0026#39; ### @3=\u0026#39;一班\u0026#39; # at 470 #220105 9:16:37 server id 1 end_log_pos 501 CRC32 0xca01d30f Xid = 15 COMMIT/*!*/; 前面的命令同时显示binlog格式的语句，使用如下命令不显示它\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 mysqlbinlog -v --base64-output=DECODE-ROWS \u0026#34;/var/lib/mysql/binlog/atguigu-bin.000002\u0026#34; #220105 9:16:37 server id 1 end_log_pos 324 CRC32 0x6b31978b Query thread_id=10 exec_time=0 error_code=0 SET TIMESTAMP=1641345397/*!*/; SET @@session.pseudo_thread_id=10/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/; SET @@session.sql_mode=1168113696/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C utf8mb3 *//*!*/; SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collatio n_server=255/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; /*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/; BEGIN /*!*/; # at 324 #220105 9:16:37 server id 1 end_log_pos 391 CRC32 0x74f89890 Table_map: `atguigu14`.`student` mapped to number 85 # at 391 #220105 9:16:37 server id 1 end_log_pos 470 CRC32 0xc9920491 Update_rows: table id 85 flags: STMT_END_F ### UPDATE `atguigu14`.`student` ### WHERE ### @1=1 ### @2=\u0026#39;张三\u0026#39; ### @3=\u0026#39;一班\u0026#39; ### SET ### @1=1 ### @2=\u0026#39;张三_back\u0026#39; ### @3=\u0026#39;一班\u0026#39; # at 470 #220105 9:16:37 server id 1 end_log_pos 501 CRC32 0xca01d30f Xid = 15 关于mysqlbinlog工具的使用技巧还有很多，例如只解析对某个库的操作或者某个时间段内的操作等。简单分享几个常用的语句，更多操作可以参考官方文档。\n1 2 3 4 5 6 7 8 # 可查看参数帮助 mysqlbinlog --no-defaults --help # 查看最后100行 mysqlbinlog --no-defaults --base64-output=decode-rows -vv atguigu-bin.000002 |tail -100 # 根据position查找 mysqlbinlog --no-defaults --base64-output=decode-rows -vv atguigu-bin.000002 |grep -A 20 \u0026#39;4939002\u0026#39; 上面这种办法读取出binlog日志的全文内容比较多，不容易分辨查看到pos点信息，下面介绍一种更为方便的查询命令：\n1 mysql\u0026gt; show binlog events [IN \u0026#39;log_name\u0026#39;] [FROM pos] [LIMIT [offset,] row_count]; IN 'log_name' ：指定要查询的binlog文件名（不指定就是第一个binlog文件）　FROM pos ：指定从哪个pos起始点开始查起（不指定就是从整个文件首个pos点开始算） LIMIT [offset] ：偏移量(不指定就是0) row_count :查询总条数（不指定就是所有行） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; show binlog events in \u0026#39;atguigu-bin.000002\u0026#39;; +--------------------+-----+----------------+-----------+-------------+--------------------------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +--------------------+-----+----------------+-----------+-------------+--------------------------------------------------------+ | atguigu-bin.000002 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.26, Binlog ver: 4 | | atguigu-bin.000002 | 125 | Previous_gtids | 1 | 156 | | | atguigu-bin.000002 | 156 | Anonymous_Gtid | 1 | 235 | SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; | | atguigu-bin.000002 | 235 | Query | 1 | 324 | BEGIN | | atguigu-bin.000002 | 324 | Table_map | 1 | 391 | table_id: 85(atguigu14.student) | | atguigu-bin.000002 | 391 | Update_rows | 1 | 470 | table_id: 85flags: STMT_END_F | | atguigu-bin.000002 | 470 | Xid | 1 | 501 | COMMIT /*xid=15 */ | | atguigu-bin.000002 | 501 | Anonymous_Gtid | 1 | 578 | SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; | | atguigu-bin.000002 | 578 | Query | 1 | 721 | use `atguigu14`; create table test(id int, title varchar(100)) /* xid=19 */ | | atguigu-bin.000002 | 721 | Anonymous_Gtid | 1 | 800 | SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; | | atguigu-bin.000002 | 800 | Query | 1 | 880 | BEGIN | | atguigu-bin.000002 | 880 | Table_map | 1 | 943 | table_id: 89(atguigu14.test) | | atguigu-bin.000002 | 943 | Write_rows | 1 | 992 | table_id: 89 flags: STMT_END_F | | atguigu-bin.000002 | 992 | Xid | 1 | 1023 | COMMIT /*xid=21 */ | +--------------------+-----+----------------+-----------+-------------+--------------------------------------------------------+ 14 行于数据集 (0.02 秒) 上面我们讲了这么多都是基于binlog的默认格式，binlog格式查看\n1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;binlog_format\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 行于数据集 (0.02 秒) 除此之外，binlog还有2种格式，分别是Statement和Mixed\nStatement\n每一条会修改数据的sql都会记录在binlog中。\n优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。\nRow\n5.1.5版本的MySQL才开始支持row level 的复制，它不记录sql语句上下文相关信息，仅保存哪条记录被修改。\n优点：row level 的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下 的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题。\nMixed\n从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。\n详细情况，下章讲解。\n5.4 使用日志恢复数据 如果MySQL服务器启用了二进制日志，在数据库出现意外丢失数据时，可以使用MySQLbinlog工具从指定的时间点开始（例如，最后一次备份）直到现在或另一个指定的时间点的日志中回复数据。\nmysqlbinlog恢复数据的语法如下：\n1 mysqlbinlog [option] filename|mysql –uuser -ppass; 这个命令可以这样理解：使用mysqlbinlog命令来读取filename中的内容，然后使用mysql命令将这些内容恢复到数据库中。\nfilename ：是日志文件名。\noption ：可选项，比较重要的两对option参数是\u0026ndash;start-date、\u0026ndash;stop-date 和 \u0026ndash;start-position、\u0026ndash; stop-position。\n--start-date 和 --stop-date ：可以指定恢复数据库的起始时间点和结束时间点。 --start-position和--stop-position ：可以指定恢复数据的开始位置和结束位置。 注意：使用mysqlbinlog命令进行恢复操作时，必须是编号小的先恢复，例如atguigu-bin.000001必须在atguigu-bin.000002之前恢复。\n详见p189，由于翻页过快，这部分没办法记录。\n5.5 删除二进制日志 MySQL的二进制文件可以配置自动删除，同时MySQL也提供了安全的手动删除二进制文件的方法。 PURGE MASTER LOGS 只删除指定部分的二进制日志文件， RESET MASTER 删除所有的二进制日志文 件。具体如下：\n1. PURGE MASTER LOGS：删除指定日志文件\nPURGE MASTER LOGS语法如下：\n1 2 PURGE {MASTER | BINARY} LOGS TO ‘指定日志文件名’ PURGE {MASTER | BINARY} LOGS BEFORE ‘指定日期’ 2. RESET MASTER: 删除所有二进制日志文件\n5.6 其它场景 二进制日志可以通过数据库的 全量备份 和二进制日志中保存的 增量信息 ，完成数据库的 无损失恢复 。 但是，如果遇到数据量大、数据库和数据表很多（比如分库分表的应用）的场景，用二进制日志进行数据恢复，是很有挑战性的，因为起止位置不容易管理。\n在这种情况下，一个有效的解决办法是配置主从数据库服务器，甚至是一主多从的架构，把二进制日志文件的内容通过中继日志，同步到从数据库服务器中，这样就可以有效避免数据库故障导致的数据异常等问题。\n6. 再谈二进制日志(binlog) 6.1 写入机制 binlog的写入时机也非常简单，事务执行过程中，先把日志写到 binlog cache ，事务提交的时候，再把binlog cache写到binlog文件中。因为一个事务的binlog不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一个块内存作为binlog cache。(保证了事务的原子性)\n我们可以通过binlog_cache_size参数控制单个线程 binlog cache 大小，如果存储内容超过了这个参数，就要暂存到磁盘（Swap）。binlog日志刷盘流程如下：\n上图的write，是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快 上图的fsync，才是将数据持久化到磁盘的操作 write和fsync的时机，可以由参数 sync_binlog 控制，默认是 0 。\n为0的时候，表示每次提交事务都只 write，由系统自行判断什么时候执行fsync。虽然性能得到提升，但是机器宕机，page cache里面的 binglog 会丢失。如下图：\n为了安全起见，可以设置为 1 ，表示每次提交事务都会执行fsync，就如同redo log 刷盘流程一样。\n最后还有一种折中方式，可以设置为N(N\u0026gt;1)，表示每次提交事务都write，但累积N个事务后才fsync。\n在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。同样的，如果机器宕机，会丢失最近N个事务的binlog日志。\n6.2 binlog与redo log对比 redo log 它是 物理日志 ，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎层产生的。 而 binlog 是 逻辑日志 ，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于 MySQL Server 层。 虽然它们都属于持久化的保证，但是侧重点不同。 redo log让InnoDB存储引擎拥有了崩溃恢复能力。 binlog保证了MySQL集群架构的数据一致性。 6.3 两阶段提交 在执行更新语句过程，会记录redo log与binlog两块日志，以基本的事务为单位，redo log在事务执行过程中可以不断写入，而binlog只有在提交事务时才会从binlog cache写入，所以redo log与binlog的 写入时机 不一样。\nredo log与binlog两份日志之间的逻辑不一致，会出现什么问题？\n以update语句为例，假设id=2的记录，字段c值是0，把字段c值更新为1，SQL语句为update T set c = 1 where id = 2。\n假设执行过程中写完redo log日志后，binlog日志写期间发生了异常，会出现什么情况呢？\n由于binlog没写完就异常，这时候binlog里面没有对应的修改记录。因此，之后用binlog日志恢复数据或主从数据同步时，就会少这一次更新，恢复出来的这一行c值为0，而原库因为redo log日志恢复，这一行c的值是1，最终数据不一致。\n为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用两阶段提交方案。原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交。\n使用两阶段提交后，写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。\n另一个场景，redo log设置commit阶段发生异常，那会不会回滚事务呢？\n并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。\n7. 中继日志(relay log) 7.1 介绍 中继日志只在主从服务器架构的从服务器上存在。从服务器为了与主服务器保持一致，要从主服务器读取二进制日志的内容，并且把读取到的信息写入 本地的日志文件 中，这个从服务器本地的日志文件就叫 中继日志 。然后，从服务器读取中继日志，并根据中继日志的内容对从服务器的数据进行更新，完成主从服务器的数据同步 。\n搭建好主从服务器之后，中继日志默认会保存在从服务器的数据目录下。\n文件名的格式是： 从服务器名 -relay-bin.序号 。中继日志还有一个索引文件：从服务器名 -relaybin.index ，用来定位当前正在使用的中继日志。\n7.2 查看中继日志 中继日志与二进制日志的格式相同，可以用 mysqlbinlog 工具进行查看。下面是中继日志的一个片段：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SET TIMESTAMP=1618558728/*!*/; BEGIN /*!*/; # at 950 #210416 15:38:48 server id 1 end_log_pos 832 CRC32 0xcc16d651 Table_map: `atguigu`.`test` mapped to number 91 # at 1000 #210416 15:38:48 server id 1 end_log_pos 872 CRC32 0x07e4047c Delete_rows: table id 91 flags: STMT_END_F -- server id 1 是主服务器，意思是主服务器删了一行数据 BINLOG \u0026#39; CD95YBMBAAAAMgAAAEADAAAAAFsAAAAAAAEABGRlbW8ABHRlc3QAAQMAAQEBAFHWFsw= CD95YCABAAAAKAAAAGgDAAAAAFsAAAAAAAEAAgAB/wABAAAAfATkBw== \u0026#39;/*!*/; # at 1040 这一段的意思是，主服务器（“server id 1”）对表 atguigu.test 进行了 2 步操作：\n1 2 定位到表 atguigu.test 编号是 91 的记录，日志位置是 832； 删除编号是 91 的记录，日志位置是 872 7.3 恢复的典型错误 如果从服务器宕机，有的时候为了系统恢复，要重装操作系统，这样就可能会导致你的 服务器名称 与之前 不同 。而中继日志里是 包含从服务器名 的。在这种情况下，就可能导致你恢复从服务器的时候，无法从宕机前的中继日志里读取数据，以为是日志文件损坏了，其实是名称不对了。\n解决的方法也很简单，把从服务器的名称改回之前的名称。\n第18章_主从复制 1. 主从复制概述 1.1 如何提升数据库并发能力 在实际工作中，我们常常将Redis作为缓存与MySQL配合来使用，当有请求的时候，首先会从缓存中进行查找，如果存在就直接取出。如果不存在再访问数据库，这样就提升了读取的效率，也减少了对后端数据库的访问压力。Redis的缓存架构是高并发架构中非常重要的一环。\n此外，一般应用对数据库而言都是“读多写少”，也就说对数据库读取数据的压力比较大，有一个思路就是采用数据库集群的方案，做 主从架构 、进行 读写分离 ，这样同样可以提升数据库的并发处理能力。但并不是所有的应用都需要对数据库进行主从架构的设置，毕竟设置架构本身是有成本的。\n如果我们的目的在于提升数据库高并发访问的效率，那么首先考虑的是如何 优化SQL和索引 ，这种方式简单有效；其次才是采用 缓存的策略 ，比如使用 Redis将热点数据保存在内存数据库中，提升读取的效率；最后才是对数据库采用 主从架构 ，进行读写分离。\n按照上面的方式进行优化，使用和维护的成本是由低到高的。\n1.2 主从复制的作用 主从同步设计不仅可以提高数据库的吞吐量，还有以下 3 个方面的作用。\n**第1个作用：读写分离。**我们可以通过主从复制的方式来同步数据，然后通过读写分离提高数据库并发处理能力。\n其中一个是Master主库，负责写入数据，我们称之为：写库。\n其他都是Slave从库，负责读取数据，我们称之为：读库。\n当主库进行更新的时候，会自动将数据复制到从库中，而我们在客户端读取数据的时候，会从从库进行读取。\n面对“读多写少”的需求，采用读写分离的方式，可以实现更高的并发访问。同时，我们还能对从服务器进行负载均衡，让不同的读请求按照策略均匀地分发到不同的从服务器上，让读取更加顺畅。读取顺畅的另一个原因，就是减少了锁表的影响，比如我们让主库负责写，当主库出现写锁的时候，不会影响到从库进行SELECT的读取。\n**第2个作用就是数据备份。**我们通过主从复制将主库上的数据复制到从库上，相当于一种热备份机制，也就是在主库正常运行的情况下进行的备份，不会影响到服务。\n**第3个作用是具有高可用性。**数据备份实际上是一种冗余的机制，通过这种冗余的方式可以换取数据库的高可用性，也就是当服务器出现故障或宕机的情况下，可以切换到从服务器上，保证服务的正常运行。\n2. 主从复制的原理 Slave 会从 Master 读取 binlog 来进行数据同步。\n2.1 原理剖析 三个线程\n实际上主从同步的原理就是基于 binlog 进行数据同步的。在主从复制过程中，会基于 3 个线程 来操 作，一个主库线程，两个从库线程。\n二进制日志转储线程 （Binlog dump thread）是一个主库线程。当从库线程连接的时候， 主库可以将二进制日志发送给从库，当主库读取事件（Event）的时候，会在Binlog上加锁 ，读取完成之后，再将锁释放掉。\n从库 I/O 线程 会连接到主库，向主库发送请求更新 Binlog。这时从库的 I/O 线程就可以读取到主库的二进制日志转储线程发送的 Binlog 更新部分，并且拷贝到本地的中继日志 （Relay log）。\n从库 SQL 线程 会读取从库中的中继日志，并且执行日志中的事件，将从库中的数据与主库保持同步。\n注意：\n不是所有版本的MySQL都默认开启服务器的二进制日志。在进行主从同步的时候，我们需要先检查服务器是否已经开启了二进制日志。\n除非特殊指定，默认情况下从服务器会执行所有主服务器中保存的事件。也可以通过配置，使从服务器执行特定的事件。\n复制三步骤\n步骤1： Master 将写操作记录到二进制日志（ binlog ）。\n步骤2： Slave 将 Master 的binary log events拷贝到它的中继日志（ relay log ）；\n步骤3： Slave 重做中继日志中的事件，将改变应用到自己的数据库中。 MySQL复制是异步的且串行化的，而且重启后从 接入点 开始复制。\n复制的问题\n复制的最大问题： 延时\n2.2 复制的基本原则 每个 Slave 只有一个 Master 每个 Slave 只能有一个唯一的服务器ID 每个 Master 可以有多个 Slave 3. 一主一从架构搭建 一台 主机 用于处理所有 写请求 ，一台 从机 负责所有 读请求 ，架构图如下:\n3.1 准备工作 1、准备 2台 CentOS 虚拟机 （具体设置内容在P192）\n2、每台虚拟机上需要安装好MySQL (可以是MySQL8.0 )\n说明：前面我们讲过如何克隆一台CentOS。大家可以在一台CentOS上安装好MySQL，进而通过克隆的方式复制出1台包含MySQL的虚拟机。\n注意：克隆的方式需要修改新克隆出来主机的：① MAC地址 ② hostname ③ IP 地址 ④ UUID 。\n此外，克隆的方式生成的虚拟机（包含MySQL Server），则克隆的虚拟机MySQL Server的UUID相同，必须修改，否则在有些场景会报错。比如： show slave status\\G ，报如下的错误：\n1 2 Last_IO_Error: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work. 修改MySQL Server 的UUID方式：\n1 2 3 vim /var/lib/mysql/auto.cnf systemctl restart mysqld 3.2 主机配置文件 建议mysql版本一致且后台以服务运行，主从所有配置项都配置在 [mysqld] 节点下，且都是小写字母。\n具体参数配置如下：\n必选 1 2 3 4 5 #[必须]主服务器唯一ID server-id=1 #[必须]启用二进制日志,指名路径。比如：自己本地的路径/log/mysqlbin log-bin=atguigu-bin 可选 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #[可选] 0（默认）表示读写（主机），1表示只读（从机） read-only=0 #设置日志文件保留的时长，单位是秒 binlog_expire_logs_seconds=6000 #控制单个二进制日志大小。此参数的最大和默认值是1GB max_binlog_size=200M #[可选]设置不要复制的数据库 binlog-ignore-db=test #[可选]设置需要复制的数据库,默认全部记录。比如：binlog-do-db=atguigu_master_slave binlog-do-db=需要复制的主数据库名字 #[可选]设置binlog格式 binlog_format=STATEMENT 重启后台mysql服务，使配置生效。\n注意：\n先搭建完主从复制，再创建数据库。\nMySQL主从复制起始时，从机不继承主机数据。\n① binlog格式设置：\n格式1： STATEMENT模式 （基于SQL语句的复制(statement-based replication, SBR)）\n1 binlog_format=STATEMENT 每一条会修改数据的sql语句会记录到binlog中。这是默认的binlog格式。\nSBR 的优点： 历史悠久，技术成熟 不需要记录每一行的变化，减少了binlog日志量，文件较小 binlog中包含了所有数据库更改信息，可以据此来审核数据库的安全等情况 binlog可以用于实时的还原，而不仅仅用于复制 主从版本可以不一样，从服务器版本可以比主服务器版本高 SBR 的缺点： 不是所有的UPDATE语句都能被复制，尤其是包含不确定操作的时候 使用以下函数的语句也无法被复制：LOAD_FILE()、UUID()、USER()、FOUND_ROWS()、SYSDATE() (除非启动时启用了 \u0026ndash;sysdate-is-now 选项) INSERT \u0026hellip; SELECT 会产生比 RBR 更多的行级锁 复制需要进行全表扫描(WHERE 语句中没有使用到索引)的 UPDATE 时，需要比 RBR 请求更多的行级锁 对于有 AUTO_INCREMENT 字段的 InnoDB表而言，INSERT 语句会阻塞其他 INSERT 语句 对于一些复杂的语句，在从服务器上的耗资源情况会更严重，而 RBR 模式下，只会对那个发 生变化的记录产生影响 执行复杂语句如果出错的话，会消耗更多资源 数据表必须几乎和主服务器保持一致才行，否则可能会导致复制出错 ② ROW模式（基于行的复制(row-based replication, RBR)）\n1 binlog_format=ROW 5.1.5版本的MySQL才开始支持，不记录每条sql语句的上下文信息，仅记录哪条数据被修改了，修改成什么样了。\nRBR 的优点： 任何情况都可以被复制，这对复制来说是最 安全可靠 的。（比如：不会出现某些特定情况下 的存储过程、function、trigger的调用和触发无法被正确复制的问题） 多数情况下，从服务器上的表如果有主键的话，复制就会快了很多 复制以下几种语句时的行锁更少：INSERT \u0026hellip; SELECT、包含 AUTO_INCREMENT 字段的 INSERT、 没有附带条件或者并没有修改很多记录的 UPDATE 或 DELETE 语句 执行 INSERT，UPDATE，DELETE 语句时锁更少 从服务器上采用 多线程 来执行复制成为可能 RBR 的缺点： binlog 大了很多 复杂的回滚时 binlog 中会包含大量的数据 主服务器上执行 UPDATE 语句时，所有发生变化的记录都会写到 binlog 中，而 SBR 只会写一次，这会导致频繁发生 binlog 的并发写问题 无法从 binlog 中看到都复制了些什么语句 ③ MIXED模式（混合模式复制(mixed-based replication, MBR)）\n1 binlog_format=MIXED 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。\n在Mixed模式下，一般的语句修改使用statment格式保存binlog。如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog。\nMySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。\n3.3 从机配置文件 要求主从所有配置项都配置在 my.cnf 的 [mysqld] 栏位下，且都是小写字母。\n必选\n1 2 #[必须]从服务器唯一ID server-id=2 可选\n1 2 #[可选]启用中继日志 relay-log=mysql-relay 重启后台mysql服务，使配置生效。\n注意：主从机都关闭防火墙 service iptables stop #CentOS 6 systemctl stop firewalld.service #CentOS 7\n3.4 主机：建立账户并授权 1 2 #在主机MySQL里执行授权主从复制的命令 GRANT REPLICATION SLAVE ON *.* TO \u0026#39;slave1\u0026#39;@\u0026#39;从机器数据库IP\u0026#39; IDENTIFIED BY \u0026#39;abc123\u0026#39;; #5.5,5.7 注意：如果使用的是MySQL8，需要如下的方式建立账户，并授权slave:\n1 2 3 4 5 6 7 8 CREATE USER \u0026#39;slave1\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;slave1\u0026#39;@\u0026#39;%\u0026#39;; #此语句必须执行。否则见下面。 ALTER USER \u0026#39;slave1\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;123456\u0026#39;; flush privileges; 注意：在从机执行show slave status\\G时报错：\nLast_IO_Error: error connecting to master \u0026lsquo;slave1@192.168.1.150:3306\u0026rsquo; - retry-time: 60 retries: 1 message:\nAuthentication plugin \u0026lsquo;caching_sha2_password\u0026rsquo; reported error: Authentication requires secure connection.\n查询Master的状态，并记录下File和Position的值。\n1 show master status; 记录下File和Position的值 注意：执行完此步骤后不要再操作主服务器MySQL，防止主服务器状态值变化。\n3.5 从机：配置需要复制的主机 **步骤1：**从机上复制主机的命令\n1 2 3 4 5 6 CHANGE MASTER TO MASTER_HOST=\u0026#39;主机的IP地址\u0026#39;, MASTER_USER=\u0026#39;主机用户名\u0026#39;, MASTER_PASSWORD=\u0026#39;主机用户名的密码\u0026#39;, MASTER_LOG_FILE=\u0026#39;mysql-bin.具体数字\u0026#39;, MASTER_LOG_POS=具体值; 举例：\n1 2 3 CHANGE MASTER TO MASTER_HOST=\u0026#39;192.168.1.150\u0026#39;,MASTER_USER=\u0026#39;slave1\u0026#39;,MASTER_PASSWORD=\u0026#39;123456\u0026#39;,MASTER_LOG_F ILE=\u0026#39;atguigu-bin.000007\u0026#39;,MASTER_LOG_POS=154; 步骤2：\n1 2 #启动slave同步 START SLAVE; 如果报错：\n可以执行如下操作，删除之前的relay_log信息。然后重新执行 CHANGE MASTER TO \u0026hellip;语句即可。\n1 mysql\u0026gt; reset slave; #删除SLAVE数据库的relaylog日志文件，并重新启用新的relaylog文件 接着，查看同步状态：\n1 SHOW SLAVE STATUS\\G; 上面两个参数都是Yes，则说明主从配置成功！\n显式如下的情况，就是不正确的。可能错误的原因有：\n1 2 3 4 5 6 1. 网络不通 2. 账户密码错误 3. 防火墙 4. mysql配置文件问题 5. 连接服务器时语法 6. 主服务器mysql权限 3.6 测试 主机新建库、新建表、insert记录，从机复制：\n1 2 3 4 5 6 7 CREATE DATABASE atguigu_master_slave; CREATE TABLE mytbl(id INT,NAME VARCHAR(16)); INSERT INTO mytbl VALUES(1, \u0026#39;zhang3\u0026#39;); INSERT INTO mytbl VALUES(2,@@hostname); 3.7 停止主从同步 停止主从同步命令：\n1 stop slave; 如何重新配置主从\n如果停止从服务器复制功能，再使用需要重新配置主从。否则会报错如下：\n重新配置主从，需要在从机上执行：\n1 2 3 stop slave; reset master; #删除Master中所有的binglog文件，并将日志索引文件清空，重新开始所有新的日志文件(慎用) 3.8 后续 搭建主从复制：双主双从\n一个主机m1用于处理所有写请求，它的从机s1和另一台主机m2还有它的从机s2负责所有读请求。当m1主机宕机后，m2主机负责写请求，m1、m2互为备机。结构图如下：\n4. 同步数据一致性问题 主从同步的要求：\n读库和写库的数据一致(最终一致)； 写数据必须写到写库； 读数据必须到读库(不一定)； 4.1 理解主从延迟问题 进行主从同步的内容是二进制日志，它是一个文件，在进行 网络传输 的过程中就一定会 存在主从延迟 （比如 500ms），这样就可能造成用户在从库上读取的数据不是最新的数据，也就是主从同步中的 数据不一致性 问题。\n4.2 主从延迟问题原因 在网络正常的时候，日志从主库传给从库所需的时间是很短的，即T2-T1的值是非常小的。即，网络正常情况下，主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。\n**主备延迟最直接的表现是，从库消费中继日志（relay log）的速度，比主库生产binlog的速度要慢。**造成原因：\n1、从库的机器性能比主库要差\n2、从库的压力大\n3、大事务的执行\n**举例1：**一次性用delete语句删除太多数据\n结论：后续再删除数据的时候，要控制每个事务删除的数据量，分成多次删除。\n**举例2：**一次性用insert\u0026hellip;select插入太多数据\n**举例3：**大表DDL\n比如在主库对一张500W的表添加一个字段耗费了10分钟，那么从节点上也会耗费10分钟。\n4.3 如何减少主从延迟 若想要减少主从延迟的时间，可以采取下面的办法：\n降低多线程大事务并发的概率，优化业务逻辑 优化SQL，避免慢SQL， 减少批量操作 ，建议写脚本以update-sleep这样的形式完成。 提高从库机器的配置 ，减少主库写binlog和从库读binlog的效率差。 尽量采用 短的链路 ，也就是主库和从库服务器的距离尽量要短，提升端口带宽，减少binlog传输的网络延时。 实时性要求的业务读强制走主库，从库只做灾备，备份。 4.4 如何解决一致性问题 如果操作的数据存储在同一个数据库中，那么对数据进行更新的时候，可以对记录加写锁，这样在读取的时候就不会发生数据不一致的情况。但这时从库的作用就是 备份 ，并没有起到 读写分离 ，分担主库 读压力 的作用。\n读写分离情况下，解决主从同步中数据不一致的问题， 就是解决主从之间 数据复制方式 的问题，如果按照数据一致性 从弱到强 来进行划分，有以下 3 种复制方式。\n方法 1：异步复制 异步模式就是客户端提交 COMMIT 之后不需要等从库返回任何结果，而是直接将结果返回给客户端，这样做的好处是不会影响主库写的效率，但可能会存在主库宕机，而Binlog还没有同步到从库的情况，也就是此时的主库和从库数据不一致。这时候从从库中选择一个作为新主，那么新主则可能缺少原来主服务器中已提交的事务。所以，这种复制模式下的数据一致性是最弱的。\n方法 2：半同步复制 方法 3：组复制 异步复制和半同步复制都无法最终保证数据的一致性问题，半同步复制是通过判断从库响应的个数来决定是否返回给客户端，虽然数据一致性相比于异步复制有提升，但仍然无法满足对数据一致性要求高的场景，比如金融领域。MGR 很好地弥补了这两种复制模式的不足。\n组复制技术，简称 MGR（MySQL Group Replication）。是 MySQL 在 5.7.17 版本中推出的一种新的数据复制技术，这种复制技术是基于 Paxos 协议的状态机复制。\nMGR 是如何工作的\n首先我们将多个节点共同组成一个复制组，在 执行读写（RW）事务 的时候，需要通过一致性协议层 （Consensus 层）的同意，也就是读写事务想要进行提交，必须要经过组里“大多数人”（对应 Node 节 点）的同意，大多数指的是同意的节点数量需要大于 （N/2+1），这样才可以进行提交，而不是原发起方一个说了算。而针对 只读（RO）事务 则不需要经过组内同意，直接 COMMIT 即可。\n在一个复制组内有多个节点组成，它们各自维护了自己的数据副本，并且在一致性协议层实现了原子消 息和全局有序消息，从而保证组内数据的一致性。\nMGR 将 MySQL 带入了数据强一致性的时代，是一个划时代的创新，其中一个重要的原因就是MGR 是基 于 Paxos 协议的。Paxos 算法是由 2013 年的图灵奖获得者 Leslie Lamport 于 1990 年提出的，有关这个算法的决策机制可以搜一下。事实上，Paxos 算法提出来之后就作为 分布式一致性算法 被广泛应用，比如 Apache 的 ZooKeeper 也是基于 Paxos 实现的。\n5. 知识延伸 在主从架构的配置中，如果想要采取读写分离的策略，我们可以 自己编写程序 ，也可以通过 第三方的中间件 来实现。\n自己编写程序的好处就在于比较自主，我们可以自己判断哪些查询在从库上来执行，针对实时性要 求高的需求，我们还可以考虑哪些查询可以在主库上执行。同时，程序直接连接数据库，减少了中间件层，相当于减少了性能损耗。 采用中间件的方法有很明显的优势， 功能强大 ， 使用简单 。但因为在客户端和数据库之间增加了 中间件层会有一些 性能损耗 ，同时商业中间件也是有使用成本的。我们也可以考虑采取一些优秀的开源工具。 ① Cobar 属于阿里B2B事业群，始于2008年，在阿里服役3年多，接管3000+个MySQL数据库的 schema,集群日处理在线SQL请求50亿次以上。由于Cobar发起人的离职，Cobar停止维护。\n② Mycat 是开源社区在阿里cobar基础上进行二次开发，解决了cobar存在的问题，并且加入了许多新的功能在其中。青出于蓝而胜于蓝。\n③ OneProxy 基于MySQL官方的proxy思想利用c语言进行开发的，OneProxy是一款商业 收费 的中 间件。舍弃了一些功能，专注在 性能和稳定性上 。\n④ kingshard 由小团队用go语言开发，还需要发展，需要不断完善。\n⑤ Vitess 是Youtube生产在使用，架构很复杂。不支持MySQL原生协议，使用 需要大量改造成 本 。\n⑥ Atlas 是360团队基于mysql proxy改写，功能还需完善，高并发下不稳定。\n⑦ MaxScale 是mariadb（MySQL原作者维护的一个版本） 研发的中间件\n⑧ MySQLRoute 是MySQL官方Oracle公司发布的中间件\n主备切换：\n主动切换 被动切换 如何判断主库出问题了？如何解决过程中的数据不一致性问题 ? 第19章_数据库备份与恢复 1. 物理备份与逻辑备份 **物理备份：**备份数据文件，转储数据库物理文件到某一目录。物理备份恢复速度比较快，但占用空间比较大，MySQL中可以用 xtrabackup 工具来进行物理备份。\n**逻辑备份：**对数据库对象利用工具进行导出工作，汇总入备份文件内。逻辑备份恢复速度慢，但占用空间小，更灵活。MySQL 中常用的逻辑备份工具为 mysqldump 。逻辑备份就是 备份sql语句 ，在恢复的 时候执行备份的sql语句实现数据库数据的重现。\n2. mysqldump实现逻辑备份 mysqldump是MySQL提供的一个非常有用的数据库备份工具。\n2.1 备份一个数据库 mysqldump命令执行时，可以将数据库备份成一个文本文件，该文件中实际上包含多个CREATE和INSERT语句，使用这些语句可以重新创建表和插入数据。\n查出需要备份的表的结构，在文本文件中生成一个CREATE语句 将表中的所有记录转换为一条INSERT语句。 基本语法：\n1 mysqldump –u 用户名称 –h 主机名称 –p密码 待备份的数据库名称[tbname, [tbname...]]\u0026gt; 备份文件名称.sql 说明： 备份的文件并非一定要求后缀名为.sql，例如后缀名为.txt的文件也是可以的。\n举例：使用root用户备份atguigu数据库：\n1 mysqldump -uroot -p atguigu\u0026gt;atguigu.sql #备份文件存储在当前目录下 1 mysqldump -uroot -p atguigudb1 \u0026gt; /var/lib/mysql/atguigu.sql 备份文件剖析：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE=\u0026#39;+00:00\u0026#39; */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=\u0026#39;NO_AUTO_VALUE_ON_ZERO\u0026#39; */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Current Database: `atguigu` -- CREATE DATABASE /*!32312 IF NOT EXISTS*/ `atguigu` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci */ /*!80016 DEFAULT ENCRYPTION=\u0026#39;N\u0026#39; */; USE `atguigu`; -- -- Table structure for table `student` -- DROP TABLE IF EXISTS `student`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `student` ( `studentno` int NOT NULL, `name` varchar(20) DEFAULT NULL, `class` varchar(20) DEFAULT NULL, PRIMARY KEY (`studentno`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; INSERT INTO `student` VALUES (1,\u0026#39;张三_back\u0026#39;,\u0026#39;一班\u0026#39;),(3,\u0026#39;李四\u0026#39;,\u0026#39;一班\u0026#39;),(8,\u0026#39;王五\u0026#39;,\u0026#39;二班\u0026#39;), (15,\u0026#39;赵六\u0026#39;,\u0026#39;二班\u0026#39;),(20,\u0026#39;钱七\u0026#39;,\u0026#39;\u0026gt;三班\u0026#39;),(22,\u0026#39;zhang3_update\u0026#39;,\u0026#39;1ban\u0026#39;),(24,\u0026#39;wang5\u0026#39;,\u0026#39;2ban\u0026#39;); /*!40000 ALTER TABLE `student` ENABLE KEYS */; UNLOCK TABLES; . . . . /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 9:58:23 2.2 备份全部数据库 若想用mysqldump备份整个实例，可以使用 \u0026ndash;all-databases 或 -A 参数：\n1 2 mysqldump -uroot -pxxxxxx --all-databases \u0026gt; all_database.sql mysqldump -uroot -pxxxxxx -A \u0026gt; all_database.sql 2.3 备份部分数据库 使用 --databases 或 -B 参数了，该参数后面跟数据库名称，多个数据库间用空格隔开。如果指定 databases参数，备份文件中会存在创建数据库的语句，如果不指定参数，则不存在。语法如下：\n1 mysqldump –u user –h host –p --databases [数据库的名称1 [数据库的名称2...]] \u0026gt; 备份文件名称.sql 举例：\n1 mysqldump -uroot -p --databases atguigu atguigu12 \u0026gt;two_database.sql 或\n1 mysqldump -uroot -p -B atguigu atguigu12 \u0026gt; two_database.sql 2.4 备份部分表 比如，在表变更前做个备份。语法如下：\n1 mysqldump –u user –h host –p 数据库的名称 [表名1 [表名2...]] \u0026gt; 备份文件名称.sql 举例：备份atguigu数据库下的book表\n1 mysqldump -uroot -p atguigu book\u0026gt; book.sql book.sql文件内容如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 mysqldump -uroot -p atguigu book\u0026gt; book.sql^C [root@node1 ~]# ls kk kubekey kubekey-v1.1.1-linux-amd64.tar.gz README.md test1.sql two_database.sql [root@node1 ~]# mysqldump -uroot -p atguigu book\u0026gt; book.sql Enter password: [root@node1 ~]# ls book.sql kk kubekey kubekey-v1.1.1-linux-amd64.tar.gz README.md test1.sql two_database.sql [root@node1 ~]# vi book.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE=\u0026#39;+00:00\u0026#39; */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=\u0026#39;NO_AUTO_VALUE_ON_ZERO\u0026#39; */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `book` -- DROP TABLE IF EXISTS `book`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `book` ( `bookid` int unsigned NOT NULL AUTO_INCREMENT, `card` int unsigned NOT NULL, `test` varchar(255) COLLATE utf8_bin DEFAULT NULL, PRIMARY KEY (`bookid`), KEY `Y` (`card`) ) ENGINE=InnoDB AUTO_INCREMENT=101 DEFAULT CHARSET=utf8mb3 COLLATE=utf8_bin; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `book` -- LOCK TABLES `book` WRITE; /*!40000 ALTER TABLE `book` DISABLE KEYS */; INSERT INTO `book` VALUES (1,9,NULL),(2,10,NULL),(3,4,NULL),(4,8,NULL),(5,7,NULL), (6,10,NULL),(7,11,NULL),(8,3,NULL),(9,1,NULL),(10,17,NULL),(11,19,NULL),(12,4,NULL), (13,1,NULL),(14,14,NULL),(15,5,NULL),(16,5,NULL),(17,8,NULL),(18,3,NULL),(19,12,NULL), (20,11,NULL),(21,9,NULL),(22,20,NULL),(23,13,NULL),(24,3,NULL),(25,18,NULL), (26,20,NULL),(27,5,NULL),(28,6,NULL),(29,15,NULL),(30,15,NULL),(31,12,NULL), (32,11,NULL),(33,20,NULL),(34,5,NULL),(35,4,NULL),(36,6,NULL),(37,17,NULL), (38,5,NULL),(39,16,NULL),(40,6,NULL),(41,18,NULL),(42,12,NULL),(43,6,NULL), (44,12,NULL),(45,2,NULL),(46,12,NULL),(47,15,NULL),(48,17,NULL),(49,2,NULL), (50,16,NULL),(51,13,NULL),(52,17,NULL),(53,7,NULL),(54,2,NULL),(55,9,NULL), (56,1,NULL),(57,14,NULL),(58,7,NULL),(59,15,NULL),(60,12,NULL),(61,13,NULL), (62,8,NULL),(63,2,NULL),(64,6,NULL),(65,2,NULL),(66,12,NULL),(67,12,NULL),(68,4,NULL), (69,5,NULL),(70,10,NULL),(71,16,NULL),(72,8,NULL),(73,14,NULL),(74,5,NULL), (75,4,NULL),(76,3,NULL),(77,2,NULL),(78,2,NULL),(79,2,NULL),(80,3,NULL),(81,8,NULL), (82,14,NULL),(83,5,NULL),(84,4,NULL),(85,2,NULL),(86,20,NULL),(87,12,NULL), (88,1,NULL),(89,8,NULL),(90,18,NULL),(91,3,NULL),(92,3,NULL),(93,6,NULL),(94,1,NULL), (95,4,NULL),(96,17,NULL),(97,15,NULL),(98,1,NULL),(99,20,NULL),(100,15,NULL); /*!40000 ALTER TABLE `book` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; 可以看到，book文件和备份的库文件类似。不同的是，book文件只包含book表的DROP、CREATE和 INSERT语句。\n备份多张表使用下面的命令，比如备份book和account表：\n1 2 #备份多张表 mysqldump -uroot -p atguigu book account \u0026gt; 2_tables_bak.sql 2.5 备份单表的部分数据 有些时候一张表的数据量很大，我们只需要部分数据。这时就可以使用 \u0026ndash;where 选项了。where后面附带需要满足的条件。\n举例：备份student表中id小于10的数据：\n1 mysqldump -uroot -p atguigu student --where=\u0026#34;id \u0026lt; 10 \u0026#34; \u0026gt; student_part_id10_low_bak.sql 内容如下所示，insert语句只有id小于10的部分\n1 2 3 4 5 6 LOCK TABLES `student` WRITE; /*!40000 ALTER TABLE `student` DISABLE KEYS */; INSERT INTO `student` VALUES (1,100002,\u0026#39;JugxTY\u0026#39;,157,280),(2,100003,\u0026#39;QyUcCJ\u0026#39;,251,277), (3,100004,\u0026#39;lATUPp\u0026#39;,80,404),(4,100005,\u0026#39;BmFsXI\u0026#39;,240,171),(5,100006,\u0026#39;mkpSwJ\u0026#39;,388,476), (6,100007,\u0026#39;ujMgwN\u0026#39;,259,124),(7,100008,\u0026#39;HBJTqX\u0026#39;,429,168),(8,100009,\u0026#39;dvQSQA\u0026#39;,61,504), (9,100010,\u0026#39;HljpVJ\u0026#39;,234,185); 2.6 排除某些表的备份 如果我们想备份某个库，但是某些表数据量很大或者与业务关联不大，这个时候可以考虑排除掉这些表，同样的，选项 --ignore-table 可以完成这个功能。\n1 mysqldump -uroot -p atguigu --ignore-table=atguigu.student \u0026gt; no_stu_bak.sql 通过如下指定判定文件中没有student表结构：\n1 grep \u0026#34;student\u0026#34; no_stu_bak.sql 2.7 只备份结构或只备份数据 只备份结构的话可以使用 --no-data 简写为 -d 选项；只备份数据可以使用 --no-create-info 简写为 -t选项。\n只备份结构\n1 2 3 4 mysqldump -uroot -p atguigu --no-data \u0026gt; atguigu_no_data_bak.sql #使用grep命令，没有找到insert相关语句，表示没有数据备份。 [root@node1 ~]# grep \u0026#34;INSERT\u0026#34; atguigu_no_data_bak.sql [root@node1 ~]# 只备份数据\n1 2 3 4 mysqldump -uroot -p atguigu --no-create-info \u0026gt; atguigu_no_create_info_bak.sql #使用grep命令，没有找到create相关语句，表示没有数据结构。 [root@node1 ~]# grep \u0026#34;CREATE\u0026#34; atguigu_no_create_info_bak.sql [root@node1 ~]# 2.8 备份中包含存储过程、函数、事件 mysqldump备份默认是不包含存储过程，自定义函数及事件的。可以使用 --routines 或 -R 选项来备份存储过程及函数，使用 --events 或 -E 参数来备份事件。\n举例：备份整个atguigu库，包含存储过程及事件：\n使用下面的SQL可以查看当前库有哪些存储过程或者函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mysql\u0026gt; SELECT SPECIFIC_NAME,ROUTINE_TYPE ,ROUTINE_SCHEMA FROM information_schema.Routines WHERE ROUTINE_SCHEMA=\u0026#34;atguigu\u0026#34;; +---------------+--------------+----------------+ | SPECIFIC_NAME | ROUTINE_TYPE | ROUTINE_SCHEMA | +---------------+--------------+----------------+ | rand_num | FUNCTION | atguigu | | rand_string | FUNCTION | atguigu | | BatchInsert | PROCEDURE | atguigu | | insert_class | PROCEDURE | atguigu | | insert_order | PROCEDURE | atguigu | | insert_stu | PROCEDURE | atguigu | | insert_user | PROCEDURE | atguigu | | ts_insert | PROCEDURE | atguigu | +---------------+--------------+----------------+ 9 rows in set (0.02 sec) 下面备份atguigu库的数据，函数以及存储过程。\n1 mysqldump -uroot -p -R -E --databases atguigu \u0026gt; fun_atguigu_bak.sql 查询备份文件中是否存在函数，如下所示，可以看到确实包含了函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 grep -C 5 \u0026#34;rand_num\u0026#34; fun_atguigu_bak.sql -- -- -- Dumping routines for database \u0026#39;atguigu\u0026#39; -- /*!50003 DROP FUNCTION IF EXISTS `rand_num` */; /*!50003 SET @saved_cs_client = @@character_set_client */ ; /*!50003 SET @saved_cs_results = @@character_set_results */ ; /*!50003 SET @saved_col_connection = @@collation_connection */ ; /*!50003 SET character_set_client = utf8mb3 */ ; /*!50003 SET character_set_results = utf8mb3 */ ; /*!50003 SET collation_connection = utf8_general_ci */ ; /*!50003 SET @saved_sql_mode = @@sql_mode */ ; /*!50003 SET sql_mode = \u0026#39;ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISIO N_BY_ZERO,NO_ENGINE_SUBSTITUTION\u0026#39; */ ; DELIMITER ;; CREATE DEFINER=`root`@`%` FUNCTION `rand_num`(from_num BIGINT ,to_num BIGINT) RETURNS bigint BEGIN DECLARE i BIGINT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END ;; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO class ( classname,address,monitor ) VALUES (rand_string(8),rand_string(10),rand_num()); UNTIL i = max_num END REPEAT; COMMIT; END ;; DELIMITER ; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO order_test (order_id, trans_id ) VALUES (rand_num(1,7000000),rand_num(100000000000000000,700000000000000000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END ;; DELIMITER ; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, name ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(),rand_num()); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END ;; DELIMITER ; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO `user` ( name,age,sex ) VALUES (\u0026#34;atguigu\u0026#34;,rand_num(1,20),\u0026#34;male\u0026#34;); UNTIL i = max_num END REPEAT; COMMIT; END ;; DELIMITER ; 2.9 mysqldump常用选项 mysqldump其他常用选项如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 --add-drop-database：在每个CREATE DATABASE语句前添加DROP DATABASE语句。 --add-drop-tables：在每个CREATE TABLE语句前添加DROP TABLE语句。 --add-locking：用LOCK TABLES和UNLOCK TABLES语句引用每个表转储。重载转储文件时插入得更快。 --all-database, -A：转储所有数据库中的所有表。与使用--database选项相同，在命令行中命名所有数据库。 --comment[=0|1]：如果设置为0，禁止转储文件中的其他信息，例如程序版本、服务器版本和主机。--skipcomments与--comments=0的结果相同。默认值为1，即包括额外信息。 --compact：产生少量输出。该选项禁用注释并启用--skip-add-drop-tables、--no-set-names、--skipdisable-keys和--skip-add-locking选项。 --compatible=name：产生与其他数据库系统或旧的MySQL服务器更兼容的输出，值可以为ansi、MySQL323、MySQL40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_table_options或者no_field_options。 --complete_insert, -c：使用包括列名的完整的INSERT语句。 --debug[=debug_options], -#[debug_options]：写调试日志。 --delete，-D：导入文本文件前清空表。 --default-character-set=charset：使用charsets默认字符集。如果没有指定，就使用utf8。 --delete--master-logs：在主复制服务器上，完成转储操作后删除二进制日志。该选项自动启用-masterdata。 --extended-insert，-e：使用包括几个VALUES列表的多行INSERT语法。这样使得转储文件更小，重载文件时可以加速插入。 --flush-logs，-F：开始转储前刷新MySQL服务器日志文件。该选项要求RELOAD权限。 --force，-f：在表转储过程中，即使出现SQL错误也继续。 --lock-all-tables，-x：对所有数据库中的所有表加锁。在整体转储过程中通过全局锁定来实现。该选项自动关闭--single-transaction和--lock-tables。 --lock-tables，-l：开始转储前锁定所有表。用READ LOCAL锁定表以允许并行插入MyISAM表。对于事务表（例如InnoDB和BDB），--single-transaction是一个更好的选项，因为它根本不需要锁定表。 --no-create-db，-n：该选项禁用CREATE DATABASE /*!32312 IF NOT EXIST*/db_name语句，如果给出--database或--all-database选项，就包含到输出中。 --no-create-info，-t：只导出数据，而不添加CREATE TABLE语句。 --no-data，-d：不写表的任何行信息，只转储表的结构。 --opt：该选项是速记，它可以快速进行转储操作并产生一个能很快装入MySQL服务器的转储文件。该选项默认开启，但可以用--skip-opt禁用。 --password[=password]，-p[password]：当连接服务器时使用的密码。 -port=port_num，-P port_num：用于连接的TCP/IP端口号。 --protocol={TCP|SOCKET|PIPE|MEMORY}：使用的连接协议。 --replace，-r –replace和--ignore：控制替换或复制唯一键值已有记录的输入记录的处理。如果指定--replace，新行替换有相同的唯一键值的已有行；如果指定--ignore，复制已有的唯一键值的输入行被跳过。如果不指定这两个选项，当发现一个复制键值时会出现一个错误，并且忽视文本文件的剩余部分。 --silent，-s：沉默模式。只有出现错误时才输出。 --socket=path，-S path：当连接localhost时使用的套接字文件（为默认主机）。 --user=user_name，-u user_name：当连接服务器时MySQL使用的用户名。 --verbose，-v：冗长模式，打印出程序操作的详细信息。 --xml，-X：产生XML输出。 运行帮助命令 mysqldump --help ，可以获得特定版本的完整选项列表。\n提示 如果运行mysqldump没有\u0026ndash;quick或\u0026ndash;opt选项，mysqldump在转储结果前将整个结果集装入内 存。如果转储大数据库可能会出现问题，该选项默认启用，但可以用\u0026ndash;skip-opt禁用。如果使用最 新版本的mysqldump程序备份数据，并用于恢复到比较旧版本的MySQL服务器中，则不要使用\u0026ndash;opt 或-e选项。\n3. mysql命令恢复数据 使用mysqldump命令将数据库中的数据备份成一个文本文件。需要恢复时，可以使用mysql命令来恢复备份的数据。\nmysql命令可以执行备份文件中的CREATE语句和INSERT语句。通过CREATE语句来创建数据库和表。通过INSERT语句来插入备份的数据。\n基本语法：\n1 mysql –u root –p [dbname] \u0026lt; backup.sql 其中，dbname参数表示数据库名称。该参数是可选参数，可以指定数据库名，也可以不指定。指定数据库名时，表示还原该数据库下的表。此时需要确保MySQL服务器中已经创建了该名的数据库。不指定数据库名，表示还原文件中所有的数据库。此时sql文件中包含有CREATE DATABASE语句，不需要MySQL服务器中已存在的这些数据库。\n3.1 单库备份中恢复单库 使用root用户，将之前练习中备份的atguigu.sql文件中的备份导入数据库中，命令如下：\n如果备份文件中包含了创建数据库的语句，则恢复的时候不需要指定数据库名称，如下所示\n1 mysql -uroot -p \u0026lt; atguigu.sql 否则需要指定数据库名称，如下所示\n1 mysql -uroot -p atguigu4\u0026lt; atguigu.sql 3.2 全量备份恢复 如果我们现在有昨天的全量备份，现在想整个恢复，则可以这样操作：\n1 mysql –u root –p \u0026lt; all.sql 1 mysql -uroot -pxxxxxx \u0026lt; all.sql 执行完后，MySQL数据库中就已经恢复了all.sql文件中的所有数据库。\n3.3 全量备份恢复 可能有这样的需求，比如说我们只想恢复某一个库，但是我们有的是整个实例的备份，这个时候我们可以从全量备份中分离出单个库的备份。\n举例：\n1 2 sed -n \u0026#39;/^-- Current Database: `atguigu`/,/^-- Current Database: `/p\u0026#39; all_database.sql \u0026gt; atguigu.sql #分离完成后我们再导入atguigu.sql即可恢复单个库 3.4 从单库备份中恢复单表 这个需求还是比较常见的。比如说我们知道哪个表误操作了，那么就可以用单表恢复的方式来恢复。\n举例：我们有atguigu整库的备份，但是由于class表误操作，需要单独恢复出这张表。\n1 2 3 4 5 6 7 8 9 10 cat atguigu.sql | sed -e \u0026#39;/./{H;$!d;}\u0026#39; -e \u0026#39;x;/CREATE TABLE `class`/!d;q\u0026#39; \u0026gt; class_structure.sql cat atguigu.sql | grep --ignore-case \u0026#39;insert into `class`\u0026#39; \u0026gt; class_data.sql #用shell语法分离出创建表的语句及插入数据的语句后 再依次导出即可完成恢复 use atguigu; mysql\u0026gt; source class_structure.sql; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql\u0026gt; source class_data.sql; Query OK, 1 row affected (0.01 sec) 4. 物理备份：直接复制整个数据库 直接将MySQL中的数据库文件复制出来。这种方法最简单，速度也最快。MySQL的数据库目录位置不一 定相同：\n在Windows平台下，MySQL 8.0存放数据库的目录通常默认为 “ C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data ”或者其他用户自定义目录； 在Linux平台下，数据库目录位置通常为/var/lib/mysql/； 在MAC OSX平台下，数据库目录位置通常为“/usr/local/mysql/data” 但为了保证备份的一致性。需要保证：\n方式1：备份前，将服务器停止。 方式2：备份前，对相关表执行 FLUSH TABLES WITH READ LOCK 操作。这样当复制数据库目录中 的文件时，允许其他客户继续查询表。同时，FLUSH TABLES语句来确保开始备份前将所有激活的索 引页写入硬盘。 这种方式方便、快速，但不是最好的备份方法，因为实际情况可能 不允许停止MySQL服务器 或者 锁住表 ，而且这种方法 对InnoDB存储引擎 的表不适用。对于MyISAM存储引擎的表，这样备份和还原很方便，但是还原时最好是相同版本的MySQL数据库，否则可能会存在文件类型不同的情况。\n注意，物理备份完毕后，执行 UNLOCK TABLES 来结算其他客户对表的修改行为。\n说明： 在MySQL版本号中，第一个数字表示主版本号，主版本号相同的MySQL数据库文件格式相同。\n此外，还可以考虑使用相关工具实现备份。比如， MySQLhotcopy 工具。MySQLhotcopy是一个Perl脚本，它使用LOCK TABLES、FLUSH TABLES和cp或scp来快速备份数据库。它是备份数据库或单个表最快的途径，但它只能运行在数据库目录所在的机器上，并且只能备份MyISAM类型的表。多用于mysql5.5之前。\n5. 物理恢复：直接复制到数据库目录 步骤：\n1）演示删除备份的数据库中指定表的数据\n2）将备份的数据库数据拷贝到数据目录下，并重启MySQL服务器\n3）查询相关表的数据是否恢复。需要使用下面的 chown 操作。\n要求：\n必须确保备份数据的数据库和待恢复的数据库服务器的主版本号相同。 因为只有MySQL数据库主版本号相同时，才能保证这两个MySQL数据库文件类型是相同的。 这种方式对 MyISAM类型的表比较有效 ，对于InnoDB类型的表则不可用。 因为InnoDB表的表空间不能直接复制。 在Linux操作系统下，复制到数据库目录后，一定要将数据库的用户和组变成mysql，命令如下： 1 chown -R mysql.mysql /var/lib/mysql/dbname 其中，两个mysql分别表示组和用户；“-R”参数可以改变文件夹下的所有子文件的用户和组；“dbname”参数表示数据库目录。\n提示 Linux操作系统下的权限设置非常严格。通常情况下，MySQL数据库只有root用户和mysql用户 组下的mysql用户才可以访问，因此将数据库目录复制到指定文件夹后，一定要使用chown命令将 文件夹的用户组变为mysql，将用户变为mysql。\n6. 表的导出与导入 6.1 表的导出 1. 使用SELECT…INTO OUTFILE导出文本文件 在MySQL中，可以使用SELECT…INTO OUTFILE语句将表的内容导出成一个文本文件。\n**举例：**使用SELECT…INTO OUTFILE将atguigu数据库中account表中的记录导出到文本文件。\n（1）选择数据库atguigu，并查询account表，执行结果如下所示。\n1 2 3 4 5 6 7 8 9 10 11 use atguigu; select * from account; mysql\u0026gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.01 sec) （2）mysql默认对导出的目录有权限限制，也就是说使用命令行进行导出的时候，需要指定目录进行操作。\n查询secure_file_priv值：\n1 2 3 4 5 6 7 8 mysql\u0026gt; SHOW GLOBAL VARIABLES LIKE \u0026#39;%secure%\u0026#39;; +--------------------------+-----------------------+ | Variable_name | Value | +--------------------------+-----------------------+ | require_secure_transport | OFF | | secure_file_priv | /var/lib/mysql-files/ | +--------------------------+-----------------------+ 2 rows in set (0.02 sec) （3）上面结果中显示，secure_file_priv变量的值为/var/lib/mysql-files/，导出目录设置为该目录，SQL语句如下。\n1 SELECT * FROM account INTO OUTFILE \u0026#34;/var/lib/mysql-files/account.txt\u0026#34;; （4）查看 /var/lib/mysql-files/account.txt`文件。\n1 2 3 1 张三 90 2 李四 100 3 王五 0 2. 使用mysqldump命令导出文本文件 **举例1：**使用mysqldump命令将将atguigu数据库中account表中的记录导出到文本文件：\n1 mysqldump -uroot -p -T \u0026#34;/var/lib/mysql-files/\u0026#34; atguigu account mysqldump命令执行完毕后，在指定的目录/var/lib/mysql-files/下生成了account.sql和account.txt文件。\n打开account.sql文件，其内容包含创建account表的CREATE语句。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [root@node1 mysql-files]# cat account.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE=\u0026#39;+00:00\u0026#39; */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=\u0026#39;\u0026#39; */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `account` -- DROP TABLE IF EXISTS `account`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `account` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `balance` int NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 23:19:27 打开account.txt文件，其内容只包含account表中的数据。\n1 2 3 4 [root@node1 mysql-files]# cat account.txt 1 张三 90 2 李四 100 3 王五 0 **举例2：**使用mysqldump将atguigu数据库中的account表导出到文本文件，使用FIELDS选项，要求字段之 间使用逗号“，”间隔，所有字符类型字段值用双引号括起来：\n1 mysqldump -uroot -p -T \u0026#34;/var/lib/mysql-files/\u0026#34; atguigu account --fields-terminatedby=\u0026#39;,\u0026#39; --fields-optionally-enclosed-by=\u0026#39;\\\u0026#34;\u0026#39; 语句mysqldump语句执行成功之后，指定目录下会出现两个文件account.sql和account.txt。\n打开account.sql文件，其内容包含创建account表的CREATE语句。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [root@node1 mysql-files]# cat account.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE=\u0026#39;+00:00\u0026#39; */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=\u0026#39;\u0026#39; */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `account` -- DROP TABLE IF EXISTS `account`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `account` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `balance` int NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 23:36:39 打开account.txt文件，其内容包含创建account表的数据。从文件中可以看出，字段之间用逗号隔开，字 符类型的值被双引号括起来。\n1 2 3 4 [root@node1 mysql-files]# cat account.txt 1,\u0026#34;张三\u0026#34;,90 2,\u0026#34;李四\u0026#34;,100 3,\u0026#34;王五\u0026#34;,0 3. 使用mysql命令导出文本文件 **举例1：**使用mysql语句导出atguigu数据中account表中的记录到文本文件：\n1 mysql -uroot -p --execute=\u0026#34;SELECT * FROM account;\u0026#34; atguigu\u0026gt; \u0026#34;/var/lib/mysql-files/account.txt\u0026#34; 打开account.txt文件，其内容包含创建account表的数据。\n1 2 3 4 5 [root@node1 mysql-files]# cat account.txt id name balance 1 张三 90 2 李四 100 3 王五 0 **举例2：**将atguigu数据库account表中的记录导出到文本文件，使用\u0026ndash;veritcal参数将该条件记录分为多行显示：\n1 mysql -uroot -p --vertical --execute=\u0026#34;SELECT * FROM account;\u0026#34; atguigu \u0026gt; \u0026#34;/var/lib/mysql-files/account_1.txt\u0026#34; 打开account_1.txt文件，其内容包含创建account表的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@node1 mysql-files]# cat account_1.txt *************************** 1. row *************************** id: 1 name: 张三 balance: 90 *************************** 2. row *************************** id: 2 name: 李四 balance: 100 *************************** 3. row *************************** id: 3 name: 王五 balance: 0 **举例3：**将atguigu数据库account表中的记录导出到xml文件，使用\u0026ndash;xml参数，具体语句如下。\n1 mysql -uroot -p --xml --execute=\u0026#34;SELECT * FROM account;\u0026#34; atguigu\u0026gt;\u0026#34;/var/lib/mysqlfiles/account_3.xml\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@node1 mysql-files]# cat account_3.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;resultset statement=\u0026#34;SELECT * FROM account\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;row\u0026gt; \u0026lt;field name=\u0026#34;id\u0026#34;\u0026gt;1\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;name\u0026#34;\u0026gt;张三\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;balance\u0026#34;\u0026gt;90\u0026lt;/field\u0026gt; \u0026lt;/row\u0026gt; \u0026lt;row\u0026gt; \u0026lt;field name=\u0026#34;id\u0026#34;\u0026gt;2\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;name\u0026#34;\u0026gt;李四\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;balance\u0026#34;\u0026gt;100\u0026lt;/field\u0026gt; \u0026lt;/row\u0026gt; \u0026lt;row\u0026gt; \u0026lt;field name=\u0026#34;id\u0026#34;\u0026gt;3\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;name\u0026#34;\u0026gt;王五\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;balance\u0026#34;\u0026gt;0\u0026lt;/field\u0026gt; \u0026lt;/row\u0026gt; \u0026lt;/resultset\u0026gt; 说明：如果要将表数据导出到html文件中，可以使用 --html 选项。然后可以使用浏览器打开。\n6.2 表的导入 1. 使用LOAD DATA INFILE方式导入文本文件 举例1：\n使用SELECT\u0026hellip;INTO OUTFILE将atguigu数据库中account表的记录导出到文本文件\n1 SELECT * FROM atguigu.account INTO OUTFILE \u0026#39;/var/lib/mysql-files/account_0.txt\u0026#39;; 删除account表中的数据：\n1 DELETE FROM atguigu.account; 从文本文件account.txt中恢复数据：\n1 LOAD DATA INFILE \u0026#39;/var/lib/mysql-files/account_0.txt\u0026#39; INTO TABLE atguigu.account; 查询account表中的数据：\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 举例2： 选择数据库atguigu，使用SELECT…INTO OUTFILE将atguigu数据库account表中的记录导出到文本文件，使用FIELDS选项和LINES选项，要求字段之间使用逗号\u0026quot;，\u0026ldquo;间隔，所有字段值用双引号括起来：\n1 SELECT * FROM atguigu.account INTO OUTFILE \u0026#39;/var/lib/mysql-files/account_1.txt\u0026#39; FIELDS TERMINATED BY \u0026#39;,\u0026#39; ENCLOSED BY \u0026#39;\\\u0026#34;\u0026#39;; 删除account表中的数据：\n1 DELETE FROM atguigu.account; 从/var/lib/mysql-files/account.txt中导入数据到account表中：\n1 LOAD DATA INFILE \u0026#39;/var/lib/mysql-files/account_1.txt\u0026#39; INTO TABLE atguigu.account FIELDS TERMINATED BY \u0026#39;,\u0026#39; ENCLOSED BY \u0026#39;\\\u0026#34;\u0026#39;; 查询account表中的数据，具体SQL如下：\n1 2 3 4 5 6 7 8 9 10 select * from account; mysql\u0026gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 2. 使用mysqlimport方式导入文本文件 举例：\n导出文件account.txt，字段之间使用逗号\u0026rdquo;，\u0026ldquo;间隔，字段值用双引号括起来：\n1 SELECT * FROM atguigu.account INTO OUTFILE \u0026#39;/var/lib/mysql-files/account.txt\u0026#39; FIELDS TERMINATED BY \u0026#39;,\u0026#39; ENCLOSED BY \u0026#39;\\\u0026#34;\u0026#39;; 删除account表中的数据：\n1 DELETE FROM atguigu.account; 使用mysqlimport命令将account.txt文件内容导入到数据库atguigu的account表中：\n1 mysqlimport -uroot -p atguigu \u0026#39;/var/lib/mysql-files/account.txt\u0026#39; --fields-terminated-by=\u0026#39;,\u0026#39; --fields-optionally-enclosed-by=\u0026#39;\\\u0026#34;\u0026#39; 查询account表中的数据：\n1 2 3 4 5 6 7 8 9 10 select * from account; mysql\u0026gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 7. 数据库迁移 7.1 概述 数据迁移（data migration）是指选择、准备、提取和转换数据，并将数据从一个计算机存储系统永久地传输到另一个计算机存储系统的过程。此外， 验证迁移数据的完整性 和 退役原来旧的数据存储 ，也被认为是整个数据迁移过程的一部分。\n数据库迁移的原因是多样的，包括服务器或存储设备更换、维护或升级，应用程序迁移，网站集成，灾难恢复和数据中心迁移。\n根据不同的需求可能要采取不同的迁移方案，但总体来讲，MySQL 数据迁移方案大致可以分为物理迁移和 逻辑迁移 两类。通常以尽可能 自动化 的方式执行，从而将人力资源从繁琐的任务中解放出来。\n7.2 迁移方案 物理迁移 物理迁移适用于大数据量下的整体迁移。使用物理迁移方案的优点是比较快速，但需要停机迁移并且要 求 MySQL 版本及配置必须和原服务器相同，也可能引起未知问题。\n物理迁移包括拷贝数据文件和使用 XtraBackup 备份工具两种。\n不同服务器之间可以采用物理迁移，我们可以在新的服务器上安装好同版本的数据库软件，创建好相同目录，建议配置文件也要和原数据库相同，然后从原数据库方拷贝来数据文件及日志文件，配置好文件组权限，之后在新服务器这边使用 mysqld 命令启动数据库。\n逻辑迁移 逻辑迁移适用范围更广，无论是 部分迁移 还是 全量迁移 ，都可以使用逻辑迁移。逻辑迁移中使用最多的就是通过 mysqldump 等备份工具。\n7.3 迁移注意点 1. 相同版本的数据库之间迁移注意点\n指的是在主版本号相同的MySQL数据库之间进行数据库移动。\n方式1： 因为迁移前后MySQL数据库的 主版本号相同 ，所以可以通过复制数据库目录来实现数据库迁移，但是物理迁移方式只适用于MyISAM引擎的表。对于InnoDB表，不能用直接复制文件的方式备份数据库。\n方式2： 最常见和最安全的方式是使用 mysqldump命令 导出数据，然后在目标数据库服务器中使用 MySQL命令导入。\n举例：\n1 2 3 #host1的机器中备份所有数据库,并将数据库迁移到名为host2的机器上 mysqldump –h host1 –uroot –p –-all-databases| mysql –h host2 –uroot –p 在上述语句中，“|”符号表示管道，其作用是将mysqldump备份的文件给mysql命令；“\u0026ndash;all-databases”表示要迁移所有的数据库。通过这种方式可以直接实现迁移。\n2. 不同版本的数据库之间迁移注意点\n例如，原来很多服务器使用5.7版本的MySQL数据库，在8.0版本推出来以后，改进了5.7版本的很多缺陷， 因此需要把数据库升级到8.0版本\n旧版本与新版本的MySQL可能使用不同的默认字符集，例如有的旧版本中使用latin1作为默认字符集，而最新版本的MySQL默认字符集为utf8mb4。如果数据库中有中文数据，那么迁移过程中需要对 默认字符集 进行修改 ，不然可能无法正常显示数据。\n高版本的MySQL数据库通常都会 兼容低版本 ，因此可以从低版本的MySQL数据库迁移到高版本的MySQL 数据库。\n3. 不同数据库之间迁移注意点\n不同数据库之间迁移是指从其他类型的数据库迁移到MySQL数据库，或者从MySQL数据库迁移到其他类 型的数据库。这种迁移没有普适的解决方法。\n迁移之前，需要了解不同数据库的架构， 比较它们之间的差异 。不同数据库中定义相同类型的数据的 关键字可能会不同 。例如，MySQL中日期字段分为DATE和TIME两种，而ORACLE日期字段只有DATE；SQL Server数据库中有ntext、Image等数据类型，MySQL数据库没有这些数据类型；MySQL支持的ENUM和SET 类型，这些SQL Server数据库不支持。\n另外，数据库厂商并没有完全按照SQL标准来设计数据库系统，导致不同的数据库系统的 SQL语句 有差别。例如，微软的SQL Server软件使用的是T-SQL语句，T-SQL中包含了非标准的SQL语句，不能和MySQL的SQL语句兼容。\n不同类型数据库之间的差异造成了互相 迁移的困难 ，这些差异其实是商业公司故意造成的技术壁垒。但 是不同类型的数据库之间的迁移并 不是完全不可能 。例如，可以使用 MyODBC 实现MySQL和SQL Server之 间的迁移。MySQL官方提供的工具 MySQL Migration Toolkit 也可以在不同数据之间进行数据迁移。 MySQL迁移到Oracle时，需要使用mysqldump命令导出sql文件，然后， 手动更改 sql文件中的CREATE语句。\n7.4 迁移小结 8. 删库了不敢跑，能干点啥？ 8.1 delete：误删行 8.2 truncate/drop ：误删库/表 8.3 预防使用truncate/drop误删库/表 8.4 rm：误删MySQL实例 对于一个有高可用机制的MySQL集群来说，不用担心 rm删除数据 了。只是删掉了其中某一个节点的数据的话，HA系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。我们要做的就是在这个节点上把数据恢复回来，再接入整个集群。\n但如果是恶意地把整个集群删除，那就需要考虑跨机房备份，跨城市备份。\n","permalink":"https://cold-bin.github.io/post/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/","tags":["mysql性能优化","mysql主从架构","mysql备份与恢复"],"title":"MySQL日志与备份篇"},{"categories":["mysql"],"contents":"[toc]\n第13章_事务基础知识 1. 数据库事务概述 1.1 存储引擎支持情况 SHOW ENGINES 命令来查看当前 MySQL 支持的存储引擎都有哪些，以及这些存储引擎是否支持事务。\n能看出在 MySQL 中，只有InnoDB 是支持事务的。\n1.2 基本概念 **事务：**一组逻辑操作单元，使数据从一种状态变换到另一种状态。\n**事务处理的原则：**保证所有事务都作为 一个工作单元 来执行，即使出现了故障，都不能改变这种执行方 式。当在一个事务中执行多个操作时，要么所有的事务都被提交( commit )，那么这些修改就 永久 地保 存下来；要么数据库管理系统将 放弃 所作的所有 修改 ，整个事务回滚( rollback )到最初状态。\n1 2 3 4 # 案例：AA用户给BB用户转账100 update account set money = money - 100 where name = \u0026#39;AA\u0026#39;; # 服务器宕机 update account set money = money + 100 where name = \u0026#39;BB\u0026#39;; 1.3 事物的ACID特性 原子性（atomicity）： 原子性是指事务是一个不可分割的工作单位，要么全部提交，要么全部失败回滚。即要么转账成功，要么转账失败，是不存在中间的状态。如果无法保证原子性会怎么样？就会出现数据不一致的情形，A账户减去100元，而B账户增加100元操作失败，系统将无故丢失100元。\n一致性（consistency）： （国内很多网站上对一致性的阐述有误，具体你可以参考 Wikipedia 对Consistency的阐述）\n根据定义，**一致性是指事务执行前后，数据从一个 合法性状态 变换到另外一个 合法性状态 。**这种状态是 语义上 的而不是语法上的，跟具体的业务有关。\n那什么是合法的数据状态呢？**满足 预定的约束 的状态就叫做合法的状态。**通俗一点，这状态是由你自己来定义的（比如满足现实世界中的约束）。满足这个状态，数据就是一致的，不满足这个状态，数据就 是不一致的！如果事务中的某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作 之前的状态。\n**举例1：**A账户有200元，转账300元出去，此时A账户余额为-100元。你自然就发现此时数据是不一致的，为什么呢？因为你定义了一个状态，余额这列必须\u0026gt;=0。\n**举例2：**A账户有200元，转账50元给B账户，A账户的钱扣了，但是B账户因为各种意外，余额并没有增加。你也知道此时的数据是不一致的，为什么呢？因为你定义了一个状态，要求A+B的总余额必须不变。\n**举例3：**在数据表中我们将姓名字段设置为唯一性约束，这时当事务进行提交或者事务发生回滚的时候，如果数据表的姓名不唯一，就破坏了事物的一致性要求。\n隔离型（isolation）： 事务的隔离性是指一个事务的执行**不能被其他事务干扰**，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能相互干扰。\n如果无法保证隔离性会怎么样？假设A账户有200元，B账户0元。A账户往B账户转账两次，每次金额为50元，分别在两个事务中执行。如果无法保证隔离性，会出现下面的情形：\n1 2 UPDATE accounts SET money = money - 50 WHERE NAME = \u0026#39;AA\u0026#39;; UPDATE accounts SET money = money + 50 WHERE NAME = \u0026#39;BB\u0026#39;; 持久性（durability）：\n持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的 ，接下来的其他操作和数据库故障不应该对其有任何影响。\n**持久性是通过 事务日志 来保证的。**日志包括了 重做日志 和 回滚日志 。当我们通过事务对数据进行修改 的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。\n总结\nACID是事务的四大特征，在这四个特性中，原子性是基础，隔离性是手段，一致性是约束条件， 而持久性是我们的目的。\n数据库事务，其实就是数据库设计者为了方便起见，把需要保证原子性、隔离性、一致性和持久性的一个或多个数据库操作称为一个事务。\n1.4 事务的状态 我们现在知道 事务 是一个抽象的概念，它其实对应着一个或多个数据库操作，MySQL根据这些操作所执 行的不同阶段把 事务 大致划分成几个状态：\n活动的（active）\n事务对应的数据库操作正在执行过程中时，我们就说该事务处在 活动的 状态。\n部分提交的（partially committed）\n当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并 没有刷新到磁盘 时，我们就说该事务处在 部分提交的 状态。\n失败的（failed）\n当事务处在 活动的 或者 部分提交的 状态时，可能遇到了某些错误（数据库自身的错误、操作系统 错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在 失败的 状态。\n中止的（aborted）\n如果事务执行了一部分而变为 失败的 状态，那么就需要把已经修改的事务中的操作还原到事务执行前的状态。换句话说，就是要撤销失败事务对当前数据库造成的影响。我们把这个撤销的过程称之为 回滚 。当 回滚 操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了 中止的 状态。\n举例：\n1 2 3 UPDATE accounts SET money = money - 50 WHERE NAME = \u0026#39;AA\u0026#39;; UPDATE accounts SET money = money + 50 WHERE NAME = \u0026#39;BB\u0026#39;; 提交的（committed）\n当一个处在 部分提交的 状态的事务将修改过的数据都 同步到磁盘 上之后，我们就可以说该事务处在了 提交的 状态。\n一个基本的状态转换图如下所示：\n图中可见，只有当事物处于提交的或者中止的状态时，一个事务的生命周期才算是结束了。对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，对于处于中止状态的事物，该事务对数据库所做的所有修改都会被回滚到没执行该事物之前的状态。\n2. 如何使用事务 使用事务有两种方式，分别为 显式事务 和 隐式事务 。\n2.1 显式事务 步骤1： START TRANSACTION 或者 BEGIN ，作用是显式开启一个事务。\n1 2 3 mysql\u0026gt; BEGIN; #或者 mysql\u0026gt; START TRANSACTION; START TRANSACTION 语句相较于 BEGIN 特别之处在于，后边能跟随几个 修饰符 ：\n① READ ONLY ：标识当前事务是一个 只读事务 ，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。\n补充：只读事务中只是不允许修改那些其他事务也能访问到的表中的数据，对于临时表来说（我们使用 CREATE TMEPORARY TABLE 创建的表），由于它们只能再当前会话中可见，所有只读事务其实也是可以对临时表进行增、删、改操作的。\n② READ WRITE ：标识当前事务是一个 读写事务 ，也就是属于该事务的数据库操作既可以读取数据， 也可以修改数据。\n③ WITH CONSISTENT SNAPSHOT ：启动一致性读。\n比如：\n1 START TRANSACTION READ ONLY; # 开启一个只读事务 1 START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT # 开启只读事务和一致性读 1 START TRANSACTION READ WRITE, WITH CONSISTENT SNAPSHOT # 开启读写事务和一致性读 注意：\nREAD ONLY和READ WRITE是用来设置所谓的事物访问模式的，就是以只读还是读写的方式来访问数据库中的数据，一个事务的访问模式不能同时即设置为只读的也设置为读写的，所以不能同时把READ ONLY和READ WRITE放到START TRANSACTION语句后边。 如果我们不显式指定事务的访问模式，那么该事务的访问模式就是读写模式 **步骤2：**一系列事务中的操作（主要是DML，不含DDL）\n**步骤3：**提交事务 或 中止事务（即回滚事务）\n1 2 # 提交事务。当提交事务后，对数据库的修改是永久性的。 mysql\u0026gt; COMMIT; 1 2 3 4 5 # 回滚事务。即撤销正在进行的所有没有提交的修改 mysql\u0026gt; ROLLBACK; # 将事务回滚到某个保存点。 mysql\u0026gt; ROLLBACK TO [SAVEPOINT] 其中关于SAVEPOINT相关操作有：\n1 2 # 在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。 SAVEPOINT 保存点名称; 1 2 # 删除某个保存点 RELEASE SAVEPOINT 保存点名称; 2.2 隐式事务 MySQL中有一个系统变量 autocommit ：\n1 2 3 4 5 6 7 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;autocommit\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | autocommit | ON | +---------------+-------+ 1 row in set (0.01 sec) 当然，如果我们想关闭这种 自动提交 的功能，可以使用下边两种方法之一：\n显式的的使用 START TRANSACTION 或者 BEGIN 语句开启一个事务。这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。\n把系统变量 autocommit 的值设置为 OFF ，就像这样：\n1 2 3 SET autocommit = OFF; #或 SET autocommit = 0; 2.3 隐式提交数据的情况 数据定义语言（Data definition language，缩写为：DDL）\n数据库对象，指的就是数据库、表、视图、存储过程等结构。当我们CREATE、ALTER、DROP等语句去修改数据库对象时，就会隐式的提交前边语句所属于的事物。即：\n1 2 3 4 5 6 7 BEGIN; SELECT ... # 事务中的一条语句 UPDATE ... # 事务中的一条语句 ... # 事务中的其他语句 CREATE TABLE ... # 此语句会隐式的提交前边语句所属于的事务 隐式使用或修改mysql数据库中的表\n当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD等语句时也会隐式的提交前边语句所属于的事务。\n事务控制或关于锁定的语句\n① 当我们在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了另一个事务时，会隐式的提交上一个事务。即：\n1 2 3 4 5 6 7 BEGIN; SELECT ... # 事务中的一条语句 UPDATE ... # 事务中的一条语句 ... # 事务中的其他语句 BEGIN; # 此语句会隐式的提交前边语句所属于的事务 ② 当前的 autocommit 系统变量的值为 OFF ，我们手动把它调为 ON 时，也会隐式的提交前边语句所属的事务。\n③ 使用 LOCK TABLES 、 UNLOCK TABLES 等关于锁定的语句也会隐式的提交 前边语句所属的事务。\n加载数据的语句\n使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。\n关于MySQL复制的一些语句\n使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句会隐式的提交前边语句所属的事务\n其他的一些语句\n使用ANALYZE TABLE、CACHE INDEX、CAECK TABLE、FLUSH、LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。\n2.4 使用举例1：提交与回滚 我们看下在 MySQL 的默认状态下，下面这个事务最后的处理结果是什么。\n情况1：\n1 2 3 4 5 6 7 8 9 10 11 12 CREATE TABLE user(name varchar(20), PRIMARY KEY (name)) ENGINE=InnoDB; BEGIN; INSERT INTO user SELECT \u0026#39;张三\u0026#39;; COMMIT; BEGIN; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; ROLLBACK; # 回滚到最近的一个提交的事务 SELECT * FROM user; 运行结果（1 行数据）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mysql\u0026gt; commit; Query OK, 0 rows affected (0.00 秒) mysql\u0026gt; BEGIN; Query OK, 0 rows affected (0.00 秒) mysql\u0026gt; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; Query OK, 1 rows affected (0.00 秒) mysql\u0026gt; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; Duplicate entry \u0026#39;李四\u0026#39; for key \u0026#39;user.PRIMARY\u0026#39; mysql\u0026gt; ROLLBACK; Query OK, 0 rows affected (0.01 秒) mysql\u0026gt; select * from user; +--------+ | name | +--------+ | 张三 | +--------+ 1 行于数据集 (0.01 秒) 情况2：\n1 2 3 4 5 6 7 8 9 CREATE TABLE user (name varchar(20), PRIMARY KEY (name)) ENGINE=InnoDB; BEGIN; INSERT INTO user SELECT \u0026#39;张三\u0026#39;; COMMIT; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; # 此条语句单独标识一个事务，自动开启事务并提交 INSERT INTO user SELECT \u0026#39;李四\u0026#39;; # 此条语句单独标识一个事务，自动开启事务并提交 ROLLBACK; # 回滚到最近的一个事务，也就是失败的插入语句前的状态。因此查询数据，没有变化 运行结果（2 行数据）：\n1 2 3 4 5 6 7 8 mysql\u0026gt; SELECT * FROM user; +--------+ | name | +--------+ | 张三 | | 李四 | +--------+ 2 行于数据集 (0.01 秒) 情况3：\n1 2 3 4 5 6 7 8 9 10 11 12 CREATE TABLE user(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB; SET @@completion_type = 1; BEGIN; INSERT INTO user SELECT \u0026#39;张三\u0026#39;; COMMIT; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; INSERT INTO user SELECT \u0026#39;李四\u0026#39;; ROLLBACK; SELECT * FROM user; 运行结果（1 行数据）：\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT * FROM user; +--------+ | name | +--------+ | 张三 | +--------+ 1 行于数据集 (0.01 秒) 当我们设置 autocommit=0 时，不论是否采用 START TRANSACTION 或者 BEGIN 的方式来开启事务，都需要用 COMMIT 进行提交，让事务生效，使用 ROLLBACK 对事务进行回滚。\n当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。 不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效， 在 ROLLBACK 时才会回滚。\n2.5 使用举例2：测试不支持事务的engine 1 2 3 CREATE TABLE test1(i INT) ENGINE=InnoDB; CREATE TABLE test2(i INT) ENGINE=MYISAM; 针对于InnoDB表\n1 2 3 4 5 BEGIN; INSERT INTO test1 VALUES(1); ROLLBACK; SELECT * FROM test1; 结果：没有数据\n针对于MYISAM表：\n1 2 3 4 5 BEGIN; INSERT INTO test2 VALUES(1); ROLLBACK; SELECT * FROM test2; 结果：有一条数据\n2.6 使用举例3：SAVEPOINT 创建表并添加数据：\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE account( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(15), balance DECIMAL(10,2) ); INSERT INTO account(NAME,balance) VALUES (\u0026#39;张三\u0026#39;,1000), (\u0026#39;李四\u0026#39;,1000); 1 2 3 4 5 6 BEGIN; UPDATE account SET balance = balance - 100 WHERE NAME = \u0026#39;张三\u0026#39;; UPDATE account SET balance = balance - 100 WHERE NAME = \u0026#39;张三\u0026#39;; SAVEPOINT s1; # 设置保存点 UPDATE account SET balance = balance + 1 WHERE NAME = \u0026#39;张三\u0026#39;; ROLLBACK TO s1; # 回滚到保存点 结果：张三：800.00\n1 ROLLBACK; 结果：张三：1000.00\n3. 事务隔离级别 MySQL是一个 客户端／服务器 架构的软件，对于同一个服务器来说，可以有若干个客户端与之连接，每个客户端与服务器连接上之后，就可以称为一个会话（ Session ）。每个客户端都可以在自己的会话中 向服务器发出请求语句，一个请求语句可能是某个事务的一部分，也就是对于服务器来说可能同时处理多个事务。事务有 隔离性 的特性，理论上在某个事务 对某个数据进行访问 时，其他事务应该进行排队 ，当该事务提交之后，其他事务才可以继续访问这个数据。但是这样对 性能影响太大 ，我们既想保持事务的隔离性，又想让服务器在处理访问同一数据的多个事务时 性能尽量高些 ，那就看二者如何权衡取舍了。\n3.1 数据准备 1 2 3 4 5 6 CREATE TABLE student ( studentno INT, name VARCHAR(20), class varchar(20), PRIMARY KEY (studentno) ) Engine=InnoDB CHARSET=utf8; 然后向这个表里插入一条数据：\n1 INSERT INTO student VALUES(1, \u0026#39;小谷\u0026#39;, \u0026#39;1班\u0026#39;); 现在表里的数据就是这样的：\n1 2 3 4 5 6 7 mysql\u0026gt; select * from student; +-----------+--------+-------+ | studentno | name | class | +-----------+--------+-------+ | 1 | 小谷 | 1班 | +-----------+--------+-------+ 1 row in set (0.00 sec) 3.2 数据并发问题 针对事务的隔离性和并发性，我们怎么做取舍呢？先看一下访问相同数据的事务在不保证串行执行（也就是执行完一个再执行另一个）的情况下可能会出现哪些问题：\n1. 脏写（ Dirty Write ）\n对于两个事务 Session A、Session B，如果事务Session A 修改了 另一个 未提交 事务Session B 修改过 的数据，之后由于回滚，导致修改无效，那就意味着发生了 脏写\u0026ndash;写无效，示意图如下：\nSession A 和 Session B 各开启了一个事务，Session B 中的事务先将studentno列为1的记录的name列更新为\u0026rsquo;李四\u0026rsquo;，然后Session A中的事务接着又把这条studentno列为1的记录的name列更新为\u0026rsquo;张三\u0026rsquo;。如果之后Session B中的事务进行了回滚，那么Session A中的更新也将不复存在，这种现象称之为脏写。这时Session A中的事务就没有效果了，明明把数据更新了，最后也提交事务了，最后看到的数据什么变化也没有。这里大家对事务的隔离性比较了解的话，会发现默认隔离级别下，上面Session A中的更新语句会处于等待状态，这里只是跟大家说明一下会出现这样的现象。\n2. 脏读（ Dirty Read ）\n对于两个事务 Session A、Session B，Session A 读取 了已经被 Session B 更新 但还 没有被提交 的字段。 之后若 Session B 回滚 ，Session A 读取 的内容就是 临时且无效 的\u0026ndash;读无效。\nSession A和Session B各开启了一个事务，Session B中的事务先将studentno列为1的记录的name列更新 为\u0026rsquo;张三\u0026rsquo;，然后Session A中的事务再去查询这条studentno为1的记录，如果读到列name的值为\u0026rsquo;张三\u0026rsquo;，而 Session B中的事务稍后进行了回滚，那么Session A中的事务相当于读到了一个不存在的数据，这种现象就称之为 脏读 。\n3. 不可重复读（ Non-Repeatable Read ）\n对于两个事务Session A、Session B，Session A 读取了一个字段，然后 Session B 更新了该字段。 之后 Session A 再次读取 同一个字段，值就不同了。那就意味着发生了不可重复读。\u0026ndash;重复读值不同\n我们在Session B中提交了几个 隐式事务 （注意是隐式事务，意味着语句结束事务就提交了），这些事务都修改了studentno列为1的记录的列name的值，每次事务提交之后，如果Session A中的事务都可以查看到最新的值，这种现象也被称之为 不可重复读 。\n4. 幻读（ Phantom ）\n对于两个事务Session A、Session B, Session A 从一个表中 读取 了一个字段, 然后 Session B 在该表中插入了一些新的行。之后, 如果 Session A 再次读取 同一个表, 就会多出几行\u0026ndash;重复读多出数据行。那就意味着发生了幻读。\nSession A 中的事务先根据条件 studentno \u0026gt; 0 这个条件查询表 student ，得到了name列值为\u0026rsquo;张三\u0026rsquo;的记录； 之后Session B中提交了一个 隐式事务 ，该事务向表student中插入了一条新记录；之后Session A中的事务 再根据相同的条件 studentno \u0026gt; 0查询表student，得到的结果集中包含Session B中的事务新插入的那条记录，这种现象也被称之为 幻读 。我们把新插入的那些记录称之为 幻影记录 。\n3.3 SQL中的四种隔离级别 上面介绍了几种并发事务执行过程中可能遇到的一些问题，这些问题有轻重缓急之分，我们给这些问题按照严重性来排一下序：\n1 脏写 \u0026gt; 脏读 \u0026gt; 不可重复读 \u0026gt; 幻读 我们愿意舍弃一部分隔离性来换取一部分性能在这里就体现在：设立一些隔离级别，隔离级别越低，并发问题发生的就越多。SQL标准中设立了4个隔离级别：\nREAD UNCOMMITTED ：读未提交，在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。不能避免脏读、不可重复读、幻读。 READ COMMITTED ：读已提交，它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。可以避免脏读，但不可重复读、幻读问题仍然存在。 REPEATABLE READ ：可重复读，事务A在读到一条数据之后，此时事务B对该数据进行了修改并提交，那么事务A再读该数据，读到的还是原来的内容。可以避免脏读、不可重复读，但幻读问题仍然存在。MySQL InnoDB默认的隔离级别是Repeatable Reads（RR），但它通过MVCC+间隙锁解决了绝大部分幻读（后面会解释为什么是绝大部分而不是全部）的问题。 SERIALIZABLE ：可串行化，确保事务可以从一个表中读取相同的行。在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作。所有的并发问题都可以避免，但性能十分低下（表级锁）。能避免脏读、不可重复读和幻读。 SQL标准中规定，针对不同的隔离级别，并发事务可以发生不同严重程度的问题，具体情况如下：\n脏写 怎么没涉及到？因为脏写这个问题太严重了，不论是哪种隔离级别，都不允许脏写的情况发生。\n不同的隔离级别有不同的现象，并有不同的锁和并发机制，隔离级别越高，数据库的并发性能就越差，4 种事务隔离级别与并发性能的关系如下：\n3.4 MySQL支持的四种隔离级别 MySQL的默认隔离级别为REPEATABLE READ，我们可以手动修改一下事务的隔离级别。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 查看隔离级别，MySQL 5.7.20的版本之前： mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;tx_isolation\u0026#39;; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | tx_isolation | REPEATABLE-READ | +---------------+-----------------+ 1 row in set (0.00 sec) # MySQL 5.7.20版本之后，引入 transaction_isolation 来替换 tx_isolation # 查看隔离级别，MySQL 5.7.20的版本及之后： mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;transaction_isolation\u0026#39;; +-----------------------+-----------------+ | Variable_name | Value | +-----------------------+-----------------+ | transaction_isolation | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set (0.02 sec) #或者不同MySQL版本中都可以使用的： SELECT @@transaction_isolation; 3.5 如何设置事务的隔离级别 通过下面的语句修改事务的隔离级别：\n1 2 3 4 5 6 SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL 隔离级别; #其中，隔离级别格式： \u0026gt; READ UNCOMMITTED \u0026gt; READ COMMITTED \u0026gt; REPEATABLE READ \u0026gt; SERIALIZABLE 或者：\n1 2 3 4 5 6 SET [GLOBAL|SESSION] TRANSACTION_ISOLATION = \u0026#39;隔离级别\u0026#39; #其中，隔离级别格式： \u0026gt; READ-UNCOMMITTED \u0026gt; READ-COMMITTED \u0026gt; REPEATABLE-READ \u0026gt; SERIALIZABLE 关于设置时使用GLOBAL或SESSION的影响：\n使用 GLOBAL 关键字（在全局范围影响）：\n1 2 3 SET GLOBAL TRANSACTION ISOLATION LEVEL SERIALIZABLE; #或 SET GLOBAL TRANSACTION_ISOLATION = \u0026#39;SERIALIZABLE\u0026#39;; 则：\n当前已经存在的会话无效 只对执行完该语句之后产生的会话起作用 使用 SESSION 关键字（在会话范围影响）：\n1 2 3 SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE; #或 SET SESSION TRANSACTION_ISOLATION = \u0026#39;SERIALIZABLE\u0026#39;; 则：\n对当前会话的所有后续的事务有效 如果在事务之间执行，则对后续的事务有效 该语句可以在已经开启的事务中间执行，但不会影响当前正在执行的事务 如果在服务器启动时想改变事务的默认隔离级别，可以修改启动参数transaction_isolation的值。比如，在启动服务器时指定了transaction_isolation=SERIALIZABLE，那么事务的默认隔离界别就从原来的REPEATABLE-READ变成了SERIALIZABLE。\n小结：\n数据库规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。\n3.6 不同隔离级别举例 初始化数据：\n1 2 TRUNCATE TABLE account; INSERT INTO account VALUES (1,\u0026#39;张三\u0026#39;,\u0026#39;100\u0026#39;), (2,\u0026#39;李四\u0026#39;,\u0026#39;0\u0026#39;); 演示1. 读未提交之脏读\n设置隔离级别为未提交读：\n脏读就是指当前事务就在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问了这个数据，然后使用了这个数据。\n演示2：读已提交\n演示3. 不可重复读\n设置隔离级别为可重复读，事务的执行流程如下：\n当我们将当前会话的隔离级别设置为可重复读的时候，当前会话可以重复读，就是每次读取的结果集都相同，而不管其他事务有没有提交。但是在可重复读的隔离级别上会出现幻读的问题。\n演示4：幻读\n4. 事务的常见分类 从事务理论的角度来看，可以把事务分为以下几种类型：\n扁平事务（Flat Transactions） 带有保存点的扁平事务（Flat Transactions with Savepoints） 链事务（Chained Transactions） 嵌套事务（Nested Transactions） 分布式事务（Distributed Transactions） 第14章_MySQL事务日志 事务有4种特性：原子性、一致性、隔离性和持久性。那么事务的四种特性到底是基于什么机制实现呢？\n事务的隔离性由 锁机制 实现。 而事务的原子性、一致性和持久性由事务的 redo 日志和undo 日志来保证。 REDO LOG 称为 重做日志 ，提供再写入操作，恢复提交事务修改页的操作，用来保证事务的持久性。 UNDO LOG 称为 回滚日志 ，回滚行记录到某个特定版本，用来保证事务的原子性、一致性。=\u0026gt; 回滚和多版本并发控制（MVCC） 有的DBA或许会认为 UNDO 是 REDO 的逆过程，其实不然。REDO 和 UNDO都可以视为是一种 恢复操作，但是：\nredo log: 是存储引擎层 (innodb) 生成的日志，记录的是\u0026quot;物理级别\u0026quot;上的页修改操作，比如页号xxx，偏移量yyy写入了\u0026rsquo;zzz\u0026rsquo;数据。主要为了保证数据的可靠性。这样内存的写入需要记录到redo log，一旦断电或死机等情况发生，那么就会导致内存数据的丢失，之后数据库恢复，就可以从redo log里再次操作上次失效的操作了。 undo log: 是存储引擎层 (innodb) 生成的日志，记录的是 逻辑操作 日志，比如对某一行数据进行了INSERT语句操作，那么undo log就记录一条与之相反的DELETE操作。主要用于 事务的回滚 (undo log 记录的是每个修改操作的 逆操作) 和 一致性非锁定读 (undo log 回滚行记录到某种特定的版本——MVCC，即多版本并发控制)。 1. redo日志 InnoDB存储引擎是以页为单位来管理存储空间的。在真正访问页面之前，需要把在磁盘上的页缓存到内存中的Buffer Pool之后才可以访问。所有的变更都必须先更新缓冲池中的数据，然后缓冲池中的脏页会以一定的频率被刷入磁盘 (checkPoint机制)，通过缓冲池来优化CPU和磁盘之间的鸿沟，这样就可以保证整体的性能不会下降太快。\n1.1 为什么需要REDO日志 一方面，缓冲池可以帮助我们消除CPU和磁盘之间的鸿沟，checkpoint机制可以保证数据的最终落盘，然而由于checkpoint 并不是每次变更的时候就触发 的，而是master线程隔一段时间去处理的。所以最坏的情况就是事务提交后，刚写完缓冲池，数据库宕机了，那么这段数据就是丢失的，无法恢复。\n另一方面，事务包含 持久性 的特性，就是说对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库中所做的更改也不能丢失。\n那么如何保证这个持久性呢？ 一个简单的做法 ：在事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但是这个简单粗暴的做法有些问题:\n修改量与刷新磁盘工作量严重不成比例\n有时候我们仅仅修改了某个页面中的一个字节，但是我们知道在InnoDB中是以页为单位来进行磁盘IO的，也就是说我们在该事务提交时不得不将一个完整的页面从内存中刷新到磁盘，我们又知道一个默认页面时16KB大小，只修改一个字节就要刷新16KB的数据到磁盘上显然是小题大做了。\n随机IO刷新较慢\n一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，假如该事务修改的这些页面可能并不相邻，这就意味着在将某个事务修改的Buffer Pool中的页面刷新到磁盘时，需要进行很多的随机IO，随机IO比顺序IO要慢，尤其对于传统的机械硬盘来说。\n另一个解决的思路 ：我们只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。所以我们其实没有必要在每次事务提交时就把该事务在内存中修改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好。比如，某个事务将系统表空间中第10号页面中偏移量为 100 处的那个字节的值 1 改成 2 。我们只需要记录一下：将第0号表空间的10号页面的偏移量为100处的值更新为 2\nInnoDB引擎的事务采用了WAL技术 (Write-Ahead Logging)，这种技术的思想就是先写日志，再写磁盘，只有日志写入成功，才算事务提交成功，这里的日志就是redo log。当发生宕机且数据未刷到磁盘的时候，可以通过redo log来恢复，保证ACID中的D（即持久性，事务的提交对数据库的影响是持久的），这就是redo log的作用。\n1.2 REDO日志的好处、特点 1. 好处 redo日志降低了刷盘频率 redo日志占用的空间非常小 存储表空间ID、页号、偏移量以及需要更新的值，所需的存储空间是很小的，刷盘快。\n2. 特点 redo日志是顺序写入磁盘的\n在执行事务的过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO，效率比随机IO快。\n事务执行过程中，redo log不断记录\nredo log跟bin log的区别，redo log是存储引擎层产生的，而bin log是数据库层产生的。假设一个事务，对表做10万行的记录插入，在这个过程中，一直不断的往redo log顺序记录，而bin log不会记录，直到这个事务提交，才会一次写入到bin log文件中。\n1.3 redo的组成 Redo log可以简单分为以下两个部分：\n重做日志的缓冲 (redo log buffer) ，保存在内存中，是易失的。 在服务器启动时就会向操作系统申请了一大片称之为 redo log buffer 的 连续内存 空间，翻译成中文就是redo日志缓冲区。这片内存空间被划分为若干个连续的redo log block。一个redo log block占用512字节大小。\n参数设置：innodb_log_buffer_size：\nredo log buffer 大小，默认 16M ，最大值是4096M，最小值为1M。\n1 2 3 4 5 6 mysql\u0026gt; show variables like \u0026#39;%innodb_log_buffer_size%\u0026#39;; +------------------------+----------+ | Variable_name | Value | +------------------------+----------+ | innodb_log_buffer_size | 16777216 | +------------------------+----------+ 重做日志文件 (redo log file) ，保存在硬盘中，是持久的。 REDO日志文件如图所示，其中ib_logfile0和ib_logfile1即为REDO日志。\n1.4 redo的整体流程 以一个更新事务为例，redo log 流转过程，如下图所示：\n1 2 3 4 第1步：先将原始数据从磁盘中读入内存中来，修改数据的内存拷贝 第2步：生成一条重做日志并写入redo log buffer，记录的是数据被修改后的值 第3步：当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式（这样才算一个事务提交成功） 第4步：定期将内存中修改的数据刷新到磁盘中 体会： Write-Ahead Log(预先日志持久化)：在持久化一个数据页之前，先将内存中相应的日志页持久化。\n1.5 redo log的刷盘策略 redo log的写入并不是直接写入磁盘的，InnoDB引擎会在写redo log的时候先写redo log buffer，之后以一 定的频率刷入到真正的redo log file 中。这里的一定频率怎么看待呢？这就是我们要说的刷盘策略。\n注意，redo log buffer刷盘到redo log file的过程并不是真正的刷到磁盘中去，只是刷入到 文件系统缓存 （page cache）中去（这是现代操作系统为了提高文件写入效率做的一个优化），真正的写入会交给系统自己来决定（比如page cache足够大了）。那么对于InnoDB来说就存在一个问题，如果交给系统来同步，同样如果系统宕机，那么数据也丢失了（虽然整个系统宕机的概率还是比较小的）。\n针对这种情况，InnoDB给出 innodb_flush_log_at_trx_commit 参数，该参数控制 commit提交事务 时，如何将 redo log buffer 中的日志刷新到 redo log file 中。它支持三种策略：（系统默认master thread每隔1s进行一次重做日志的同步）\n设置为0 ：表示每次事务提交时不进行刷盘操作，也就是只把 redo log 留在 redo log buffer 内存里中\n第1步：先将原始数据从磁盘中读入内存中来，修改数据的内存拷贝\n第2步：生成一条重做日志并写入redo log buffer，记录的是数据被修改后的值\n第3步：当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式\n第4步：定期将内存中修改的数据刷新到磁盘中\n这种方式存在缺陷：redo log持久化磁盘的策略只有一种：系统master线程来做定期的write+fsync操作。如果master线程挂了或者说mysql宕机了，那么这宕机之后的所有数据（前一秒在redo log buffer里记录的）就会丢失。虽然这种方式是最快的，但是也是最危险的。所以，一般不会采用这种策略。\n设置为1 ：表示每次事务提交时都将进行同步，刷盘持久化到操作（ 默认值 ），也就是每次提交事务都将redo log 持久化到磁盘上，也就是write+fsync\nwrite：刷盘 fsync：持久化到磁盘 write(刷盘)指的是MySQL从buffer pool中将内容写到系统的page cache中，并没有持久化到系统磁盘上。这个速度其实是很快的。(磁盘读写很慢，算是现代操作系统这一层做的优化) fsync指的是从系统的cache中将数据持久化到系统磁盘上。这个速度可以认为比较慢，而且也是IOPS升高的真正原因。\n设置为2 ：表示每次事务提交时都只把 redo log buffer 内容写入 page cache，不进行同步。由os自己决定什么时候同步到磁盘文件（fsync）。\n另外，InnoDB存储引擎有一个后台线程，每隔1秒，就会把redo log buffer中的内容写到文件系统缓存(page cache)，然后调用刷盘操作。\n也就是说，一个没有提交事务的redo log记录，也可能会刷盘。因为在事务执行过程 redo log 记录是会写入 redo log buffer中，这些redo log 记录会被后台线程刷盘。事务执行过程中，而且在提交之前，中间的执行语句首先会被redo log记录，之后，后台线程再每隔1秒进行异步地刷盘。\n除了后台线程每秒1次的轮询操作，还有一种情况，当redo log buffer占用的空间即将达到innodb_log_buffer_size（这个参数默认是16M）的一半的时候，后台线程会主动刷盘。\n1.6 不同刷盘策略演示 1. 流程图 注：为1的情况下，事务提交成功的前提之一：需要将redo log持久化（write+fsync）到redo log file。如果刷盘失败，事务不会提交成功，这样就保证了提交事务前要持久化redo log，这样就保证了ACID里的D特性。\n2. 举例 比较innodb_flush_log_at_trx_commit对事务的影响。\n1 2 3 4 CREATE TABLE test_load( a INT, b CHAR(80) )ENGINE=INNODB; 1 2 3 4 5 6 7 8 9 10 11 12 DELIMITER// CREATE PROCEDURE p_load(COUNT INT UNSIGNED) BEGIN DECLARE s INT UNSIGNED DEFAULT 1; DECLARE c CHAR(80) DEFAULT REPEAT(\u0026#39;a\u0026#39;,80); WHILE s\u0026lt;=COUNT DO INSERT INTO test_load SELECT NULL, c; COMMIT; SET s=s+1; END WHILE; END // DELIMITER; 1 2 mysql\u0026gt; CALL p_load(30000); Query OK, 0 rows affected(1 min 23 sec) 1 min 23 sec的时间显然是不能接受的。而造成时间比较长的原因就在于fsync操作所需要的时间。\n修改参数innodb_flush_log_at_trx_commit，设置为0：\n1 mysql\u0026gt; set global innodb_flush_log_at_trx_commit = 0; 1 2 mysql\u0026gt; CALL p_load(30000); Query OK, 0 rows affected(38 sec) 修改参数innodb_flush_log_at_trx_commit，设置为2：\n1 mysql\u0026gt; set global innodb_flush_log_at_trx_commit = 2; 1 2 mysql\u0026gt; CALL p_load(30000); Query OK, 0 rows affected(46 sec) 总结：innodb_flush_log_at_trx_commit为0或2时，虽然可以显著提升mysql事务的性能，但是在某些情况下就无法完全保证ACID的D特性了\n当为0的时候，mysql只有master线程来做持久化操作（fsync），但是如果mysql宕机或系统宕机，显然会丢掉redo log buffer里记录的前一秒的数据。 当为2的时候，mysql除了master线程来定期做持久化操作之外，还会有一个主动刷盘的策略（write）。主要是每次事务提交成功的时候，会将redo log写入系统的page cache（系统缓存）里，然后再由系统决定何时真正地放到硬盘里。显然在这个过程里，如果mysql宕机，系统没宕机，还是满足D特性；但是如果系统宕机了，那么D特性就无法满足了，系统会丢掉没有来得及刷入磁盘地缓存数据。 当为1的时候（默认），mysql有两种策略：一种还是有一个mysql的master线程来定期扫描持久化；另一种就是事务提交前会强制持久化redo log（write+fsync）操作。第二种策略才是真的保证了事务的D特性。 1.7 写入redo log buffer 过程 1. 补充概念：Mini-Transaction MySQL把对底层页面中的一次原子访问过程称之为一个Mini-Transaction，简称mtr，比如，向某个索引对应的B+树中插入一条记录的过程就是一个Mini-Transaction。一个所谓的mtr可以包含一组redo日志，在进行崩溃恢复时这一组redo日志可以作为一个不可分割的整体。\n一个事务可以包含若干条语句，每一条语句其实是由若干个 mtr 组成，每一个 mtr 又可以包含若干条 redo 日志，画个图表示它们的关系就是这样：\n2. redo 日志写入log buffer 不同的事务可能是 并发 执行的，所以 T1 、 T2 之间的 mtr 可能是 交替执行 的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有redo日志当做一个整体来画）：\n有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志占用空间比较大，占用了3个block来存储。\n3. redo log block的结构图 一个redo log block是由日志头、日志体、日志尾组成。日志头占用12字节，日志尾占用8字节，所以一个block真正能存储的数据是512-12-8=492字节。\n真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。我们来看看这些所谓管理信息都有什么。\n1.8 redo log file 1. 相关参数设置 innodb_log_group_home_dir ：指定 redo log 文件组所在的路径，默认值为 ./ ，表示在数据库的数据目录下。MySQL的默认数据目录（ var/lib/mysql）下默认有两个名为 ib_logfile0 和 ib_logfile1 的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。此redo日志文件位置还可以修改。\ninnodb_log_files_in_group：指明redo log file的个数，命名方式如：ib_logfile0，iblogfile1\u0026hellip; iblogfilen。默认2个，最大100个。\n1 2 3 4 5 6 7 8 mysql\u0026gt; show variables like \u0026#39;innodb_log_files_in_group\u0026#39;; +---------------------------+-------+ | Variable_name | Value | +---------------------------+-------+ | innodb_log_files_in_group | 2 | +---------------------------+-------+ #ib_logfile0 #ib_logfile1 innodb_flush_log_at_trx_commit：控制 redo log 刷新到磁盘的策略，默认为1，也是最慢最安全的持久化策略。\ninnodb_log_file_size：单个 redo log 文件设置大小，默认值为 48M 。最大值为512G，注意最大值指的是整个 redo log 系列文件之和，即（innodb_log_files_in_group * innodb_log_file_size ）不能大于最大值512G。\n1 2 3 4 5 6 mysql\u0026gt; show variables like \u0026#39;innodb_log_file_size\u0026#39;; +----------------------+----------+ | Variable_name | Value | +----------------------+----------+ | innodb_log_file_size | 50331648 | +----------------------+----------+ 根据业务修改其大小，以便容纳较大的事务。编辑my.cnf文件并重启数据库生效，如下所示\n1 2 [root@localhost ~]# vim /etc/my.cnf innodb_log_file_size=200M 在数据库实例更新比较频繁的情况下，可以适当加大 redo log 数组和大小。但也不推荐 redo log 设置过大，在MySQL崩溃时会重新执行REDO日志中的记录。\n2. 日志文件组 总共的redo日志文件大小其实就是： innodb_log_file_size × innodb_log_files_in_group 。\n采用循环使用的方式向redo日志文件组里写数据的话，会导致后写入的redo日志覆盖掉前边写的redo日志？当然！所以InnoDB的设计者提出了checkpoint的概念。\n3. checkpoint 在整个日志文件组中还有两个重要的属性，分别是 write pos、checkpoint\nwrite pos是当前记录的位置，一边写一边后移 checkpoint是当前要擦除的位置，也是往后推移 每次刷盘 redo log 记录到日志文件组中，write pos 位置就会后移更新。每次MySQL加载日志文件组恢复数据时，会清空加载过的 redo log 记录，并把check point后移更新。write pos 和 checkpoint 之间的还空着的部分可以用来写入新的 redo log 记录。\n如果 write pos 追上 checkpoint ，表示日志文件组满了，这时候不能再写入新的 redo log记录，MySQL 得停下来，清空一些记录，把 checkpoint 推进一下。\n1.9 redo log 小结 2. Undo日志 redo log是事务持久性的保证，undo log是事务原子性的保证。在事务中更新数据的前置操作其实是要先写入一个undo log。\n2.1 如何理解Undo日志 事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但有时候事务执行到一半会出现一些情况，比如：\n情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 情况二：程序员可以在事务执行过程中手动输入ROLLBACK语句结束当前事务的执行。 以上情况出现，我们需要把数据改回原先的样子，这个过程称之为回滚，这样就可以造成一个假象：这个事务看起来什么都没做，所以符合原子性要求。\n2.2 Undo日志的作用 作用1：回滚数据 回滚只是使用逻辑反操作\n作用2：MVCC undo的另一个作用是MVCC，即在InnoDB存储引擎中MVCC的实现是通过undo来完成。当用户读取一行记录时，若该记录以及被其他事务占用，当前事务可以通过undo读取之前的行版本信息，以此实现非锁定读取。\n2.3 undo的存储结构 1. 回滚段与undo页 InnoDB对undo log的管理采用段的方式，也就是回滚段（rollback segment）。每个回滚段记录了 1024 个 undo log segment ，而在每个undo log segment段中进行undo页的申请。\n在InnoDB1.1版本之前（不包括1.1版本），只有一个rollback segment，因此支持同时在线的事务限制为1024。虽然对绝大多数的应用来说都已经够用。 从1.1版本开始InnoDB支持最大 128个rollback segment ，故其支持同时在线的事务限制提高到了 128*1024 。 1 2 3 4 5 6 mysql\u0026gt; show variables like \u0026#39;innodb_undo_logs\u0026#39;; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | innodb_undo_logs | 128 | +------------------+-------+ 2. 回滚段与事务 每个事务只会使用一个回滚段，一个回滚段在同一时刻可能会服务于多个事务。\n当一个事务开始的时候，会制定一个回滚段，在事务进行的过程中，当数据被修改时，原始的数据会被复制到回滚段。\n在回滚段中，事务会不断填充盘区，直到事务结束或所有的空间被用完。如果当前的盘区不够用，事务会在段中请求扩展下一个盘区，如果所有已分配的盘区都被用完，事务会覆盖最初的盘 区或者在回滚段允许的情况下扩展新的盘区来使用。\n回滚段存在于undo表空间中，在数据库中可以存在多个undo表空间，但同一时刻只能使用一个undo表空间。\n1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;innodb_undo_tablespaces\u0026#39;; +-------------------------+-------+ | Variable_name | Value | +-------------------------+-------+ | innodb_undo_tablespaces | 2 | +-------------------------+-------+ # undo log的数量，最少为2. undo log的truncate操作有purge协调线程发起。在truncate某个undo log表空间的过程中，保证有一个可用的undo log可用。 当事务提交时，InnoDB存储引擎会做以下两件事情：\n将undo log放入列表中，以供之后的purge操作 判断undo log所在的页是否可以重用，若可以分配给下个事务使用 3. 回滚段中的数据分类 未提交的回滚数据(uncommitted undo information)：该数据所关联的事务并未提交，用于实现读一致性，所以该数据不能被其他事务的数据覆盖。 已经提交但未过期的回滚数据(committed undo information)：该数据关联的事务已经提交，但是仍受到undo retention参数的保持时间的影响。 事务已经提交并过期的数据(expired undo information)：事务已经提交，而且数据保存时间已经超过 undo retention参数指定的时间，属于已经过期的数据。当回滚段满了之后，就优先覆盖“事务已经提交并过期的数据\u0026quot;。 事务提交后不能马上删除undo log及undo log所在的页。这是因为可能还有其他事务需要通过undo log来得到行记录之前的版本。故事务提交时将undo log放入一个链表中，是否可以最终删除undo log以undo log所在页由purge线程来判断。\n2.4 undo的类型 在InnoDB存储引擎中，undo log分为：\ninsert undo log\ninsert undo log是指insert操作中产生的undo log。因为insert操作的记录，只对事务本身可见，对其他事务不可见（这是事务隔离性的要求），故该undo log可以在事务提交后直接删除。不需要进行purge操作。（事务里的insert操作不需要旧版本数据\nupdate undo log\nupdate undo log记录的是对delete和update操作产生的undo log。该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。（事务里的delete和update操作提交后的undo日志就相当于旧版本数据\n当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。\n2.5 undo log的生命周期 1. 简要生成过程 以下是undo+redo事务的简化过程\n假设有两个数值，分别为A=1和B=2，然后将A修改为3，B修改为4\n只有Buffer Pool的流程：\n有了Redo Log和Undo Log之后：\n在更新Buffer Pool中的数据之前，我们需要先将该数据事务开始之前的状态写入Undo Log中。假设更新到一半出错了，我们就可以通过Undo Log来回滚到事务开始前。\n2. 详细生成过程 当我们执行INSERT时：\n1 2 begin; INSERT INTO user (name) VALUES (\u0026#34;tom\u0026#34;); 插入的数据都会生成一条insert undo log，并且数据的回滚指针会指向它。undo log会记录undo log的序号、插入主键的列和值\u0026hellip;，那么在进行rollback的时候，通过主键直接把对应的数据删除即可。\n当我们执行UPDATE时：\n对应更新的操作会产生update undo log，并且会分更新主键和不更新主键的，假设现在执行：\n1 UPDATE user SET name=\u0026#34;Sun\u0026#34; WHERE id=1; 这时会把老的记录写入新的undo log，让回滚指针指向新的undo log，它的undo no是1，并且新的undo log会指向老的undo log（undo no=0）。(单链表)\n假设现在执行：\n1 UPDATE user SET id=2 WHERE id=1; # 更新主键 对于更新主键的操作，会先把原来的数据deletemark标识打开，这时并没有真正的删除数据，真正的删除会交给清理线程去判断，然后在后面插入一条新的数据，新的数据也会产生undo log，并且undo log的序号会递增。\n可以发现每次对数据的变更都会产生一个undo log，当一条记录被变更多次时，那么就会产生多条undo log，undo log记录的是变更前的日志，并且每个undo log的序号是递增的，那么当要回滚的时候，按照序号依次向前推，就可以找到我们的原始数据了。(有点类似于git)\n3. undo log是如何回滚的 以上面的例子来说，假设执行rollback，那么对应的流程应该是这样：\n通过undo no=3的日志把id=2的数据删除 通过undo no=2的日志把id=1的数据的deletemark还原成0 通过undo no=1的日志把id=1的数据的name还原成Tom 通过undo no=0的日志把id=1的数据删除 4. undo log的删除 针对于insert undo log\n因为insert操作的记录，只对事务本身可见，对其他事务不可见。故该undo log可以在事务提交后直接删除，不需要进行purge操作。\n针对于update undo log\n该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。\n补充：\npurge线程两个主要作用是：清理undo页和清理page里面带有Delete_Bit标识的数据行。在InnoDB中，事务中的Delete操作实际上并不是真正的删除掉数据行，而是一种Delete Mark操作，在记录上标识Delete_Bit，而不删除记录。是一种“假删除”，只是做了个标记，真正的删除工作需要后台purge线程去完成。\n2.6 小结 undo log是逻辑日志，对事务回滚时，只是将数据库逻辑地恢复到原来的样子。\nredo log是物理日志，记录的是数据页的物理变化，undo log不是redo log的逆过程。\n第15章_锁 1. 概述 在数据库中，除传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供许多用户共享的资源。为保证数据的一致性，需要对 并发操作进行控制 ，因此产生了 锁 。同时 锁机制 也为实现MySQL 的各个隔离级别提供了保证。锁冲突也是影响数据库并发访问性能的一个重要因素。所以锁对数据库而言显得尤其重要，也更加复杂。\n2. MySQL并发事务访问相同记录 并发事务访问相同记录的情况大致可以划分为3种：\n2.1 读-读情况 读-读情况，即并发事务相继读取相同的记录。读取操作本身不会对记录有任何影响，并不会引起什么问题，所以允许这种情况的发生。\n2.2 写-写情况 写-写情况，即并发事务相继对相同的记录做出改动。\n在这种情况下可能会发生 脏写 的问题，任何一种隔离级别都不允许这种问题的发生。所以在多个未提交事务相继对一条记录做改动时，需要让它们 排队执行 ，这个排队的过程其实是通过 锁 来实现的。这个所谓的锁其实是一个内存中的结构 ，在事务执行前本来是没有锁的，也就是说一开始是没有 锁结构 和记录进行关联的，如图所示：\n当一个事务想对这条记录做改动时，首先会看看内存中有没有与这条记录关联的锁结构，当没有的时候就会在内存中生成一个锁结构与之关联。比如，事务 T1要对这条记录做改动，就需要生成一个锁结构与之关联：\n在锁结构里有很多信息，为了简化理解，只把两个比较重要的属性拿了出来：\ntrx信息：代表这个锁结构是哪个事务生成的。 is_waiting：代表当前事务是否在等待。 在事务T1改动了这条记录后，就生成了一个锁结构与该记录关联，因为之前没有别的事务为这条记录加锁，所以is_waiting属性就是false，我们把这个场景就称值为获取锁成功，或者加锁成功，然后就可以继续执行操作了。\n在事务T1提交之前，另一个事务T2也想对该记录做改动，那么先看看有没有锁结构与这条记录关联，发现有一个锁结构与之关联后，然后也生成了一个锁结构与这条记录关联，不过锁结构的is_waiting属性值为true，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，图示：\n在事务T1提交之后，就会把该事务生成的锁结构释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务T2还在等待获取锁，所以把事务T2对应的锁结构的is_waiting属性设置为false，然后把该事务对应的线程唤醒，让它继续执行，此时事务T2就算获取到锁了。效果就是这样。\n小结几种说法：\n不加锁\n意思就是不需要在内存中生成对应的 锁结构 ，可以直接执行操作。\n获取锁成功，或者加锁成功\n意思就是在内存中生成了对应的 锁结构 ，而且锁结构的 is_waiting 属性为 false ，也就是事务可以继续执行操作。\n获取锁失败，或者加锁失败，或者没有获取到锁\n意思就是在内存中生成了对应的 锁结构 ，不过锁结构的 is_waiting 属性为 true ，也就是事务需要等待，不可以继续执行操作。\n2.3 读-写或写-读情况 读-写 或 写-读 ，即一个事务进行读取操作，另一个进行改动操作。这种情况下可能发生脏读、不可重复读、幻读的问题。\n各个数据库厂商对 SQL标准 的支持都可能不一样。比如MySQL在 REPEATABLE READ 隔离级别上就已经解决了 幻读 问题。\n2.4 并发问题的解决方案 怎么解决 脏读 、 不可重复读 、 幻读 这些问题呢？其实有两种可选的解决方案：\n方案一：读操作利用多版本并发控制（ MVCC ，下章讲解），写操作进行 加锁 。 普通的SELECT语句在READ COMMITTED和REPEATABLE READ隔离级别下会使用到MVCC读取记录。\n在 READ COMMITTED 隔离级别下，一个事务在执行过程中每次执行SELECT操作时都会生成一个ReadView，ReadView的存在本身就保证了事务不可以读取到未提交的事务所做的更改，也就是避免了脏读现象； 在 REPEATABLE READ 隔离级别下，一个事务在执行过程中只有 第一次执行SELECT操作 才会生成一个ReadView，之后的SELECT操作都 复用 这个ReadView，这样也就避免了不可重复读和幻读的问题。 方案二：读、写操作都采用 加锁 的方式。 小结对比发现：\n采用 MVCC 方式的话， 读-写 操作彼此并不冲突性能更高 。 采用 加锁 方式的话， 读-写 操作彼此需要 排队执行 ，影响性能。 一般情况下我们当然愿意采用 MVCC 来解决 读-写 操作并发执行的问题，但是业务在某些特殊情况下，要求必须采用加锁的方式执行。下面就讲解下MySQL中不同类别的锁。\n3. 锁的不同角度分类 锁的分类图，如下：\n3.1 从数据操作的类型划分：读锁、写锁 增删改查：查可以是加X或S锁，但是删改只能是加X锁，增是隐式锁\n读锁 ：也称为共享锁、英文用 S 表示。针对同一份数据，多个事务的读操作可以同时进行而不会互相影响，相互不阻塞的，但是会阻塞另外一个事务的写锁获取。 写锁 ：也称为排他锁、英文用 X 表示。当前写操作没有完成前，它会阻断其他写和读。这样就能确保在给定的时间里，只有一个事务能执行写入，并防止其他用户读取或写入正在写入的同一资源。 需要注意的是对于 InnoDB 引擎来说，读锁和写锁可以加在表上，也可以加在行上。\n1. 锁定读 当前事务select...持有S锁，不阻塞持有S锁的事务来读，但是阻塞持有X锁的写；当前事务select...持有X锁，阻塞持有X锁读和写或阻塞S锁的事务读\n2. 写操作 3.2 从数据操作的粒度划分：表级锁、页级锁、行锁 1. 表锁（Table Lock） ① 表级别的S锁、X锁 在对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，InnoDB存储引擎是不会为这个表添加表级别的 S锁 或者 X锁 的。在对某个表执行一些诸如 ALTER TABLE 、 DROP TABLE 这类的 DDL 语句时，其 他事务对这个表并发执行诸如SELECT、INSERT、DELETE、UPDATE的语句会发生阻塞。同理，某个事务中对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，在其他会话中对这个表执行 DDL 语句也会 发生阻塞。这个过程其实是通过在 server层使用一种称之为 元数据锁 （英文名： Metadata Locks ， 简称 MDL ）结构来实现的。\n一般情况下，不会使用InnoDB存储引擎提供的表级别的 S锁 和 X锁 。只会在一些特殊情况下，比方说 崩溃恢复 过程中用到。比如，在系统变量 autocommit=0，innodb_table_locks = 1 时，手动获取InnoDB存储引擎提供的表t的 S锁 或者 X锁 可以这么写：\nLOCK TABLES t READ ：InnoDB存储引擎会对表 t 加表级别的 S锁 。\nLOCK TABLES t WRITE ：InnoDB存储引擎会对表 t 加表级别的 X锁。\n不过尽量避免在使用InnoDB存储引擎的表上使用 LOCK TABLES 这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB的厉害之处还是实现了更细粒度的 行锁 ，关于 InnoDB表级别的 S锁 和 X锁 大家了解一下就可以了。\n**举例：**下面我们讲解MyISAM引擎下的表锁。\n步骤1：创建表并添加数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE mylock( id INT NOT NULL PRIMARY KEY auto_increment, NAME VARCHAR(20) )ENGINE myisam; # 插入一条数据 INSERT INTO mylock(NAME) VALUES(\u0026#39;a\u0026#39;); # 查询表中所有数据 SELECT * FROM mylock; +----+------+ | id | Name | +----+------+ | 1 | a | +----+------+ 步骤二：查看表上加过的锁\n1 2 3 SHOW OPEN TABLES; # 主要关注In_use字段的值 或者 SHOW OPEN TABLES where In_use \u0026gt; 0; 或者\n上面的结果表明，当前数据库中没有被锁定的表\n步骤3：手动增加表锁命令\n1 2 LOCK TABLES t READ; # 存储引擎会对表t加表级别的共享锁。共享锁也叫读锁或S锁（Share的缩写） LOCK TABLES t WRITE; # 存储引擎会对表t加表级别的排他锁。排他锁也叫独占锁、写锁或X锁（exclusive的缩写） 比如：\n步骤4：释放表锁\n1 UNLOCK TABLES; # 使用此命令解锁当前加锁的表 比如：\n步骤5：加读锁\n我们为mylock表加read锁（读阻塞写），观察阻塞的情况，流程如下：\n步骤6：加写锁\n为mylock表加write锁，观察阻塞的情况，流程如下：\n总结：\nMyISAM在执行查询语句（SELECT）前，会给涉及的所有表加读锁，在执行增删改操作前，会给涉及的表加写锁。InnoDB存储引擎是不会为这个表添加表级别的读锁和写锁的。\nMySQL的表级锁有两种模式：（以MyISAM表进行操作的演示）\n表共享读锁（Table Read Lock）\n表独占写锁（Table Write Lock）\n② 意向锁 （intention lock） InnoDB 支持 多粒度锁（multiple granularity locking） ，它允许 行级锁 与 表级锁 共存，而意向锁就是其中的一种 表锁 。\n意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁和行锁）的锁并存。 意向锁是一种不与行级锁、表级锁冲突，这一点非常重要。 表明“某个事务正在某些行持有了锁或该事务准备去持有锁” 意向锁分为两种：\n意向共享锁（intention shared lock, IS）：事务有意向对表中的某些行加共享锁（S锁）\n1 2 # 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。 SELECT column FROM table ... LOCK IN SHARE MODE; 意向排他锁（intention exclusive lock, IX）：事务有意向对表中的某些行加排他锁（X锁）\n1 2 -- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁。 SELECT column FROM table ... FOR UPDATE; 即：意向锁是由存储引擎 自己维护的 ，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前， InooDB 会先获取该数据行 所在数据表的对应意向锁 。\n1. 意向锁要解决的问题\n**举例：**创建表teacher,插入6条数据，事务的隔离级别默认为Repeatable-Read，如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE TABLE `teacher` ( `id` int NOT NULL, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; INSERT INTO `teacher` VALUES (\u0026#39;1\u0026#39;, \u0026#39;zhangsan\u0026#39;), (\u0026#39;2\u0026#39;, \u0026#39;lisi\u0026#39;), (\u0026#39;3\u0026#39;, \u0026#39;wangwu\u0026#39;), (\u0026#39;4\u0026#39;, \u0026#39;zhaoliu\u0026#39;), (\u0026#39;5\u0026#39;, \u0026#39;songhongkang\u0026#39;), (\u0026#39;6\u0026#39;, \u0026#39;leifengyang\u0026#39;); 1 2 3 4 5 6 mysql\u0026gt; SELECT @@transaction_isolation; +-------------------------+ | @@transaction_isolation | +-------------------------+ | REPEATABLE-READ | +-------------------------+ 假设事务A获取了某一行的排他锁，并未提交，语句如下所示:\n1 2 3 BEGIN; SELECT * FROM teacher WHERE id = 6 FOR UPDATE; 事务B想要获取teacher表的表读锁，语句如下：\n1 2 3 BEGIN; LOCK TABLES teacher READ; 1 2 3 BEGIN; SELECT * FROM teacher WHERE id = 6 FOR UPDATE; 此时teacher表存在两把锁：teacher表上的意向排他锁与id未6的数据行上的排他锁。事务B想要获取teacher表的共享锁。\n1 2 3 BEGIN; LOCK TABLES teacher READ; 此时事务B检测事务A持有teacher表的意向排他锁，就可以得知事务A必须持有该表中某些数据行的排他锁，那么事务B对teacher表的加锁请求就会被排斥（阻塞），而无需去检测表中的每一行数据是否存在排他锁。\n意向锁的并发性\n意向锁不会与行级的共享 / 排他锁互斥！正因为如此，意向锁并不会影响到多个事务对不同数据行加排他锁时的并发性。（不然我们直接用普通的表锁就行了）\n我们扩展一下上面 teacher表的例子来概括一下意向锁的作用（一条数据从被锁定到被释放的过程中，可 能存在多种不同锁，但是这里我们只着重表现意向锁）。\n事务A先获得了某一行的排他锁，并未提交：\n1 2 3 BEGIN; SELECT * FROM teacher WHERE id = 6 FOR UPDATE; 事务A获取了teacher表上的意向排他锁。事务A获取了id为6的数据行上的排他锁。之后事务B想要获取teacher表上的共享锁。\n1 2 3 BEGIN; LOCK TABLES teacher READ; 事务B检测到事务A持有teacher表的意向排他锁。事务B对teacher表的加锁请求被阻塞（排斥）。最后事务C也想获取teacher表中某一行的排他锁。\n1 2 3 BEGIN; SELECT * FROM teacher WHERE id = 5 FOR UPDATE; 事务C申请teacher表的意向排他锁。事务C检测到事务A持有teacher表的意向排他锁。因为意向锁之间并不互斥，所以事务C获取到了teacher表的意向排他锁。因为id为5的数据行上不存在任何排他锁，最终事务C成功获取到了该数据行上的排他锁。\n从上面的案例可以得到如下结论：\nInnoDB 支持 多粒度锁 ，特定场景下，行级锁可以与表级锁共存。 意向锁之间互不排斥，但除了 IS 与 S 兼容外， 意向锁会与 共享锁 / 排他锁 互斥 。 IX，IS是表级锁，不会和行级的X，S锁发生冲突。只会和表级的X，S发生冲突。 意向锁在保证并发性的前提下，实现了 行锁和表锁共存 且 满足事务隔离性 的要求。 ③ 自增锁（AUTO-INC锁） 在使用MySQL过程中，我们可以为表的某个列添加 AUTO_INCREMENT 属性。举例：\n1 2 3 4 5 CREATE TABLE `teacher` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; 由于这个表的id字段声明了AUTO_INCREMENT，意味着在书写插入语句时不需要为其赋值，SQL语句修改 如下所示。\n1 INSERT INTO `teacher` (name) VALUES (\u0026#39;zhangsan\u0026#39;), (\u0026#39;lisi\u0026#39;); 上边的插入语句并没有为id列显式赋值，所以系统会自动为它赋上递增的值，结果如下所示。\n1 2 3 4 5 6 7 8 mysql\u0026gt; select * from teacher; +----+----------+ | id | name | +----+----------+ | 1 | zhangsan | | 2 | lisi | +----+----------+ 2 rows in set (0.00 sec) 现在我们看到的上面插入数据只是一种简单的插入模式，所有插入数据的方式总共分为三类，分别是 “ Simple inserts ”，“ Bulk inserts ”和“ Mixed-mode inserts ”。\n1. “Simple inserts” （简单插入）\n可以 预先确定要插入的行数 （当语句被初始处理时）的语句。包括没有嵌套子查询的单行和多行INSERT...VALUES()和 REPLACE 语句。比如我们上面举的例子就属于该类插入，已经确定要插入的行 数。\n2. “Bulk inserts” （批量插入）\n事先不知道要插入的行数 （和所需自动递增值的数量）的语句。比如 INSERT ... SELECT ， REPLACE ... SELECT 和 LOAD DATA 语句，但不包括纯INSERT。 InnoDB在每处理一行，为AUTO_INCREMENT列\n3. “Mixed-mode inserts” （混合模式插入）\n这些是“Simple inserts”语句但是指定部分新行的自动递增值。例如 INSERT INTO teacher (id,name) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d'); 只是指定了部分id的值。另一种类型的“混合模式插入”是 INSERT ... ON DUPLICATE KEY UPDATE 。\ninnodb_autoinc_lock_mode有三种取值，分别对应与不同锁定模式：\n（1）innodb_autoinc_lock_mode = 0(“传统”锁定模式)\n在此锁定模式下，所有类型的insert语句都会获得一个特殊的表级AUTO-INC锁，用于插入具有 AUTO_INCREMENT列的表。这种模式其实就如我们上面的例子，即每当执行insert的时候，都会得到一个 表级锁(AUTO-INC锁)，使得语句中生成的auto_increment为顺序，且在binlog中重放的时候，可以保证 master与slave中数据的auto_increment是相同的。因为是表级锁，当在同一时间多个事务中执行insert的 时候，对于AUTO-INC锁的争夺会 限制并发 能力。\n（2）innodb_autoinc_lock_mode = 1(“连续”锁定模式)\n在 MySQL 8.0 之前，连续锁定模式是 默认 的。\n在这个模式下，“bulk inserts”仍然使用AUTO-INC表级锁，并保持到语句结束。这适用于所有INSERT \u0026hellip; SELECT，REPLACE \u0026hellip; SELECT和LOAD DATA语句。同一时刻只有一个语句可以持有AUTO-INC锁。\n对于“Simple inserts”（要插入的行数事先已知），则通过在 mutex（轻量锁） 的控制下获得所需数量的自动递增值来避免表级AUTO-INC锁， 它只在分配过程的持续时间内保持，而不是直到语句完成。不使用表级AUTO-INC锁，除非AUTO-INC锁由另一个事务保持。如果另一个事务保持AUTO-INC锁，则“Simple inserts”等待AUTO-INC锁，如同它是一个“bulk inserts”。\n（3）innodb_autoinc_lock_mode = 2(“交错”锁定模式)\n从 MySQL 8.0 开始，交错锁模式是 默认 设置。\n在此锁定模式下，自动递增值 保证 在所有并发执行的所有类型的insert语句中是 唯一 且 单调递增 的。但是，由于多个语句可以同时生成数字（即，跨语句交叉编号），为任何给定语句插入的行生成的值可能不是连续的。\n如果执行的语句是“simple inserts\u0026quot;，其中要插入的行数已提前知道，除了\u0026quot;Mixed-mode inserts\u0026quot;之外，为单个语句生成的数字不会有间隙。然后，当执行\u0026quot;bulk inserts\u0026quot;时，在由任何给定语句分配的自动递增值中可能存在间隙。\n④ 元数据锁（MDL锁） MySQL5.5引入了meta data lock，简称MDL锁，属于表锁范畴。MDL 的作用是，保证读写的正确性。比 如，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个 表结构做变更 ，增加了一 列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n因此，当对一个表做增删改查操作的时候，加 MDL读锁；当要对表做结构变更操作的时候，加 MDL 写锁。\n读锁之间不互斥，因此你可以有多个线程同时对一张表增删查改。读写锁之间、写锁之间都是互斥的，用来保证变更表结构操作的安全性，解决了DML和DDL操作之间的一致性问题。不需要显式使用，在访问一个表的时候会被自动加上。\n举例：元数据锁的使用场景模拟\n**会话A：**从表中查询数据\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; BEGIN; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SELECT COUNT(1) FROM teacher; +----------+ | COUNT(1) | +----------+ | 2 | +----------+ 1 row int set (7.46 sec) **会话B：**修改表结构，增加新列\n1 2 3 mysql\u0026gt; BEGIN; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; alter table teacher add age int not null; **会话C：**查看当前MySQL的进程\n1 mysql\u0026gt; show processlist; 通过会话C可以看出会话B被阻塞，这是由于会话A拿到了teacher表的元数据读锁，会话B想申请teacher表的元数据写锁，由于读写锁互斥，会话B需要等待会话A释放元数据锁才能执行。\n2. InnoDB中的行锁 行锁（Row Lock）也称为记录锁，顾名思义，就是锁住某一行（某条记录 row）。需要注意的是，MySQL服务器层并没有实现行锁机制，行级锁只在存储引擎层实现。\n**优点：**锁定力度小，发生锁冲突概率低，可以实现的并发度高。\n**缺点：**对于锁的开销比较大，加锁会比较慢，容易出现死锁情况。\nInnoDB与MyISAM的最大不同有两点：一是支持事物（TRANSACTION）；二是采用了行级锁。\n首先我们创建表如下：\n1 2 3 4 5 6 CREATE TABLE student ( id INT, name VARCHAR(20), class VARCHAR(10), PRIMARY KEY (id) ) Engine=InnoDB CHARSET=utf8; 向这个表里插入几条记录：\n1 2 3 4 5 6 7 8 INSERT INTO student VALUES (1, \u0026#39;张三\u0026#39;, \u0026#39;一班\u0026#39;), (3, \u0026#39;李四\u0026#39;, \u0026#39;一班\u0026#39;), (8, \u0026#39;王五\u0026#39;, \u0026#39;二班\u0026#39;), (15, \u0026#39;赵六\u0026#39;, \u0026#39;二班\u0026#39;), (20, \u0026#39;钱七\u0026#39;, \u0026#39;三班\u0026#39;); mysql\u0026gt; SELECT * FROM student; student表中的聚簇索引的简图如下所示。\n这里把B+树的索引结构做了超级简化，只把索引中的记录给拿了出来，下面看看都有哪些常用的行锁类型。\n① 记录锁（Record Locks） 记录锁也就是仅仅把一条记录锁，官方的类型名称为：LOCK_REC_NOT_GAP。\n记录锁其实很好理解，对表中的记录加锁，叫做记录锁，简称行锁。比如\n1 SELECT * FROM `test` WHERE `id`=1 FOR UPDATE; 它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行。\n需要注意的是：\nid 列必须为唯一索引列或主键列，否则上述语句加的锁就会变成临键锁(有关临键锁下面会讲)。 同时查询语句必须为精准匹配（=），不能为 \u0026gt;、\u0026lt;、like等，否则也会退化成临键锁。 其他实现\n在通过 主键索引 与 唯一索引 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁：\n1 2 -- id 列为主键列或唯一索引列 UPDATE SET age = 50 WHERE id = 1; 记录锁是锁住记录，锁住索引记录，而不是真正的数据记录.\n如果要锁的列没有索引，进行全表记录加锁\n记录锁也是排它(X)锁,所以会阻塞其他事务对其插入、更新、删除。\n② 间隙锁（Gap Locks） MySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方案解决（读undo log的历史版本来实现MVCC多版本并发控制），也可以采用加锁 方案解决。\n但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上记录锁。\nInnoDB提出了一种称之为 Gap Locks 的锁，官方的类型名称为： LOCK_GAP ，我们可以简称为 gap锁 。\nMySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方 案解决，也可以采用 加锁 方案解决。但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些 幻影记录 加上 记录锁 。InnoDB提出了一种称之为 Gap Locks 的锁，官方的类型名称为： LOCK_GAP ，我们可以简称为 gap锁 。比如，把id值为8的那条 记录加一个gap锁的示意图如下。\n图中id值为8的记录加了gap锁，意味着 不允许别的事务在id值为8的记录前边的间隙插入新记录 ，其实就是 id列的值(3, 8)这个区间的新记录是不允许立即插入的。比如，有另外一个事务再想插入一条id值为4的新 记录，它定位到该条新记录的下一条记录的id值为8，而这条记录上又有一个gap锁，所以就会阻塞插入 操作，直到拥有这个gap锁的事务提交了之后，id列的值在区间(3, 8)中的新记录才可以被插入。\n**gap锁的提出仅仅是为了防止插入幻影记录而提出的。**虽然有共享gap锁和独占gap锁这样的说法，但是它们起到的作用是相同的。而且如果对一条记录加了gap锁（不论是共享gap锁还是独占gap锁），并不会限制其他事务对这条记录加记录锁或者继续加gap锁。\n举例：\nSession1 Session2 select * from student where id=5 lock in share mode; select * from student where id=5 for update; 这里session2并不会被堵住。因为表里并没有id=5这条记录，因此session1嘉的是间隙锁(3,8)。而session2也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙锁，不允许插入值。但，它们之间是不冲突的。\nInfimum记录，表示该页面中最小的记录。 Supremun记录，表示该页面中最大的记录。 为了实现阻止其他事务插入id值再(20,正无穷)这个区间的新纪录，我们可以给索引中的最后一条记录，也就是id值为20的那条记录所在页面的Supremun记录加上一个gap锁，如图所示。\n1 2 mysql\u0026gt; select * from student where id \u0026gt; 20 lock in share mode; Empty set (0.01 sec) 检测：\n③ 临键锁（Next-Key Locks） 有时候我们既想锁住某条记录，又想阻止其他事务在该记录前边的间隙插入新记录，所以InnoDB就提出了一种称之为 Next-Key Locks 的锁，官方的类型名称为：LOCK_ORDINARY，我们也可以简称为next-key锁。\nNext-Key Locks是在存储引擎innodb、事务级别在可重复读的情况下使用的数据库锁，innodb默认的锁就是Next-Key locks。比如，我们把id值为8的那条记录加一个next-key锁的示意图如下：\nnext-key锁的本质就是一个记录锁和一个gap锁的合体，它既能保护该条记录，又能阻止别的事务将新记录插入被保护记录前边的间隙。\n1 2 begin; select * from student where id \u0026lt;=8 and id \u0026gt; 3 for update; ④ 插入意向锁（Inert Intention Locks） ⑤ 总结 记录锁的出现可以解决脏读、不可重复读的数据库并发问题；针对幻读问题，记录锁是无法给那些“幻影”记录上锁的，因为不存在。MySQL在RR隔离级别下是通过间隙锁+MVCC来解决幻读问题。举例子：在事务提交前，我使用查询语句查询数据表记录时，如果条件里出现针对唯一索引范围，那么会将这个范围加上一个锁————\u0026ldquo;间隙锁\u0026quot;来防止其他事务在这个范围插入数据，这样就避免了幻读问题。\n3. 页锁 页锁就是在 页的粒度 上进行锁定，锁定的数据资源比行锁要多，因为一个页中可以有多个行记录。当我 们使用页锁的时候，会出现数据浪费的现象，但这样的浪费最多也就是一个页上的数据行。页锁的开销介于表锁和行锁之间，会出现死锁。锁定粒度介于表锁和行锁之间，并发度一般。\n每个层级的锁数量是有限制的，因为锁会占用内存空间， 锁空间的大小是有限的 。当某个层级的锁数量 超过了这个层级的阈值时，就会进行 锁升级 。锁升级就是用更大粒度的锁替代多个更小粒度的锁，比如 InnoDB 中行锁升级为表锁，这样做的好处是占用的锁空间降低了，但同时数据的并发度也下降了。\n3.3 从对待锁的态度划分:乐观锁、悲观锁 从对待锁的态度来看锁的话，可以将锁分成乐观锁和悲观锁，从名字中也可以看出这两种锁是两种看待 数据并发的思维方式 。需要注意的是，乐观锁和悲观锁并不是锁，而是锁的 设计思想 。\n1. 悲观锁（Pessimistic Locking） 悲观锁是一种思想，顾名思义，就是很悲观，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据操作的排它性。\n悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会 阻塞 直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞， 用完后再把资源转让给其它线程）。比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁，当其他线程想要访问数据时，都需要阻塞挂起。Java中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。\n秒杀案例1：\n2. 乐观锁（Optimistic Locking） 乐观锁认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，也就是不采用数据库自身的锁机制，而是通过程序来实现。在程序上，我们可以采用 版本号机制 或者 CAS机制 实现。乐观锁适用于多读的应用类型， 这样可以提高吞吐量。在Java中 java.util.concurrent.atomic 包下的原子变量类就是使用了乐观锁的一种实现方式：CAS实现的。\n1. 乐观锁的版本号机制\n在表中设计一个 版本字段 version ，第一次读的时候，会获取 version 字段的取值。然后对数据进行更新或删除操作时，会执行 UPDATE ... SET version=version+1 WHERE version=version 。此时如果已经有事务对这条数据进行了更改，修改就不会成功。\n这种方式类似我们熟悉的SVN、CVS版本管理系统，当我们修改了代码进行提交时，首先会检查当前版本号与服务器上的版本号是否一致，如果一致就可以直接提交，如果不一致就需要更新服务器上的最新代码，然后再进行提交。\n2. 乐观锁的时间戳机制\n时间戳和版本号机制一样，也是在更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行 比较，如果两者一致则更新成功，否则就是版本冲突。\n你能看到乐观锁就是程序员自己控制数据并发操作的权限，基本是通过给数据行增加一个戳（版本号或者时间戳），从而证明当前拿到的数据是否最新。\n3. 两种锁的适用场景 从这两种锁的设计思想中，我们总结一下乐观锁和悲观锁的适用场景：\n乐观锁 适合 读操作多 的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。而且，如果我们对同一条数据又很多个事务进行修改，那么只有其中一个事务成功。因为一个事务更新成功就会修改当前数据行的版本号，那么其他事务的修改就会失败，需要读取数据新的版本从而基于这个新版本更新。这样做可以更好地保证数据库的并发正确性，但是如果修改操作过多，可能会引起雪崩式的反应：长时间会有大量的事务失败返回。虽然保证了并发场景下的修改正确性，但是在修改较多的场景下并发性能也下降了。所以，乐观锁不适合在写操作较多的情况下使用。 悲观锁 适合 写操作多 的场景，因为写的操作具有 排它性 。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止 读 - 写 和 写 - 写 的冲突。 3.4 按加锁的方式划分：显式锁、隐式锁 1. 隐式锁 情景一：对于聚簇索引记录来说，有一个 trx_id 隐藏列，该隐藏列记录着最后改动该记录的 事务 id 。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的 trx_id 隐藏列代表的的就是 当前事务的 事务id ，如果其他事务此时想对该记录添加 S锁 或者 X锁 时，首先会看一下该记录的 trx_id 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个 X 锁 （也就是为当前事务创建一个锁结构， is_waiting 属性是 false ），然后自己进入等待状态 （也就是为自己也创建一个锁结构， is_waiting 属性是 true ）。 情景二：对于二级索引记录来说，本身并没有 trx_id 隐藏列，但是在二级索引页面的 Page Header 部分有一个 PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的 事务id ，如 果 PAGE_MAX_TRX_ID 属性值小于当前最小的活跃 事务id ，那么说明对该页面做修改的事务都已 经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记 录，然后再重复 情景一 的做法。 session 1:\n1 2 3 4 mysql\u0026gt; begin; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; insert INTO student VALUES(34,\u0026#34;周八\u0026#34;,\u0026#34;二班\u0026#34;); # 当前行的聚簇索引行格式里的trx_id为当前事务的id，这里算一个隐式锁（还没形成锁结构，一旦另外一个事务检测到了当前行，当前行的隐式锁回转化为显示的锁结构。这样另外一个事务就会进入阻塞等待状态） Query OK, 1 row affected (0.00 sec) session 2:\n1 2 3 mysql\u0026gt; begin; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select * from student lock in share mode; # 执行完，当前事务被阻塞，因为当前操作检测到的行出现了隐式锁，然后隐式锁会转化为显示锁 执行下述语句，输出结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 mysql\u0026gt; SELECT * FROM performance_schema.data_lock_waits\\G; *************************** 1. row *************************** ENGINE: INNODB REQUESTING_ENGINE_LOCK_ID: 140562531358232:7:4:9:140562535668584 REQUESTING_ENGINE_TRANSACTION_ID: 422037508068888 REQUESTING_THREAD_ID: 64 REQUESTING_EVENT_ID: 6 REQUESTING_OBJECT_INSTANCE_BEGIN: 140562535668584 BLOCKING_ENGINE_LOCK_ID: 140562531351768:7:4:9:140562535619104 BLOCKING_ENGINE_TRANSACTION_ID: 15902 BLOCKING_THREAD_ID: 64 BLOCKING_EVENT_ID: 6 BLOCKING_OBJECT_INSTANCE_BEGIN: 140562535619104 1 row in set (0.00 sec) 隐式锁的逻辑过程如下：\nA. InnoDB的每条记录中都一个隐含的trx_id字段，这个字段存在于聚簇索引的B+Tree中。\nB. 在操作一条记录前，首先根据记录中的trx_id检查该事务是否是活动的事务(未提交或回滚)。如果是活动的事务，首先将 隐式锁 转换为 显式锁 (就是为该事务添加一个锁)。\nC. 检查是否有锁冲突，如果有冲突，创建锁，并设置为 is waiting 状态（给自己加一个意向锁）。如果没有冲突不加锁，跳到E。\nD. 等待加锁成功，被唤醒，或者超时。\nE. 写数据，并将自己的trx_id写入trx_id字段。\n2. 显式锁 通过特定的语句进行加锁，我们一般称之为显示加锁，例如：\n显示加共享锁：\n1 select .... lock in share mode 显示加排它锁：\n1 select .... for update 3.5 其它锁之：全局锁 全局锁就是对 整个数据库实例 加锁。当你需要让整个库处于 只读状态 的时候，可以使用这个命令，之后 其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结 构等）和更新类事务的提交语句。全局锁的典型使用 场景 是：做 全库逻辑备份 。\n全局锁的命令：\n1 Flush tables with read lock 3.6 其它锁之：死锁 1. 概念 两个事务都持有对方需要的锁，并且在等待对方释放，并且双方都不会释放自己的锁。\n举例1：\n下面这个例子加的锁显然是隐式锁\n举例2：\n用户A给用户B转账100，再次同时，用户B也给用户A转账100。这个过程，可能导致死锁。\n2. 产生死锁的必要条件 两个或者两个以上事务 每个事务都已经持有锁并且申请新的锁 锁资源同时只能被同一个事务持有或者不兼容 事务之间因为持有锁和申请锁导致彼此循环等待 死锁的关键在于：两个（或以上）的Session加锁的顺序不一致。\n3. 如何处理死锁 **方式1：**等待，直到超时（innodb_lock_wait_timeout=50s)\n**方式2：**使用死锁检测处理死锁程序\n方式1检测死锁太过被动，innodb还提供了wait-for graph算法来主动进行死锁检测，每当加锁请求无法立即满足需要并进入等待时，wait-for graph 算法都会被触发。\n这是一种较为主动的死锁检测机制，要求数据库保存锁的信息链表和事物等待链表两部分信息。\n基于这两个信息，可以绘制wait-for graph（等待图）\n死锁检测的原理是构建一个以事务为顶点，锁为边的有向图，判断有向图是否存在环，存在既有死锁。\n一旦检测到回路、有死锁，这时候InnoDB存储引擎会选择回滚undo量最小的事务，让其他事务继续执行（innodb_deadlock_detect=on表示开启这个逻辑）。\n缺点：每个新的被阻塞的线程，都要判断是不是由于自己的加入导致了死锁，这个操作时间复杂度是O(n)。如果100个并发线程同时更新同一行，意味着要检测100*100=1万次，1万个线程就会有1千万次检测。\n如何解决？\n方式1：关闭死锁检测，但意味着可能会出现大量的超时，会导致业务有损。 方式2：控制并发访问的数量。比如在中间件中实现对于相同行的更新，在进入引擎之前排队，这样在InnoDB内部就不会有大量的死锁检测工作。 进一步的思路：\n可以考虑通过将一行改成逻辑上的多行来减少锁冲突。比如，连锁超市账户总额的记录，可以考虑放到多条记录上。账户总额等于这多个记录的值的总和。\n4. 如何避免死锁 4. 锁的内部结构 我们前边说对一条记录加锁的本质就是在内存中创建一个锁结构与之关联，那么是不是一个事务对多条记录加锁，就要创建多个锁结构呢？比如：\n1 2 # 事务T1 SELECT * FROM user LOCK IN SHARE MODE; 理论上创建多个锁结构没问题，但是如果一个事务要获取10000条记录的锁，生成10000个锁结构也太崩溃了！所以决定在对不同记录加锁时，如果符合下边这些条件的记录会放在一个锁结构中。\n在同一个事务中进行加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的 等待状态是一样的 InnoDB 存储引擎中的 锁结构 如下：\n结构解析：\n1. 锁所在的事务信息 ：\n不论是 表锁 还是 行锁 ，都是在事务执行过程中生成的，哪个事务生成了这个锁结构 ，这里就记录这个事务的信息。\n此 锁所在的事务信息 在内存结构中只是一个指针，通过指针可以找到内存中关于该事务的更多信息，比方说事务id等。\n2. 索引信息 ：\n对于 行锁 来说，需要记录一下加锁的记录是属于哪个索引的。这里也是一个指针。\n3. 表锁／行锁信息 ：\n表锁结构 和 行锁结构 在这个位置的内容是不同的：\n表锁：\n记载着是对哪个表加的锁，还有其他的一些信息。\n行锁：\n记载了三个重要的信息：\nSpace ID ：记录所在表空间。 Page Number ：记录所在页号。 n_bits ：对于行锁来说，一条记录就对应着一个比特位，一个页面中包含很多记录，用不同 的比特位来区分到底是哪一条记录加了锁。为此在行锁结构的末尾放置了一堆比特位，这个n_bis属性代表使用了多少比特位。 n_bits的值一般都比页面中记录条数多一些。主要是为了之后在页面中插入了新记录后 也不至于重新分配锁结构\n4. type_mode ：\n这是一个32位的数，被分成了 lock_mode 、 lock_type 和 rec_lock_type 三个部分，如图所示：\n锁的模式（ lock_mode ），占用低4位，可选的值如下： LOCK_IS （十进制的 0 ）：表示共享意向锁，也就是 IS锁 。 LOCK_IX （十进制的 1 ）：表示独占意向锁，也就是 IX锁 。 LOCK_S （十进制的 2 ）：表示共享锁，也就是 S锁 。 LOCK_X （十进制的 3 ）：表示独占锁，也就是 X锁 。 LOCK_AUTO_INC （十进制的 4 ）：表示 AUTO-INC锁 。 在InnoDB存储引擎中，LOCK_IS，LOCK_IX，LOCK_AUTO_INC都算是表级锁的模式，LOCK_S和 LOCK_X既可以算是表级锁的模式，也可以是行级锁的模式。\n锁的类型（ lock_type ），占用第5～8位，不过现阶段只有第5位和第6位被使用： LOCK_TABLE （十进制的 16 ），也就是当第5个比特位置为1时，表示表级锁。 LOCK_REC （十进制的 32 ），也就是当第6个比特位置为1时，表示行级锁。 行锁的具体类型（ rec_lock_type ），使用其余的位来表示。只有在 lock_type 的值为 LOCK_REC 时，也就是只有在该锁为行级锁时，才会被细分为更多的类型： LOCK_ORDINARY （十进制的 0 ）：表示 next-key锁 。 LOCK_GAP （十进制的 512 ）：也就是当第10个比特位置为1时，表示 gap锁 。 LOCK_REC_NOT_GAP （十进制的 1024 ）：也就是当第11个比特位置为1时，表示正经 记录锁 。 LOCK_INSERT_INTENTION （十进制的 2048 ）：也就是当第12个比特位置为1时，表示插入意向锁。其他的类型：还有一些不常用的类型我们就不多说了。 is_waiting 属性呢？基于内存空间的节省，所以把 is_waiting 属性放到了 type_mode 这个32 位的数字中： LOCK_WAIT （十进制的 256 ） ：当第9个比特位置为 1 时，表示 is_waiting 为 true ，也 就是当前事务尚未获取到锁，处在等待状态；当这个比特位为 0 时，表示 is_waiting 为 false ，也就是当前事务获取锁成功。 5. 其他信息 ：\n为了更好的管理系统运行过程中生成的各种锁结构而设计了各种哈希表和链表。\n6. 一堆比特位 ：\n如果是 行锁结构 的话，在该结构末尾还放置了一堆比特位，比特位的数量是由上边提到的 n_bits 属性 表示的。InnoDB数据页中的每条记录在 记录头信息 中都包含一个 heap_no 属性，伪记录 Infimum 的 heap_no 值为 0 ， Supremum 的 heap_no 值为 1 ，之后每插入一条记录， heap_no 值就增1。 锁结构最后的一堆比特位就对应着一个页面中的记录，一个比特位映射一个 heap_no ，即一个比特位映射 到页内的一条记录。\n5. 锁监控 关于MySQL锁的监控，我们一般可以通过检查 InnoDB_row_lock 等状态变量来分析系统上的行锁的争夺情况\n1 2 3 4 5 6 7 8 9 10 11 mysql\u0026gt; show status like \u0026#39;innodb_row_lock%\u0026#39;; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | Innodb_row_lock_current_waits | 0 | | Innodb_row_lock_time | 0 | | Innodb_row_lock_time_avg | 0 | | Innodb_row_lock_time_max | 0 | | Innodb_row_lock_waits | 0 | +-------------------------------+-------+ 5 rows in set (0.01 sec) 对各个状态量的说明如下：\nInnodb_row_lock_current_waits：当前正在等待锁定的数量； Innodb_row_lock_time：从系统启动到现在锁定总时间长度；（等待总时长） Innodb_row_lock_time_avg：每次等待所花平均时间；（等待平均时长） Innodb_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间； Innodb_row_lock_waits：系统启动后到现在总共等待的次数；（等待总次数） 对于这5个状态变量，比较重要的3个见上面（灰色）。\n尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。\n其他监控方法：\nMySQL把事务和锁的信息记录在了 information_schema 库中，涉及到的三张表分别是 INNODB_TRX 、 INNODB_LOCKS 和 INNODB_LOCK_WAITS 。\nMySQL5.7及之前 ，可以通过information_schema.INNODB_LOCKS查看事务的锁情况，但只能看到阻塞事务的锁；如果事务并未被阻塞，则在该表中看不到该事务的锁情况。\nMySQL8.0删除了information_schema.INNODB_LOCKS，添加了 performance_schema.data_locks，可以通过performance_schema.data_locks查看事务的锁情况，和MySQL5.7及之前不同，performance_schema.data_locks不但可以看到阻塞该事务的锁，还可以看到该事务所持有的锁。\n同时，information_schema.INNODB_LOCK_WAITS也被 performance_schema.data_lock_waits所代替。\n我们模拟一个锁等待的场景，以下是从这三张表收集的信息锁等待场景，我们依然使用记录锁中的案例，当事务2进行等待时，查询情况如下：\n（1）查询正在被锁阻塞的sql语句。\n1 SELECT * FROM information_schema.INNODB_TRX\\G; 重要属性代表含义已在上述中标注。\n（2）查询锁等待情况\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT * FROM data_lock_waits\\G; *************************** 1. row *************************** ENGINE: INNODB REQUESTING_ENGINE_LOCK_ID: 139750145405624:7:4:7:139747028690608 REQUESTING_ENGINE_TRANSACTION_ID: 13845 #被阻塞的事务ID REQUESTING_THREAD_ID: 72 REQUESTING_EVENT_ID: 26 REQUESTING_OBJECT_INSTANCE_BEGIN: 139747028690608 BLOCKING_ENGINE_LOCK_ID: 139750145406432:7:4:7:139747028813248 BLOCKING_ENGINE_TRANSACTION_ID: 13844 #正在执行的事务ID，阻塞了13845 BLOCKING_THREAD_ID: 71 BLOCKING_EVENT_ID: 24 BLOCKING_OBJECT_INSTANCE_BEGIN: 139747028813248 1 row in set (0.00 sec) （3）查询锁的情况\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 mysql \u0026gt; SELECT * from performance_schema.data_locks\\G; *************************** 1. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145405624:1068:139747028693520 ENGINE_TRANSACTION_ID: 13847 THREAD_ID: 72 EVENT_ID: 31 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: NULL OBJECT_INSTANCE_BEGIN: 139747028693520 LOCK_TYPE: TABLE LOCK_MODE: IX LOCK_STATUS: GRANTED LOCK_DATA: NULL *************************** 2. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145405624:7:4:7:139747028690608 ENGINE_TRANSACTION_ID: 13847 THREAD_ID: 72 EVENT_ID: 31 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: PRIMARY OBJECT_INSTANCE_BEGIN: 139747028690608 LOCK_TYPE: RECORD LOCK_MODE: X,REC_NOT_GAP LOCK_STATUS: WAITING LOCK_DATA: 1 *************************** 3. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145406432:1068:139747028816304 ENGINE_TRANSACTION_ID: 13846 THREAD_ID: 71 EVENT_ID: 28 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: NULL OBJECT_INSTANCE_BEGIN: 139747028816304 LOCK_TYPE: TABLE LOCK_MODE: IX LOCK_STATUS: GRANTED LOCK_DATA: NULL *************************** 4. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145406432:7:4:7:139747028813248 ENGINE_TRANSACTION_ID: 13846 THREAD_ID: 71 EVENT_ID: 28 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: PRIMARY OBJECT_INSTANCE_BEGIN: 139747028813248 LOCK_TYPE: RECORD LOCK_MODE: X,REC_NOT_GAP LOCK_STATUS: GRANTED LOCK_DATA: 1 4 rows in set (0.00 sec) ERROR: No query specified 从锁的情况可以看出来，两个事务分别获取了IX锁，我们从意向锁章节可以知道，IX锁互相时兼容的。所 以这里不会等待，但是事务1同样持有X锁，此时事务2也要去同一行记录获取X锁，他们之间不兼容，导致等待的情况发生。\n6. 附录 间隙锁加锁规则（共11个案例）\n间隙锁是在可重复读隔离级别下才会生效的：next-key lock 实际上是由间隙锁加行锁实现的，如果切换到读提交隔离级别 (read-committed) 的话，就好理解了，过程中去掉间隙锁的部分，也就是只剩下行锁的部分。而在读提交隔离级别下间隙锁就没有了，为了解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row 。也就是说，许多公司的配置为：读提交隔离级别加 binlog_format=row。业务不需要可重复读的保证，这样考虑到读提交下操作数据的锁范围更小（没有间隙锁），这个选择是合理的。\nnext-key lock 的加锁规则（临建锁），总结的加锁规则里面，包含了两个原则、两个优化和一个bug：\n原则 1 ：加锁的基本单位是 next-key lock 。 next-key lock 是前开后闭区间。 原则 2 ：查找过程中访问到的对象才会加锁。任何辅助索引上的锁，或者非索引列上的锁，最终都要回溯到主键上，在主键上也要加一把锁。 优化 1 ：索引上的等值查询，给唯一索引加锁的时候， next-key lock 退化为行锁。也就是说如果 InnoDB扫描的是一个主键、或是一个唯一索引的话，那InnoDB只会采用行锁方式来加锁 优化 2 ：索引上（不一定是唯一索引）的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug ：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 我们以表test作为例子，建表语句和初始化语句如下：其中id为主键索引。\n1 2 3 4 5 6 7 8 9 CREATE TABLE `test` ( `id` int(11) NOT NULL, `col1` int(11) DEFAULT NULL, `col2` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `col1` (`col1`) ) ENGINE=InnoDB; insert into test values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 案例一：唯一索引等值查询间隙锁\n由于表 test 中没有 id=7 的记录\n根据原则 1 ，加锁单位是 next-key lock ， session A 加锁范围就是 (5,10] ； 同时根据优化 2 ，这是一个等值查询 (id=7) ，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)\n案例二：非唯一索引等值查询锁\n这里 session A 要给索引 col1 上 col1=5 的这一行加上读锁。\n根据原则 1 ，加锁单位是 next-key lock ，左开右闭，5是闭上的，因此会给 (0,5] 加上 next-key lock。 要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的（可能有col1=5的其他记录），需要向右遍历，查到c=10才放弃。根据原则 2 ，访问到的都要加锁，因此要给 (5,10] 加 next-key lock 。 但是同时这个符合优化 2 ：等值判断，向右遍历，最后一个值不满足 col1=5 这个等值条件，因此退化成间隙锁 (5,10) 。 根据原则 2 ， 只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。 但 session C 要插入一个 (7,7,7) 的记录，就会被 session A 的间隙锁 (5,10) 锁住。这个例子说明，锁是加在索引上的。\n执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。\n如果你要用 lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，因为覆盖索引不会访问主键索引，不会给主键索引上加锁\n案例三：主键索引范围查询锁\n上面两个例子是等值查询的，这个例子是关于范围查询的，也就是说下面的语句\n1 2 select * from test where id=10 for update; # 此处只加记录锁，没有间隙锁、临键锁 select * from tets where id\u0026gt;=10 and id\u0026lt;11 for update; 这两条查语句肯定是等价的，但是它们的加锁规则不太一样\n开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock (5,10] 。 根据优化 1 ，主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。 它是范围查询，范围查找就往后继续找，找到 id=15 这一行停下来，不满足条件，因此需要加 next-key lock (10,15]。 session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15] 。首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。\n案例四：非唯一索引范围查询锁\n与案例三不同的是，案例四中查询语句的 where 部分用的是字段 col1 ，它是普通索引\n这两条查语句肯定是等价的，但是它们的加锁规则不太一样\n在第一次用 col1=10 定位记录的时候，索引 c 上加了 (5,10] 这个 next-key lock 后，由于索引 col1 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-keylock 。\n这里需要扫描到 col1=15 才停止扫描，是合理的，因为 InnoDB 要扫到 col1=15 ，才知道不需要继续往后找了。\n案例五：唯一索引范围查询锁bug\nsession A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15] 这个 next-key lock ，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。\n**但是实现上， InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20 。而且由于这是个范围扫描，因此索引 id 上的 (15,20] 这个 next-key lock 也会被锁上。**照理说，这里锁住 id=20 这一行的行为，其实是没有必要的。因为扫描到 id=15 ，就可以确定不用往后再找了。\n案例六：非唯一索引上存在等值的例子\n这里，我给表 t 插入一条新记录：insert into t values(30,10,30);也就是说，现在表里面有两个c=10的行\n但是它们的主键值 id 是不同的（分别是 10 和 30 ），因此这两个c=10 的记录之间，也是有间隙的。\n这次我们用 delete 语句来验证。注意， delete 语句加锁的逻辑，其实跟 select \u0026hellip; for update 是类似的， 也就是我在文章开始总结的两个 “ 原则 ” 、两个 “ 优化 ” 和一个 “bug” 。\n这时， session A 在遍历的时候，先访问第一个 col1=10 的记录。同样地，根据原则 1 ，这里加的是 (col1=5,id=5) 到 (col1=10,id=10) 这个 next-key lock 。\n由于c是普通索引，所以继续向右查找，直到碰到 (col1=15,id=15) 这一行循环才结束。根据优化 2 ，这是 一个等值查询，向右查找到了不满足条件的行，所以会退化成 (col1=10,id=10) 到 (col1=15,id=15) 的间隙锁。\n这个 delete 语句在索引 c 上的加锁范围，就是上面图中蓝色区域覆盖的部分。这个蓝色区域左右两边都 是虚线，表示开区间，即 (col1=5,id=5) 和 (col1=15,id=15) 这两行上都没有锁\n案例七： limit 语句加锁\n例子 6 也有一个对照案例，场景如下所示：\nsession A 的 delete 语句加了 limit 2 。你知道表 t 里 c=10 的记录其实只有两条，因此加不加 limit 2 ，删除的效果都是一样的。但是加锁效果却不一样\n这是因为，案例七里的 delete 语句明确加了 limit 2 的限制，因此在遍历到 (col1=10, id=30) 这一行之后， 满足条件的语句已经有两条，循环就结束了。因此，索引 col1 上的加锁范围就变成了从（ col1=5,id=5) 到（ col1=10,id=30) 这个前开后闭区间，如下图所示：\n这个例子对我们实践的指导意义就是， 在删除数据的时候尽量加 limit 。\n这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。\n案例八：一个死锁的例子\nsession A 启动事务后执行查询语句加 lock in share mode ，在索引 col1 上加了 next-keylock(5,10] 和 间隙锁 (10,15) （索引向右遍历退化为间隙锁）； session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待； 实际上分成了两步， 先是加 (5,10) 的间隙锁，加锁成功；然后加 col1=10 的行锁，因为sessionA上已经给这行加上了读 锁，此时申请死锁时会被阻塞 然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁， InnoDB 让 session B 回滚 案例九：order by索引排序的间隙锁 1\n如下面一条语句\n1 2 begin; select * from test where id\u0026gt;9 and id\u0026lt;12 order by id desc for update; 下图为这个表的索引id的示意图。\n首先这个查询语句的语义是 order by id desc ，要拿到满足条件的所有行，优化器必须先找到第一个 id\u0026lt;12 的值。 这个过程是通过索引树的搜索过程得到的，在引擎内部，其实是要找到 id=12 的这个值，只是最终没找到，但找到了 (10,15) 这个间隙。（ id=15 不满足条件，所以 next-key lock 退化为了间隙锁(10, 15)。) 然后向左遍历，在遍历过程中，就不是等值查询了，会扫描到 id=5 这一行，又因为区间是左开右闭的，所以会加一个next-key lock (0,5] 。也就是说，在执行过程中，通过树搜索的方式定位记录的时候，用的是“等值查询”的方法。 案例十：order by索引排序的间隙锁 2\n由于order by col1 desc，第一个要定位的是索引 col1 上“最右边的”col1=20的行。这是一个非唯一索引的等值查询：\n左开右闭区间，首先加上 next-key lock (15,20] 。向右遍历，col1=25不满足条件，退化为间隙锁,所以会加上间隙锁(20,25) 和 next-key lock (15,20]。\n在索引 col1 上向左遍历，要扫描到 col1=10 才停下来。同时又因为左开右闭区间，所以 next-key lock 会加到 (5,10] ，这正是阻塞session B 的 insert 语句的原因。\n在扫描过程中， col1=20 、 col1=15 、 col1=10 这三行都存在值，由于是 select * ，所以会在主键 id 上加三个行锁。 因此， session A 的 select 语句锁的范围就是：\n索引 col1 上 (5, 25) ； 主键索引上 id=15 、 20 两个行锁。 案例十一：update修改数据的例子-先插入后删除\n注意：根据 col1\u0026gt;5 查到的第一个记录是 col1=10 ，因此不会加 (0,5] 这个 next-key lock 。\nsession A 的加锁范围是索引 col1 上的(5,10] 、(10,15] 、(15,20] 、(20,25] 和(25,supremum]。\n之后 session B 的第一个 update 语句，要把 col1=5 改成 col1=1 ，你可以理解为两步：\n插入 (col1=1, id=5) 这个记录； 删除 (col1=5, id=5) 这个记录。 通过这个操作， session A 的加锁范围变成了图 7 所示的样子:\n好，接下来 session B 要执行 update t set col1 = 5 where col1 = 1 这个语句了，一样地可以拆成两步：\n插入 (col1=5, id=5) 这个记录； 删除 (col1=1, id=5) 这个记录。 第一步试图在已经加了间隙锁的 (1,10) 中插入数据，所以就被堵住了。 第16章_多版本并发控制 1. 什么是MVCC MVCC （Multiversion Concurrency Control），多版本并发控制。顾名思义，MVCC 是通过数据行的多个版本管理来实现数据库的 并发控制 。这项技术使得在InnoDB的事务隔离级别下执行 一致性读 操作有了保证。换言之，就是为了查询一些正在被另一个事务更新的行，并且可以看到它们被更新之前的值，这样 在做查询的时候就不用等待另一个事务释放锁。\nMVCC没有正式的标准，在不同的DBMS中MVCC的实现方式可能是不同的，也不是普遍使用的（大家可以参考相关的DBMS文档）。这里讲解InnoDB中MVCC的实现机制（MySQL其他的存储引擎并不支持它）。\n2. 快照读与当前读 MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理 读-写冲突 ，做到 即使有读写冲突时，也能做到 不加锁 ， 非阻塞并发读 ，而这个读指的就是 快照读 , 而非 当前读 。当前 读实际上是一种加锁的操作，是悲观锁的实现。而MVCC本质是采用乐观锁思想的一种方式。\n2.1 快照读 快照读又叫一致性读，读取的是快照数据。不加锁的简单的 SELECT 都属于快照读，即不加锁的非阻塞读；比如这样：\n1 SELECT * FROM player WHERE ... 之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于MVCC，它在很多情况下， 避免了加锁操作，降低了开销。\n既然是基于多版本，那么快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本。\n快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读。\n2.2 当前读 当前读读取的是记录的最新版本（最新数据，而不是历史版本的数据），读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。加锁的 SELECT，或者对数据进行增删改都会进行当前读。比如：\n1 2 3 4 5 SELECT * FROM student LOCK IN SHARE MODE; # 共享锁 SELECT * FROM student FOR UPDATE; # 排他锁 INSERT INTO student values ... # 排他锁 DELETE FROM student WHERE ... # 排他锁 UPDATE student SET ... # 排他锁 3. 复习 3.1 再谈隔离级别 我们知道事务有 4 个隔离级别，可能存在三种并发问题：\n3.2 隐藏字段、Undo Log版本链 回顾一下undo日志的版本链，对于使用 InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列。\ntrx_id ：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的 事务id 赋值给 trx_id 隐藏列。(类似于一个隐式锁，当有另外一个事务来查询当前行时，如果发现存在一个trx_id，隐式锁会蜕变为显示锁) roll_pointer ：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo日志 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 假设插入该记录的事务id为8，那么此刻该条记录的示意图如下所示：\ninsert undo只在事务回滚时起作用，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收（也就是该undo日志占用的Undo页面链表要么被重用，要么被释放）。\n假设之后两个事务id分别为10、20的事务对这条记录进行 UPDATE 操作，操作流程如下：\n每次对记录进行改动，都会记录一条undo日志，每条undo日志也都有一个roll_pointer属性（INSERT操作对应的undo日志没有该属性，因为该记录并没有更早的版本），可以将这些undo日志都连起来，串成一个链表：\n对该记录每次更新后，都会将旧值放到一条 undo日志 中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，我们把这个链表称之为 版本链 ，版本链的头节点就是当前记录最新的值。\n每个版本中还包含生成该版本时对应的事务id。\n4. MVCC实现原理之ReadView MVCC 的实现依赖于：隐藏字段、Undo Log、Read View。\n4.1 什么是ReadView 4.2 设计思路 使用 READ UNCOMMITTED 隔离级别的事务，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了。\n使用 SERIALIZABLE 隔离级别的事务，InnoDB规定使用加锁的方式来访问记录。\n使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务，都必须保证读到 已经提交了的 事务修改过的记录。假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是需要判断一下版本链中的哪个版本是当前事务可见的，这是ReadView要解决的主要问题。\n这个ReadView中主要包含4个比较重要的内容，分别如下：\ncreator_trx_id，创建这个 Read View 的事务 ID。\n说明：只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。\ntrx_ids ，表示在生成ReadView时当前系统中活跃的读写事务的 事务id列表 。\nup_limit_id ，活跃的事务中最小的事务 ID。\nlow_limit_id ，表示生成ReadView时系统中应该分配给下一个事务的 id 值。low_limit_id 是系统最大的事务id值，这里要注意是系统中的事务id，需要区别于正在活跃的事务ID。\n注意：low_limit_id并不是trx_ids中的最大值，事务id是递增分配的。比如，现在有id为1， 2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，trx_ids就包括1和2,up_limit_id的值就是1，low_limit_id的值就是4。\n4.3 ReadView的规则 有了这个ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见。\n如果被访问版本的trx_id属性值与ReadView中的 creator_trx_id 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值小于ReadView中的 up_limit_id 值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值大于或等于ReadView中的 low_limit_id 值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的trx_id属性值在ReadView的 up_limit_id 和 low_limit_id 之间，那就需要判断一下trx_id属性值是不是在 trx_ids 列表中。 如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问。 如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。 4.4 MVCC整体操作流程 了解了这些概念之后，我们来看下当查询一条记录的时候，系统如何通过MVCC找到它：\n首先获取事务自己的版本号，也就是事务ID； 获取 ReadView； 查询得到的数据，然后与 ReadView 中的事务版本号进行比较； 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照； 最后返回符合规则的数据。 在隔离级别为读已提交（Read Committed）时，一个事务中的每一次 SELECT 查询都会重新获取一次 Read View。\n如表所示：\n注意，此时同样的查询语句都会重新获取一次 Read View，这时如果 Read View 不同，就可能产生不可重复读或者幻读的情况。\n当隔离级别为可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT 的时候会获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View，如下表所示：\n5. 举例说明 5.1 READ COMMITTED隔离级别下 READ COMMITTED ：每次读取数据前都生成一个ReadView。\n现在有两个事务id分别为10、20的事务在执行:\n1 2 3 4 5 6 7 8 # Transaction 10 BEGIN; UPDATE student SET name=\u0026#34;李四\u0026#34; WHERE id=1; UPDATE student SET name=\u0026#34;王五\u0026#34; WHERE id=1; # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... 说明：事务执行过程中，只有在第一次真正修改记录时（比如使用INSERT、DELETE、UPDATE语句），才会被分配一个单独的事务id，这个事务id是递增的。所以我们才在事务2中更新一些别的表的记录，目的是让它分配事务id。\n此刻，表student 中 id 为 1 的记录得到的版本链表如下所示：\n假设现在有一个使用 READ COMMITTED 隔离级别的事务开始执行：\n1 2 3 4 5 # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为\u0026#39;张三\u0026#39; 之后，我们把 事务id 为 10 的事务提交一下：\n1 2 3 4 5 # Transaction 10 BEGIN; UPDATE student SET name=\u0026#34;李四\u0026#34; WHERE id=1; UPDATE student SET name=\u0026#34;王五\u0026#34; WHERE id=1; COMMIT; 然后再到 事务id 为 20 的事务中更新一下表 student 中 id 为 1 的记录：\n1 2 3 4 5 6 # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... UPDATE student SET name=\u0026#34;钱七\u0026#34; WHERE id=1; UPDATE student SET name=\u0026#34;宋八\u0026#34; WHERE id=1; 此刻，表student中 id 为 1 的记录的版本链就长这样：\n然后再到刚才使用 READ COMMITTED 隔离级别的事务中继续查找这个 id 为 1 的记录，如下：\n1 2 3 4 5 6 7 8 9 10 # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20均未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为\u0026#39;张三\u0026#39; # SELECT2：Transaction 10提交，Transaction 20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为\u0026#39;王五\u0026#39; -- 出现不可重复读 5.2 REPEATABLE READ隔离级别下 使用 REPEATABLE READ 隔离级别的事务来说，只会在第一次执行查询语句时生成一个 ReadView ，之后的查询就不会重复生成了。\n比如，系统里有两个 事务id 分别为 10 、 20 的事务在执行：\n1 2 3 4 5 6 7 8 # Transaction 10 BEGIN; UPDATE student SET name=\u0026#34;李四\u0026#34; WHERE id=1; UPDATE student SET name=\u0026#34;王五\u0026#34; WHERE id=1; # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... 此刻，表student 中 id 为 1 的记录得到的版本链表如下所示：\n假设现在有一个使用 REPEATABLE READ 隔离级别的事务开始执行：\n1 2 3 4 5 # 使用 REPEATABLE READ 隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为\u0026#39;张三\u0026#39; 之后，我们把 事务id 为 10 的事务提交一下，就像这样：\n1 2 3 4 5 6 7 # Transaction 10 BEGIN; UPDATE student SET name=\u0026#34;李四\u0026#34; WHERE id=1; UPDATE student SET name=\u0026#34;王五\u0026#34; WHERE id=1; COMMIT; 然后再到 事务id 为 20 的事务中更新一下表 student 中 id 为 1 的记录：\n1 2 3 4 5 6 # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... UPDATE student SET name=\u0026#34;钱七\u0026#34; WHERE id=1; UPDATE student SET name=\u0026#34;宋八\u0026#34; WHERE id=1; 此刻，表 student 中id为1的记录的版本链长这样：\n然后再到刚才使用 REPEATABLE READ 隔离级别的事务中继续查找这个 id 为 1 的记录，如下：\n1 2 3 4 5 6 7 8 # 使用REPEATABLE READ隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20均未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为\u0026#39;张三\u0026#39; # SELECT2：Transaction 10提交，Transaction 20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值仍为\u0026#39;张三\u0026#39; -- 解决了不可重复读问题 这次SELECT查询得到的结果是重复的，记录的列c值都是张三，这就是可重复读的含义。如果我们之后再把事务id为20的记录提交了，然后再到刚才使用REPEATABLE READ隔离级别的事务中继续查找这个id为1的记录，得到的结果还是张三，具体执行过程大家可以自己分析一下。\n5.3 如何解决幻读 接下来说明 InnoDB 是如何解决幻读的。\n假设现在表 student 中只有一条数据，数据内容中，主键 id=1，隐藏的 trx_id=10，它的 undo log 如下图所示。\n假设现在有事务 A 和事务 B 并发执行，事务 A 的事务 id 为 20 ， 事务 B 的事务 id 为 30 。\n步骤1：事务 A 开始第一次查询数据，查询的 SQL 语句如下。\n1 select * from student where id \u0026gt;= 1; 在开始查询之前，MySQL 会为事务 A 产生一个 ReadView，此时 ReadView 的内容如下： trx_ids= [20,30] ， up_limit_id=20 ， low_limit_id=31 ， creator_trx_id=20 。\n由于此时表 student 中只有一条数据，且符合 where id\u0026gt;=1 条件，因此会查询出来。然后根据 ReadView 机制，发现该行数据的trx_id=10，小于事务 A 的 ReadView 里 up_limit_id，这表示这条数据是事务 A 开启之前，其他事务就已经提交了的数据，因此事务 A 可以读取到。\n结论：事务 A 的第一次查询，能读取到一条数据，id=1。\n步骤2：接着事务 B(trx_id=30)，往表 student 中新插入两条数据，并提交事务。\n1 2 insert into student(id,name) values(2,\u0026#39;李四\u0026#39;); insert into student(id,name) values(3,\u0026#39;王五\u0026#39;); 此时表student 中就有三条数据了，对应的 undo 如下图所示：\n步骤3：接着事务 A 开启第二次查询，根据可重复读隔离级别的规则，此时事务 A 并不会再重新生成 ReadView。此时表 student 中的 3 条数据都满足 where id\u0026gt;=1 的条件，因此会先查出来。然后根据 ReadView 机制，判断每条数据是不是都可以被事务 A 看到。\n1）首先 id=1 的这条数据，前面已经说过了，可以被事务 A 看到。\n2）然后是 id=2 的数据，它的 trx_id=30，此时事务 A 发现，这个值处于 up_limit_id 和 low_limit_id 之间，因此还需要再判断 30 是否处于 trx_ids 数组内。由于事务 A 的 trx_ids=[20,30]，因此在数组内，这表示 id=2 的这条数据是与事务 A 在同一时刻启动的其他事务未提交的，所以这条数据不能让事务 A 看到。\n3）同理，id=3 的这条数据，trx_id 也为 30，因此也不能被事务 A 看见。\n结论：最终事务 A 的第二次查询，只能查询出 id=1 的这条数据。这和事务 A 的第一次查询的结果是一样的，因此没有出现幻读现象，所以说在 MySQL 的可重复读隔离级别下，不存在幻读问题。\n6. 总结 这里介绍了 MVCC 在 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行快照读操作时访问记录的版本链的过程。这样使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。\n核心点在于 ReadView 的原理，READ COMMITTD、REPEATABLE READ这两个隔离级别的一个很大不同就是生成ReadView的时机不同：\nREAD COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，显然这样不能避免幻读 REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了，这样就解决了幻读问题。 通过MVCC我们可以解决：\n","permalink":"https://cold-bin.github.io/post/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/","tags":["mysql事务","mysql隔离级别","redo与undo","mvcc多版本并发控制"],"title":"MySQL事务篇"},{"categories":["杂"],"contents":"burpsuitepro的模块 Proxy\n提供一个直观、友好的用户界面，它的代理服务器包含非常详细的拦截规则，并能准确分析 HTTP 消息的结构与内容。\nSpide\n爬行蜘蛛工具，可以用来抓取目标网站，以显示网站的内容，基本结构，和其他功能。\nScanner\nWeb 应用程序的安全漏洞进行自动发现工具。它被设计用于渗透测试，并密切与您现有的技术和方法，以适应执行手动和半自动化的 Web 应用程序渗透测试。这个对于抓包分析来说用处不大。\nRepeater\n可让您手动重新发送单个HTTP请求以查看服务器回应。这个对于重新构造请求头和请求体比较方便。\nIntruder\n是burp套件的优势,它提供一组特别有用的功能。它可以自动实施各种定制攻击，包括资源枚举、数据提取、模糊测试等常见漏洞等。在各种有效的扫描工具中，它能够以最细化、最简单的方式访问它生产的请求与响应，允许组合利用个人智能与该工具的控制优点。\nSequencer\n对会话令牌，会话标识符或其他出于安全原因需要随机产生的键值的可预测性进行分析。\nComparer\n是一个简单的工具，执行比较数据之间的任何两个项目（一个可视化的“差异”）。在攻击一个Web 应用程序的情况下，这一要求通常会出现当你想快速识别两个应用程序的响应之间的差异（例如，入侵者攻击的过程中收到的两种反应之间之间，或登录失败的反应使用有效的和无效的用户名）之间，或两个应用程序请求（例如，确定不同的行为引起不同的请求参数）。\nDecoder\n使用各种编码绕过服务器端输入过滤，smart decode 自动识别编码格式。\ngoogle浏览器网页抓包 下载证书 burpsuitepro默认的证书地址在120.0.0.1:8080端口，当然也可以更改。这样在本地就可以访问127.0.0.1:8080地址获取CA证书了。如果发生端口冲突，可以手动修改burpsuitepro的代理地址端口，这样只需要在对应代理端口访问即可。然后点击 CA Certificate即可下载证书\n安装证书 设置\u0026mdash;\u0026gt;隐私设置和安全性\u0026mdash;\u0026gt;更多\u0026mdash;\u0026gt;管理证书\n导入证书\n下一步到浏览本地证书位置。选所有文件，不然可能你找不到你的证书!! 选择证书后打开进入下一步。\n按下图位置设置进入下一步，完成。\n最后设置证书信任\n按图示操作找到刚才安装的证书。\n选中证书点高级\n按下图勾选，确认。最后重启浏览器即可。\n抓包 挂代理 需要使用google浏览器抓包时，需要将burpsuitepro的proxy端口地址开个代理。可以全局代理，也建议局部代理，这里推荐一款好用的插件 SwitchyOmega ,它可以定制代理规则，将浏览器的请求转发到对应地址上。这里我们需要将burpsuitepro监听的端口地址，使用代理进行转发，这样浏览器发出的请求就会被转到代理地址，就可以收到请求报文信息。\n打开拦截 proxy-\u0026gt;intercept\u0026ndash;\u0026gt;intercept is off=\u0026gt;intercept is on\n打开代理和拦截后，那么对满足代理规则的网址的请求都会拦截并解析。\n选项解释 这里说说，Forward Drop Action intercept is off/on 这几个选项\nForward\n拦截并编辑请求之后，发送请求到服务器或浏览器\nDrop\n不想要发送本次请求，可以点击Drop放弃这个拦截请求\nintercept is off/on\n关闭/开启所有拦截.如果时 on ，表示请求和响应将被拦截或自动转发根据配置的拦截规则配置的代理选项。如果显示 off 则显示拦截之后的所有信息将自动转发。相当于没有对这个请求做任何事情。\nAction\n对当前拦截的请求做一些操作，如Repeater可以篡改请求之后再发送请求。\n微信小程序抓包 由于目前微信升级了。最新版本的微信小程序应用使用代理，请求却无法走代理。目前好像不能抓包。微信小程序的请求不走代理端口地址。\nAPP抓包 todo\n","permalink":"https://cold-bin.github.io/post/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/","tags":["抓包"],"title":"BurpsuitePro抓包之旅"},{"categories":["golang"],"contents":"sync.Pool使用及源码浅析 sync.Pool使用 背景 “频繁创建对象，频繁销毁对象”是在项目开发里算比较常见。sync.Pool的出现就是为了解决这个问题。\nGo语言从1.3版本开始提供了对象重用的机制，即sync.Pool。sync.Pool是可伸缩的，同时也是并发安全的，其大小仅受限于内存的大小。sync.Pool用于存储那些被分配了但是没有被使用，而未来可能会使用的值。这样就可以不用再次经过内存分配，可直接复用已有对象，减轻GC的压力，从而提升系统的性能。\nsync.Pool的大小是可伸缩的，高负载时会动态扩容，存放在池中的对象如果不活跃了会被自动清理。\nGC是一种自动内存管理机制，回收不再使用的对象的内存。\n需要注意的是，sync.Pool 缓存的对象随时可能被无通知的清除，因此不能将 sync.Pool 用于存储持久对象的场景。\n声明对象池 只需要实现New函数即可。对象池中没有对象时，将会调用New函数创建。\n1 2 3 4 5 var machinePool = sync.Pool{ New: func() interface{} { return new(Machine) }, } Get\u0026amp;Put 1 2 3 m := machinePool.Get().(*Machine) json.Unmarshal(buf, m) machinePool.Put(m) Get() 用于从对象池中获取对象，因为返回值是 interface{}，因此需要类型转换。 Put() 则是在对象使用完毕后，返回对象池。 性能测试 struct反序列化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func BenchmarkUnmarshal(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { m := \u0026amp;Machine{} if err := json.Unmarshal([]byte(\u0026#34;{\\\u0026#34;A\\\u0026#34;:2}\u0026#34;), m); err != nil { log.Println(err) return } } } func BenchmarkUnmarshalWithPool(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { m := machinePool.Get().(*Machine) if err := json.Unmarshal([]byte(\u0026#34;{\\\u0026#34;A\\\u0026#34;:2}\u0026#34;), m); err != nil { log.Println(err) return } machinePool.Put(m) } } 测试：\n1 2 3 4 5 6 7 8 9 $ go test -bench . -benchmem goos: windows goarch: amd64 pkg: demo cpu: Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz BenchmarkUnmarshal-8 2188940 528.5 ns/op 232 B/op 6 allocs/op BenchmarkUnmarshalWithPool-8 2264995 518.5 ns/op 224 B/op 5 allocs/op PASS ok demo 3.805s 可以从结果看到：使用池化复用对象的方式要比不使用具有更好的性能。当然，本例之中的结构体比较小，没能凸显出较大区别。\nbytes.Buffer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 var bufferPool = sync.Pool{ New: func() interface{} { return \u0026amp;bytes.Buffer{} }, } var data = make([]byte, 10000) func BenchmarkBufferWithPool(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { buf := bufferPool.Get().(*bytes.Buffer) buf.Write(data) buf.Reset() bufferPool.Put(buf) } } func BenchmarkBuffer(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { var buf bytes.Buffer buf.Write(data) } } 测试：\n1 2 3 4 5 6 7 8 9 $ go test -bench . -benchmem goos: windows goarch: amd64 pkg: demo cpu: Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz BenchmarkBufferWithPool-8 10175354 109.2 ns/op 0 B/op 0 allocs/op BenchmarkBuffer-8 750318 1669 ns/op 10240 B/op 1 allocs/op PASS ok demo 2.879s 使用池化进行对象的复用和不使用有明显的性能差异\n标准库应用 json.Marshal\nencodeState对象的复用\nfmt.Printf\n对pp的复用\nsync.Pool浅析 参考博客 -\u0026gt; https://www.cyhone.com/articles/think-in-sync-pool/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // A Pool must not be copied after first use. type Pool struct { noCopy noCopy local unsafe.Pointer // local fixed-size per-P pool, actual type is [P]poolLocal localSize uintptr // size of the local array victim unsafe.Pointer // local from previous cycle victimSize uintptr // size of victims array // New optionally specifies a function to generate // a value when Get would otherwise return nil. // It may not be changed concurrently with calls to Get. New func() interface{} } no copy no copy的原因是为了安全，因为结构体对象中包含引用类型的话，直接赋值拷贝是浅拷贝，是不安全的。因为浅拷贝之后，就相当于有两个指针指向同一个地址上的对象，任意一个指针引起的更新删除等操作都会影响到另一个指针。\nno copy的实现也很简单，只需要有实现sync.Locker接口，然后再把实现的类型嵌入目标结构体，就可以实现。这种实现不是直接禁掉复制这个功能，嵌入了no copy字段的程序依然可以正常执行。通过go vet分析，拷贝了嵌入no copy字段的类型时会报错，提示不能对当前类型进行值拷贝（goland也能在一定程度上提示，变黄）。\n当然除了使用no copy字段来约束类型不能出现复制以外，还可以在代码逻辑层面实现（不是范式，不能总结）。\nlocal \u0026amp; local size local 是个数组，长度为 P 的个数。其元素类型是 poolLocal。这里面存储着各个 P 对应的本地对象池。可以近似的看做 [P]poolLocal。（P，指的是GMP里的Processor） localSize。代表 local 数组的长度。因为 P 可以在运行时通过调用 runtime.GOMAXPROCS 进行修改, 因此我们还是得通过 localSize 来对应 local 数组的长度。 由于每个 P 都有自己的一个本地对象池 poolLocal，Get 和 Put 操作都会优先存取本地对象池。由于 P 的特性，操作本地对象池的时候整个并发问题就简化了很多，可以尽量避免并发冲突。\n我们再看下本地对象池 poolLocal 的定义，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 // 每个 P 都会有一个 poolLocal 的本地 type poolLocal struct { poolLocalInternal pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte // 128 - unsafe.Sizeof(poolLocalInternal{})%128 + unsafe.Sizeof(poolLocalInternal{}) = n*128 } type poolLocalInternal struct { private interface{} shared poolChain } pad 变量的作用在下文会讲到，这里暂时不展开讨论。我们可以直接看 poolLocalInternal 的定义，其中每个本地对象池，都会包含两项：\nprivate 私有变量。Get 和 Put 操作都会优先存取 private 变量，如果 private 变量可以满足情况，则不再深入进行其他的复杂操作。 shared。其类型为 poolChain，这个是链表结构，这个就是 P 的本地对象池了。 Get方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func (p *Pool) Get() interface{} { if race.Enabled { race.Disable() } // 禁掉M调度，固定住P，并拿到当前P的poolLocal数组 l, pid := p.pin() // 途径一：拿私有 x := l.private l.private = nil // 途径二：私有没有，就拿公共存储区shared双端队列缓存 if x == nil { x, _ = l.shared.popHead() if x == nil { // 途径三：还没有，在当前P进行地址偏移，获取数组里其他所有P的公共存储区shared双端队列缓存；还没有就取 pool.victim x = p.getSlow(pid) } } // 取消P的固定 runtime_procUnpin() if race.Enabled { race.Enable() if x != nil { race.Acquire(poolRaceAddr(x)) } } // 途径四：实在没得，就内存分配一个对象 if x == nil \u0026amp;\u0026amp; p.New != nil { x = p.New() } return x } Put方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // Put adds x to the pool. func (p *Pool) Put(x interface{}) { if x == nil { return } // 竟态检测 if race.Enabled { if fastrand()%4 == 0 { // Randomly drop x on floor. return } race.ReleaseMerge(poolRaceAddr(x)) race.Disable() } // 先禁用M调度，固定当前G l, _ := p.pin() // 如果私有为空，就放到私有上 if l.private == nil { l.private = x x = nil } // 私有已经有了，就放到公共缓存区 if x != nil { l.shared.pushHead(x) } // 取消固定 runtime_procUnpin() if race.Enabled { race.Enable() } } 清理对象 每个被使用的 sync.Pool，都会在初始化阶段被添加到全局变量 allPools []*Pool 对象中。Golang 的 runtime 将会在 每轮 GC 前，触发调用 poolCleanup 函数，清理 allPools。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func poolCleanup() { // Drop victim caches from all pools. for _, p := range oldPools { p.victim = nil p.victimSize = 0 } // Move primary cache to victim cache. for _, p := range allPools { p.victim = p.local p.victimSize = p.localSize p.local = nil p.localSize = 0 } oldPools, allPools = allPools, nil } var ( allPoolsMu Mutex allPools []*Pool // get 或 put时，会将pool对象放到allPools里 oldPools []*Pool ) func init() { runtime_registerPoolCleanup(poolCleanup) } 可以看到，每次GC前，都会将当前p里的local放到victim里，这样，需要两次GC才能将sync.Pool里的对象池的对象，完全清掉。\nsync.Pool结构 ","permalink":"https://cold-bin.github.io/post/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":["sync.pool"],"title":"Sync.Pool的使用及源码分析"},{"categories":["mysql"],"contents":"[toc]\n第06章_索引的数据结构 1. 为什么使用索引 索引是存储引擎用于快速找到数据记录的一种数据结构，就好比一本教科书的目录部分，通过目录中找到对应文章的页码，便可快速定位到需要的文章。MySQL中也是一样的道理，进行数据查找时，首先查看查询条件是否命中某条索引，符合则通过索引查找相关数据，如果不符合则需要全表扫描，即需要一条一条地查找记录，直到找到与条件符合的记录。\n如上图所示，数据库没有索引的情况下，数据分布在硬盘不同的位置上面，读取数据时，摆臂需要前后摆动查询数据，这样操作非常消耗时间。如果数据顺序摆放，那么也需要从1到6行按顺序读取，这样就相当于进行了6次IO操作，依旧非常耗时。如果我们不借助任何索引结构帮助我们快速定位数据的话，我们查找 Col 2 = 89 这条记录，就要逐行去查找、去比较。从Col 2 = 34 开始，进行比较，发现不是，继续下一行。我们当前的表只有不到10行数据，但如果表很大的话，有上千万条数据，就意味着要做很多很多次硬盘I/0才能找到。现在要查找 Col 2 = 89 这条记录。CPU必须先去磁盘查找这条记录，找到之后加载到内存，再对数据进行处理。这个过程最耗时间就是磁盘I/O（涉及到磁盘的旋转时间（速度较快），磁头的寻道时间(速度慢、费时)）\n假如给数据使用 二叉树 这样的数据结构进行存储，如下图所示\n对字段 Col 2 添加了索引，就相当于在硬盘上为 Col 2 维护了一个索引的数据结构，即这个 二叉搜索树。二叉搜索树的每个结点存储的是 (K, V) 结构，key 是 Col 2，value 是该 key 所在行的文件指针（地址）。比如：该二叉搜索树的根节点就是：(34, 0x07)。现在对 Col 2 添加了索引，这时再去查找 Col 2 = 89 这条记录的时候会先去查找该二叉搜索树（二叉树的遍历查找）。读 34 到内存，89 \u0026gt; 34; 继续右侧数据，读 89 到内存，89==89；找到数据返回。找到之后就根据当前结点的 value 快速定位到要查找的记录对应的地址。我们可以发现，只需要 查找两次 就可以定位到记录的地址，查询速度就提高了。\n这就是我们为什么要建索引，目的就是为了 减少磁盘I/O的次数，加快查询速率。\n2. 索引及其优缺点 2.1 索引概述 MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。\n索引的本质：索引是数据结构。你可以简单理解为“排好序的快速查找数据结构”，满足特定查找算法。 这些数据结构以某种方式指向数据， 这样就可以在这些数据结构的基础上实现 高级查找算法 。\n索引是在存储引擎中实现的，因此每种存储引擎的索引不一定完全相同，并且每种存储引擎不一定支持所有索引类型。同时，存储引擎可以定义每个表的 最大索引数和 最大索引长度。所有存储引擎支持每个表至少16个索引，总索引长度至少为256字节。有些存储引擎支持更多的索引数和更大的索引长度。\n2.2 优点 （1）类似大学图书馆建书目索引，提高数据检索的效率，降低 数据库的IO成本 ，这也是创建索引最主 要的原因。\n（2）通过创建唯一索引，可以保证数据库表中每一行 数据的唯一性 。\n（3）在实现数据的 参考完整性方面，可以 加速表和表之间的连接 。换句话说，对于有依赖关系的子表和父表联合查询时， 可以提高查询速度。\n（4）在使用分组和排序子句进行数据查询时，可以显著 减少查询中分组和排序的时间 ，降低了CPU的消耗。\n2.3 缺点 增加索引也有许多不利的方面，主要表现在如下几个方面：\n（1）创建索引和维护索引要 耗费时间 ，并 且随着数据量的增加，所耗费的时间也会增加。\n（2）索引需要占 磁盘空间 ，除了数据表占数据空间之 外，每一个索引还要占一定的物理空间， 存储在磁盘上 ，如果有大量的索引，索引文件就可能比数据文 件更快达到最大文件尺寸。\n（3）虽然索引大大提高了查询速度，同时却会 降低更新表的速度 。当对表 中的数据进行增加、删除和修改的时候，索引也要动态地维护，这样就降低了数据的维护速度。 因此，选择使用索引时，需要综合考虑索引的优点和缺点。\n因此，选择使用索引时，需要综合考虑索引的优点和缺点。\n提示：\n索引可以提高查询的速度，但是会影响插入记录的速度。这种情况下，最好的办法是先删除表中的索引，然后插入数据，插入完成后再创建索引。\n3. InnoDB中索引的推演 3.1 索引之前的查找 先来看一个精确匹配的例子：\n1 SELECT [列名列表] FROM 表名 WHERE 列名 = xxx; 1. 在一个页中的查找 假设目前表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况：\n以主键为搜索条件\n可以在页目录中使用 二分法 快速定位到对应的槽，然后再遍历该槽对用分组中的记录即可快速找到指定记录。\n以其他列作为搜索条件\n因为在数据页中并没有对非主键列简历所谓的页目录，所以我们无法通过二分法快速定位相应的槽。这种情况下只能从 最小记录 开始 依次遍历单链表中的每条记录， 然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。\n2. 在很多页中查找 在很多页中查找记录的活动可以分为两个步骤：\n定位到记录所在的页。 从所在的页内中查找相应的记录。 在没有索引的情况下，不论是根据主键列或者其他列的值进行查找，由于我们并不能快速的定位到记录所在的页，所以只能 从第一个页沿着双向链表 一直往下找，在每一个页中根据我们上面的查找方式去查 找指定的记录。因为要遍历所有的数据页，所以这种方式显然是 超级耗时 的。如果一个表有一亿条记录呢？此时 索引 应运而生。\n3.2 设计索引 建一个表：\n1 2 3 4 5 6 mysql\u0026gt; CREATE TABLE index_demo( -\u0026gt; c1 INT, -\u0026gt; c2 INT, -\u0026gt; c3 CHAR(1), -\u0026gt; PRIMARY KEY(c1) -\u0026gt; ) ROW_FORMAT = Compact; 这个新建的 index_demo 表中有2个INT类型的列，1个CHAR(1)类型的列，而且我们规定了c1列为主键， 这个表使用 Compact 行格式来实际存储记录的。这里我们简化了index_demo表的行格式示意图：\n我们只在示意图里展示记录的这几个部分：\nrecord_type ：记录头信息的一项属性，表示记录的类型， 0 表示普通记录、 2 表示最小记 录、 3 表示最大记录、 1 暂时还没用过，下面讲。 mysql\u0026gt; CREATE TABLE index_demo( -\u0026gt; c1 INT, -\u0026gt; c2 INT, -\u0026gt; c3 CHAR(1), -\u0026gt; PRIMARY KEY(c1) -\u0026gt; ) ROW_FORMAT = Compact; next_record ：记录头信息的一项属性，表示下一条地址相对于本条记录的地址偏移量，我们用 箭头来表明下一条记录是谁。 各个列的值 ：这里只记录在 index_demo 表中的三个列，分别是 c1 、 c2 和 c3 。 其他信息 ：除了上述3种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。 将记录格式示意图的其他信息项暂时去掉并把它竖起来的效果就是这样：\n把一些记录放到页里的示意图就是：\n1. 一个简单的索引设计方案 我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以不得不依次遍历所有的数据页。所以如果我们 想快速的定位到需要查找的记录在哪些数据页 中该咋办？我们可以为快速定位记录所在的数据页而建立一个目录 ，建这个目录必须完成下边这些事：\n下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。\n假设：每个数据结构最多能存放3条记录（实际上一个数据页非常大，可以存放下好多记录）。\n1 INSERT INTO index_demo VALUES(1, 4, \u0026#39;u\u0026#39;), (3, 9, \u0026#39;d\u0026#39;), (5, 3, \u0026#39;y\u0026#39;); 那么这些记录以及按照主键值的大小串联成一个单向链表了，如图所示：\r从图中可以看出来， index_demo 表中的3条记录都被插入到了编号为10的数据页中了。此时我们再来插入一条记录\r1 INSERT INTO index_demo VALUES(4, 4, \u0026#39;a\u0026#39;); 因为 页10 最多只能放3条记录，所以我们不得不再分配一个新页：\n注意：新分配的 数据页编号可能并不是连续的。它们只是通过维护者上一个页和下一个页的编号而建立了 链表 关系。另外，页10中用户记录最大的主键值是5，而页28中有一条记录的主键值是4，因为5\u0026gt;4，所以这就不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值的要求，所以在插入主键值为4的记录的时候需要伴随着一次 记录移动，也就是把主键值为5的记录移动到页28中，然后再把主键值为4的记录插入到页10中，这个过程的示意图如下：\n这个过程表明了在对页中的记录进行增删改查操作的过程中，我们必须通过一些诸如 记录移动 的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程称为 页分裂。\n给所有的页建立一个目录项。 由于数据页的 编号可能是不连续 的，所以在向 index_demo 表中插入许多条记录后，可能是这样的效果：\n我们需要给它们做个 目录，每个页对应一个目录项，每个目录项包括下边两个部分：\n1）页的用户记录中最小的主键值，我们用 key 来表示。\n2）页号，我们用 page_on 表示。\n以 页28 为例，它对应 目录项2 ，这个目录项中包含着该页的页号 28 以及该页中用户记录的最小主 键值 5 。我们只需要把几个目录项在物理存储器上连续存储（比如：数组），就可以实现根据主键 值快速查找某条记录的功能了。比如：查找主键值为 20 的记录，具体查找过程分两步：\n先从目录项中根据 二分法 快速确定出主键值为 20 的记录在 目录项3 中（因为 12 \u0026lt; 20 \u0026lt; 209 ），它对应的页是 页9 。 再根据前边说的在页中查找记录的方式去 页9 中定位具体的记录。 至此，针对数据页做的简易目录就搞定了。这个目录有一个别名，称为 索引 。\n2. InnoDB中的索引方案 ① 迭代1次：目录项纪录的页 InnoDB怎么区分一条记录是普通的 用户记录 还是 目录项记录 呢？使用记录头信息里的 record_type 属性，它的各自取值代表的意思如下：\n0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 我们把前边使用到的目录项放到数据页中的样子就是这样：\n从图中可以看出来，我们新分配了一个编号为30的页来专门存储目录项记录。这里再次强调 目录项记录 和普通的 用户记录 的不同点：\n目录项记录 的 record_type 值是1，而 普通用户记录 的 record_type 值是0。 目录项记录只有 主键值和页的编号 两个列，而普通的用户记录的列是用户自己定义的，可能包含 很多列 ，另外还有InnoDB自己添加的隐藏列。 了解：记录头信息里还有一个叫 min_rec_mask 的属性，只有在存储 目录项记录 的页中的主键值最小的 目录项记录 的 min_rec_mask 值为 1 ，其他别的记录的 min_rec_mask 值都是 0 。 相同点：两者用的是一样的数据页，都会为主键值生成 Page Directory （页目录），从而在按照主键值进行查找时可以使用 二分法 来加快查询速度。\n现在以查找主键为 20 的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步：\n先到存储 目录项记录 的页，也就是页30中通过 二分法 快速定位到对应目录项，因为 12 \u0026lt; 20 \u0026lt; 209 ，所以定位到对应的记录所在的页就是页9。 再到存储用户记录的页9中根据 二分法 快速定位到主键值为 20 的用户记录。 ② 迭代2次：多个目录项纪录的页 从图中可以看出，我们插入了一条主键值为320的用户记录之后需要两个新的数据页：\n为存储该用户记录而新生成了 页31 。 因为原先存储目录项记录的 页30的容量已满 （我们前边假设只能存储4条目录项记录），所以不得 不需要一个新的 页32 来存放 页31 对应的目录项。 现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为 20 的记录为例：\n确定 目录项记录页 我们现在的存储目录项记录的页有两个，即 页30 和 页32 ，又因为页30表示的目录项的主键值的 范围是 [1, 320) ，页32表示的目录项的主键值不小于 320 ，所以主键值为 20 的记录对应的目 录项记录在 页30 中。 通过目录项记录页 确定用户记录真实所在的页 。 在一个存储 目录项记录 的页中通过主键值定位一条目录项记录的方式说过了。 在真实存储用户记录的页中定位到具体的记录。 ③ 迭代3次：目录项记录页的目录页 如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？那就为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子：\n如图，我们生成了一个存储更高级目录项的 页33 ，这个页中的两条记录分别代表页30和页32，如果用 户记录的主键值在 [1, 320) 之间，则到页30中查找更详细的目录项记录，如果主键值 不小于320 的 话，就到页32中查找更详细的目录项记录。\n我们可以用下边这个图来描述它：\n这个数据结构，它的名称是 B+树 。\n④ B+Tree 一个B+树的节点其实可以分成好多层，规定最下边的那层，也就是存放我们用户记录的那层为第 0 层， 之后依次往上加。之前我们做了一个非常极端的假设：存放用户记录的页 最多存放3条记录 ，存放目录项 记录的页 最多存放4条记录 。其实真实环境中一个页存放的记录数量是非常大的，假设所有存放用户记录 的叶子节点代表的数据页可以存放 100条用户记录 ，所有存放目录项记录的内节点代表的数据页可以存 放 1000条目录项记录 ，那么：\n如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放 100 条记录。 如果B+树有2层，最多能存放 1000×100=10,0000 条记录。 如果B+树有3层，最多能存放 1000×1000×100=1,0000,0000 条记录。 如果B+树有4层，最多能存放 1000×1000×1000×100=1000,0000,0000 条记录。相当多的记录！ 你的表里能存放 100000000000 条记录吗？所以一般情况下，我们用到的 B+树都不会超过4层 ，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的 Page Directory （页目录），所以在页面内也可以通过 二分法 实现快速 定位记录。\n而且最重要的是：应当尽可能减少B+树的高度，因为每深入B+树的下一层寻找页都会将该页从磁盘加载到内存，这个过程是IO入内存，非常非常慢\n3.3 常见索引概念 索引按照物理实现方式，索引可以分为 2 种：聚簇（聚集）和非聚簇（非聚集）索引。我们也把非聚集 索引称为二级索引或者辅助索引。\n1. 聚簇索引 聚簇索引并不是一种单独的索引类型，而是一种数据存储方式（所有的用户记录都存储在了叶子结点），也就是所谓的 索引即数据，数据即索引。一般情况下主键就是默认的聚簇索引\n聚簇索引默认是主键，如果表中没有定义主键，InnoDB 会选择一个唯一的非空索引代替（“唯一的非空索引”是指列不能出现null值的唯一索引，跟主键性质一样）。如果没有这样的索引，InnoDB会隐式地定义一个主键来作为聚簇索引。\n术语\u0026quot;聚簇\u0026quot;表示当前数据行和相邻的键值聚簇的存储在一起\n特点：\n使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义：\n页内的记录是按照主键的大小顺序排成一个 单向链表 。 各个存放 用户记录的页 也是根据页中用户记录的主键大小顺序排成一个 双向链表 。 存放 目录项记录的页 分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个 双向链表 。 B+树的 叶子节点 存储的是完整的用户记录。\n所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。\n我们把具有这两种特性的 B+树 称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用 INDEX 语句去创建， InnDB 存储引擎会 自动 的为我们创建聚簇索引。\n优点：\n数据访问更快 ，因为聚簇索引将索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快 聚簇索引对于主键的 排序查找 和 范围查找 速度非常快 按照聚簇索引排列顺序，查询显示一定范围数据的时候，由于数据都是紧密相连，数据库不用从多个数据块中提取数据，所以 节省了大量的io操作 。 缺点：\n插入速度严重依赖于插入顺序 ，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键。自增ID列，这样就可以保证插入时一定是按照主键顺序插入，这样就不会存在页分裂、重新构造上层页目录的情况，毕竟这样做可能就是又会出现多次磁盘IO才能办到，大大拖低了MySQL服务器的性能 更新主键的代价很高 ，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新 二级索引访问需要两次索引查找 ，第一次找到主键值，第二次根据主键值找到行数据 \u0026ndash;\u0026gt; 回表，因为二级索引，不会将一行数据全部放到B+树得叶子节点上 2. 二级索引（辅助索引、非聚簇索引） 如果我们想以别的列作为搜索条件该怎么办？肯定不能是从头到尾沿着链表依次遍历记录一遍。\n答案：我们可以多建几颗B+树，不同的B+树中的数据采用不同的排列规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一课B+树，效果如下图所示：\n这个B+树与上边介绍的聚簇索引有几处不同：\n下图里的“大小”表示数据值\n**概念：回表 **\n我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程称为回表 。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！\n问题：为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不OK吗？\n回答：\n如果把完整的用户记录放到叶子结点是可以不用回表。但是太占地方了，相当于每建立一课B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。\n因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引，或者辅助索引。由于使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为c2列建立的索引。\n非聚簇索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个非聚簇索引。\n小结：聚簇索引与非聚簇索引的原理不同，在使用上也有一些区别：\n聚簇索引的叶子节点存储的就是我们的数据记录, 非聚簇索引的叶子节点存储的是数据位置。非聚簇索引不会影响数据表的物理存储顺序。 一个表只能有一个聚簇索引，因为只能有一种排序存储的方式，但可以有多个非聚簇索引，也就是多个索引目录提供数据检索。 使用聚簇索引的时候，数据的查询效率高，但如果对数据进行插入，删除，更新等操作，效率会比非聚簇索引低。 3.联合索引 我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义：\n先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3建立的索引的示意图如下：\n如图所示，我们需要注意以下几点：\n每条目录项都有c2、c3、页号这三个部分组成，各条记录先按照c2列的值进行排序，如果记录的c2列相同，则按照c3列的值进行排序 B+树叶子节点处的用户记录由c2、c3和主键c1列组成 注意一点，以c2和c3列的大小为排序规则建立的B+树称为 联合索引 ，本质上也是一个二级索引。它的意思与分别为c2和c3列分别建立索引的表述是不同的，不同点如下：\n建立 联合索引 只会建立如上图一样的1棵B+树。 为c2和c3列分别建立索引会分别以c2和c3列的大小为排序规则建立2棵B+树。 3.4 InnoDB的B+树索引的注意事项 1. 根页面位置万年不动 实际上B+树的形成过程是这样的：\n每当为某个表创建一个B+树索引（聚簇索引不是人为创建的，默认就有）的时候，都会为这个索引创建一个 根结点 页面。最开始表中没有数据的时候，每个B+树索引对应的 根结点 中即没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点 中。 当根节点中的可用 空间用完时 继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如 页a 中，然后对这个新页进行 页分裂 的操作，得到另一个新页，比如页b 。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到 页a 或者 页b 中，而 根节点 便升级为存储目录项记录的页。 这个过程特别注意的是：一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建议一个索引，那么它的根节点的页号便会被记录到某个地方。然后凡是 InnoDB 存储引擎需要用到这个索引的时候，都会从哪个固定的地方取出根节点的页号，从而来访问这个索引。\n2. 内节点中目录项记录的唯一性 我们知道B+树索引的内节点中目录项记录的内容是 索引列 + 页号 的搭配，但是这个搭配对于二级索引来说有点不严谨。还拿 index_demo 表为例，假设这个表中的数据是这样的：\n如果二级索引中目录项记录的内容只是 索引列 + 页号 的搭配的话，那么为 c2 列简历索引后的B+树应该长这样：\n如果我们想新插入一行记录，其中 c1 、c2 、c3 的值分别是: 9、1、c, 那么在修改这个为 c2 列建立的二级索引对应的 B+ 树时便碰到了个大问题：由于 页3 中存储的目录项记录是由 c2列 + 页号 的值构成的，页3 中的两条目录项记录对应的 c2 列的值都是1，而我们 新插入的这条记录 的 c2 列的值也是 1，那我们这条新插入的记录到底应该放在 页4 中，还是应该放在 页5 中？答案：对不起，懵了\n为了让新插入记录找到自己在那个页面，我们需要保证在B+树的同一层页节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的：\n索引列的值 主键值 页号 也就是我们把主键值也添加到二级索引内节点中的目录项记录，这样就能保住 B+ 树每一层节点中各条目录项记录除页号这个字段外是唯一的，所以我们为c2建立二级索引后的示意图实际上应该是这样子的：（事实上，叶子节点也应该带上主键和索引列，下图画的有点毛病\u0026hellip;\n这样我们再插入记录(9, 1, 'c') 时，由于 页3 中存储的目录项记录是由 c2列 + 主键 + 页号 的值构成的，可以先把新纪录的 c2 列的值和 页3 中各目录项记录的 c2 列的值作比较，如果 c2 列的值相同的话，可以接着比较主键值，因为B+树同一层中不同目录项记录的 c2列 + 主键的值肯定是不一样的，所以最后肯定能定位唯一的一条目录项记录，在本例中最后确定新纪录应该被插入到 页5 中。\n3. 一个页面最少存储 2 条记录 一个B+树只需要很少的层级就可以轻松存储数亿条记录，查询速度相当不错！这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录是个啥效果呢？那就是目录层级非常非常多，而且最后的那个存放真实数据的目录中只存放一条数据。所以 InnoDB 的一个数据页至少可以存放两条记录。\n4. MyISAM中的索引方案 B树索引使用存储引擎如表所示：\n索引 / 存储引擎 MyISAM InnoDB Memory B-Tree索引 支持 支持 支持 即使多个存储引擎支持同一种类型的索引，但是他们的实现原理也是不同的。Innodb和MyISAM默认的索 引是Btree索引；而Memory默认的索引是Hash索引。\nMyISAM引擎使用 B+Tree 作为索引结构，叶子节点的data域存放的是数据记录的地址。\n4.1 MyISAM索引的原理 4.2 MyISAM 与 InnoDB对比 MyISAM的索引方式都是“非聚簇”的，与InnoDB包含1个聚簇索引是不同的。小结两种引擎中索引的区别：\n① 在InnoDB存储引擎中，我们只需要根据主键值对 聚簇索引 进行一次查找就能找到对应的记录，而在 MyISAM 中却需要进行一次 回表 操作，意味着MyISAM中建立的索引相当于全部都是 二级索引 。\n② InnoDB的数据文件本身就是索引文件，而MyISAM索引文件和数据文件是 分离的 ，索引文件仅保存数 据记录的地址。\n③ InnoDB的非聚簇索引data域存储相应记录 主键的值 ，而MyISAM索引记录的是 地址 。换句话说， InnoDB的所有非聚簇索引都引用主键作为data域。\n④ MyISAM的回表操作是十分 快速 的，因为是拿着地址偏移量直接到文件中取数据的，反观InnoDB是通 过获取主键之后再去聚簇索引里找记录，虽然说也不慢，但还是比不上直接用地址去访问。\n⑤ InnoDB要求表 必须有主键 （ MyISAM可以没有 ）。如果没有显式指定，则MySQL系统会自动选择一个 可以非空且唯一标识数据记录的列作为主键。如果不存在这种列，则MySQL自动为InnoDB表生成一个隐 含字段作为主键，这个字段长度为6个字节，类型为长整型。\n小结：\n5. 索引的代价 索引是个好东西，可不能乱建，它在空间和时间上都会有消耗：\n空间上的代价\n每建立一个索引都要为它建立一棵B+树，每一棵B+树的每一个节点都是一个数据页，一个页默认会 占用 16KB 的存储空间，一棵很大的B+树由许多数据页组成，那就是很大的一片存储空间。\n时间上的代价\n每次对表中的数据进行 增、删、改 操作时，都需要去修改各个B+树索引。而且我们讲过，B+树每 层节点都是按照索引列的值 从小到大的顺序排序 而组成了 双向链表 。不论是叶子节点中的记录，还 是内节点中的记录（也就是不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序 而形成了一个单向链表。而增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需 要额外的时间进行一些 记录移位 ， 页面分裂 、 页面回收 等操作来维护好节点和记录的排序。如果 我们建了许多索引，每个索引对应的B+树都要进行相关的维护操作，会给性能拖后腿。\n一个表上索引建的越多，就会占用越多的存储空间，在增删改记录的时候性能就越差。为了能建立又好又少的索引，我们得学学这些索引在哪些条件下起作用的。\n6. MySQL数据结构选择的合理性 6.1 全表查询 这里都懒得说了。\n6.2 Hash查询 加快查找速度的数据结构，常见的有两类：\n(1) 树，例如平衡二叉搜索树，查询/插入/修改/删除的平均时间复杂度都是 O(log2N);\n(2)哈希，例如HashMap，查询/插入/修改/删除的平均时间复杂度都是 O(1); (key, value)\n上图中哈希函数h有可能将两个不同的关键字映射到相同的位置，这叫做 碰撞 ，在数据库中一般采用 链 接法 来解决。在链接法中，将散列到同一槽位的元素放在一个链表中，如下图所示：\n实验：体会数组和hash表的查找方面的效率区别\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 算法复杂度为 O(n) @Test public void test1(){ int[] arr = new int[100000]; for(int i = 0;i \u0026lt; arr.length;i++){ arr[i] = i + 1; } long start = System.currentTimeMillis(); for(int j = 1; j\u0026lt;=100000;j++){ int temp = j; for(int i = 0;i \u0026lt; arr.length;i++){ if(temp == arr[i]){ break; } } } long end = System.currentTimeMillis(); System.out.println(\u0026#34;time： \u0026#34; + (end - start)); //time： 823 } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 算法复杂度为 O(1) @Test public void test2(){ HashSet\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(100000); for(int i = 0;i \u0026lt; 100000;i++){ set.add(i + 1); } long start = System.currentTimeMillis(); for(int j = 1; j\u0026lt;=100000;j++) { int temp = j; boolean contains = set.contains(temp); } long end = System.currentTimeMillis(); System.out.println(\u0026#34;time： \u0026#34; + (end - start)); //time： 5 } Hash结构效率高，那为什么索引结构要设计成树型呢？\nHash索引适用存储引擎如表所示：\n索引 / 存储引擎 MyISAM InnoDB Memory HASH索引 不支持 不支持 支持 Hash索引的适用性：\n采用自适应 Hash 索引目的是方便根据 SQL 的查询条件加速定位到叶子节点，特别是当 B+ 树比较深的时 候，通过自适应 Hash 索引可以明显提高数据的检索效率。\n我们可以通过 innodb_adaptive_hash_index 变量来查看是否开启了自适应 Hash，比如：\n1 mysql\u0026gt; show variables like \u0026#39;%adaptive_hash_index\u0026#39;; 6.3 二叉搜索树 如果我们利用二叉树作为索引结构，那么磁盘的IO次数和索引树的高度是相关的。\n1. 二叉搜索树的特点\n一个节点只能有两个子节点，也就是一个节点度不能超过2 左子节点 \u0026lt; 本节点; 右子节点 \u0026gt;= 本节点，比我大的向右，比我小的向左 2. 查找规则\n但是特殊情况，就是有时候二叉树的深度非常大，比如：\n为了提高查询效率，就需要 减少磁盘IO数 。为了减少磁盘IO的次数，就需要尽量 降低树的高度 ，需要把 原来“瘦高”的树结构变的“矮胖”，树的每层的分叉越多越好。\n6.4 AVL树 `每访问一次节点就需要进行一次磁盘 I/O 操作，对于上面的树来说，我们需要进行 5次 I/O 操作。虽然平衡二叉树的效率高，但是树的深度也同样高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率。\n针对同样的数据，如果我们把二叉树改成 M 叉树 （M\u0026gt;2）呢？当 M=3 时，同样的 31 个节点可以由下面 的三叉树来进行存储：\n你能看到此时树的高度降低了，当数据量 N 大的时候，以及树的分叉树 M 大的时候，M叉树的高度会远小于二叉树的高度 (M \u0026gt; 2)。所以，我们需要把 `树从“瘦高” 变 “矮胖”。\n6.5 B-Tree B 树的英文是 Balance Tree，也就是 多路平衡查找树。简写为 B-Tree。它的高度远小于平衡二叉树的高度。\nB 树的结构如下图所示：\n一个 M 阶的 B 树（M\u0026gt;2）有以下的特性：\n根节点的儿子数的范围是 [2,M]。 每个中间节点包含 k-1 个关键字和 k 个孩子，孩子的数量 = 关键字的数量 +1，k 的取值范围为 [ceil(M/2), M]。 叶子节点包括 k-1 个关键字（叶子节点没有孩子），k 的取值范围为 [ceil(M/2), M]。 假设中间节点节点的关键字为：Key[1], Key[2], …, Key[k-1]，且关键字按照升序排序，即 Key[i]\u0026lt;Key[i+1]。此时 k-1 个关键字相当于划分了 k 个范围，也就是对应着 k 个指针，即为：P[1], P[2], …, P[k]，其中 P[1] 指向关键字小于 Key[1] 的子树，P[i] 指向关键字属于 (Key[i-1], Key[i]) 的子树，P[k] 指向关键字大于 Key[k-1] 的子树。 所有叶子节点位于同一层。 上面那张图所表示的 B 树就是一棵 3 阶的 B 树。我们可以看下磁盘块 2，里面的关键字为（8，12），它 有 3 个孩子 (3，5)，(9，10) 和 (13，15)，你能看到 (3，5) 小于 8，(9，10) 在 8 和 12 之间，而 (13，15) 大于 12，刚好符合刚才我们给出的特征。\n然后我们来看下如何用 B 树进行查找。假设我们想要 查找的关键字是 9 ，那么步骤可以分为以下几步：\n我们与根节点的关键字 (17，35）进行比较，9 小于 17 那么得到指针 P1； 按照指针 P1 找到磁盘块 2，关键字为（8，12），因为 9 在 8 和 12 之间，所以我们得到指针 P2； 按照指针 P2 找到磁盘块 6，关键字为（9，10），然后我们找到了关键字 9。 你能看出来在 B 树的搜索过程中，我们比较的次数并不少，但如果把数据读取出来然后在内存中进行比 较，这个时间就是可以忽略不计的。而读取磁盘块本身需要进行 I/O 操作，消耗的时间比在内存中进行 比较所需要的时间要多，是数据查找用时的重要因素。 B 树相比于平衡二叉树来说磁盘 I/O 操作要少 ， 在数据查询中比平衡二叉树效率要高。所以 只要树的高度足够低，IO次数足够少，就可以提高查询性能 。\n再举例1：\n6.6 B+Tree MySQL官网说明： B+ 树和 B 树的差异在于以下几点：\n有 k 个孩子的节点就有 k 个关键字。也就是孩子数量 = 关键字数，而 B 树中，孩子数量 = 关键字数 +1。 非叶子节点的关键字也会同时存在在子节点中，并且是在子节点中所有关键字的最大（或最 小）。 非叶子节点仅用于索引，不保存数据记录，跟记录有关的信息都放在叶子节点中。而 B 树中， 非 叶子节点既保存索引，也保存数据记录 。 所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大 小从小到大顺序链接。 B 树和 B+ 树都可以作为索引的数据结构，在 MySQL 中采用的是 B+ 树。 但B树和B+树各有自己的应用场景，不能说B+树完全比B树好，反之亦然。\n思考题：为了减少IO，索引树会一次性加载吗？\n思考题：B+树的存储能力如何？为何说一般查找行记录，最多只需1~3次磁盘IO\n思考题：为什么说B+树比B-树更适合实际应用中操作系统的文件索引和数据库索引？\n思考题：Hash 索引与 B+ 树索引的区别\n思考题：Hash 索引与 B+ 树索引是在建索引的时候手动指定的吗？\n6.7 R树 R-Tree在MySQL很少使用，仅支持 geometry数据类型 ，支持该类型的存储引擎只有myisam、bdb、 innodb、ndb、archive几种。举个R树在现实领域中能够解决的例子：查找20英里以内所有的餐厅。如果 没有R树你会怎么解决？一般情况下我们会把餐厅的坐标(x,y)分为两个字段存放在数据库中，一个字段记 录经度，另一个字段记录纬度。这样的话我们就需要遍历所有的餐厅获取其位置信息，然后计算是否满 足要求。如果一个地区有100家餐厅的话，我们就要进行100次位置计算操作了，如果应用到谷歌、百度 地图这种超大数据库中，这种方法便必定不可行了。R树就很好的 解决了这种高维空间搜索问题 。它把B 树的思想很好的扩展到了多维空间，采用了B树分割空间的思想，并在添加、删除操作时采用合并、分解 结点的方法，保证树的平衡性。因此，R树就是一棵用来 存储高维数据的平衡树 。相对于B-Tree，R-Tree 的优势在于范围查找。\n索引 / 存储引擎 MyISAM InnoDB Memory R-Tree索引 支持 支持 不支持 6.8 小结 附录：算法的时间复杂度 同一问题可用不同算法解决，而一个算法的质量优劣将影响到算法乃至程序的效率。算法分析的目的在 于选择合适算法和改进算法。\n第7章_InnoDB数据存储结构 1. 数据库的存储结构：页 1.1 磁盘与内存交互基本单位：页 1.2 页结构概述 1.3 页的大小 不同的数据库管理系统（简称DBMS）的页大小不同。比如在 MySQL 的 InnoDB 存储引擎中，默认页的大小是 16KB，我们可以通过下面的命令来进行查看：\n1 show variables like \u0026#39;%innodb_page_size%\u0026#39;; SQL Server 中页的大小为 8KB，而在 Oracle 中我们用术语 \u0026ldquo;块\u0026rdquo; （Block）来表示 \u0026ldquo;页\u0026rdquo;，Oracle 支持的快大小为2KB, 4KB, 8KB, 16KB, 32KB 和 64KB。\n1.4 页的上层结构 另外在数据库中，还存在着区（Extent）、段（Segment）和表空间（Tablespace）的概念。行、页、区、段、表空间的关系如下图所示：\n2. 页的内部结构 页如果按类型划分的话，常见的有 数据页（保存B+树节点）、系统表、Undo 页 和 事物数据页 等。数据页是我们最常使用的页。\n数据页的 16KB 大小的存储空间被划分为七个部分，分别是文件头（File Header）、页头（Page Header）、最大最小记录（Infimum + supremum）、用户记录（User Records）、空闲空间（Free Space）、页目录（Page Directory）和文件尾（File Tailer）。\n页结构的示意图如下所示：\n如下表所示：\n我们可以把这7个结构分为3个部分。\n第一部分：File Header (文件头部) 和 File Trailer (文件尾部) 见文件InnoDB数据库存储结构.mmap\n第二部分：User Records (用户记录)、最大最小记录、Free Space (空闲空间) 见文件InnoDB数据库存储结构.mmap\n第三部分：Page Directory (页目录) 和 Page Header (页面头部) 见文件InnoDB数据库存储结构.mmap\n2.3 从数据库页的角度看B+树如何查询 一颗B+树按照字节类型可以分为两部分：\n叶子节点，B+ 树最底层的节点，节点的高度为0，存储行记录。 非叶子节点，节点的高度大于0，存储索引键和页面指针，并不存储行记录本身。(索引键肯定包括主键，聚簇索引只有主键，辅助索引这些一般是主键+索引列形成新的索引结构) 当我们从页结构来理解 B+ 树的结构的时候，可以帮我们理解一些通过索引进行检索的原理：\n当然，一般只要普通索引字段的重复值不要太多，保持在同一个数据页，这样就和唯一索引差别不大，毕竟每次处理查询都是以页为单位把数据从磁盘加载到内存里。这样普通索引临近的重复数据都能快速找到，但是如果重复数据太多，导致多页的重复数据，那么就会导致要多一次甚至多次磁盘io才能查询到数据。当然，也可能普通索引重复数据不多，但是刚好被安排在两页的尾首，这样也会耽误运行的效率。综合来说，一般唯一索引肯定要优于或等于普通索引的检索效率。\n3. InnoDB行格式 (或记录格式) 见文件InnoDB数据库存储结构.mmap下\n4. 区、段与碎片区 4.1 为什么要有区？ 4.2 为什么要有段？ 4.3 为什么要有碎片区？ 4.4 区的分类 区大体上可以分为4种类型：\n空闲的区 (FREE) : 现在还没有用到这个区中的任何页面。 有剩余空间的碎片区 (FREE_FRAG)：表示碎片区中还有可用的页面。 没有剩余空间的碎片区 (FULL_FRAG)：表示碎片区中的所有页面都被使用，没有空闲页面。 附属于某个段的区 (FSEG)：每一个索引都可以分为叶子节点段和非叶子节点段。 处于FREE、FREE_FRAG 以及 FULL_FRAG 这三种状态的区都是独立的，直属于表空间。而处于 FSEG 状态的区是附属于某个段的。\n如果把表空间比作是一个集团军，段就相当于师，区就相当于团。一般的团都是隶属于某个师的，就像是处于 FSEG 的区全部隶属于某个段，而处于 FREE、FREE_FRAG 以及 FULL_FRAG 这三种状态的区却直接隶属于表空间，就像独立团直接听命于军部一样。\n5. 表空间 5.1 独立表空间 独立表空间，即每张表有一个独立的表空间，也就是数据和索引信息都会保存在自己的表空间中。独立的表空间 (即：单表) 可以在不同的数据库之间进行 迁移。\n空间可以回收 (DROP TABLE 操作可自动回收表空间；其他情况，表空间不能自己回收) 。如果对于统计分析或是日志表，删除大量数据后可以通过：alter table TableName engine=innodb; 回收不用的空间。对于使用独立表空间的表，不管怎么删除，表空间的碎片不会太严重的影响性能，而且还有机会处理。\n独立表空间结构\n独立表空间由段、区、页组成。\n真实表空间对应的文件大小\n我们到数据目录里看，会发现一个新建的表对应的 .ibd 文件只占用了 96K，才6个页面大小 (MySQL5.7中)，这是因为一开始表空间占用的空间很小，因为表里边都没有数据。不过别忘了这些 .ibd 文件是自扩展的，随着表中数据的增多，表空间对应的文件也逐渐增大。\n查看 InnoDB 的表空间类型：\n1 show variables like \u0026#39;innodb_file_per_table\u0026#39; 你能看到 innodb_file_per_table=ON, 这就意味着每张表都会单词保存一个 .ibd 文件。\n5.2 系统表空间 系统表空间的结构和独立表空间基本类似，只不过由于整个MySQL进程只有一个系统表空间，在系统表空间中会额外记录一些有关整个系统信息的页面，这部分是独立表空间中没有的。\nInnoDB数据字典\n删除这些数据并不是我们使用 INSERT 语句插入的用户数据，实际上是为了更好的管理我们这些用户数据而不得以引入的一些额外数据，这些数据页称为 元数据。InnoDB 存储引擎特意定义了一些列的 内部系统表 (internal system table) 来记录这些元数据：\n这些系统表也称为 数据字典，它们都是以 B+ 树的形式保存在系统表空间的某个页面中。其中 SYS_TABLES、SYS_COLUMNS、SYS_INDEXES、SYS_FIELDS 这四个表尤其重要，称之为基本系统表 (basic system tables) ，我们先看看这4个表的结构：\n注意：用户不能直接访问 InnoDB 的这些内部系统表，除非你直接去解析系统表空间对应文件系统上的文件。不过考虑到查看这些表的内容可能有助于大家分析问题，所以在系统数据库 information_schema 中提供了一些以 innodb_sys 开头的表:\n1 USE information_schema; 1 SHOW TABLES LIKE \u0026#39;innodb_sys%\u0026#39;; 在 information_scheme 数据库中的这些以 INNODB_SYS 开头的表并不是真正的内部系统表 (内部系统表就是我们上边以 SYS 开头的那些表)，而是在存储引擎启动时读取这些以 SYS 开头的系统表，然后填充到这些以 INNODB_SYS 开头的表中。以 INNODB_SYS 开头的表和以 SYS 开头的表中的字段并不完全一样，但仅供大家参考已经足矣。\n附录：数据页加载的三种方式 InnoDB从磁盘中读取数据 最小单位 是数据页。而你想得到的 id = xxx 的数据，就是这个数据页众多行中的一行。\n对于MySQL存放的数据，逻辑概念上我们称之为表，在磁盘等物理层面而言是按 数据页 形式进行存放的，当其加载到 MySQL 中我们称之为 缓存页。\n如果缓冲池没有该页数据，那么缓冲池有以下三种读取数据的方式，每种方式的读取速率是不同的：\n1. 内存读取\n如果该数据存在于内存中，基本上执行时间在 1ms 左右，效率还是很高的。\n2. 随机读取\n3. 顺序读取\n第8章_索引的创建与设计原则 1. 索引的声明与使用 1.1 索引的分类 MySQL的索引包括普通索引、唯一性索引、全文索引、单列索引、多列索引和空间索引等。\n从 功能逻辑 上说，索引主要有 4 种，分别是普通索引、唯一索引、主键索引、全文索引。\n按照 物理实现方式 ，索引可以分为 2 种：聚簇索引和非聚簇索引。\n按照 作用字段个数 进行划分，分成单列索引和联合索引。\n1. 普通索引\n2. 唯一性索引\n3. 主键索引\n4. 单列索引\n5. 多列 (组合、联合) 索引\n6. 全文检索\n7. 补充：空间索引\n**小结：不同的存储引擎支持的索引类型也不一样 **\nInnoDB ：支持 B-tree、Full-text 等索引，不支持 Hash 索引；\nMyISAM ： 支持 B-tree、Full-text 等索引，不支持 Hash 索引；\nMemory ：支持 B-tree、Hash 等 索引，不支持 Full-text 索引；\nNDB ：支持 Hash 索引，不支持 B-tree、Full-text 等索引；\nArchive ：不支 持 B-tree、Hash、Full-text 等索引；\n1.2 创建索引 MySQL支持多种方法在单个或多个列上创建索引：在创建表的定义语句 CREATE TABLE 中指定索引列，使用 ALTER TABLE 语句在存在的表上创建索引，或者使用 CREATE INDEX 语句在已存在的表上添加索引。\n1. 创建表的时候创建索引 使用CREATE TABLE创建表时，除了可以定义列的数据类型外，还可以定义主键约束、外键约束或者唯一性约束，而不论创建哪种约束，在定义约束的同时相当于在指定列上创建了一个索引。\n举例：\n1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE dept( dept_id INT PRIMARY KEY AUTO_INCREMENT, dept_name VARCHAR(20) ); CREATE TABLE emp( emp_id INT PRIMARY KEY AUTO_INCREMENT, emp_name VARCHAR(20) UNIQUE, dept_id INT, CONSTRAINT emp_dept_id_fk FOREIGN KEY(dept_id) REFERENCES dept(dept_id) ) 但是，如果显式创建表时创建索引的话，基本语法格式如下：\n1 2 3 CREATE TABLE table_name [col_name data_type] [UNIQUE | FULLTEXT | SPATIAL] [INDEX | KEY] [index_name] (col_name [length]) [ASC | DESC] UNIQUE 、 FULLTEXT 和 SPATIAL 为可选参数，分别表示唯一索引、全文索引和空间索引； INDEX 与 KEY 为同义词，两者的作用相同，用来指定创建索引； index_name 指定索引的名称，为可选参数，如果不指定，那么MySQL默认col_name为索引名； col_name 为需要创建索引的字段列，该列必须从数据表中定义的多个列中选择； length 为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度； ASC 或 DESC 指定升序或者降序的索引值存储。 1. 创建普通索引\n在book表中的year_publication字段上建立普通索引，SQL语句如下：\n1 2 3 4 5 6 7 8 9 CREATE TABLE book( book_id INT , book_name VARCHAR(100), `authors` VARCHAR(100), info VARCHAR(100) , `comment` VARCHAR(100), year_publication YEAR, INDEX(year_publication) ); 2. 创建唯一索引\n1 2 3 4 5 CREATE TABLE test1( id INT NOT NULL, name varchar(30) NOT NULL, UNIQUE INDEX uk_idx_id(id) ); 该语句执行完毕之后，使用SHOW CREATE TABLE查看表结构：\n1 SHOW INDEX FROM test1 \\G 3. 主键索引\n设定为主键后数据库会自动建立索引，innodb为聚簇索引，语法：\n随表一起建索引： 1 2 3 4 5 6 CREATE TABLE student ( id INT(10) UNSIGNED AUTO_INCREMENT , student_no VARCHAR(200), student_name VARCHAR(200), PRIMARY KEY(id) ); 删除主键索引： 1 2 ALTER TABLE student drop PRIMARY KEY; 修改主键索引：必须先删除掉(drop)原索引，再新建(add)索引 4. 创建单列索引\n引举:\n1 2 3 4 5 CREATE TABLE test2( id INT NOT NULL, name CHAR(50) NULL, INDEX single_idx_name(name(20)) ); 该语句执行完毕之后，使用SHOW CREATE TABLE查看表结构：\n1 SHOW INDEX FROM test2 \\G 5. 创建组合索引\n举例：创建表test3，在表中的id、name和age字段上建立组合索引，SQL语句如下：\n1 2 3 4 5 6 7 CREATE TABLE test3( id INT(11) NOT NULL, name CHAR(30) NOT NULL, age INT(11) NOT NULL, info VARCHAR(255), INDEX multi_idx(id,name,age) ); 该语句执行完毕之后，使用SHOW INDEX 查看：\n1 SHOW INDEX FROM test3 \\G 在test3表中，查询id和name字段，使用EXPLAIN语句查看索引的使用情况：\n1 EXPLAIN SELECT * FROM test3 WHERE id=1 AND name=\u0026#39;songhongkang\u0026#39; \\G 可以看到，查询id和name字段时，使用了名称为MultiIdx的索引，如果查询 (name, age) 组合或者单独查询name和age字段，会发现结果中possible_keys和key值为NULL, 并没有使用在t3表中创建的索引进行查询。虽然查询条件为(id,name)字段时是可以使用到索引的，因为创建索引时的字段是(id,name,age),也就是说，B+tree是先按照id递增，id相同时根据name字段，name又相同时根据age字段进行排序的，所以，从整个B+tree来看，查询条件为(name,age)是无法使用到索引的，因为在联合索引(id,name,age)构成的B+树整棵树来看name和age都是无序的，只能说局部有序\n6. 创建全文索引\nFULLTEXT全文索引可以用于全文检索，并且只为 CHAR 、VARCHAR 和 TEXT 列创建索引。索引总是对整个列进行，不支持局部 (前缀) 索引。\n举例1：创建表test4，在表中的info字段上建立全文索引，SQL语句如下：\n1 2 3 4 5 6 7 CREATE TABLE test4( id INT NOT NULL, name CHAR(30) NOT NULL, age INT NOT NULL, info VARCHAR(255), FULLTEXT INDEX futxt_idx_info(info) ) ENGINE=MyISAM; 在MySQL5.7及之后版本中可以不指定最后的ENGINE了，因为在此版本中InnoDB支持全文索引。\n语句执行完毕之后，使用SHOW CREATE TABLE查看表结构：\n1 SHOW INDEX FROM test4 \\G 由结果可以看到，info字段上已经成功建立了一个名为futxt_idx_info的FULLTEXT索引。\n举例2：\n1 2 3 4 5 6 CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY, title VARCHAR (200), body TEXT, FULLTEXT index (title, body) ) ENGINE = INNODB; 创建了一个给title和body字段添加全文索引的表。\n举例3：\n1 2 3 4 5 6 7 CREATE TABLE `papers` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `title` varchar(200) DEFAULT NULL, `content` text, PRIMARY KEY (`id`), FULLTEXT KEY `title` (`title`,`content`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8; 不同于like方式的的查询：\n1 SELECT * FROM papers WHERE content LIKE ‘%查询字符串%’; 全文索引用match+against方式查询：\n1 SELECT * FROM papers WHERE MATCH(title,content) AGAINST (‘查询字符串’); 明显的提高查询效率。\n注意点\n使用全文索引前，搞清楚版本支持情况； 全文索引比 like + % 快 N 倍，但是可能存在精度问题； 如果需要全文索引的是大量数据，建议先添加数据，再创建索引。 7. 创建空间索引\n空间索引创建中，要求空间类型的字段必须为 非空 。\n举例：创建表test5，在空间类型为GEOMETRY的字段上创建空间索引，SQL语句如下：\n1 2 3 4 CREATE TABLE test5( geo GEOMETRY NOT NULL, SPATIAL INDEX spa_idx_geo(geo) ) ENGINE=MyISAM; 该语句执行完毕之后，使用SHOW CREATE TABLE查看表结构：\n1 SHOW INDEX FROM test5 \\G 可以看到，test5表的geo字段上创建了名称为spa_idx_geo的空间索引。注意创建时指定空间类型字段值的非空约束，并且表的存储引擎为MyISAM。\n2. 在已经存在的表上创建索引 在已经存在的表中创建索引可以使用ALTER TABLE语句或者CREATE INDEX语句。\n1. 使用ALTER TABLE语句创建索引 ALTER TABLE语句创建索引的基本语法如下：\n1 2 ALTER TABLE table_name ADD [UNIQUE | FULLTEXT | SPATIAL] [INDEX | KEY] [index_name] (col_name[length],...) [ASC | DESC] 2. 使用CREATE INDEX创建索引 CREATE INDEX语句可以在已经存在的表上添加索引，在MySQL中， CREATE INDEX被映射到一个ALTER TABLE语句上，基本语法结构为：\n1 2 CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name ON table_name (col_name[length],...) [ASC | DESC] 1.3 删除索引 1. 使用ALTER TABLE删除索引 ALTER TABLE删除索引的基本语法格式如下：\n1 ALTER TABLE table_name DROP INDEX index_name; 2. 使用DROP INDEX语句删除索引 DROP INDEX删除索引的基本语法格式如下：\n1 DROP INDEX index_name ON table_name; 提示: 删除表中的列时，如果要删除的列为索引的组成部分，则该列也会从索引中删除。如果组成索引的所有列都被删除，则整个索引将被删除。\n2. MySQL8.0索引新特性 2.1 支持降序索引 降序索引以降序存储键值。虽然在语法上，从MySQL 4版本开始就已经支持降序索引的语法了，但实际上DESC定义是被忽略的，直到MySQL 8.x版本才开始真正支持降序索引 (仅限于InnoDBc存储引擎)。\nMySQL在8.0版本之前创建的仍然是升序索引，使用时进行反向扫描，这大大降低了数据库的效率。在某些场景下，降序索引意义重大。例如，如果一个查询，需要对多个列进行排序，且顺序要求不一致，那么使用降序索引将会避免数据库使用额外的文件排序操作，从而提高性能。\n举例：分别在MySQL 5.7版本和MySQL 8.0版本中创建数据表ts1，结果如下：\n1 CREATE TABLE ts1(a int,b int,index idx_a_b(a,b desc)); 在MySQL 5.7版本中查看数据表ts1的结构，结果如下:\n从结果可以看出，索引仍然是默认的升序\n在MySQL 8.0版本中查看数据表ts1的结构，结果如下：\n从结果可以看出，索引已经是降序了。下面继续测试降序索引在执行计划中的表现。\n分别在MySQL 5.7版本和MySQL 8.0版本的数据表ts1中插入800条随机数据，执行语句如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DELIMITER // CREATE PROCEDURE ts_insert() BEGIN DECLARE i INT DEFAULT 1; WHILE i \u0026lt; 800 DO insert into ts1 select rand()*80000, rand()*80000; SET i = i+1; END WHILE; commit; END // DELIMITER; # 调用 CALL ts_insert(); 在MySQL 5.7版本中查看数据表ts1的执行计划，结果如下:\n1 EXPLAIN SELECT * FROM ts1 ORDER BY a, b DESC LIMIT 5; 在MySQL 8.0版本中查看数据表 ts1 的执行计划。\n从结果可以看出，修改后MySQL 5.7 的执行计划要明显好于MySQL 8.0。\n2.2 隐藏索引 在MySQL 5.7版本及之前，只能通过显式的方式删除索引。此时，如果发展删除索引后出现错误，又只能通过显式创建索引的方式将删除的索引创建回来。如果数据表中的数据量非常大，或者数据表本身比较 大，这种操作就会消耗系统过多的资源，操作成本非常高。\n从MySQL 8.x开始支持 隐藏索引（invisible indexes） ，只需要将待删除的索引设置为隐藏索引，使 查询优化器不再使用这个索引（即使使用force index（强制使用索引），优化器也不会使用该索引）， 确认将索引设置为隐藏索引后系统不受任何响应，就可以彻底删除索引。 这种通过先将索引设置为隐藏索 引，再删除索引的方式就是软删除。\n同时，如果你想验证某个索引删除之后的 查询性能影响，就可以暂时先隐藏该索引。\n注意：\n主键不能被设置为隐藏索引。当表中没有显式主键时，表中第一个唯一非空索引会成为隐式主键，也不能设置为隐藏索引。\n索引默认是可见的，在使用CREATE TABLE, CREATE INDEX 或者 ALTER TABLE 等语句时可以通过 VISIBLE 或者 INVISIBLE 关键词设置索引的可见性。\n1. 创建表时直接创建\n在MySQL中创建隐藏索引通过SQL语句INVISIBLE来实现，其语法形式如下：\n1 2 3 4 5 6 7 CREATE TABLE tablename( propname1 type1[CONSTRAINT1], propname2 type2[CONSTRAINT2], …… propnamen typen, INDEX [indexname](propname1 [(length)]) INVISIBLE ); 上述语句比普通索引多了一个关键字INVISIBLE，用来标记索引为不可见索引。\n2. 在已经存在的表上创建\n可以为已经存在的表设置隐藏索引，其语法形式如下：\n1 2 CREATE INDEX indexname ON tablename(propname[(length)]) INVISIBLE; 3. 通过ALTER TABLE语句创建\n语法形式如下：\n1 2 ALTER TABLE tablename ADD INDEX indexname (propname [(length)]) INVISIBLE; 4. 切换索引可见状态\n已存在的索引可通过如下语句切换可见状态：\n1 2 ALTER TABLE tablename ALTER INDEX index_name INVISIBLE; #切换成隐藏索引 ALTER TABLE tablename ALTER INDEX index_name VISIBLE; #切换成非隐藏索引 如果将index_cname索引切换成可见状态，通过explain查看执行计划，发现优化器选择了index_cname索引。\n注意 当索引被隐藏时，它的内容仍然是和正常索引一样实时更新的。如果一个索引需要长期被隐藏，那么可以将其删除，因为索引的存在会影响插入、更新和删除的性能。\n通过设置隐藏索引的可见性可以查看索引对调优的帮助。\n5. 使隐藏索引对查询优化器可见\n在MySQL 8.x版本中，为索引提供了一种新的测试方式，可以通过查询优化器的一个开关 (use_invisible_indexes) 来打开某个设置，使隐藏索引对查询优化器可见。如果use_invisible_indexes 设置为off (默认)，优化器会忽略隐藏索引。如果设置为on，即使隐藏索引不可见，优化器在生成执行计 划时仍会考虑使用隐藏索引。\n（1）在MySQL命令行执行如下命令查看查询优化器的开关设置。\n1 mysql\u0026gt; select @@optimizer_switch \\G 在输出的结果信息中找到如下属性配置。\n1 use_invisible_indexes=off 此属性配置值为off，说明隐藏索引默认对查询优化器不可见。\n（2）使隐藏索引对查询优化器可见，需要在MySQL命令行执行如下命令：\n1 2 mysql\u0026gt; set session optimizer_switch=\u0026#34;use_invisible_indexes=on\u0026#34;; Query OK, 0 rows affected (0.00 sec) SQL语句执行成功，再次查看查询优化器的开关设置。\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; select @@optimizer_switch \\G *************************** 1. row *************************** @@optimizer_switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_ intersection=on,engine_condition_pushdown=on,index_condition_pushdown=on,mrr=on,mrr_co st_based=on,block_nested_loop=on,batched_key_access=off,materialization=on,semijoin=on ,loosescan=on,firstmatch=on,duplicateweedout=on,subquery_materialization_cost_based=on ,use_index_extensions=on,condition_fanout_filter=on,derived_merge=on,use_invisible_ind exes=on,skip_scan=on,hash_join=on 1 row in set (0.00 sec) 此时，在输出结果中可以看到如下属性配置。\n1 use_invisible_indexes=on use_invisible_indexes属性的值为on，说明此时隐藏索引对查询优化器可见。\n（3）使用EXPLAIN查看以字段invisible_column作为查询条件时的索引使用情况。\n1 explain select * from classes where cname = \u0026#39;高一2班\u0026#39;; 查询优化器会使用隐藏索引来查询数据。\n（4）如果需要使隐藏索引对查询优化器不可见，则只需要执行如下命令即可。\n1 2 mysql\u0026gt; set session optimizer_switch=\u0026#34;use_invisible_indexes=off\u0026#34;; Query OK, 0 rows affected (0.00 sec) 再次查看查询优化器的开关设置。\n1 mysql\u0026gt; select @@optimizer_switch \\G 此时，use_invisible_indexes属性的值已经被设置为“off”。\n3. 索引的设计原则 为了使索引的使用效率更高，在创建索引时，必须考虑在哪些字段上创建索引和创建什么类型的索引。**索引设计不合理或者缺少索引都会对数据库和应用程序的性能造成障碍。**高效的索引对于获得良好的性能非常重要。设计索引时，应该考虑相应准则。\n3.1 数据准备 第1步：创建数据库、创建表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 CREATE DATABASE atguigudb1; USE atguigudb1; #1.创建学生表和课程表 CREATE TABLE `student_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `student_id` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `course_id` INT NOT NULL , `class_id` INT(11) DEFAULT NULL, `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; CREATE TABLE `course` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `course_id` INT NOT NULL , `course_name` VARCHAR(40) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 第2步：创建模拟数据必需的存储函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #函数1：创建随机产生字符串函数 DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ\u0026#39;; DECLARE return_str VARCHAR(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; 1 2 3 4 5 6 7 8 9 #函数2：创建随机数函数 DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; 创建函数，假如报错：\n1 This function has none of DETERMINISTIC...... 由于开启过慢查询日志bin-log, 我们就必须为我们的function指定一个参数。\n主从复制，主机会将写操作记录在bin-log日志中。从机读取bin-log日志，执行语句来同步数据。如果使用函数来操作数据，会导致从机和主机操作时间不一致。所以，默认情况下，mysql不开启创建函数设置。\n查看mysql是否允许创建函数： 1 show variables like \u0026#39;log_bin_trust_function_creators\u0026#39;; 命令开启：允许创建函数设置： 1 set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 mysqld重启，上述参数又会消失。永久方法：\nwindows下：my.ini[mysqld]加上：\n1 log_bin_trust_function_creators=1 linux下：/etc/my.cnf下my.cnf[mysqld]加上：\n1 log_bin_trust_function_creators=1 第3步：创建插入模拟数据的存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 存储过程1：创建插入课程表存储过程 DELIMITER // CREATE PROCEDURE insert_course( max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO course (course_id, course_name ) VALUES (rand_num(10000,10100),rand_string(6)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 存储过程2：创建插入学生信息表存储过程 DELIMITER // CREATE PROCEDURE insert_stu( max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student_info (course_id, class_id ,student_id ,NAME ) VALUES (rand_num(10000,10100),rand_num(10000,10200),rand_num(1,200000),rand_string(6)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; 第4步：调用存储过程\n1 CALL insert_course(100); 1 CALL insert_stu(1000000); 3.2 哪些情况适合创建索引 1. 字段的数值有唯一性的限制 业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。（来源：Alibaba） 说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的。\n2. 频繁作为 WHERE 查询条件的字段 某个字段在SELECT语句的 WHERE 条件中经常被使用到，那么就需要给这个字段创建索引了。尤其是在数据量大的情况下，创建普通索引就可以大幅提升数据查询的效率。\n比如 student_info 数据表（含100万条数据），假设我们想要查询 student_id=123110 的用户信息。\n3. 经常 GROUP BY 和 ORDER BY 的列 索引就是让数据按照某种顺序进行存储或检索，因此当我们使用 GROUP BY 对数据进行分组查询，或者使用 ORDER BY 对数据进行排序的时候，就需要对分组或者排序的字段进行索引。如果待排序的列有多个，那么可以在这些列上建立组合索引。\n4. UPDATE、DELETE 的 WHERE 条件列 对数据按照某个条件进行查询后再进行 UPDATE 或 DELETE 的操作，如果对 WHERE 字段创建了索引，就能大幅提升效率。原理是因为我们需要先根据 WHERE 条件列检索出来这条记录，然后再对它进行更新或删除。如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段更新不需要对索引进行维护。所以，频繁地更新删除的字段，也要考虑索引是否建立的问题。\n5.DISTINCT 字段需要创建索引 有时候我们需要对某个字段进行去重，使用 DISTINCT，那么对这个字段创建索引，也会提升查询效率。\n比如，我们想要查询课程表中不同的 student_id 都有哪些，如果我们没有对 student_id 创建索引，执行 SQL 语句：\n1 SELECT DISTINCT(student_id) FROM `student_info`; 运行结果（600637 条记录，运行时间 0.683s ）\n如果我们对 student_id 创建索引，再执行 SQL 语句：\n1 SELECT DISTINCT(student_id) FROM `student_info`; 运行结果（600637 条记录，运行时间 0.010s ）\n你能看到 SQL 查询效率有了提升，同时显示出来的 student_id 还是按照递增的顺序进行展示的。这是因为索引会对数据按照某种顺序进行排序，所以在去重的时候也会快很多。\n6. 多表 JOIN 连接操作时，创建索引注意事项 首先， 连接表的数量尽量不要超过 3 张 ，因为每增加一张表就相当于增加了一次嵌套的循环，数量级增长会非常快，严重影响查询的效率。\n其次， 对 WHERE 条件创建索引 ，因为 WHERE 才是对数据条件的过滤。如果在数据量非常大的情况下， 没有 WHERE 条件过滤是非常可怕的。\n最后， 对用于连接的字段创建索引 ，并且该字段在多张表中的类型必须一致 。比如 course_id 在 student_info 表和 course 表中都为 int(11) 类型，而不能一个为 int 另一个为 varchar 类型。因为，类型不一致时，mysql会使用函数来做一个隐式地类型转换，一旦使用函数，索引就会失效，失效就使用不到索引了，变成全表扫描。\n举个例子，如果我们只对 student_id 创建索引，执行 SQL 语句：\n1 2 3 4 SELECT s.course_id, name, s.student_id, c.course_name FROM student_info s JOIN course c ON s.course_id = c.course_id WHERE name = \u0026#39;462eed7ac6e791292a79\u0026#39;; 运行结果（1 条数据，运行时间 0.189s ）\n这里我们对 name 创建索引，再执行上面的 SQL 语句，运行时间为 0.002s 。\n7. 使用列的类型小的创建索引 8. 使用字符串前缀创建索引 创建一张商户表，因为地址字段比较长，在地址字段上建立前缀索引\n1 2 create table shop(address varchar(120) not null); alter table shop add index(address(12)); 问题是，截取多少呢？截取得少了，达不到节省索引存储空间的目的；截取得多了，重复内容太多，字的散列度(选择性)会降低。怎么计算不同的长度的选择性呢？\n先看一下字段在全部数据中的选择度：\n1 select count(distinct address) / count(*) from shop 通过不同长度去计算，与全表的选择性对比：\n公式：\n1 count(distinct left(列名, 索引长度))/count(*) 例如：\n1 2 3 4 5 select count(distinct left(address,10)) / count(*) as sub10, -- 截取前10个字符的选择度 count(distinct left(address,15)) / count(*) as sub11, -- 截取前15个字符的选择度 count(distinct left(address,20)) / count(*) as sub12, -- 截取前20个字符的选择度 count(distinct left(address,25)) / count(*) as sub13 -- 截取前25个字符的选择度 from shop; 越接近于1越好，说明越有区分度\n引申另一个问题：索引列前缀对排序的影响\n如果使用了索引列前缀，比方说前边只把address列的 前12个字符 放到了二级索引中，下边这个查询可能就有点尴尬了：\n1 2 3 SELECT * FROM shop ORDER BY address LIMIT 12; 因为二级索引中不包含完整的address列信息，所以无法对前12个字符相同，后边的字符不同的记录进行排序，也就是使用索引列前缀的方式 无法支持使用索引排序 ，只能使用文件排序。\n拓展：Alibaba《Java开发手册》\n【 强制 】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\n说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90% 以上 ，可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定。\n9. 区分度高(散列性高)的列适合作为索引 列的基数 指的是某一列中不重复数据的个数，比方说某个列包含值 2, 5, 8, 2, 5, 8, 2, 5, 8，虽然有9条记录，但该列的基数却是3。也就是说**在记录行数一定的情况下，列的基数越大，该列中的值越分散；列的基数越小，该列中的值越集中。**这个列的基数指标非常重要，直接影响我们是否能有效的利用索引。最好为列的基数大的列简历索引，为基数太小的列的简历索引效果可能不好。\n可以使用公式select count(distinct a) / count(*) from t1 计算区分度，越接近1越好，一般超过33%就算比较高效的索引了。\n扩展：联合索引把区分度搞(散列性高)的列放在前面。\n10. 使用最频繁的列放到联合索引的左侧 这样也可以较少的建立一些索引。同时，由于\u0026quot;最左前缀原则\u0026quot;，可以增加联合索引的使用率。不满足最左前缀原则，可能会导致无法使用到联合索引，只能文件排序\n11. 在多个字段都要创建索引的情况下，联合索引优于单值索引 3.3 限制索引的数目 3.4 哪些情况不适合创建索引 1. 在where中使用不到的字段，不要设置索引 WHERE条件 (包括 GROUP BY、ORDER BY) 里用不到的字段不需要创建索引，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的。因为索引在update、insert、delete数据时，数据库会维护索引的结构，造成性能损耗。\n举个例子：\n1 2 3 SELECT course_id, student_id, create_time FROM student_info WHERE student_id = 41251; 因为我们是按照 student_id 来进行检索的，所以不需要对其他字段创建索引，即使这些字段出现在SELECT字段中。\n2. 数据量小的表最好不要使用索引 如果表记录太少，比如少于1000个，那么是不需要创建索引的。表记录太少，是否创建索引 对查询效率的影响并不大。甚至说，查询花费的时间可能比遍历索引的时间还要短，索引可能不会产生优化效果。\n举例：创建表1：\n1 2 3 4 CREATE TABLE t_without_index( a INT PRIMARY KEY AUTO_INCREMENT, b INT ); 提供存储过程1：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #创建存储过程 DELIMITER // CREATE PROCEDURE t_wout_insert() BEGIN DECLARE i INT DEFAULT 1; WHILE i \u0026lt;= 900 DO INSERT INTO t_without_index(b) SELECT RAND()*10000; SET i = i + 1; END WHILE; COMMIT; END // DELIMITER ; #调用 CALL t_wout_insert() 创建表2：\n1 2 3 4 5 CREATE TABLE t_with_index( a INT PRIMARY KEY AUTO_INCREMENT, b INT, INDEX idx_b(b) ); 创建存储过程2：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #创建存储过程 DELIMITER // CREATE PROCEDURE t_with_insert() BEGIN DECLARE i INT DEFAULT 1; WHILE i \u0026lt;= 900 DO INSERT INTO t_with_index(b) SELECT RAND()*10000; SET i = i + 1; END WHILE; COMMIT; END // DELIMITER ; #调用 CALL t_with_insert(); 查询对比：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mysql\u0026gt; select * from t_without_index where b = 9879; +------+------+ | a | b | +------+------+ | 1242 | 9879 | +------+------+ 1 row in set (0.00 sec) mysql\u0026gt; select * from t_with_index where b = 9879; +-----+------+ | a | b | +-----+------+ | 112 | 9879 | +-----+------+ 1 row in set (0.00 sec) 你能看到运行结果相同，但是在数据量不大的情况下，索引就发挥不出作用了。\n结论：在数据表中的数据行数比较少的情况下，比如不到 1000 行，是不需要创建索引的。\n3. 有大量重复数据的列上不要建立索引 在条件表达式中经常用到的不同值较多的列上建立索引，但字段中如果有大量重复数据，也不用创建索引。比如在学生表的\u0026quot;性别\u0026quot;字段上只有“男”与“女”两个不同值，因此无须建立索引。如果建立索引，不但不会提高查询效率，反而会严重降低数据更新速度。\n举例1：要在 100 万行数据中查找其中的 50 万行（比如性别为男的数据），一旦创建了索引，你需要先 访问 50 万次索引，然后再访问 50 万次数据表，这样加起来的开销比不使用索引可能还要大。\n举例2：假设有一个学生表，学生总数为 100 万人，男性只有 10 个人，也就是占总人口的 10 万分之 1。\n学生表 student_gender 结构如下。其中数据表中的 student_gender 字段取值为 0 或 1，0 代表女性，1 代表男性。\n1 2 3 4 5 6 CREATE TABLE student_gender( student_id INT(11) NOT NULL, student_name VARCHAR(50) NOT NULL, student_gender TINYINT(1) NOT NULL, PRIMARY KEY(student_id) )ENGINE = INNODB; 如果我们要筛选出这个学生表中的男性，可以使用：\n1 SELECT * FROM student_gender WHERE student_gender = 1; 结论：当数据重复度大，比如 高于 10% 的时候，也不需要对这个字段使用索引。\n4. 避免对经常更新的表创建过多的索引 第一层含义：频繁更新的字段不一定要创建索引。因为更新数据的时候，也需要更新索引，如果索引太多，在更新索引的时候也会造成负担，从而影响效率。\n第二层含义：避免对经常更新的表创建过多的索引，并且索引中的列尽可能少。此时，虽然提高了查询速度，同时却降低更新表的速度。\n5. 不建议用无序的值作为索引 例如身份证、UUID(在索引比较时需要转为ASCII，并且插入时可能造成页分裂)、MD5、HASH、无序长字 符串等。\n6. 删除不再使用或者很少使用的索引 表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。\n7. 不要定义夯余或重复的索引 ① 冗余索引\n举例：建表语句如下\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE person_info( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name(10), birthday, phone_number), KEY idx_name (name(10)) ); 我们知道，通过 idx_name_birthday_phone_number 索引就可以对 name 列进行快速搜索，再创建一 个专门针对 name 列的索引就算是一个 冗余索引 ，维护这个索引只会增加维护的成本，并不会对搜索有 什么好处。\n② 重复索引\n另一种情况，我们可能会对某个列 重复建立索引 ，比方说这样：\n1 2 3 4 5 6 CREATE TABLE repeat_index_demo ( col1 INT PRIMARY KEY, col2 INT, UNIQUE uk_idx_c1 (col1), INDEX idx_c1 (col1) ); 我们看到，col1 既是主键、又给它定义为一个唯一索引，还给它定义了一个普通索引，可是主键本身会生成聚簇索引，所以定义的唯一索引和普通索引是重复的，这种情况要避免。\n第09章_性能分析工具的使用 在数据库调优中，我们的目标是 响应时间更快, 吞吐量更大 。利用宏观的监控工具和微观的日志分析可以帮我们快速找到调优的思路和方式。\n1. 数据库服务器的优化步骤 当我们遇到数据库调优问题的时候，该如何思考呢？这里把思考的流程整理成下面这张图。\n整个流程划分成了 观察（Show status） 和 行动（Action） 两个部分。字母 S 的部分代表观察（会使 用相应的分析工具），字母 A 代表的部分是行动（对应分析可以采取的行动）。\n我们可以通过观察了解数据库整体的运行状态，通过性能分析工具可以让我们了解执行慢的SQL都有哪些，查看具体的SQL执行计划，甚至是SQL执行中的每一步的成本代价，这样才能定位问题所在，找到了问题，再采取相应的行动。\n详细解释一下这张图：\n2. 查看系统性能参数 在MySQL中，可以使用 SHOW STATUS 语句查询一些MySQL数据库服务器的性能参数、执行频率。\nSHOW STATUS语句语法如下：\n1 SHOW [GLOBAL|SESSION] STATUS LIKE \u0026#39;参数\u0026#39;; 一些常用的性能参数如下：\nConnections：连接MySQL服务器的次数。 Uptime：MySQL服务器的上线时间。 Slow_queries：慢查询的次数。 Innodb_rows_read：Select查询返回的行数 Innodb_rows_inserted：执行INSERT操作插入的行数 Innodb_rows_updated：执行UPDATE操作更新的 行数 Innodb_rows_deleted：执行DELETE操作删除的行数 Com_select：查询操作的次数。 Com_insert：插入操作的次数。对于批量插入的 INSERT 操作，只累加一次。 Com_update：更新操作的次数。 Com_delete：删除操作的次数。 若查询MySQL服务器的连接次数，则可以执行如下语句:\n1 SHOW STATUS LIKE \u0026#39;Connections\u0026#39;; 若查询服务器工作时间，则可以执行如下语句:\n1 SHOW STATUS LIKE \u0026#39;Uptime\u0026#39;; 若查询MySQL服务器的慢查询次数，则可以执行如下语句:\n1 SHOW STATUS LIKE \u0026#39;Slow_queries\u0026#39;; 慢查询次数参数可以结合慢查询日志找出慢查询语句，然后针对慢查询语句进行表结构优化或者查询语句优化。\n再比如，如下的指令可以查看相关的指令情况：\n1 SHOW STATUS LIKE \u0026#39;Innodb_rows_%\u0026#39;; 3. 统计SQL的查询成本: last_query_cost 一条SQL查询语句在执行前需要查询执行计划，如果存在多种执行计划的话，MySQL会计算每个执行计划所需要的成本，从中选择成本最小的一个作为最终执行的执行计划。\n如果我们想要查看某条SQL语句的查询成本，可以在执行完这条SQL语句之后，通过查看当前会话中的last_query_cost变量值来得到当前查询的成本。它通常也是我们评价一个查询的执行效率的一个常用指标。这个查询成本对应的是SQL 语句所需要读取的读页的数量。\n我们依然使用第8章的 student_info 表为例：\n1 2 3 4 5 6 7 8 9 CREATE TABLE `student_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `student_id` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `course_id` INT NOT NULL , `class_id` INT(11) DEFAULT NULL, `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 如果我们想要查询 id=900001 的记录，然后看下查询成本，我们可以直接在聚簇索引上进行查找：\n1 SELECT student_id, class_id, NAME, create_time FROM student_info WHERE id = 900001; 运行结果（1 条记录，运行时间为 0.042s ）\n然后再看下查询优化器的成本，实际上我们只需要检索一个页即可：\n1 2 3 4 5 6 mysql\u0026gt; SHOW STATUS LIKE \u0026#39;last_query_cost\u0026#39;; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | Last_query_cost | 1.000000 | +-----------------+----------+ 如果我们想要查询 id 在 900001 到 9000100 之间的学生记录呢？\n1 SELECT student_id, class_id, NAME, create_time FROM student_info WHERE id BETWEEN 900001 AND 900100; 运行结果（100 条记录，运行时间为 0.046s ）：\n然后再看下查询优化器的成本，这时我们大概需要进行 20 个页的查询。\n1 2 3 4 5 6 mysql\u0026gt; SHOW STATUS LIKE \u0026#39;last_query_cost\u0026#39;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | Last_query_cost | 21.134453 | +-----------------+-----------+ 你能看到页的数量是刚才的 20 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间 。\n**使用场景：**它对于比较开销是非常有用的，特别是我们有好几种查询方式可选的时候。\nSQL查询时一个动态的过程，从页加载的角度来看，我们可以得到以下两点结论：\n位置决定效率。如果页就在数据库 缓冲池 中，那么效率是最高的，否则还需要从 内存 或者 磁盘 中进行读取，当然针对单个页的读取来说，如果页存在于内存中，会比在磁盘中读取效率高很多。 批量决定效率。如果我们从磁盘中对单一页进行随机读，那么效率是很低的(差不多10ms)，而采用顺序读取的方式，批量对页进行读取，平均一页的读取效率就会提升很多，甚至要快于单个页面在内存中的随机读取。 所以说，遇到I/O并不用担心，方法找对了，效率还是很高的。我们首先要考虑数据存放的位置，如果是进程使用的数据就要尽量放到缓冲池中，其次我们可以充分利用磁盘的吞吐能力，一次性批量读取数据，这样单个页的读取效率也就得到了提升。\n4. 定位执行慢的 SQL：慢查询日志 4.1 开启慢查询日志参数 1. 开启 slow_query_log\n在使用前，我们需要先查下慢查询是否已经开启，使用下面这条命令即可：\n1 mysql \u0026gt; show variables like \u0026#39;%slow_query_log\u0026#39;; 我们可以看到 slow_query_log=OFF，我们可以把慢查询日志打开，注意设置变量值的时候需要使用 global，否则会报错：\n1 mysql \u0026gt; set global slow_query_log=\u0026#39;ON\u0026#39;; 然后我们再来查看下慢查询日志是否开启，以及慢查询日志文件的位置：\n你能看到这时慢查询分析已经开启，同时文件保存在 /var/lib/mysql/atguigu02-slow.log 文件 中。\n2. 修改 long_query_time 阈值\n接下来我们来看下慢查询的时间阈值设置，使用如下命令：\n1 mysql \u0026gt; show variables like \u0026#39;%long_query_time%\u0026#39;; 这里如果我们想把时间缩短，比如设置为 1 秒，可以这样设置：\n1 2 3 4 5 6 7 #测试发现：设置global的方式对当前session的long_query_time失效。对新连接的客户端有效。所以可以一并 执行下述语句 mysql \u0026gt; set global long_query_time = 1; mysql\u0026gt; show global variables like \u0026#39;%long_query_time%\u0026#39;; mysql\u0026gt; set long_query_time=1; mysql\u0026gt; show variables like \u0026#39;%long_query_time%\u0026#39;; 补充：配置文件中一并设置参数\n如下的方式相较于前面的命令行方式，可以看做是永久设置的方式。\n修改 my.cnf 文件，[mysqld] 下增加或修改参数 long_query_time、slow_query_log 和 slow_query_log_file 后，然后重启 MySQL 服务器。\n1 2 3 4 5 [mysqld] slow_query_log=ON # 开启慢查询日志开关 slow_query_log_file=/var/lib/mysql/atguigu-low.log # 慢查询日志的目录和文件名信息 long_query_time=3 # 设置慢查询的阈值为3秒，超出此设定值的SQL即被记录到慢查询日志 log_output=FILE 如果不指定存储路径，慢查询日志默认存储到MySQL数据库的数据文件夹下。如果不指定文件名，默认文件名为hostname_slow.log。\n4.2 查看慢查询数目 查询当前系统中有多少条慢查询记录\n1 SHOW GLOBAL STATUS LIKE \u0026#39;%Slow_queries%\u0026#39;; 4.3 案例演示 步骤1. 建表\n1 2 3 4 5 6 7 8 CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 步骤2：设置参数 log_bin_trust_function_creators\n创建函数，假如报错：\n1 This function has none of DETERMINISTIC...... 命令开启：允许创建函数设置： 1 set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 步骤3：创建函数\n随机产生字符串：（同上一章）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ\u0026#39;; DECLARE return_str VARCHAR(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; # 测试 SELECT rand_string(10); 产生随机数值：（同上一章）\n1 2 3 4 5 6 7 8 9 10 11 DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; #测试： SELECT rand_num(10,100); 步骤4：创建存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 DELIMITER // CREATE PROCEDURE insert_stu1( START INT , max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, NAME ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(10,100),rand_num(10,1000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; 步骤5：调用存储过程\n1 2 3 #调用刚刚写好的函数, 4000000条记录,从100001号开始 CALL insert_stu1(100001,4000000); 4.4 测试及分析 1. 测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; SELECT * FROM student WHERE stuno = 3455655; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 3523633 | 3455655 | oQmLUr | 19 | 39 | +---------+---------+--------+------+---------+ 1 row in set (2.09 sec) mysql\u0026gt; SELECT * FROM student WHERE name = \u0026#39;oQmLUr\u0026#39;; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 1154002 | 1243200 | OQMlUR | 266 | 28 | | 1405708 | 1437740 | OQMlUR | 245 | 439 | | 1748070 | 1680092 | OQMlUR | 240 | 414 | | 2119892 | 2051914 | oQmLUr | 17 | 32 | | 2893154 | 2825176 | OQMlUR | 245 | 435 | | 3523633 | 3455655 | oQmLUr | 19 | 39 | +---------+---------+--------+------+---------+ 6 rows in set (2.39 sec) 从上面的结果可以看出来，查询学生编号为“3455655”的学生信息花费时间为2.09秒。查询学生姓名为 “oQmLUr”的学生信息花费时间为2.39秒。已经达到了秒的数量级，说明目前查询效率是比较低的，下面的小节我们分析一下原因。\n2. 分析\n1 show status like \u0026#39;slow_queries\u0026#39;; 4.5 慢查询日志分析工具：mysqldumpslow 在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具 mysqldumpslow 。\n查看 mysqldumpslow 的帮助信息\n1 mysqldumpslow --help mysqldumpslow 命令的具体参数如下：\n-a: 不将数字抽象成N，字符串抽象成S -s: 是表示按照何种方式排序： c: 访问次数 l: 锁定时间 r: 返回记录 t: 查询时间 al:平均锁定时间 ar:平均返回记录数 at:平均查询时间 （默认方式） ac:平均查询次数 -t: 即为返回前面多少条的数据； -g: 后边搭配一个正则匹配模式，大小写不敏感的； 举例：我们想要按照查询时间排序，查看前五条 SQL 语句，这样写即可：\n1 mysqldumpslow -s t -t 5 /var/lib/mysql/atguigu01-slow.log 1 2 3 4 5 6 7 8 9 10 [root@bogon ~] mysqldumpslow -s t -t 5 /var/lib/mysql/atguigu01-slow.log Reading mysql slow query log from /var/lib/mysql/atguigu01-slow.log Count: 1 Time=2.39s (2s) Lock=0.00s (0s) Rows=13.0 (13), root[root]@localhost SELECT * FROM student WHERE name = \u0026#39;S\u0026#39; Count: 1 Time=2.09s (2s) Lock=0.00s (0s) Rows=2.0 (2), root[root]@localhost SELECT * FROM student WHERE stuno = N Died at /usr/bin/mysqldumpslow line 162, \u0026lt;\u0026gt; chunk 2. 工作常用参考：\n1 2 3 4 5 6 7 8 9 10 11 #得到返回记录集最多的10个SQL mysqldumpslow -s r -t 10 /var/lib/mysql/atguigu-slow.log #得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /var/lib/mysql/atguigu-slow.log #得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g \u0026#34;left join\u0026#34; /var/lib/mysql/atguigu-slow.log #另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现爆屏情况 mysqldumpslow -s r -t 10 /var/lib/mysql/atguigu-slow.log | more 4.6 关闭慢查询日志 MySQL服务器停止慢查询日志功能有两种方法：\n方式1：永久性方式\n1 2 [mysqld] slow_query_log=OFF 或者，把slow_query_log一项注释掉 或 删除\n1 2 [mysqld] #slow_query_log =OFF 重启MySQL服务，执行如下语句查询慢日志功能。\n1 2 SHOW VARIABLES LIKE \u0026#39;%slow%\u0026#39;; #查询慢查询日志所在目录 SHOW VARIABLES LIKE \u0026#39;%long_query_time%\u0026#39;; #查询超时时长 方式2：临时性方式\n使用SET语句来设置。\n（1）停止MySQL慢查询日志功能，具体SQL语句如下。\n1 SET GLOBAL slow_query_log=off; （2）重启MySQL服务，使用SHOW语句查询慢查询日志功能信息，具体SQL语句如下。\n1 2 3 SHOW VARIABLES LIKE \u0026#39;%slow%\u0026#39;; #以及 SHOW VARIABLES LIKE \u0026#39;%long_query_time%\u0026#39;; 4.7 删除慢查询日志 使用SHOW语句显示慢查询日志信息，具体SQL语句如下。\n1 SHOW VARIABLES LIKE `slow_query_log%`; 从执行结果可以看出，慢查询日志的目录默认为MySQL的数据目录，在该目录下手动删除慢查询日志文件即可。\n使用命令 mysqladmin flush-logs 来重新生成查询日志文件，具体命令如下，执行完毕会在数据目录下重新生成慢查询日志文件。\n1 mysqladmin -uroot -p flush-logs slow 提示\n慢查询日志都是使用mysqladmin flush-logs命令来删除重建的。使用时一定要注意，一旦执行了这个命令，慢查询日志都只存在新的日志文件中，如果需要旧的查询日志，就必须事先备份。\n5. 查看 SQL 执行成本：SHOW PROFILE show profile 在《逻辑架构》章节中讲过，这里作为复习。\nshow profile 是 MySQL 提供的可以用来分析当前会话中 SQL 都做了什么、执行的资源消耗工具的情况，可用于 sql 调优的测量。默认情况下处于关闭状态，并保存最近15次的运行结果。\nshow profile 命令可以显示一条sql的每个过程里花费的时间以及各种资源开销，如果是 executing 时间过长，表示sql执行时间太长，需要使用explain来分析sql效率慢的原因；如果是其他问题，可能是网络延时，锁等情况\n我们可以在会话级别开启这个功能。\n1 mysql \u0026gt; show variables like \u0026#39;profiling\u0026#39;; 通过设置 profiling=\u0026lsquo;ON\u0026rsquo; 来开启 show profile:\n1 mysql \u0026gt; set profiling = \u0026#39;ON\u0026#39;; 然后执行相关的查询语句。接着看下当前会话都有哪些 profiles，使用下面这条命令：\n1 mysql \u0026gt; show profiles; 你能看到当前会话一共有 2 个查询。如果我们想要查看最近一次查询的开销，可以使用：\n1 mysql \u0026gt; show profile; 1 mysql\u0026gt; show profile cpu,block io for query 2 **show profile的常用查询参数： **\n① ALL：显示所有的开销信息。\n② BLOCK IO：显示块IO开销。\n③ CONTEXT SWITCHES：上下文切换开销。\n④ CPU：显示CPU开销信息。\n⑤ IPC：显示发送和接收开销信息。\n⑥ MEMORY：显示内存开销信息。\n⑦ PAGE FAULTS：显示页面错误开销信息。\n⑧ SOURCE：显示和Source_function，Source_file， Source_line相关的开销信息。\n⑨ SWAPS：显示交换次数开销信息。\n日常开发需注意的结论：\n① converting HEAP to MyISAM: 查询结果太大，内存不够，数据往磁盘上搬了。\n② Creating tmp table：创建临时表。先拷贝数据到临时表，用完后再删除临时表。\n③ Copying to tmp table on disk：把内存中临时表复制到磁盘上，警惕！\n④ locked。\n如果在show profile诊断结果中出现了以上4条结果中的任何一条，则sql语句需要优化。\n注意：不过SHOW PROFILE命令将被启用，我们可以从 information_schema 中的 profiling 数据表进行查看。\n6. 分析查询语句：EXPLAIN 先了解在join连接时哪个表是驱动表，哪个表是被驱动表：\n当使用join时，mysql会选择数据量比较小的表作为驱动表，大表作为被驱动表\n驱动表的含义:\nMySQL 表关联的算法是 Nest Loop Join，是通过驱动表的结果集作为循环基础数据，然后一条一条地通过该结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果。如果还有第三个参与Join，则再通过前两个表的Join结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此往复。\n例如：\n小表驱动大表：for(140条){for(20万条){}}\n大表驱动小表：for(20万条){for(140条){}}\n大表驱动小表，要通过20万次的连接\n小表驱动小表，只需要通过140多次的连接就可以了.\n所以也可以得出结论: 如果A表，B表数据量差不多大的时候，那么选择谁作为驱动表也是无所谓了; 以小表作为驱动表，大表作为被驱动表的方式查询速度更快\n6.1 概述 1. 能做什么？\n表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 2. 官网介绍\nhttps://dev.mysql.com/doc/refman/5.7/en/explain-output.html\nhttps://dev.mysql.com/doc/refman/8.0/en/explain-output.html\n3. 版本情况\nMySQL 5.6.3以前只能 EXPLAIN SELECT ；MYSQL 5.6.3以后就可以 EXPLAIN SELECT，UPDATE， DELETE 在5.7以前的版本中，想要显示 partitions 需要使用 explain partitions 命令；想要显示 filtered 需要使用 explain extended 命令。在5.7版本后，默认explain直接显示partitions和 filtered中的信息。 6.2 基本语法 EXPLAIN 或 DESCRIBE语句的语法形式如下：\n1 2 3 EXPLAIN SELECT select_options 或者 DESCRIBE SELECT select_options 如果我们想看看某个查询的执行计划的话，可以在具体的查询语句前边加一个 EXPLAIN ，就像这样：\n1 mysql\u0026gt; EXPLAIN SELECT 1; EXPLAIN 语句输出的各个列的作用如下：\n在这里把它们都列出来知识为了描述一个轮廓，让大家有一个大致的印象。\n6.3 数据准备 1. 建表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE s1 ( id INT AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), INDEX idx_key1 (key1), UNIQUE INDEX idx_key2 (key2), INDEX idx_key3 (key3), INDEX idx_key_part(key_part1, key_part2, key_part3) ) ENGINE=INNODB CHARSET=utf8; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE s2 ( id INT AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), INDEX idx_key1 (key1), UNIQUE INDEX idx_key2 (key2), INDEX idx_key3 (key3), INDEX idx_key_part(key_part1, key_part2, key_part3) ) ENGINE=INNODB CHARSET=utf8; 2. 设置参数 log_bin_trust_function_creators\n创建函数，假如报错，需开启如下命令：允许创建函数设置：\n1 set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 3. 创建函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DELIMITER // CREATE FUNCTION rand_string1(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ\u0026#39;; DECLARE return_str VARCHAR(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; 4. 创建存储过程\n创建往s1表中插入数据的存储过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 DELIMITER // CREATE PROCEDURE insert_s1 (IN min_num INT (10),IN max_num INT (10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO s1 VALUES( (min_num + i), rand_string1(6), (min_num + 30 * i + 5), rand_string1(6), rand_string1(10), rand_string1(5), rand_string1(10), rand_string1(10)); UNTIL i = max_num END REPEAT; COMMIT; END // DELIMITER ; 创建往s2表中插入数据的存储过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 DELIMITER // CREATE PROCEDURE insert_s2 (IN min_num INT (10),IN max_num INT (10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO s2 VALUES( (min_num + i), rand_string1(6), (min_num + 30 * i + 5), rand_string1(6), rand_string1(10), rand_string1(5), rand_string1(10), rand_string1(10)); UNTIL i = max_num END REPEAT; COMMIT; END // DELIMITER ; 5. 调用存储过程\ns1表数据的添加：加入1万条记录：\n1 CALL insert_s1(10001,10000); s2表数据的添加：加入1万条记录：\n1 CALL insert_s2(10001,10000); 6.4 EXPLAIN各列作用 为了让大家有比较好的体验，我们调整了下 EXPLAIN 输出列的顺序。\n1. table 不论我们的查询语句有多复杂，里边儿包含了多少个表 ，到最后也是需要对每个表进行单表访问的，所以MySQL规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名（有时不是真实的表名字，可能是简称）。\n1 mysql \u0026gt; EXPLAIN SELECT * FROM s1; 这个查询语句只涉及对s1表的单表查询，所以 EXPLAIN 输出中只有一条记录，其中的table列的值为s1，表明这条记录是用来说明对s1表的单表访问方法的。\n边我们看一个连接查询的执行计划\n1 mysql \u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2; 可以看出这个连接查询的执行计划中有两条记录，这两条记录的table列分别是s1和s2，这两条记录用来分别说明对s1表和s2表的访问方法是什么。\n2. id 我们写的查询语句一般都以 SELECT 关键字开头，比较简单的查询语句里只有一个 SELECT 关键字，比如下边这个查询语句：\n1 SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;; 稍微复杂一点的连接查询中也只有一个 SELECT 关键字，比如：\n1 2 3 SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = \u0026#39;a\u0026#39;; 但是下边两种情况下在一条查询语句中会出现多个SELECT关键字：\n1 mysql \u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;; 对于连接查询来说，一个SELECT关键字后边的FROM字句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的，比如：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2; 可以看到，上述连接查询中参与连接的s1和s2表分别对应一条记录，但是这两条记录对应的id都是1。这里需要大家记住的是，在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后面的表表示被驱动表。所以从上边的EXPLAIN输出中我们可以看到，查询优化器准备让s1表作为驱动表，让s2表作为被驱动表来执行查询。\n对于包含子查询的查询语句来说，就可能涉及多个SELECT关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT关键字都会对应一个唯一的id值，比如这样：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = \u0026#39;a\u0026#39;; 1 2 # 查询优化器可能对涉及子查询的查询语句进行重写，转变为多表查询的操作。 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key2 FROM s2 WHERE common_field = \u0026#39;a\u0026#39;); 可以看到，虽然我们的查询语句是一个子查询，但是执行计划中s1和s2表对应的记录的id值全部是1，这就表明查询优化器将子查询转换为了连接查询。\n对于包含UNION子句的查询语句来说，每个SELECT关键字对应一个id值也是没错的，不过还是有点儿特别的东西，比方说下边的查询：\n1 2 # Union去重 mysql\u0026gt; EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2; 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 UNION ALL SELECT * FROM s2; 小结:\nid如果相同，可以认为是一组，从上往下顺序执行 在所有组中，id值越大，优先级越高，越先执行 关注点：id号每个号码，表示一趟独立的查询, 一个sql的查询趟数越少越好 3. select_type 具体分析如下：\nSIMPLE\n查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，比方说下边这个单表查询select_type的值就是SIMPLE:\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1; 当然，连接查询也算是 SIMPLE 类型，比如：\r1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2; PRIMARY\n对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type的值就是PRIMARY,比方说：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2; 从结果中可以看到，最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type的值就是PRIMARY。\nUNION\n对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询意外，其余的小查询的select_type值就是UNION，可以对比上一个例子的效果。\nUNION RESULT\nMySQL 选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT, 例子上边有。\nSUBQUERY\n如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY，比如下边这个查询：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = \u0026#39;a\u0026#39;; DEPENDENT SUBQUERY\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = \u0026#39;a\u0026#39;; DEPENDENT UNION\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = \u0026#39;a\u0026#39; UNION SELECT key1 FROM s1 WHERE key1 = \u0026#39;b\u0026#39;); DERIVED\n1 mysql\u0026gt; EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c \u0026gt; 1; 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED, 说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的table列显示的是derived2，表示该查询时针对将派生表物化之后的表进行查询的。\nMATERIALIZED\n当查询优化器在执行包含子查询的语句时，选择将子查询物化之后的外层查询进行连接查询时，该子查询对应的select_type属性就是DERIVED，比如下边这个查询：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2); UNCACHEABLE SUBQUERY\n不常用，就不多说了。\nUNCACHEABLE UNION\n不常用，就不多说了。\n4. partitions (可略) 代表分区表中的命中情况，非分区表，该项为NULL。一般情况下我们的查询语句的执行计划的partitions列的值为NULL。 https://dev.mysql.com/doc/refman/5.7/en/alter-table-partition-operations.html 如果想详细了解，可以如下方式测试。创建分区表： 1 2 3 4 5 6 7 8 -- 创建分区表， -- 按照id分区，id\u0026lt;100 p0分区，其他p1分区 CREATE TABLE user_partitions (id INT auto_increment, NAME VARCHAR(12),PRIMARY KEY(id)) PARTITION BY RANGE(id)( PARTITION p0 VALUES less than(100), PARTITION p1 VALUES less than MAXVALUE ); 1 DESC SELECT * FROM user_partitions WHERE id\u0026gt;200; 查询id大于200（200\u0026gt;100，p1分区）的记录，查看执行计划，partitions是p1，符合我们的分区规则\n5. type ☆ 执行计划的一条记录就代表着MySQL对某个表的 执行查询时的访问方法 , 又称“访问类型”，其中的 type 列就表明了这个访问方法是啥，是较为重要的一个指标。比如，看到type列的值是ref，表明MySQL即将使用ref访问方法来执行对s1表的查询。\n完整的访问方法如下： system ， const ， eq_ref ， ref ， fulltext ， ref_or_null ， index_merge ， unique_subquery ， index_subquery ， range ， index ， ALL 。\n我们详细解释一下：\nsystem\n当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是system。比方说我们新建一个MyISAM表，并为其插入一条记录：\n1 2 3 4 5 mysql\u0026gt; CREATE TABLE t(i int) Engine=MyISAM; Query OK, 0 rows affected (0.05 sec) mysql\u0026gt; INSERT INTO t VALUES(1); Query OK, 1 row affected (0.01 sec) 然后我们看一下查询这个表的执行计划：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM t; 可以看到type列的值就是system了，\n测试，可以把表改成使用InnoDB存储引擎，试试看执行计划的type列是什么。ALL\nconst\n当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const, 比如：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE id = 10005; eq_ref\n在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref，比方说：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id; 从执行计划的结果中可以看出，MySQL打算将s2作为驱动表，s1作为被驱动表，重点关注s1的访问方法是 eq_ref ，表明在访问s1表的时候可以 通过主键的等值匹配 来进行访问。\nref\n当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref，比方说下边这个查询：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;; fulltext\n全文索引\nref_or_null\n当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null，比如说：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; OR key1 IS NULL; index_merge\n一般情况下对于某个表的查询只能使用到一个索引，但单表访问方法时在某些场景下可以使用Interseation、union、Sort-Union这三种索引合并的方式来执行查询。我们看一下执行计划中是怎么体现MySQL使用索引合并的方式来对某个表执行查询的：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; OR key3 = \u0026#39;a\u0026#39;; 从执行计划的 type 列的值是 index_merge 就可以看出，MySQL 打算使用索引合并的方式来执行 对 s1 表的查询。\nunique_subquery\n类似于两表连接中被驱动表的eq_ref访问方法，unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery，比如下边的这个查询语句：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 = s2.key1) OR key3 = \u0026#39;a\u0026#39;; index_subquery\nindex_subquery 与 unique_subquery 类似，只不过访问子查询中的表时使用的是普通的索引，比如这样：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE common_field IN (SELECT key3 FROM s2 where s1.key1 = s2.key1) OR key3 = \u0026#39;a\u0026#39;; range\n如果使用索引获取某些范围区间记录，那么type应该为range\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;); 或者：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;a\u0026#39; AND key1 \u0026lt; \u0026#39;b\u0026#39;; index\n先说说覆盖索引：我们知道依据非聚簇索引的列作为查询条件时，由于索引的B+树叶子节点里存的是主键+索引列，并非是完整的一行数据，所以当我们select查询的字段出现了不是主键+索引列，而是包括其他列那么就需要回表\u0026ndash;拿着主键的值，到聚簇索引里拿出你想要的数据；如果查询的字段里只出现了索引列或主键的字段，这样就不需要回表了，我们要查的数据就在索引列上，因此索引覆盖了我们要查询的数据。\n当我们可以使用**索引覆盖**，但需要扫描全部的索引记录时，该表的访问方法就是index，比如这样：（注意：这个确实不满足最左前缀原则，但是会将使用到 idx_key_part 这个索引来遍历一次，也就是说，使用不到索引的好处，但是使用到避免回表的好处）\n1 mysql\u0026gt; EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = \u0026#39;a\u0026#39;; 上述查询中的所有列表中只有 key_part2 一个列，而且搜索条件中也只有 key_part3 一个列，这两个列又恰好包含在idx_key_part这个索引中，可是搜索条件key_part3不能直接使用该索引进行ref和range方式的访问，只能扫描整个idx_key_part索引的记录，所以查询计划的type列的值就是index。\n再一次强调，对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，也就是扫描聚簇索引的代价更低一些。\u0026ndash;回表\nALL\n最熟悉的全表扫描，就不多说了，直接看例子：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1; **小结: **\n**结果值从最好到最坏依次是： **\nsystem \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; fulltext \u0026gt; ref_or_null \u0026gt; index_merge \u0026gt; unique_subquery \u0026gt; index_subquery \u0026gt; range \u0026gt; index \u0026gt; ALL\nSQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，最好是 const 级别。（阿里巴巴 开发手册要求）\n6. possible_keys和key ☆ 在EXPLAIN语句输出的执行计划中，possible_keys列表示在某个查询语句中，对某个列执行单表查询时可能用到的索引有哪些。一般查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用。key列表示实际用到的索引有哪些，如果为NULL，则没有使用索引。比方说下面这个查询：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND key3 = \u0026#39;a\u0026#39;; 上述执行计划的possible_keys列的值是idx_key1, idx_key3，表示该查询可能使用到idx_key1, idx_key3两个索引，然后key列的值是idx_key3，表示经过查询优化器计算使用不同索引的成本后，最后决定采用idx_key3。\n7. key_len ☆ 实际使用到的索引长度 (即：字节数)\n帮你检查是否充分的利用了索引，值越大越好，主要针对于联合索引，有一定的参考意义。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE id = 10005; int 占用 4 个字节\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key2 = 10126; key2上有一个唯一性约束，是否为NULL占用一个字节，那么就是5个字节\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;; key1 VARCHAR(100) 一个字符占3个字节，100*3，是否为NULL占用一个字节，varchar的长度信息占两个字节。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key_part1 = \u0026#39;a\u0026#39;; 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key_part1 = \u0026#39;a\u0026#39; AND key_part2 = \u0026#39;b\u0026#39;; 联合索引中可以比较，key_len=606的好于key_len=303\n**练习： **\nkey_len的长度计算公式：\n1 2 3 4 5 6 7 varchar(10)变长字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL)+2(变长字段) varchar(10)变长字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+2(变长字段) char(10)固定字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL) char(10)固定字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1) 8. ref 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;; 可以看到ref列的值是const，表明在使用idx_key1索引执行查询时，与key1列作等值匹配的对象是一个常数，当然有时候更复杂一点:\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id; 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s2.key1 = UPPER(s1.key1); 9. rows ☆ 预估的需要读取的记录条数，值越小越好。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39;; 10. filtered 某个表经过搜索条件过滤后剩余记录条数的百分比\n如果使用的是索引执行的单表扫描，那么计算时需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND common_field = \u0026#39;a\u0026#39;; 对于单表查询来说，这个filtered的值没有什么意义，我们更关注在连接查询中驱动表对应的执行计划记录的filtered值，它决定了被驱动表要执行的次数 (即: rows * filtered)\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = \u0026#39;a\u0026#39;; 从执行计划中可以看出来，查询优化器打算把s1作为驱动表，s2当做被驱动表。我们可以看到驱动表s1表的执行计划的rows列为9688，filtered列为10.00，这意味着驱动表s1的扇出值就是9688 x 10.00% = 968.8，这说明还要对被驱动表执行大约968次查询。\n11. Extra ☆ 顾名思义，Extra列是用来说明一些额外信息的，包含不适合在其他列中显示但十分重要的额外信息。我们可以通过这些额外信息来更准确的理解MySQL到底将如何执行给定的查询语句。MySQL提供的额外信息有好几十个，我们就不一个一个介绍了，所以我们只挑选比较重要的额外信息介绍给大家。\nNo tables used\n当查询语句没有FROM子句时将会提示该额外信息，比如：\n1 mysql\u0026gt; EXPLAIN SELECT 1; Impossible WHERE\n当查询语句的WHERE子句永远为FALSE时将会提示该额外信息\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE 1 != 1; Using where\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE common_field = \u0026#39;a\u0026#39;; 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; AND common_field = \u0026#39;a\u0026#39;; No matching min/max row\n当查询列表处有MIN或者MAX聚合函数，但是并没有符合WHERE子句中的搜索条件的记录时。\n1 mysql\u0026gt; EXPLAIN SELECT MIN(key1) FROM s1 WHERE key1 = \u0026#39;abcdefg\u0026#39;; Using index\n当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用覆盖索引的情况下，在Extra列将会提示该额外信息。比方说下边这个查询中只需要用到idx_key1而不需要回表操作:\n1 mysql\u0026gt; EXPLAIN SELECT key1 FROM s1 WHERE key1 = \u0026#39;a\u0026#39;; Using index condition\n有些搜索条件中虽然出现了索引列，但却不能使用到索引，比如下边这个查询：(索引下推)\n1 SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND key1 LIKE \u0026#39;%a\u0026#39;; 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND key1 LIKE \u0026#39;%b\u0026#39;; Using join buffer (Block Nested Loop)\n在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.common_field = s2.common_field; Not exists\n当我们使用左(外)连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示这个信息：\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.id IS NULL; Using intersect(...) 、 Using union(...) 和 Using sort_union(...)\n如果执行计划的Extra列出现了Using intersect(...)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的...表示需要进行索引合并的索引名称；\n如果出现Using union(...)提示，说明准备使用Union索引合并的方式执行查询;\n如果出现Using sort_union(...)提示，说明准备使用Sort-Union索引合并的方式执行查询。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; OR key3 = \u0026#39;a\u0026#39;; Zero limit\n当我们的LIMIT子句的参数为0时，表示压根儿不打算从表中读取任何记录，将会提示该额外信息\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 LIMIT 0; Using filesort\n有一些情况下对结果集中的记录进行排序是可以使用到索引的。\n1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 ORDER BY key1 LIMIT 10; 1 mysql\u0026gt; EXPLAIN SELECT * FROM s1 ORDER BY common_field LIMIT 10; 需要注意的是，如果查询中需要使用filesort的方式进行排序的记录非常多，那么这个过程是很耗费性能的，我们最好想办法将使用文件排序的执行方式改为索引进行排序。\nUsing temporary\n1 mysql\u0026gt; EXPLAIN SELECT DISTINCT common_field FROM s1; 再比如：\n1 mysql\u0026gt; EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field; 执行计划中出现Using temporary并不是一个好的征兆，因为建立与维护临时表要付出很大的成本的，所以我们最好能使用索引来替代掉使用临时表，比方说下边这个包含GROUP BY子句的查询就不需要使用临时表：\n1 mysql\u0026gt; EXPLAIN SELECT key1, COUNT(*) AS amount FROM s1 GROUP BY key1; 从 Extra 的 Using index 的提示里我们可以看出，上述查询只需要扫描 idx_key1 索引就可以搞定了，不再需要临时表了。\n其他\n其它特殊情况这里省略。\n12. 小结 EXPLAIN不考虑各种Cache EXPLAIN不能显示MySQL在执行查询时所作的优化工作 EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 部分统计信息是估算的，并非精确值 7. EXPLAIN的进一步使用 7.1 EXPLAIN四种输出格式 这里谈谈EXPLAIN的输出格式。EXPLAIN可以输出四种格式： 传统格式 ，JSON格式 ， TREE格式 以及 可视化输出 。用户可以根据需要选择适用于自己的格式。\n1. 传统格式 传统格式简单明了，输出是一个表格形式，概要说明查询计划。\n1 mysql\u0026gt; EXPLAIN SELECT s1.key1, s2.key1 FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.common_field IS NOT NULL; 2. JSON格式 第1种格式中介绍的EXPLAIN语句输出中缺少了一个衡量执行好坏的重要属性 —— 成本。而JSON格式是四种格式里面输出信息最详尽的格式，里面包含了执行的成本信息。\nJSON格式：在EXPLAIN单词和真正的查询语句中间加上 FORMAT=JSON 。 1 EXPLAIN FORMAT=JSON SELECT .... EXPLAIN的Column与JSON的对应关系：(来源于MySQL 5.7文档) 这样我们就可以得到一个json格式的执行计划，里面包含该计划花费的成本。比如这样：\n1 mysql\u0026gt; EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = \u0026#39;a\u0026#39;\\G 我们使用 # 后边跟随注释的形式为大家解释了 EXPLAIN FORMAT=JSON 语句的输出内容，但是大家可能 有疑问 \u0026ldquo;cost_info\u0026rdquo; 里边的成本看着怪怪的，它们是怎么计算出来的？先看 s1 表的 \u0026ldquo;cost_info\u0026rdquo; 部 分：\n1 2 3 4 5 6 \u0026#34;cost_info\u0026#34;: { \u0026#34;read_cost\u0026#34;: \u0026#34;1840.84\u0026#34;, \u0026#34;eval_cost\u0026#34;: \u0026#34;193.76\u0026#34;, \u0026#34;prefix_cost\u0026#34;: \u0026#34;2034.60\u0026#34;, \u0026#34;data_read_per_join\u0026#34;: \u0026#34;1M\u0026#34; } read_cost 是由下边这两部分组成的：\nIO 成本 检测 rows × (1 - filter) 条记录的 CPU 成本 小贴士： rows和filter都是我们前边介绍执行计划的输出列，在JSON格式的执行计划中，rows 相当于rows_examined_per_scan，filtered名称不变。\neval_cost 是这样计算的：\n检测 rows × filter 条记录的成本。\nprefix_cost 就是单独查询 s1 表的成本，也就是：\nread_cost + eval_cost\ndata_read_per_join 表示在此次查询中需要读取的数据量。\n对于 s2 表的 \u0026ldquo;cost_info\u0026rdquo; 部分是这样的：\n1 2 3 4 5 6 \u0026#34;cost_info\u0026#34;: { \u0026#34;read_cost\u0026#34;: \u0026#34;968.80\u0026#34;, \u0026#34;eval_cost\u0026#34;: \u0026#34;193.76\u0026#34;, \u0026#34;prefix_cost\u0026#34;: \u0026#34;3197.16\u0026#34;, \u0026#34;data_read_per_join\u0026#34;: \u0026#34;1M\u0026#34; } 由于 s2 表是被驱动表，所以可能被读取多次，这里的read_cost 和 eval_cost 是访问多次 s2 表后累加起来的值，大家主要关注里边儿的 prefix_cost 的值代表的是整个连接查询预计的成本，也就是单次查询 s1 表和多次查询 s2 表后的成本的和，也就是：\n1 968.80 + 193.76 + 2034.60 = 3197.16 3. TREE格式 TREE格式是8.0.16版本之后引入的新格式，主要根据查询的 各个部分之间的关系 和 各部分的执行顺序 来描述如何查询。\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; EXPLAIN FORMAT=tree SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = \u0026#39;a\u0026#39;\\G *************************** 1. row *************************** EXPLAIN: -\u0026gt; Nested loop inner join (cost=1360.08 rows=990) -\u0026gt; Filter: ((s1.common_field = \u0026#39;a\u0026#39;) and (s1.key1 is not null)) (cost=1013.75 rows=990) -\u0026gt; Table scan on s1 (cost=1013.75 rows=9895) -\u0026gt; Single-row index lookup on s2 using idx_key2 (key2=s1.key1), with index condition: (cast(s1.key1 as double) = cast(s2.key2 as double)) (cost=0.25 rows=1) 1 row in set, 1 warning (0.00 sec) 4. 可视化输出 可视化输出，可以通过MySQL Workbench可视化查看MySQL的执行计划。通过点击Workbench的放大镜图标，即可生成可视化的查询计划。\n上图按从左到右的连接顺序显示表。红色框表示 全表扫描 ，而绿色框表示使用 索引查找 。对于每个表， 显示使用的索引。还要注意的是，每个表格的框上方是每个表访问所发现的行数的估计值以及访问该表的成本。\n7.2 SHOW WARNINGS的使用 在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息，比如这样：\n1 mysql\u0026gt; EXPLAIN SELECT s1.key1, s2.key1 FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.common_field IS NOT NULL; 1 2 3 4 5 6 7 8 mysql\u0026gt; SHOW WARNINGS\\G *************************** 1. row *************************** Level: Note Code: 1003 Message: /* select#1 */ select `atguigu`.`s1`.`key1` AS `key1`,`atguigu`.`s2`.`key1` AS `key1` from `atguigu`.`s1` join `atguigu`.`s2` where ((`atguigu`.`s1`.`key1` = `atguigu`.`s2`.`key1`) and (`atguigu`.`s2`.`common_field` is not null)) 1 row in set (0.00 sec) 大家可以看到SHOW WARNINGS展示出来的信息有三个字段，分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。比如我们上边的查询本来是一个左(外)连接查询，但是有一个s2.common_field IS NOT NULL的条件，这就会导致查询优化器把左(外)连接查询优化为内连接查询，从SHOW WARNINGS的Message字段也可以看出来，原本的LEFE JOIN已经变成了JOIN。\n但是大家一定要注意，我们说Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，并不是等价于，也就是说Message字段展示的信息并不是标准的查询语句，在很多情况下并不能直接拿到黑框框中运行，它只能作为帮助我们理解MySQL将如何执行查询语句的一个参考依据而已。\n8. 分析优化器执行计划：trace 1 2 SET optimizer_trace=\u0026#34;enabled=on\u0026#34;,end_markers_in_json=on; set optimizer_trace_max_mem_size=1000000; 开启后，可分析如下语句：\nSELECT INSERT REPLACE UPDATE DELETE EXPLAIN SET DECLARE CASE IF RETURN CALL 测试：执行如下SQL语句\n1 select * from student where id \u0026lt; 10; 最后， 查询 information_schema.optimizer_trace 就可以知道MySQL是如何执行SQL的 ：\n1 select * from information_schema.optimizer_trace\\G 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 *************************** 1. row *************************** //第1部分：查询语句 QUERY: select * from student where id \u0026lt; 10 //第2部分：QUERY字段对应语句的跟踪信息 TRACE: { \u0026#34;steps\u0026#34;: [ { \u0026#34;join_preparation\u0026#34;: { //预备工作 \u0026#34;select#\u0026#34;: 1, \u0026#34;steps\u0026#34;: [ { \u0026#34;expanded_query\u0026#34;: \u0026#34;/* select#1 */ select `student`.`id` AS `id`,`student`.`stuno` AS `stuno`,`student`.`name` AS `name`,`student`.`age` AS `age`,`student`.`classId` AS `classId` from `student` where (`student`.`id` \u0026lt; 10)\u0026#34; } ] /* steps */ } /* join_preparation */ }, { \u0026#34;join_optimization\u0026#34;: { //进行优化 \u0026#34;select#\u0026#34;: 1, \u0026#34;steps\u0026#34;: [ { \u0026#34;condition_processing\u0026#34;: { //条件处理 \u0026#34;condition\u0026#34;: \u0026#34;WHERE\u0026#34;, \u0026#34;original_condition\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;transformation\u0026#34;: \u0026#34;equality_propagation\u0026#34;, \u0026#34;resulting_condition\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34; }, { \u0026#34;transformation\u0026#34;: \u0026#34;constant_propagation\u0026#34;, \u0026#34;resulting_condition\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34; }, { \u0026#34;transformation\u0026#34;: \u0026#34;trivial_condition_removal\u0026#34;, \u0026#34;resulting_condition\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34; } ] /* steps */ } /* condition_processing */ }, { \u0026#34;substitute_generated_columns\u0026#34;: { //替换生成的列 } /* substitute_generated_columns */ }, { \u0026#34;table_dependencies\u0026#34;: [ //表的依赖关系 { \u0026#34;table\u0026#34;: \u0026#34;`student`\u0026#34;, \u0026#34;row_may_be_null\u0026#34;: false, \u0026#34;map_bit\u0026#34;: 0, \u0026#34;depends_on_map_bits\u0026#34;: [ ] /* depends_on_map_bits */ } ] /* table_dependencies */ }, { \u0026#34;ref_optimizer_key_uses\u0026#34;: [ //使用键 ] /* ref_optimizer_key_uses */ }, { \u0026#34;rows_estimation\u0026#34;: [ //行判断 { \u0026#34;table\u0026#34;: \u0026#34;`student`\u0026#34;, \u0026#34;range_analysis\u0026#34;: { \u0026#34;table_scan\u0026#34;: { \u0026#34;rows\u0026#34;: 3973767, \u0026#34;cost\u0026#34;: 408558 } /* table_scan */, //扫描表 \u0026#34;potential_range_indexes\u0026#34;: [ //潜在的范围索引 { \u0026#34;index\u0026#34;: \u0026#34;PRIMARY\u0026#34;, \u0026#34;usable\u0026#34;: true, \u0026#34;key_parts\u0026#34;: [ \u0026#34;id\u0026#34; ] /* key_parts */ } ] /* potential_range_indexes */, \u0026#34;setup_range_conditions\u0026#34;: [ //设置范围条件 ] /* setup_range_conditions */, \u0026#34;group_index_range\u0026#34;: { \u0026#34;chosen\u0026#34;: false, \u0026#34;cause\u0026#34;: \u0026#34;not_group_by_or_distinct\u0026#34; } /* group_index_range */, \u0026#34;skip_scan_range\u0026#34;: { \u0026#34;potential_skip_scan_indexes\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;PRIMARY\u0026#34;, \u0026#34;usable\u0026#34;: false, \u0026#34;cause\u0026#34;: \u0026#34;query_references_nonkey_column\u0026#34; } ] /* potential_skip_scan_indexes */ } /* skip_scan_range */, \u0026#34;analyzing_range_alternatives\u0026#34;: { //分析范围选项 \u0026#34;range_scan_alternatives\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;PRIMARY\u0026#34;, \u0026#34;ranges\u0026#34;: [ \u0026#34;id \u0026lt; 10\u0026#34; ] /* ranges */, \u0026#34;index_dives_for_eq_ranges\u0026#34;: true, \u0026#34;rowid_ordered\u0026#34;: true, \u0026#34;using_mrr\u0026#34;: false, \u0026#34;index_only\u0026#34;: false, \u0026#34;rows\u0026#34;: 9, \u0026#34;cost\u0026#34;: 1.91986, \u0026#34;chosen\u0026#34;: true } ] /* range_scan_alternatives */, \u0026#34;analyzing_roworder_intersect\u0026#34;: { \u0026#34;usable\u0026#34;: false, \u0026#34;cause\u0026#34;: \u0026#34;too_few_roworder_scans\u0026#34; } /* analyzing_roworder_intersect */ } /* analyzing_range_alternatives */, \u0026#34;chosen_range_access_summary\u0026#34;: { //选择范围访问摘要 \u0026#34;range_access_plan\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;range_scan\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;PRIMARY\u0026#34;, \u0026#34;rows\u0026#34;: 9, \u0026#34;ranges\u0026#34;: [ \u0026#34;id \u0026lt; 10\u0026#34; ] /* ranges */ } /* range_access_plan */, \u0026#34;rows_for_plan\u0026#34;: 9, \u0026#34;cost_for_plan\u0026#34;: 1.91986, \u0026#34;chosen\u0026#34;: true } /* chosen_range_access_summary */ } /* range_analysis */ } ] /* rows_estimation */ }, { \u0026#34;considered_execution_plans\u0026#34;: [ //考虑执行计划 { \u0026#34;plan_prefix\u0026#34;: [ ] /* plan_prefix */, \u0026#34;table\u0026#34;: \u0026#34;`student`\u0026#34;, \u0026#34;best_access_path\u0026#34;: { //最佳访问路径 \u0026#34;considered_access_paths\u0026#34;: [ { \u0026#34;rows_to_scan\u0026#34;: 9, \u0026#34;access_type\u0026#34;: \u0026#34;range\u0026#34;, \u0026#34;range_details\u0026#34;: { \u0026#34;used_index\u0026#34;: \u0026#34;PRIMARY\u0026#34; } /* range_details */, \u0026#34;resulting_rows\u0026#34;: 9, \u0026#34;cost\u0026#34;: 2.81986, \u0026#34;chosen\u0026#34;: true } ] /* considered_access_paths */ } /* best_access_path */, \u0026#34;condition_filtering_pct\u0026#34;: 100, //行过滤百分比 \u0026#34;rows_for_plan\u0026#34;: 9, \u0026#34;cost_for_plan\u0026#34;: 2.81986, \u0026#34;chosen\u0026#34;: true } ] /* considered_execution_plans */ }, { \u0026#34;attaching_conditions_to_tables\u0026#34;: { //将条件附加到表上 \u0026#34;original_condition\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34;, \u0026#34;attached_conditions_computation\u0026#34;: [ ] /* attached_conditions_computation */, \u0026#34;attached_conditions_summary\u0026#34;: [ //附加条件概要 { \u0026#34;table\u0026#34;: \u0026#34;`student`\u0026#34;, \u0026#34;attached\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34; } ] /* attached_conditions_summary */ } /* attaching_conditions_to_tables */ }, { \u0026#34;finalizing_table_conditions\u0026#34;: [ { \u0026#34;table\u0026#34;: \u0026#34;`student`\u0026#34;, \u0026#34;original_table_condition\u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34;, \u0026#34;final_table_condition \u0026#34;: \u0026#34;(`student`.`id` \u0026lt; 10)\u0026#34; } ] /* finalizing_table_conditions */ }, { \u0026#34;refine_plan\u0026#34;: [ //精简计划 { \u0026#34;table\u0026#34;: \u0026#34;`student`\u0026#34; } ] /* refine_plan */ } ] /* steps */ } /* join_optimization */ }, { \u0026#34;join_execution\u0026#34;: { //执行 \u0026#34;select#\u0026#34;: 1, \u0026#34;steps\u0026#34;: [ ] /* steps */ } /* join_execution */ } ] /* steps */ } //第3部分：跟踪信息过长时，被截断的跟踪信息的字节数。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 0 //丢失的超出最大容量的字节 //第4部分：执行跟踪语句的用户是否有查看对象的权限。当不具有权限时，该列信息为1且TRACE字段为空，一般在 调用带有SQL SECURITY DEFINER的视图或者是存储过程的情况下，会出现此问题。 INSUFFICIENT_PRIVILEGES: 0 //缺失权限 1 row in set (0.00 sec) 9. MySQL监控分析视图-sys schema 9.1 Sys schema视图摘要 主机相关：以host_summary开头，主要汇总了IO延迟的信息。 Innodb相关：以innodb开头，汇总了innodb buffer信息和事务等待innodb锁的信息。 I/o相关：以io开头，汇总了等待I/O、I/O使用量情况。 内存使用情况：以memory开头，从主机、线程、事件等角度展示内存的使用情况 连接与会话信息：processlist和session相关视图，总结了会话相关信息。 表相关：以schema_table开头的视图，展示了表的统计信息。 索引信息：统计了索引的使用情况，包含冗余索引和未使用的索引情况。 语句相关：以statement开头，包含执行全表扫描、使用临时表、排序等的语句信息。 用户相关：以user开头的视图，统计了用户使用的文件I/O、执行语句统计信息。 等待事件相关信息：以wait开头，展示等待事件的延迟情况。 9.2 Sys schema视图使用场景 索引情况\n1 2 3 4 5 6 7 #1. 查询冗余索引 select * from sys.schema_redundant_indexes; #2. 查询未使用过的索引 select * from sys.schema_unused_indexes; #3. 查询索引的使用情况 select index_name,rows_selected,rows_inserted,rows_updated,rows_deleted from sys.schema_index_statistics where table_schema=\u0026#39;dbname\u0026#39;; 表相关\n1 2 3 4 5 6 7 8 # 1. 查询表的访问量 select table_schema,table_name,sum(io_read_requests+io_write_requests) as io from sys.schema_table_statistics group by table_schema,table_name order by io desc; # 2. 查询占用bufferpool较多的表 select object_schema,object_name,allocated,data from sys.innodb_buffer_stats_by_table order by allocated limit 10; # 3. 查看表的全表扫描情况 select * from sys.statements_with_full_table_scans where db=\u0026#39;dbname\u0026#39;; 语句相关\n1 2 3 4 5 6 7 8 9 10 #1. 监控SQL执行的频率 select db,exec_count,query from sys.statement_analysis order by exec_count desc; #2. 监控使用了排序的SQL select db,exec_count,first_seen,last_seen,query from sys.statements_with_sorting limit 1; #3. 监控使用了临时表或者磁盘临时表的SQL select db,exec_count,tmp_tables,tmp_disk_tables,query from sys.statement_analysis where tmp_tables\u0026gt;0 or tmp_disk_tables \u0026gt;0 order by (tmp_tables+tmp_disk_tables) desc; IO相关\n1 2 3 #1. 查看消耗磁盘IO的文件 select file,avg_read,avg_write,avg_read+avg_write as avg_io from sys.io_global_by_file_by_bytes order by avg_read limit 10; Innodb 相关\n1 2 #1. 行锁阻塞情况 select * from sys.innodb_lock_waits; 10. 小结 查询是数据库中最频繁的操作，提高查询速度可以有效地提高MySQL数据库的性能。通过对查询语句的分析可以了解查询语句的执行情况，找出查询语句执行的瓶颈，从而优化查询语句。\n第10章_索引优化与查询优化 都有哪些维度可以进行数据库调优？简言之：\n索引失效、没有充分利用到索引——建立索引 关联查询太多JOIN（设计缺陷或不得已的需求）——SQL优化 服务器调优及各个参数设置（缓冲、线程数等）——调整my.cnf 数据过多——分库分表 关于数据库调优的知识非常分散。不同的DBMS，不同的公司，不同的职位，不同的项目遇到的问题都不尽相同。这里我们分为三个章节进行细致讲解。\n虽然SQL查询优化的技术有很多，但是大方向上完全可以分成物理查询优化和逻辑查询优化两大块。\n物理查询优化是通过索引和表连接方式等技术来进行优化，这里重点需要掌握索引的使用。 逻辑查询优化就是通过SQL等价变换提升查询效率，直白一点就是说，换一种查询写法效率可能更高。 1. 数据准备 学员表 插 50万 条， 班级表 插 1万 条。\n1 2 CREATE DATABASE atguigudb2; USE atguigudb2; 步骤1：建表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE `class` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `className` VARCHAR(30) DEFAULT NULL, `address` VARCHAR(40) DEFAULT NULL, `monitor` INT NULL , PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) #CONSTRAINT `fk_class_id` FOREIGN KEY (`classId`) REFERENCES `t_class` (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 步骤2：设置参数\n命令开启：允许创建函数设置： 1 set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 步骤3：创建函数\n保证每条数据都不同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #随机产生字符串 DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) BEGIN DECLARE chars_str VARCHAR(100) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ\u0026#39;; DECLARE return_str VARCHAR(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; #假如要删除 #drop function rand_string; 随机产生班级编号\n1 2 3 4 5 6 7 8 9 10 11 #用于随机产生多少到多少的编号 DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; #假如要删除 #drop function rand_num; 步骤4：创建存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #创建往stu表中插入数据的存储过程 DELIMITER // CREATE PROCEDURE insert_stu( START INT , max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, name ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(1,50),rand_num(1,1000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; #假如要删除 #drop PROCEDURE insert_stu; 创建往class表中插入数据的存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #执行存储过程，往class表添加随机数据 DELIMITER // CREATE PROCEDURE `insert_class`( max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO class ( classname,address,monitor ) VALUES (rand_string(8),rand_string(10),rand_num(1,100000)); UNTIL i = max_num END REPEAT; COMMIT; END // DELIMITER ; #假如要删除 #drop PROCEDURE insert_class; 步骤5：调用存储过程\nclass\n1 2 #执行存储过程，往class表添加1万条数据 CALL insert_class(10000); stu\n1 2 #执行存储过程，往stu表添加50万条数据 CALL insert_stu(100000,500000); 步骤6：删除某表上的索引\n创建存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 DELIMITER // CREATE PROCEDURE `proc_drop_index`(dbname VARCHAR(200),tablename VARCHAR(200)) BEGIN DECLARE done INT DEFAULT 0; DECLARE ct INT DEFAULT 0; DECLARE _index VARCHAR(200) DEFAULT \u0026#39;\u0026#39;; DECLARE _cur CURSOR FOR SELECT index_name FROM information_schema.STATISTICS WHERE table_schema=dbname AND table_name=tablename AND seq_in_index=1 AND index_name \u0026lt;\u0026gt;\u0026#39;PRIMARY\u0026#39; ; #每个游标必须使用不同的declare continue handler for not found set done=1来控制游标的结束 DECLARE CONTINUE HANDLER FOR NOT FOUND set done=2 ; #若没有数据返回,程序继续,并将变量done设为2 OPEN _cur; FETCH _cur INTO _index; WHILE _index\u0026lt;\u0026gt;\u0026#39;\u0026#39; DO SET @str = CONCAT(\u0026#34;drop index \u0026#34; , _index , \u0026#34; on \u0026#34; , tablename ); PREPARE sql_str FROM @str ; EXECUTE sql_str; DEALLOCATE PREPARE sql_str; SET _index=\u0026#39;\u0026#39;; FETCH _cur INTO _index; END WHILE; CLOSE _cur; END // DELIMITER ; 执行存储过程\n1 CALL proc_drop_index(\u0026#34;dbname\u0026#34;,\u0026#34;tablename\u0026#34;); 2. 索引失效案例 2.1 全值匹配我最爱 系统中经常出现的sql语句如下：\n1 2 3 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age=30; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4 AND name = \u0026#39;abcd\u0026#39;; 建立索引前执行：（关注执行时间）\n1 2 mysql\u0026gt; SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4 AND name = \u0026#39;abcd\u0026#39;; Empty set, 1 warning (0.28 sec) 建立索引\n1 2 3 CREATE INDEX idx_age ON student(age); CREATE INDEX idx_age_classid ON student(age,classId); CREATE INDEX idx_age_classid_name ON student(age,classId,name); 建立索引后执行：\n1 2 mysql\u0026gt; SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4 AND name = \u0026#39;abcd\u0026#39;; Empty set, 1 warning (0.01 sec) 2.2 最佳左前缀法则 在MySQL建立联合索引时会遵守最佳左前缀原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。\n举例1：\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = \u0026#39;abcd\u0026#39;; 举例2：\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.classId=1 AND student.name = \u0026#39;abcd\u0026#39;; 举例3：索引idx_age_classid_name还能否正常使用？\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.classId=4 AND student.age=30 AND student.name = \u0026#39;abcd\u0026#39;; 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。\n1 mysql\u0026gt; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = \u0026#39;abcd\u0026#39;; 虽然可以正常使用，但是只有部分被使用到了。\n1 mysql\u0026gt; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.classId=1 AND student.name = \u0026#39;abcd\u0026#39;; 完全没有使用上索引。\n结论：MySQL可以为多个字段创建索引，一个索引可以包含16个字段。对于多列索引，过滤条件要使用索引必须按照索引建立时的顺序，依次满足，一旦跳过某个字段，索引后面的字段都无法被使用。如果查询条件中没有用这些字段中第一个字段时，多列（或联合）索引不会被使用。\n原因：假设我们现在给student表建立一个多列索引 idx_age_classid_name(CREATE INDEX idx_age_classid_name ON student(age,classId,name);)。mysql 就会先根据 age 字段排序，age 相同的时候呢，再根据 classid 排序，classid 又相同的时候就根据 name 字段排序。如此就很清晰：如果不满足最左前缀匹配原则，也就是说，要在局部有序，全局无序的索引列（如classid、name字段）进行全局的查找，显然没有使用到索引，索引失效\n注：只有在查询范围内，有序，才能使用n分叉快速定位\n拓展：Alibaba《Java开发手册》\n索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。\n2.3 主键插入顺序 如果此时再插入一条主键值为 9 的记录，那它插入的位置就如下图：\n可这个数据页已经满了，再插进来咋办呢？我们需要把当前 页面分裂 成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的 主键值依次递增 ，这样就不会发生这样的性能损耗了。所以我们建议：让主键具有AUTO_INCREMENT，让存储引擎自己为表生成主键，而不是我们手动插入，比如：person_info表：\n1 2 3 4 5 6 7 8 9 CREATE TABLE person_info( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name(10), birthday, phone_number) ); 我们自定义的主键列 id 拥有 AUTO_INCREMENT 属性，在插入记录时存储引擎会自动为我们填入自增的主键值。这样的主键占用空间小，顺序写入，减少页分裂。\n2.4 计算、函数、类型转换(自动或手动)导致索引失效 这两条sql哪种写法更好\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE \u0026#39;abc%\u0026#39;; 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = \u0026#39;abc\u0026#39;; 创建索引\n1 CREATE INDEX idx_name ON student(NAME); 第一种：索引优化生效\n1 mysql\u0026gt; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE \u0026#39;abc%\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 mysql\u0026gt; SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE \u0026#39;abc%\u0026#39;; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 5301379 | 1233401 | AbCHEa | 164 | 259 | | 7170042 | 3102064 | ABcHeB | 199 | 161 | | 1901614 | 1833636 | ABcHeC | 226 | 275 | | 5195021 | 1127043 | abchEC | 486 | 72 | | 4047089 | 3810031 | AbCHFd | 268 | 210 | | 4917074 | 849096 | ABcHfD | 264 | 442 | | 1540859 | 141979 | abchFF | 119 | 140 | | 5121801 | 1053823 | AbCHFg | 412 | 327 | | 2441254 | 2373276 | abchFJ | 170 | 362 | | 7039146 | 2971168 | ABcHgI | 502 | 465 | | 1636826 | 1580286 | ABcHgK | 71 | 262 | | 374344 | 474345 | abchHL | 367 | 212 | | 1596534 | 169191 | AbCHHl | 102 | 146 | ... | 5266837 | 1198859 | abclXe | 292 | 298 | | 8126968 | 4058990 | aBClxE | 316 | 150 | | 4298305 | 399962 | AbCLXF | 72 | 423 | | 5813628 | 1745650 | aBClxF | 356 | 323 | | 6980448 | 2912470 | AbCLXF | 107 | 78 | | 7881979 | 3814001 | AbCLXF | 89 | 497 | | 4955576 | 887598 | ABcLxg | 121 | 385 | | 3653460 | 3585482 | AbCLXJ | 130 | 174 | | 1231990 | 1283439 | AbCLYH | 189 | 429 | | 6110615 | 2042637 | ABcLyh | 157 | 40 | +---------+---------+--------+------+---------+ 401 rows in set, 1 warning (0.01 sec) 第二种：索引优化失效\n1 mysql\u0026gt; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = \u0026#39;abc\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 mysql\u0026gt; SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = \u0026#39;abc\u0026#39;; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 5301379 | 1233401 | AbCHEa | 164 | 259 | | 7170042 | 3102064 | ABcHeB | 199 | 161 | | 1901614 | 1833636 | ABcHeC | 226 | 275 | | 5195021 | 1127043 | abchEC | 486 | 72 | | 4047089 | 3810031 | AbCHFd | 268 | 210 | | 4917074 | 849096 | ABcHfD | 264 | 442 | | 1540859 | 141979 | abchFF | 119 | 140 | | 5121801 | 1053823 | AbCHFg | 412 | 327 | | 2441254 | 2373276 | abchFJ | 170 | 362 | | 7039146 | 2971168 | ABcHgI | 502 | 465 | | 1636826 | 1580286 | ABcHgK | 71 | 262 | | 374344 | 474345 | abchHL | 367 | 212 | | 1596534 | 169191 | AbCHHl | 102 | 146 | ... | 5266837 | 1198859 | abclXe | 292 | 298 | | 8126968 | 4058990 | aBClxE | 316 | 150 | | 4298305 | 399962 | AbCLXF | 72 | 423 | | 5813628 | 1745650 | aBClxF | 356 | 323 | | 6980448 | 2912470 | AbCLXF | 107 | 78 | | 7881979 | 3814001 | AbCLXF | 89 | 497 | | 4955576 | 887598 | ABcLxg | 121 | 385 | | 3653460 | 3585482 | AbCLXJ | 130 | 174 | | 1231990 | 1283439 | AbCLYH | 189 | 429 | | 6110615 | 2042637 | ABcLyh | 157 | 40 | +---------+---------+--------+------+---------+ 401 rows in set, 1 warning (3.62 sec) type为“ALL”，表示没有使用到索引，查询时间为 3.62 秒，查询效率较之前低很多。\n再举例：\nstudent表的字段stuno上设置有索引\n1 CREATE INDEX idx_sno ON student(stuno); 索引优化失效：（假设：student表的字段stuno上设置有索引）\n1 EXPLAIN SELECT SQL_NO_CACHE id, stuno, NAME FROM student WHERE stuno+1 = 900001; 运行结果：\n索引优化生效：\n1 EXPLAIN SELECT SQL_NO_CACHE id, stuno, NAME FROM student WHERE stuno = 900000; 再举例：\nstudent表的字段name上设置有索引\n1 CREATE INDEX idx_name ON student(NAME); 1 EXPLAIN SELECT id, stuno, name FROM student WHERE SUBSTRING(name, 1,3)=\u0026#39;abc\u0026#39;; 索引优化生效\n1 EXPLAIN SELECT id, stuno, NAME FROM student WHERE NAME LIKE \u0026#39;abc%\u0026#39;; 2.5 类型转换导致索引失效 下列哪个sql语句可以用到索引。（假设name字段上设置有索引）\n1 2 # 未使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name=123; 1 2 # 使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name=\u0026#39;123\u0026#39;; name=123发生类型转换，索引失效。\n2.6 范围条件右边的列索引失效 系统经常出现的sql如下： 1 2 3 4 5 6 7 ALTER TABLE student DROP INDEX idx_name; ALTER TABLE student DROP INDEX idx_age; ALTER TABLE student DROP INDEX idx_age_classid; # 只剩下 idx_age_classid_name 索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = \u0026#39;abc\u0026#39; AND student.classId\u0026gt;20; 那么索引 idx_age_classId_name 这个索引还能正常使用么？\n注：where字段条件的顺序，mysql优化器会根据索引调整的\n不能，范围右边的列不能使用。比如：(\u0026lt;) (\u0026lt;=) (\u0026gt;) (\u0026gt;=) 和 between 等。因为，范围查询时，会有多条记录，每条记录对应的右索引列，局部上是有序的，但是多条记录整体上看又变成无序的了，所以范围条件右边的列索引也会失效 如果这种sql出现较多，应该建立： 1 create index idx_age_name_classId on student(age,name,classId); 将范围查询条件放置语句最后： 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.classId\u0026gt;20 AND student.name = \u0026#39;abc\u0026#39;; 应用开发中范围查询，例如：金额查询，日期查询往往都是范围查询。应将查询条件放置where语句最后。（创建的联合索引中，务必把范围涉及到的字段写在最后）\n效果 2.7 不等于(!= 或者\u0026lt;\u0026gt;)索引失效 为什么呢？其实，不等于 操作运算符也是范围查询的一种，他会挨个儿比较不同，只有 等于 才能使用到n分叉定位查找\n为name字段创建索引 1 CREATE INDEX idx_name ON student(NAME); 查看索引是否失效 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name \u0026lt;\u0026gt; \u0026#39;abc\u0026#39;; 或者\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name != \u0026#39;abc\u0026#39;; 场景举例：用户提出需求，将财务数据，产品利润金额不等于0的都统计出来。\n2.8 is null可以使用索引，is not null无法使用索引 IS NULL: 可以触发索引 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age IS NULL; IS NOT NULL: 无法触发索引 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age IS NOT NULL; 结论：最好在设计数据库的时候就将字段设置为 NOT NULL 约束，比如你可以将 INT 类型的字段，默认值设置为0。将字符类型的默认值设置为空字符串(\u0026rsquo;\u0026rsquo;)。\n扩展：同理，在查询中使用not like也无法使用索引，导致全表扫描。\n2.9 like以通配符%开头索引失效 在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为\u0026rsquo;%\u0026rsquo;，索引就不会起作用。只有\u0026rsquo;%\u0026lsquo;不在第一个位置，索引才会起作用。\n为什么呢？因为，如果第一个字符是%，说明前缀是无法确定的，可以匹配任意值，那么就无法定位索引了\n使用到索引 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name LIKE \u0026#39;ab%\u0026#39;; 未使用到索引 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name LIKE \u0026#39;%ab%\u0026#39;; 拓展：Alibaba《Java开发手册》\n【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\n2.10 OR 前后存在非索引的列，索引失效 在WHERE子句中，如果在OR前的条件列进行了索引，而在OR后的条件列没有进行索引，那么索引会失效。也就是说，OR前后的两个条件中的列都是索引时，查询中才使用索引。\n因为OR的含义就是两个只要满足一个即可，因此只有一个条件列进行了索引是没有意义的，只要有条件列没有进行索引，就会进行全表扫描，因此所以的条件列也会失效。\n查询语句使用OR关键字的情况：\n1 2 # 未使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 10 OR classid = 100; 因为classId字段上没有索引，所以上述查询语句没有使用索引。\n1 2 # 使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 10 OR name = \u0026#39;Abel\u0026#39;; 因为age字段和name字段上都有索引，所以查询中使用了索引。你能看到这里使用到了index_merge，简单来说index_merge就是对age和name分别进行了扫描，然后将这两个结果集进行了合并。这样做的好处就是避免了全表扫描。\n2.11 数据库和表的字符集统一使用utf8mb4 统一使用utf8mb4( 5.5.3版本以上支持)兼容性更好，统一字符集可以避免由于字符集转换产生的乱码。不同的字符集进行比较前需要进行 转换 会造成索引失效。\n2.12 练习及一般性建议 **练习：**假设：index(a,b,c)\n一般性建议\n对于单列索引，尽量选择针对当前query过滤性更好的索引 在选择组合索引的时候，当前query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择组合索引的时候，尽量选择能够当前query中where子句中更多的索引。 在选择组合索引的时候，如果某个字段可能出现范围查询时，尽量把这个字段放在索引次序的最后面。 总之，书写SQL语句时，尽量避免造成索引失效的情况\n3. 关联查询优化 3.1 数据准备 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # 分类 CREATE TABLE IF NOT EXISTS `type` ( `id` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, `card` INT(10) UNSIGNED NOT NULL, PRIMARY KEY (`id`) ); # 图书 CREATE TABLE IF NOT EXISTS `book` ( `bookid` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, `card` INT(10) UNSIGNED NOT NULL, PRIMARY KEY (`bookid`) ); # 向分类表中添加20条记录 INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); # 向图书表中添加20条记录 INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); 3.2 采用左外连接 下面开始 EXPLAIN 分析\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; 结论：type 有All\n添加索引优化\n1 2 3 ALTER TABLE book ADD INDEX Y (card); #【被驱动表】，可以避免全表扫描 EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; # 注意，驱动表type是all，无可厚非，本来连接查询本来就是将驱动表的结果根据连接条件逐行与被驱动表里的所有行进行比较筛选，只要能够让被驱动表使用到索引，从而避免【依据驱动表的结果在被驱动表全表扫描的情况】发生，这样就如同下图，可以达到优化的目的 可以看到第二行的 type 变为了 ref，rows 也变成了优化比较明显。这是由左连接特性决定的。LEFT JOIN 条件用于确定如何从右表搜索行，左边一定都有，所以 右边是我们的关键点,一定需要建立索引 。\n1 2 ALTER TABLE `type` ADD INDEX X (card); #【驱动表】，无法避免全表扫描 EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; 接着：\n1 2 DROP INDEX Y ON book; EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; 3.3 采用内连接 1 2 drop index X on type; drop index Y on book; #（如果已经删除了可以不用再执行该操作） 换成 inner join（MySQL自动选择驱动表，一般优化器会选择小的表作为驱动表，大的表作为被驱动表）\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM type INNER JOIN book ON type.card=book.card; 添加索引优化\n1 2 ALTER TABLE book ADD INDEX Y (card); EXPLAIN SELECT SQL_NO_CACHE * FROM type INNER JOIN book ON type.card=book.card; 1 2 ALTER TABLE type ADD INDEX X (card); EXPLAIN SELECT SQL_NO_CACHE * FROM type INNER JOIN book ON type.card=book.card; 对于内连接来说，查询优化器可以决定谁作为驱动表，谁作为被驱动表出现的\n接着：\n1 2 DROP INDEX X ON `type`; EXPLAIN SELECT SQL_NO_CACHE * FROM TYPE INNER JOIN book ON type.card=book.card; 接着：\n1 2 ALTER TABLE `type` ADD INDEX X (card); EXPLAIN SELECT SQL_NO_CACHE * FROM `type` INNER JOIN book ON type.card=book.card; 接着：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 向图书表中添加20条记录 INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); ALTER TABLE book ADD INDEX Y (card); EXPLAIN SELECT SQL_NO_CACHE * FROM `type` INNER JOIN book ON `type`.card = book.card; 图中发现，由于type表数据大于book表数据，MySQL选择将type作为被驱动表。\u0026ndash;\u0026gt; 小表驱动大表\n3.4 join语句原理 join方式连接多个表，本质就是各个表之间数据的循环匹配。MySQL5.5版本之前，MySQL只支持一种表间关联方式，就是嵌套循环(Nested Loop Join)。如果关联表的数据量很大，则join关联的执行时间会很长。在MySQL5.5以后的版本中，MySQL通过引入BNLJ算法来优化嵌套执行。\n1. 驱动表和被驱动表 驱动表就是主表，被驱动表就是从表、非驱动表。\n对于内连接来说： 1 SELECT * FROM A JOIN B ON ... A一定是驱动表吗？不一定，优化器会根据你查询语句做优化，决定先查哪张表。先查询的那张表就是驱动表，反之就是被驱动表。通过explain关键字可以查看。一般来说，小表驱动大表。\n对于外连接来说： 1 2 3 SELECT * FROM A LEFT JOIN B ON ... # 或 SELECT * FROM B RIGHT JOIN A ON ... **通常，大家会认为A就是驱动表，B就是被驱动表。但也未必。**测试如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE TABLE a(f1 INT, f2 INT, INDEX(f1)) ENGINE=INNODB; CREATE TABLE b(f1 INT, f2 INT) ENGINE=INNODB; INSERT INTO a VALUES(1,1),(2,2),(3,3),(4,4),(5,5),(6,6); INSERT INTO b VALUES(3,3),(4,4),(5,5),(6,6),(7,7),(8,8); SELECT * FROM b; # 测试1 EXPLAIN SELECT * FROM a LEFT JOIN b ON(a.f1=b.f1) WHERE (a.f2=b.f2); # 测试2 EXPLAIN SELECT * FROM a LEFT JOIN b ON(a.f1=b.f1) AND (a.f2=b.f2); 当连接查询没有where条件时，左连接查询时，前面的表是驱动表，后面的表是被驱动表，右连接查询时相反，内连接查询时，哪张表的数据较少，哪张表就是驱动表 当连接查询有where条件时，不好说了 2. Simple Nested-Loop Join (简单嵌套循环连接) 算法相当简单，从表A中取出一条数据1，遍历表B，将匹配到的数据放到result.. 以此类推，驱动表A中的每一条记录与被驱动表B的记录进行判断：\n可以看到这种方式效率是非常低的，以上述表A数据100条，表B数据1000条计算，则A*B=10万次。开销统计如下:\n当然mysql肯定不会这么粗暴的去进行表的连接，所以就出现了后面的两种对Nested-Loop Join优化算法。\n可以看到，A越小，内表扫描次数就越少，性能越好，因此一般优化器小表驱动大表\n3. Index Nested-Loop Join （索引嵌套循环连接） Index Nested-Loop Join 其优化的思路主要是为了减少被驱动表数据的匹配次数，所以要求被驱动表上必须有索引才行。通过驱动表匹配条件直接与被驱动表索引进行匹配，避免和被驱动表的每条记录去进行比较，这样极大的减少了对内存表的匹配次数。\n驱动表中的每条记录通过被驱动表的索引进行访问，因为索引查询的成本是比较固定的，故mysql优化器都倾向于使用记录数少的表作为驱动表（外表）。\n如果被驱动表加索引，效率是非常高的，但如果索引不是主键索引，所以还得进行一次回表查询。相比，被驱动表的索引是主键索引，效率会更高。\n4. Block Nested-Loop Join（块嵌套循环连接） 注意：\n这里缓存的不只是关联表的列，select后面的列也会缓存起来。\n在一个有N个join关联的sql中会分配N-1个join buffer。所以查询的时候尽量减少不必要的字段，可以让join buffer中可以存放更多的列。\n参数设置：\nblock_nested_loop 通过show variables like '%optimizer_switch% 查看 block_nested_loop状态。默认是开启的。\njoin_buffer_size 驱动表能不能一次加载完，要看join buffer能不能存储所有的数据，默认情况下join_buffer_size=256k。\n1 mysql\u0026gt; show variables like \u0026#39;%join_buffer%\u0026#39;; join_buffer_size的最大值在32位操作系统可以申请4G，而在64位操作系统下可以申请大于4G的Join Buffer空间（64位Windows除外，其大值会被截断为4GB并发出警告）。\n5. Join小结 1、整体效率比较：INLJ \u0026gt; BNLJ \u0026gt; SNLJ\n2、永远用小结果集驱动大结果集（其本质就是减少外层循环的数据数量）（小的度量单位指的是表行数 * 每行大小）\n1 2 select t1.b,t2.* from t1 straight_join t2 on (t1.b=t2.b) where t2.id\u0026lt;=100; # 推荐 select t1.b,t2.* from t2 straight_join t1 on (t1.b=t2.b) where t2.id\u0026lt;=100; # 不推荐 3、为被驱动表匹配的条件增加索引(减少内层表的循环匹配次数)\n4、增大join buffer size的大小（一次索引的数据越多，那么内层表的扫描次数就越少）\n5、减少驱动表不必要的字段查询（字段越少，join buffer所缓存的数据就越多）\n6. Hash Join 从MySQL的8.0.20版本开始将废弃BNLJ，因为从MySQL8.0.18版本开始就加入了hash join，默认都会使用hash join\nNested Loop:\n对于被连接的数据子集较小的情况，Nested Loop是个较好的选择。\nHash Join是做大数据集连接时的常用方式，优化器使用两个表中较小（相对较小）的表利用Join Key在内存中建立散列表，然后扫描较大的表并探测散列表，找出与Hash表匹配的行。\n这种方式适合于较小的表完全可以放于内存中的情况，这样总成本就是访问两个表的成本之和。 在表很大的情况下并不能完全放入内存，这时优化器会将它分割成若干不同的分区，不能放入内存的部分就把该分区写入磁盘的临时段，此时要求有较大的临时段从而尽量提高I/O的性能。 它能够很好的工作于没有索引的大表和并行查询的环境中，并提供最好的性能。大多数人都说它是Join的重型升降机。Hash Join只能应用于等值连接（如WHERE A.COL1 = B.COL2），这是由Hash的特点决定的。 3.5 小结 保证被驱动表的JOIN字段已经创建了索引 需要 JOIN 的字段，数据类型保持绝对一致，避免类型隐式或显示转换，使索引失效。 LEFT JOIN 时，选择小表作为驱动表，大表作为被驱动表。减少外层循环的次数。 INNER JOIN 时，MySQL会自动将小结果集的表选为驱动表 。选择相信MySQL优化策略。 能够直接多表关联的尽量直接关联，不用子查询。(减少查询的趟数) 不建议使用子查询，建议将子查询SQL拆开结合程序多次查询，或使用 JOIN 来代替子查询。 衍生表建不了索引 4. 子查询优化 MySQL从4.1版本开始支持子查询，使用子查询可以进行SELECT语句的嵌套查询，即一个SELECT查询的结 果作为另一个SELECT语句的条件。 子查询可以一次性完成很多逻辑上需要多个步骤才能完成的SQL操作 。\n子查询是 MySQL 的一项重要的功能，可以帮助我们通过一个 SQL 语句实现比较复杂的查询。但是，子查询的执行效率不高。\n原因：\n① 执行子查询时，MySQL需要为内层查询语句的查询结果建立一个临时表，然后外层查询语句从临时表中查询记录。查询完毕后，再撤销这些临时表。这样会消耗过多的CPU和IO资源，产生大量的慢查询。\n② 子查询的结果集存储的临时表，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。\n③ 对于返回结果集比较大的子查询，其对查询性能的影响也就越大。\n在MySQL中，可以使用连接（JOIN）查询来替代子查询。\n连接查询不需要建立临时表，其速度比子查询要快，如果查询中使用索引的话，性能就会更好。\n举例1：查询学生表中是班长的学生信息\n使用子查询 1 2 3 4 5 6 7 8 9 # 创建班级表中班长的索引 CREATE INDEX idx_monitor ON class(monitor); EXPLAIN SELECT * FROM student stu1 WHERE stu1.`stuno` IN ( SELECT monitor FROM class c WHERE monitor IS NOT NULL # is not null 使用不到索引 ) 推荐使用多表查询 1 2 3 EXPLAIN SELECT stu1.* FROM student stu1 JOIN class c ON stu1.`stuno` = c.`monitor` WHERE c.`monitor` is NOT NULL; 举例2：取所有不为班长的同学\n不推荐 1 2 3 4 5 6 7 EXPLAIN SELECT SQL_NO_CACHE a.* FROM student a WHERE a.stuno NOT IN ( SELECT monitor FROM class b WHERE monitor IS NOT NULL ); SELECT a.* FROM student a JOIN class b ON a.stuno != b.monitor; # 不等于符号，容易让索引失效，giao 执行结果如下：\n推荐： 1 2 3 4 5 EXPLAIN SELECT SQL_NO_CACHE a.* FROM student a LEFT OUTER JOIN class b ON a.stuno = b.monitor WHERE b.monitor IS NULL; # 首先，保留student表所有记录，连接class表，使用等号 结论：尽量不要使用NOT IN或者NOT EXISTS，用LEFT JOIN xxx ON xx WHERE xx IS NULL替代\n5. 排序优化 5.1 排序优化 问题：在 WHERE 条件字段上加索引，但是为什么在 ORDER BY 字段上还要加索引呢？\n回答：\n在MySQL中，支持两种排序方式，分别是 FileSort 和 Index 排序。\nIndex 排序中，索引可以保证数据的有序性，不需要再进行排序，效率更高。 FileSort 排序则一般在 内存中 进行排序，占用CPU较多。如果待排结果较大，会产生临时文件 I/O 到磁盘进行排序的情况，效率较低。 优化建议：\nSQL 中，可以在 WHERE 子句和 ORDER BY 子句中使用索引，目的是在 WHERE 子句中 避免全表扫描 ，在 ORDER BY 子句 避免使用 FileSort 排序 。当然，某些情况下全表扫描，或者 FileSort 排序不一定比索引慢。但总的来说，我们还是要避免，以提高查询效率。 尽量使用 Index 完成 ORDER BY 排序。如果 WHERE 和 ORDER BY 后面是相同的列就使用单索引列； 如果不同就使用联合索引。 无法使用 Index 时，需要对 FileSort 方式进行调优。 5.2 测试 删除student表和class表中已创建的索引。\n1 2 3 4 5 6 7 8 9 10 # 方式1 DROP INDEX idx_monitor ON class; DROP INDEX idx_cid ON student; DROP INDEX idx_age ON student; DROP INDEX idx_name ON student; DROP INDEX idx_age_name_classId ON student; DROP INDEX idx_age_classId_name ON student; # 方式2 call proc_drop_index(\u0026#39;atguigudb2\u0026#39;,\u0026#39;student\u0026#39;;) 以下是否能使用到索引，能否去掉using filesort\n过程一：\n过程二： order by 时不limit,索引失效\n过程三：order by 时顺序错误，索引失效\n过程四：order by 时规则不一致，索引失效（顺序错，不索引；方向反，不索引）\n结论：ORDER BY 子句，尽量使用 Index 方式排序，避免使用 FileSort 方式排序\n过程五：无过滤，不索引\n小结\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 INDEX a_b_c(a,b,c) order by 能使用索引最左前缀 - ORDER BY a - ORDER BY a,b - ORDER BY a,b,c - ORDER BY a DESC,b DESC,c DESC 如果 WHERE 使用索引的最左前缀定义为常量，则 order by 能使用索引 - WHERE a = const ORDER BY b,c - WHERE a = const AND b = const ORDER BY c - WHERE a = const ORDER BY b,c - WHERE a = const AND b \u0026gt; const ORDER BY b,c 不能使用索引进行排序 - ORDER BY a ASC,b DESC,c DESC /* 排序不一致 */ - WHERE g = const ORDER BY b,c /*丢失a索引*/ - WHERE a = const ORDER BY c /*丢失b索引*/ - WHERE a = const ORDER BY a,d /*d不是索引的一部分*/ - WHERE a in (...) ORDER BY b,c /*对于排序来说，多个相等条件也是范围查询*/ 5.3 案例实战 ORDER BY子句，尽量使用Index方式排序，避免使用FileSort方式排序。\n执行案例前先清除student上的索引，只留主键：\n1 2 3 4 5 6 DROP INDEX idx_age ON student; DROP INDEX idx_age_classid_stuno ON student; DROP INDEX idx_age_classid_name ON student; #或者 call proc_drop_index(\u0026#39;atguigudb2\u0026#39;,\u0026#39;student\u0026#39;); 场景:查询年龄为30岁的，且学生编号小于101000的学生，按用户名称排序\n1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u0026lt;101000 ORDER BY NAME; 查询结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 mysql\u0026gt; SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u0026lt;101000 ORDER BY NAME; +---------+--------+--------+------+---------+ | id | stuno | name | age | classId | +---------+--------+--------+------+---------+ | 922 | 100923 | elTLXD | 30 | 249 | | 3723263 | 100412 | hKcjLb | 30 | 59 | | 3724152 | 100827 | iHLJmh | 30 | 387 | | 3724030 | 100776 | LgxWoD | 30 | 253 | | 30 | 100031 | LZMOIa | 30 | 97 | | 3722887 | 100237 | QzbJdx | 30 | 440 | | 609 | 100610 | vbRimN | 30 | 481 | | 139 | 100140 | ZqFbuR | 30 | 351 | +---------+--------+--------+------+---------+ 8 rows in set, 1 warning (3.16 sec) 结论：type 是 ALL，即最坏的情况。Extra 里还出现了 Using filesort,也是最坏的情况。优化是必须的。\n方案一: 为了去掉filesort我们可以把索引建成\n1 2 #创建新索引 CREATE INDEX idx_age_name ON student(age,NAME); 1 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u0026lt;101000 ORDER BY NAME; 这样我们优化掉了 using filesort\n查询结果如下：\n方案二：尽量让where的过滤条件和排序使用上索引\n建一个三个字段的组合索引：\n1 2 3 4 DROP INDEX idx_age_name ON student; CREATE INDEX idx_age_stuno_name ON student (age,stuno,NAME); EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u0026lt;101000 ORDER BY NAME; # 范围查询，右边的 name 列索引失效，排序为filesort，但是由于where条件过滤了很多数据，剩余数据量不大，所以是否filesort对处理的效率影响不大 我们发现using filesort依然存在，所以name并没有用到索引，而且type还是range光看名字其实并不美好。原因是，因为stuno是一个范围过滤，所以索引后面的字段不会在使用索引了。\n结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 mysql\u0026gt; SELECT SQL_NO_CACHE * FROM student -\u0026gt; WHERE age = 30 AND stuno \u0026lt;101000 ORDER BY NAME; +-----+--------+--------+------+---------+ | id | stuno | name | age | classId | +-----+--------+--------+------+---------+ | 167 | 100168 | AClxEF | 30 | 319 | | 323 | 100324 | bwbTpQ | 30 | 654 | | 651 | 100652 | DRwIac | 30 | 997 | | 517 | 100518 | HNSYqJ | 30 | 256 | | 344 | 100345 | JuepiX | 30 | 329 | | 905 | 100906 | JuWALd | 30 | 892 | | 574 | 100575 | kbyqjX | 30 | 260 | | 703 | 100704 | KJbprS | 30 | 594 | | 723 | 100724 | OTdJkY | 30 | 236 | | 656 | 100657 | Pfgqmj | 30 | 600 | | 982 | 100983 | qywLqw | 30 | 837 | | 468 | 100469 | sLEKQW | 30 | 346 | | 988 | 100989 | UBYqJl | 30 | 457 | | 173 | 100174 | UltkTN | 30 | 830 | | 332 | 100333 | YjWiZw | 30 | 824 | +-----+--------+--------+------+---------+ 15 rows in set, 1 warning (0.00 sec) 结果竟然有 filesort 的 sql 运行速度， 超过了已经优化掉 filesort 的 sql ，而且快了很多，几乎一瞬间就出现了结果。\n原因：\n结论：\n两个索引同时存在，mysql自动选择最优的方案。（对于这个例子，mysql选择 idx_age_stuno_name）。但是，随着数据量的变化，选择的索引也会随之变化的 。 当【范围条件】和【group by 或者 order by】的字段出现二选一时，优先观察条件字段的过滤数量，如果过滤的数据足够多，而需要排序的数据并不多时，优先把索引放在范围字段上。反之，亦然。 思考：这里我们使用如下索引，是否可行？\n1 2 3 DROP INDEX idx_age_stuno_name ON student; CREATE INDEX idx_age_stuno ON student(age,stuno); 当然可以。\n5.4 filesort算法：双路排序和单路排序 排序的字段若不在索引列上，则filesort会有两种算法：双路排序和单路排序\n双路排序 （慢）\nMySQL 4.1之前是使用双路排序 ，字面意思就是两次扫描磁盘，最终得到数据，读取行指针和 order by 列 ，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出 从磁盘取排序字段，在buffer进行排序，再从磁盘取其他字段。 取一批数据，要对磁盘进行两次扫描，众所周知，IO是很耗时的，所以在mysql4.1之后，出现了第二种改进的算法，就是单路排序。\n单路排序 （快）\n从磁盘读取查询需要的所有列 ，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了。\n结论及引申出的问题\n由于单路是后出的，总体而言好过双路 但是用单路有问题 在sort_buffer中，单路要比多路多占用很多空间，因为单路是把所有字段都取出，所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取sort_buffer容量大小，再排\u0026hellip;从而多次I/O。 单路本来想省一次I/O操作，反而导致了大量的I/O操作，反而得不偿失。 优化策略\n1. 尝试提高 sort_buffer_size\n2. 尝试提高 max_length_for_sort_data\n3. Order by 时select * 是一个大忌。最好只Query需要的字段。\n6. GROUP BY优化 group by 使用索引的原则几乎跟order by一致 ，group by 即使没有过滤条件用到索引，也可以直接使用索引。 group by 先排序再分组，遵照索引建的最佳左前缀法则 当无法使用索引列，增大 max_length_for_sort_data 和 sort_buffer_size 参数的设置 where效率高于having，能写在where限定的条件就不要写在having中了（having里可以使用聚合函数，where不可以 减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。Order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。 包含了order by、group by、distinct 这些查询的语句，where条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。 7. 优化分页查询 limit offset,row分页常用语句。当offset比较小的时候，影响不大；但是比较大的时候，也就是一些用户翻到了后面的数据页了。\n如limit 2000000,10，意味着需要先扫描2000010这么多行数据，但是只会丢弃前面2000000而取后面十条数据，这样分页效率低下。\n可以考虑使用子查询分页、join连接分页或者limit查询转化为某个位置的查询（当然这种情况还是不太适用）\n优化思路一\n在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。\n1 EXPLAIN SELECT * FROM student t,(SELECT id FROM student ORDER BY id LIMIT 2000000,10) a WHERE t.id = a.id; 优化思路二\n该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询。\n1 EXPLAIN SELECT * FROM student WHERE id \u0026gt; 2000000 LIMIT 10; 8. 优先考虑覆盖索引 8.1 什么是覆盖索引？ 理解方式一：索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据；当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个索引包含了满足查询结果的数据就叫做覆盖索引（省掉了回表过程）。\n理解方式二：非聚簇复合索引的一种形式，它包括在查询里的SELECT、JOIN和WHERE子句用到的所有列 （即建索引的字段正好是覆盖查询条件中所涉及的字段）。\n简单说就是，索引列+主键 包含 SELECT 到 FROM之间查询的列 。\n举例一：\n1 2 3 4 # 删除之前的索引 DROP INDEX idx_age_stuno ON student; CREATE INDEX idx_age_name ON student(age, NAME); EXPLAIN SELECT * FROM student WHERE age \u0026lt;\u0026gt; 20; 举例二：\n1 EXPLAIN SELECT * FROM student WHERE NAME LIKE \u0026#39;%abc\u0026#39;; 1 2 CREATE INDEX idx_age_name ON student(age, NAME); EXPLAIN SELECT id,age,NAME FROM student WHERE NAME = \u0026#39;abc\u0026#39;; 上述都使用到了声明的索引，下面的情况则不然，查询列依然多了classId,结果是未使用到索引：\n1 EXPLAIN SELECT id,age,NAME,classId FROM student WHERE NAME LIKE \u0026#39;%abc\u0026#39;; 8.2 覆盖索引的利弊 9. 如何给字符串添加索引 有一张教师表，表定义如下：\n1 2 3 4 5 create table teacher( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 讲师要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：\n1 mysql\u0026gt; select col1, col2 from teacher where email=\u0026#39;xxx\u0026#39;; 如果email这个字段上没有索引，那么这个语句就只能做 全表扫描 。\n9.1 前缀索引 MySQL是支持前缀索引的。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。\n1 2 3 mysql\u0026gt; alter table teacher add index index1(email); #或 mysql\u0026gt; alter table teacher add index index2(email(6)); 这两种不同的定义在数据结构和存储上有什么区别呢？下图就是这两个索引的示意图。\n以及\n如果使用的是index1（即email整个字符串的索引结构），执行顺序是这样的：\n从index1索引树找到满足索引值是zhangssxyz@xxx.com的这条记录，取得ID2的值； 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集； 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email=zhangssxyz@xxx.com的 条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。\n如果使用的是index2（即email(6)索引结构），执行顺序是这样的：\n从index2索引树找到满足索引值是zhangs的记录，找到的第一个是ID1； 到主键上查到主键值是ID1的行，判断出email的值不是zhangssxyz@xxx.com，这行记录丢弃； 取index2上刚刚查到的位置的下一条记录，发现仍然是zhangs，取出ID2，再到ID索引上取整行然后判断，这次值对了，将这行记录加入结果集； 重复上一步，直到在idxe2上取到的值不是zhangs时，循环结束。 也就是说**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**前面已经讲过区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。\n9.2 前缀索引对覆盖索引的影响 结论： 使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。\n10. 索引下推 10.1 使用前后对比 Index Condition Pushdown(ICP) 是MySQL 5.6中新特性，是一种在存储引擎层使用索引过滤数据的一种优化方式。\n10.2 ICP的开启/关闭 默认情况下启动索引条件下推。可以通过设置系统变量optimizer_switch控制：index_condition_pushdown 1 2 3 4 5 # 打开索引下推 SET optimizer_switch = \u0026#39;index_condition_pushdown=on\u0026#39;; # 关闭索引下推 SET optimizer_switch = \u0026#39;index_condition_pushdown=off\u0026#39;; 当使用索引条件下推是，EXPLAIN语句输出结果中Extra列内容显示为Using index condition。 10.3 ICP使用案例 主键索引 (简图) 二级索引zip_last_first (简图，这里省略了数据页等信息)\n10.4 开启和关闭ICP性能对比 10.5 ICP的使用条件 如果表的访问类型为 range 、 ref 、 eq_ref 或者 ref_or_null 可以使用ICP。 ICP可以使用InnDB和MyISAM表，包括分区表InnoDB和MyISAM表 对于InnoDB表，ICP仅用于二级索引。ICP的目标是减少全行读取次数，从而减少I/O操作。 当SQL使用覆盖索引时，不支持ICP优化方法。因为这种情况下使用ICP不会减少I/O。 相关子查询的条件不能使用ICP 11. 普通索引 vs 唯一索引 从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢？\n假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引，假设字段 k 上的值都不重复。\n这个表的建表语句是：\n1 2 3 4 5 6 mysql\u0026gt; create table test( id int primary key, k int not null, name varchar(16), index (k) )engine=InnoDB; 表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)。\n11.1 查询过程 假设，执行查询的语句是 select id from test where k=5。\n对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。\n11.2 更新过程 为了说明普通索引和唯一索引对更新语句性能的影响这个问题，介绍一下change buffer。\n当需要更新一个数据页时，如果数据页在内存中,就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下， InooDB会将这些更新操作缓存在change buffer中 ，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。\n将change buffer中的操作应用到原数据页，得到最新结果的过程称为 merge 。除了 访问这个数据页 会触发merge外，系统有 后台线程会定期 merge。在 数据库正常关闭（shutdown） 的过程中，也会执行merge操作。\n如果能够将更新操作先记录在change buffer， 减少读磁盘 ，语句的执行速度会得到明显的提升。而且， 数据读入内存是需要占用 buffer pool 的，所以这种方式还能够 避免占用内存 ，提高内存利用率。\n唯一索引的更新就不能使用change buffer ，实际上也只有普通索引可以使用。\n如果要在这张表中插入一个新记录(4,400)的话，InnoDB的处理流程是怎样的？\n11.3 change buffer的使用场景 普通索引和唯一索引应该怎么选择？其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，建议你尽量选择普通索引 。 在实际使用中会发现，普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。 如果所有的更新后面，都马上伴随着对这个记录的查询 ，那么你应该 关闭change buffer 。而在 其他情况下，change buffer都能提升更新性能。 由于唯一索引用不上change buffer的优化机制，因此如果业务可以接受，从性能角度出发建议优先考虑非唯一索引。但是如果\u0026quot;业务可能无法确保\u0026quot;的情况下，怎么处理呢？ 首先，业务正确性优先。我们的前提是“业务代码已经保证不会写入重复数据”的情况下，讨论性能问题。如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选，必须创建唯一索引。这种情况下，本节的意义在于，如果碰上了大量插入数据慢、内存命中率低的时候，给你多提供一个排查思路。 然后，在一些“ 归档库 ”的场景，你是可以考虑使用唯一索引的。比如，线上数据只需要保留半年， 然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。 12. 其它查询优化策略 12.1 EXISTS 和 IN 的区分 问题：\n不太理解哪种情况下应该使用 EXISTS，哪种情况应该用 IN。选择的标准是看能否使用表的索引吗？\n回答：\n12.2 COUNT(*)与COUNT(具体字段)效率 问：在 MySQL 中统计数据表的行数，可以使用三种方式： SELECT COUNT(*) 、 SELECT COUNT(1) 和 SELECT COUNT(具体字段) ，使用这三者之间的查询效率是怎样的？\n答：\n12.3 关于SELECT(*) 在表查询中，建议明确字段，不要使用 * 作为查询的字段列表，推荐使用SELECT \u0026lt;字段列表\u0026gt; 查询。原因：\n① MySQL 在解析的过程中，会通过查询数据字典 将\u0026quot;*\u0026ldquo;按序转换成所有列名，这会大大的耗费资源和时间。\n② 无法使用覆盖索引\n12.4 LIMIT 1 对优化的影响 针对的是会扫描全表的 SQL 语句，如果你可以确定结果集只有一条，那么加上 LIMIT 1 的时候，当找到一条结果的时候就不会继续扫描了，这样会加快查询速度。\n如果数据表已经对字段建立了唯一索引，那么可以通过索引进行查询，不会全表扫描的话，就不需要加上 LIMIT 1 了。\n12.5 多使用COMMIT 只要有可能，在程序中尽量多使用 COMMIT，这样程序的性能得到提高，需求也会因为 COMMIT 所释放 的资源而减少。\nCOMMIT 所释放的资源：\n回滚段上用于恢复数据的信息 被程序语句获得的锁 redo / undo log buffer 中的空间 管理上述 3 种资源中的内部花费 13. 淘宝数据库，主键如何设计的？ 聊一个实际问题：淘宝的数据库，主键是如何设计的？\n某些错的离谱的答案还在网上年复一年的流传着，甚至还成为了所谓的MySQL军规。其中，一个最明显的错误就是关于MySQL的主键设计。\n大部分人的回答如此自信：用8字节的 BIGINT 做主键，而不要用INT。 错 ！\n这样的回答，只站在了数据库这一层，而没有 从业务的角度 思考主键。主键就是一个自增ID吗？站在 2022年的新年档口，用自增做主键，架构设计上可能 连及格都拿不到 。\n13.1 自增ID的问题 自增ID做主键，简单易懂，几乎所有数据库都支持自增类型，只是实现上各自有所不同而已。自增ID除了简单，其他都是缺点，总体来看存在以下几方面的问题：\n可靠性不高\n存在自增ID回溯的问题，这个问题直到最新版本的MySQL 8.0才修复。\n**安全性不高 **\n对外暴露的接口可以非常容易猜测对应的信息。比如：/User/1/这样的接口，可以非常容易猜测用户ID的 值为多少，总用户数量有多少，也可以非常容易地通过接口进行数据的爬取。\n性能差\n自增ID的性能较差，需要在数据库服务器端生成。\n交互多\n业务还需要额外执行一次类似 last_insert_id() 的函数才能知道刚才插入的自增值，这需要多一次的 网络交互。在海量并发的系统中，多1条SQL，就多一次性能上的开销。\n**局部唯一性 **\n最重要的一点，自增ID是局部唯一，只在当前数据库实例中唯一，而不是全局唯一，在任意服务器间都是唯一的。对于目前分布式系统来说，这简直就是噩梦。\n13.2 业务字段做主键 为了能够唯一地标识一个会员的信息，需要为会员信息表设置一个主键。那么，怎么为这个表设置主键，才能达到我们理想的目标呢？这里我们考虑业务字段做主键。\n表数据如下：\n在这个表里，哪个字段比较合适呢？\n选择卡号（cardno） 会员卡号（cardno）看起来比较合适，因为会员卡号不能为空，而且有唯一性，可以用来标识一条会员记录。\n1 2 3 4 5 6 7 8 9 10 11 mysql\u0026gt; CREATE TABLE demo.membermaster -\u0026gt; ( -\u0026gt; cardno CHAR(8) PRIMARY KEY, -- 会员卡号为主键 -\u0026gt; membername TEXT, -\u0026gt; memberphone TEXT, -\u0026gt; memberpid TEXT, -\u0026gt; memberaddress TEXT, -\u0026gt; sex TEXT, -\u0026gt; birthday DATETIME -\u0026gt; ); Query OK, 0 rows affected (0.06 sec) 不同的会员卡号对应不同的会员，字段“cardno”唯一地标识某一个会员。如果都是这样，会员卡号与会员一一对应，系统是可以正常运行的。\n但实际情况是，会员卡号可能存在重复使用的情况。比如，张三因为工作变动搬离了原来的地址，不再到商家的门店消费了 （退还了会员卡），于是张三就不再是这个商家门店的会员了。但是，商家不想让这个会员卡空着，就把卡号是“10000001”的会员卡发给了王五。\n从系统设计的角度看，这个变化只是修改了会员信息表中的卡号是“10000001”这个会员信息，并不会影响到数据一致性。也就是说，修改会员卡号是“10000001”的会员信息，系统的各个模块，都会获取到修改后的会员信息，不会出现“有的模块获取到修改之前的会员信息，有的模块获取到修改后的会员信息，而导致系统内部数据不一致”的情况。因此，从 信息系统层面 上看是没问题的。\n但是从使用 系统的业务层面 来看，就有很大的问题 了，会对商家造成影响。\n比如，我们有一个销售流水表（trans），记录了所有的销售流水明细。2020 年 12 月 01 日，张三在门店 购买了一本书，消费了 89 元。那么，系统中就有了张三买书的流水记录，如下所示：\n接着，我们查询一下 2020 年 12 月 01 日的会员销售记录：\n1 2 3 4 5 6 7 8 9 10 11 mysql\u0026gt; SELECT b.membername,c.goodsname,a.quantity,a.salesvalue,a.transdate -\u0026gt; FROM demo.trans AS a -\u0026gt; JOIN demo.membermaster AS b -\u0026gt; JOIN demo.goodsmaster AS c -\u0026gt; ON (a.cardno = b.cardno AND a.itemnumber=c.itemnumber); +------------+-----------+----------+------------+---------------------+ | membername | goodsname | quantity | salesvalue | transdate | +------------+-----------+----------+------------+---------------------+ | 张三 | 书 | 1.000 | 89.00 | 2020-12-01 00:00:00 | +------------+-----------+----------+------------+---------------------+ 1 row in set (0.00 sec) 如果会员卡“10000001”又发给了王五，我们会更改会员信息表。导致查询时：\n1 2 3 4 5 6 7 8 9 10 11 mysql\u0026gt; SELECT b.membername,c.goodsname,a.quantity,a.salesvalue,a.transdate -\u0026gt; FROM demo.trans AS a -\u0026gt; JOIN demo.membermaster AS b -\u0026gt; JOIN demo.goodsmaster AS c -\u0026gt; ON (a.cardno = b.cardno AND a.itemnumber=c.itemnumber); +------------+-----------+----------+------------+---------------------+ | membername | goodsname | quantity | salesvalue | transdate | +------------+-----------+----------+------------+---------------------+ | 王五 | 书 | 1.000 | 89.00 | 2020-12-01 00:00:00 | +------------+-----------+----------+------------+---------------------+ 1 row in set (0.01 sec) 这次得到的结果是：王五在 2020 年 12 月 01 日，买了一本书，消费 89 元。显然是错误的！结论：千万不能把会员卡号当做主键。\n选择会员电话 或 身份证号 会员电话可以做主键吗？不行的。在实际操作中，手机号也存在被运营商收回 ，重新发给别人用的情况。\n那身份证号行不行呢？好像可以。因为身份证决不会重复，身份证号与一个人存在一一对 应的关系。可问题是，身份证号属于个人隐私 ，顾客不一定愿意给你。要是强制要求会员必须登记身份证号，会把很多客人赶跑的。其实，客户电话也有这个问题，这也是我们在设计会员信息表的时候，允许身份证号和 电话都为空的原因。\n所以，建议尽量不要用跟业务有关的字段做主键。毕竟，作为项目设计的技术人员，我们谁也无法预测 在项目的整个生命周期中，哪个业务字段会因为项目的业务需求而有重复，或者重用之类的情况出现。\n经验： 刚开始使用 MySQL 时，很多人都很容易犯的错误是喜欢用业务字段做主键，想当然地认为了解业务需求，但实际情况往往出乎意料，而更改主键设置的成本非常高。\n13.3 淘宝的主键设计 在淘宝的电商业务中，订单服务是一个核心业务。请问， 订单表的主键 淘宝是如何设计的呢？是自增ID 吗？\n打开淘宝，看一下订单信息：\n从上图可以发现，订单号不是自增ID！我们详细看下上述4个订单号：\n1 2 3 4 1550672064762308113 1481195847180308113 1431156171142308113 1431146631521308113 订单号是19位的长度，且订单的最后5位都是一样的，都是08113。且订单号的前面14位部分是单调递增的。\n大胆猜测，淘宝的订单ID设计应该是：\n1 订单ID = 时间 + 去重字段 + 用户ID后6位尾号 这样的设计能做到全局唯一，且对分布式系统查询及其友好。\n13.4 推荐的主键设计 非核心业务 ：对应表的主键自增ID，如告警、日志、监控等信息。\n核心业务 ：主键设计至少应该是全局唯一且是单调递增。全局唯一保证在各系统之间都是唯一的，单调递增是希望插入时不影响数据库性能（避免聚簇索引里的数据页的分裂）。\n这里推荐最简单的一种主键设计：UUID。\nUUID的特点：\n全局唯一，占用36字节，数据无序，插入性能差。\n认识UUID：\n为什么UUID是全局唯一的？ 为什么UUID占用36个字节？ 为什么UUID是无序的？ MySQL数据库的UUID组成如下所示：\n1 UUID = 时间+UUID版本（16字节）- 时钟序列（4字节） - MAC地址（12字节） 我们以UUID值e0ea12d4-6473-11eb-943c-00155dbaa39d举例：\n为什么UUID是全局唯一的？\n在UUID中时间部分占用60位，存储的类似TIMESTAMP的时间戳，但表示的是从1582-10-15 00：00：00.00 到现在的100ns的计数。可以看到UUID存储的时间精度比TIMESTAMPE更高，时间维度发生重复的概率降 低到1/100ns。\n时钟序列是为了避免时钟被回拨导致产生时间重复的可能性。MAC地址用于全局唯一。\n为什么UUID占用36个字节？\nUUID根据字符串进行存储，设计时还带有无用\u0026rdquo;-\u0026ldquo;字符串，因此总共需要36个字节。\n为什么UUID是随机无序的呢？\n因为UUID的设计中，将时间低位放在最前面，而这部分的数据是一直在变化的，并且是无序。\n改造UUID\n若将时间高低位互换，则时间就是单调递增的了，也就变得单调递增了。MySQL 8.0可以更换时间低位和时间高位的存储方式，这样UUID就是有序的UUID了。\nMySQL 8.0还解决了UUID存在的空间占用的问题，除去了UUID字符串中无意义的\u0026rdquo;-\u0026ldquo;字符串，并且将字符串用二进制类型保存，这样存储空间降低为了16字节。\n可以通过MySQL8.0提供的uuid_to_bin函数实现上述功能，同样的，MySQL也提供了bin_to_uuid函数进行转化：\n1 2 SET @uuid = UUID(); SELECT @uuid,uuid_to_bin(@uuid),uuid_to_bin(@uuid,TRUE); 通过函数uuid_to_bin(@uuid,true)将UUID转化为有序UUID了。全局唯一 + 单调递增，这不就是我们想要的主键！\n有序UUID性能测试\n16字节的有序UUID，相比之前8字节的自增ID，性能和存储空间对比究竟如何呢？\n我们来做一个测试，插入1亿条数据，每条数据占用500字节，含有3个二级索引，最终的结果如下所示：\n从上图可以看到插入1亿条数据有序UUID是最快的，而且在实际业务使用中有序UUID在 业务端就可以生成 。还可以进一步减少SQL的交互次数。\n另外，虽然有序UUID相比自增ID多了8个字节，但实际只增大了3G的存储空间，还可以接受。\n在当今的互联网环境中，非常不推荐自增ID作为主键的数据库设计。更推荐类似有序UUID的全局唯一的实现。\n另外在真实的业务系统中，主键还可以加入业务和系统属性，如用户的尾号，机房的信息等。这样的主键设计就更为考验架构师的水平了。\n如果不是MySQL8.0 肿么办？\n手动赋值字段做主键！\n比如，设计各个分店的会员表的主键，因为如果每台机器各自产生的数据需要合并，就可能会出现主键重复的问题。\n可以在总部 MySQL 数据库中，有一个管理信息表，在这个表中添加一个字段，专门用来记录当前会员编号的最大值。\n门店在添加会员的时候，先到总部 MySQL 数据库中获取这个最大值，在这个基础上加 1，然后用这个值 作为新会员的“id”，同时，更新总部 MySQL 数据库管理信息表中的当前会员编号的最大值。\n这样一来，各个门店添加会员的时候，都对同一个总部 MySQL 数据库中的数据表字段进行操作，就解决了各门店添加会员时会员编号冲突的问题。\n第11章_数据库的设计规范 1. 为什么需要数据库设计 2. 范 式 2.1 范式简介 在关系型数据库中，关于数据表设计的基本原则、规则就称为范式。可以理解为，一张数据表的设计结构需要满足的某种设计标准的级别。要想设计一个结构合理的关系型数据库，必须满足一定的范式。\n2.2 范式都包括哪些 目前关系型数据库有六种常见范式，按照范式级别，从低到高分别是：第一范式（1NF）、第二范式 （2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。\n数据库的范式设计越高阶，夯余度就越低，同时高阶的范式一定符合低阶范式的要求，满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范的要求称为第二范式（2NF），其余范式以此类推。\n一般来说，在关系型数据库设计中，最高也就遵循到BCNF, 普遍还是3NF。但也不绝对，有时候为了提高某些查询性能，我们还需要破坏范式规则，也就是反规范化。\n2.3 键和相关属性的概念 举例:\n这里有两个表：\n球员表(player) ：球员编号 | 姓名 | 身份证号 | 年龄 | 球队编号\n球队表(team) ：球队编号 | 主教练 | 球队所在地\n超键 ：对于球员表来说，超键就是包括 球员编号 或者 身份证号 的任意组合，比如（球员编号）、（球员编号，姓名）、（身份证号，年龄）等。 候选键 ：就是最小的超键，对于球员表来说，候选键就是（球员编号）或者（身份证号）。 主键 ：我们自己选定，也就是从候选键中选择一个，比如（球员编号），那么另一个候选键就不是主键了。 外键 ：球员表中的球队编号（球队表的主键）。 主属性 、 非主属性 ：在球员表中，主属性是（球员编号）（身份证号），其他的属性（姓名） （年龄）（球队编号）都是非主属性。 2.4 第一范式(1st NF) 第一范式主要确保数据库中每个字段的值必须具有原子性，也就是说数据表中每个字段的值为不可再次拆分的最小数据单元。\n我们在设计某个字段的时候，对于字段X来说，不能把字段X拆分成字段X-1和字段X-2。事实上，任何的DBMS都会满足第一范式的要求，不会将字段进行拆分。\n举例1：\n假设一家公司要存储员工的姓名和联系方式。它创建一个如下表：\n该表不符合 1NF ，因为规则说“表的每个属性必须具有原子（单个）值”，lisi 和 zhaoliu 员工的 emp_mobile 值违反了该规则。为了使表符合 1NF ，我们应该有如下表数据：\n举例2：\nuser 表的设计不符合第一范式\n其中，user_info 字段为用户信息，可以进一步拆分成更小粒度的字段，不符合数据库设计对第一范式的 要求。将 user_info 拆分后如下：\n举例3：\n**属性的原子性是主观的。**例如，Employees关系中雇员姓名应当使用1个（fullname）、2个（firstname 和lastname）还是3个（firstname、middlename和lastname）属性表示呢？**答案取决于应用程序。**如果应用程序需要分别处理雇员的姓名部分（如：用于搜索目的），则有必要把它们分开。否则，不需要。\n表1：\n表2：\n2.5 第二范式(2nd NF) 第二范式要求，在满足第一范式的基础上，还要满足数据库里的每一条数据记录，都是可唯一标识的。而且所有非主键字段，都必须完全依赖主键，不能只依赖主键的一部分。如果知道主键的所有属性的值，就可以检索到任何元组（行）的任何属性的任何值。（要求中的主键，其实可以扩展替换为候选键）。\n举例1：\n成绩表（学号，课程号，成绩）关系中，（学号，课程号）可以决定成绩，但是学号不能决定成绩，课程号也不能决定成绩，所以“（学号，课程号）→成绩”就是完全依赖关系。\n举例2：\n比赛表 player_game，里面包含 球员编号、姓名、年龄、比赛编号、比赛时间和比赛场地 等属性，这里候选键和主键都为（球员编号，比赛编号），我们可以通过候选键（或主键）来决定如下的关系：\n1 (球员编号, 比赛编号) → (姓名, 年龄, 比赛时间, 比赛场地，得分) 但是这个数据表不满足第二范式，因为数据表中的字段之间还存在着如下的对应关系：\n1 2 3 (球员编号) → (姓名，年龄) (比赛编号) → (比赛时间, 比赛场地) 对于非主属性来说，并非完全依赖候选键。这样会产生怎样的问题呢？\n数据冗余 ：如果一个球员可以参加 m 场比赛，那么球员的姓名和年龄就重复了 m-1 次。一个比赛 也可能会有 n 个球员参加，比赛的时间和地点就重复了 n-1 次。 插入异常 ：如果我们想要添加一场新的比赛，但是这时还没有确定参加的球员都有谁，那么就没法插入。 删除异常 ：如果我要删除某个球员编号，如果没有单独保存比赛表的话，就会同时把比赛信息删 除掉。 更新异常 ：如果我们调整了某个比赛的时间，那么数据表中所有这个比赛的时间都需要进行调 整，否则就会出现一场比赛时间不同的情况。 为了避免出现上述的情况，我们可以把球员比赛表设计为下面的三张表。\n这样的话，每张数据表都符合第二范式，也就避免了异常情况的发生。\n1NF 告诉我们字段属性需要是原子性的，而 2NF 告诉我们一张表就是一个独立的对象，一张表只表达一个意思。\n举例3：\n定义了一个名为 Orders 的关系，表示订单和订单行的信息：\n违反了第二范式，因为有非主键属性仅依赖于候选键（或主键）的一部分。例如，可以仅通过orderid找 到订单的 orderdate，以及 customerid 和 companyname，而没有必要再去使用productid。\n修改：\nOrders表和OrderDetails表如下，此时符合第二范式。\n小结：第二范式（2NF）要求实体的属性完全依赖主关键字。如果存在不完全依赖，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与元实体之间是一对多的关系。\n2.6 第三范式(3rd NF) 第三范式是在第二范式的基础上，确保数据表中的每一个非主键字段都和主键字段直接相关，也就是说，要求数据表中的所有非主键字段不能依赖于其他非主键字段。（即，不能存在非主属性A依赖于非主属性B，非主属性B依赖于主键C的情况，即存在“A-\u0026gt;B-\u0026gt;C\u0026quot;的决定关系）通俗地讲，该规则的意思是所有非主键属性之间不能由依赖关系，必须相互独立。\n这里的主键可以扩展为候选键。\n举例1：\n部门信息表 ：每个部门有部门编号（dept_id）、部门名称、部门简介等信息。\n员工信息表 ：每个员工有员工编号、姓名、部门编号。列出部门编号后就不能再将部门名称、部门简介 等与部门有关的信息再加入员工信息表中。\n如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。\n举例2：\n商品类别名称依赖于商品类别编号，不符合第三范式。\n修改：\n表1：符合第三范式的 商品类别表 的设计\n表2：符合第三范式的 商品表 的设计\n商品表goods通过商品类别id字段（category_id）与商品类别表goods_category进行关联。\n举例3：\n球员player表 ：球员编号、姓名、球队名称和球队主教练。现在，我们把属性之间的依赖关系画出来，如下图所示:\n你能看到球员编号决定了球队名称，同时球队名称决定了球队主教练，非主属性球队主教练就会传递依 赖于球员编号，因此不符合 3NF 的要求。\n如果要达到 3NF 的要求，需要把数据表拆成下面这样：\n举例4：\n修改第二范式中的举例3。\n此时的Orders关系包含 orderid、orderdate、customerid 和 companyname 属性，主键定义为 orderid。 customerid 和companyname均依赖于主键——orderid。例如，你需要通过orderid主键来查找代表订单中 客户的customerid，同样，你需要通过 orderid 主键查找订单中客户的公司名称（companyname）。然 而， customerid和companyname也是互相依靠的。为满足第三范式，可以改写如下：\n符合3NF后的数据模型通俗地讲，2NF和3NF通常以这句话概括：“每个非键属性依赖于键，依赖于 整个键，并且除了键别无他物”。\n2.7 小结 3. 反范式化 3.1 概述 规范化 vs 性能\n为满足某种商业目标 , 数据库性能比规范化数据库更重要 在数据规范化的同时 , 要综合考虑数据库的性能 通过在给定的表中添加额外的字段，以大量减少需要从中搜索信息所需的时间 通过在给定的表中插入计算列，以方便查询 3.2 应用举例 举例1：\n员工的信息存储在 employees 表 中，部门信息存储在 departments 表 中。通过 employees 表中的 department_id字段与 departments 表建立关联关系。如果要查询一个员工所在部门的名称：\n1 2 3 select employee_id,department_name from employees e join departments d on e.department_id = d.department_id; 如果经常需要进行这个操作，连接查询就会浪费很多时间。可以在 employees 表中增加一个冗余字段 department_name，这样就不用每次都进行连接操作了。\n举例2：\n反范式化的 goods商品信息表 设计如下：\n举例3：\n我们有 2 个表，分别是 商品流水表（atguigu.trans ）和 商品信息表 （atguigu.goodsinfo） 。商品流水表里有 400 万条流水记录，商品信息表里有 2000 条商品记录。\n商品流水表：\n商品信息表：\n新的商品流水表如下所示：\n举例4：\n课程评论表 class_comment ，对应的字段名称及含义如下：\n学生表 student ，对应的字段名称及含义如下：\n在实际应用中，我们在显示课程评论的时候，通常会显示这个学生的昵称，而不是学生 ID，因此当我们 想要查询某个课程的前 1000 条评论时，需要关联 class_comment 和 student这两张表来进行查询。\n实验数据：模拟两张百万量级的数据表\n为了更好地进行 SQL 优化实验，我们需要给学生表和课程评论表随机模拟出百万量级的数据。我们可以 通过存储过程来实现模拟数据。\n反范式优化实验对比\n如果我们想要查询课程 ID 为 10001 的前 1000 条评论，需要写成下面这样：\n1 2 3 4 5 6 SELECT p.comment_text, p.comment_time, stu.stu_name FROM class_comment AS p LEFT JOIN student AS stu ON p.stu_id = stu.stu_id WHERE p.class_id = 10001 ORDER BY p.comment_id DESC LIMIT 1000; 运行结果（1000 条数据行）：\n运行时长为 0.395 秒，对于网站的响应来说，这已经很慢了，用户体验会非常差。\n如果我们想要提升查询的效率，可以允许适当的数据冗余，也就是在商品评论表中增加用户昵称字段， 在 class_comment 数据表的基础上增加 stu_name 字段，就得到了 class_comment2 数据表。\n这样一来，只需单表查询就可以得到数据集结果：\n1 2 3 4 SELECT comment_text, comment_time, stu_name FROM class_comment2 WHERE class_id = 10001 ORDER BY class_id DESC LIMIT 1000; 运行结果（1000 条数据）：\n优化之后只需要扫描一次聚集索引即可，运行时间为 0.039 秒，查询时间是之前的 1/10。 你能看到， 在数据量大的情况下，查询效率会有显著的提升。\n3.3 反范式的新问题 存储空间变大了 一个表中字段做了修改，另一个表中冗余的字段也需要做同步修改，否则数据不一致 若采用存储过程来支持数据的更新、删除等额外操作，如果更新频繁，会非常消耗系统资源 在 数据量小 的情况下，反范式不能体现性能的优势，可能还会让数据库的设计更加复杂 3.4 反范式的适用场景 当冗余信息有价值或者能 大幅度提高查询效率 的时候，我们才会采取反范式的优化。\n1. 增加冗余字段的建议 增加冗余字段一定要符合如下两个条件。只要满足这两个条件，才可以考虑增加夯余字段。\n1）这个冗余字段不需要经常进行修改。\n2）这个冗余字段查询的时候不可或缺。\n2. 历史快照、历史数据的需要 在现实生活中，我们经常需要一些冗余信息，比如订单中的收货人信息，包括姓名、电话和地址等。每次发生的 订单收货信息 都属于 历史快照 ，需要进行保存，但用户可以随时修改自己的信息，这时保存这些冗余信息是非常有必要的。\n反范式优化也常用在 数据仓库 的设计中，因为数据仓库通常存储历史数据 ，对增删改的实时性要求不强，对历史数据的分析需求强。这时适当允许数据的冗余度，更方便进行数据分析。\n我简单总结下数据仓库和数据库在使用上的区别：\n数据库设计的目的在于捕捉数据，而数据仓库设计的目的在于分析数据。 数据库对数据的增删改实时性要求强，需要存储在线的用户数据，而数据仓库存储的一般是历史数据。 数据库设计需要尽量避免冗余，但为了提高查询效率也允许一定的冗余度，而数据仓库在设计上更偏向采用反范式设计， 4. BCNF(巴斯范式) 人们在3NF的基础上进行了改进，提出了巴斯范式（BCNF），页脚巴斯 - 科德范式（Boyce - Codd Normal Form）。BCNF被认为没有新的设计规范加入，只是对第三范式中设计规范要求更强，使得数据库冗余度更小。所以，称为是修正的第三范式，或扩充的第三范式，BCNF不被称为第四范式。\n若一个关系达到了第三范式，并且它只有一个候选键，或者它的每个候选键都是单属性，则该关系自然达到BC范式。\n一般来说，一个数据库设符合3NF或者BCNF就可以了。\n1. 案例\n我们分析如下表的范式情况：\n在这个表中，一个仓库只有一个管理员，同时一个管理员也只管理一个仓库。我们先来梳理下这些属性之间的依赖关系。\n仓库名决定了管理员，管理员也决定了仓库名，同时（仓库名，物品名）的属性集合可以决定数量这个属性。这样，我们就可以找到数据表的候选键。\n候选键 ：是（管理员，物品名）和（仓库名，物品名），然后我们从候选键中选择一个作为主键 ，比 如（仓库名，物品名）。\n主属性 ：包含在任一候选键中的属性，也就是仓库名，管理员和物品名。\n非主属性 ：数量这个属性。\n2. 是否符合三范式\n如何判断一张表的范式呢？我们需要根据范式的等级，从低到高来进行判断。\n首先，数据表每个属性都是原子性的，符合 1NF 的要求；\n其次，数据表中非主属性”数量“都与候选键全部依赖，（仓库名，物品名）决定数量，（管理员，物品名）决定数量。因此，数据表符合 2NF 的要求；\n最后，数据表中的非主属性，不传递依赖于候选键。因此符合 3NF 的要求。\n3. 存在的问题\n既然数据表已经符合了 3NF 的要求，是不是就不存在问题了呢？我们来看下面的情况：\n增加一个仓库，但是还没有存放任何物品。根据数据表实体完整性的要求，主键不能有空值，因此会出现插入异常； 如果仓库更换了管理员，我们就可能会修改数据表中的多条记录 ； 如果仓库里的商品都卖空了，那么此时仓库名称和相应的管理员名称也会随之被删除。 你能看到，即便数据表符合 3NF 的要求，同样可能存在插入，更新和删除数据的异常情况。\n4. 问题解决\n首先我们需要确认造成异常的原因：主属性仓库名对于候选键（管理员，物品名）是部分依赖的关系， 这样就有可能导致上面的异常情况。因此引入BCNF，它在 3NF 的基础上消除了主属性对候选键的部分依赖或者传递依赖关系。\n如果在关系R中，U为主键，A属性是主键的一个属性，若存在A-\u0026gt;Y，Y为主属性，则该关系不属于 BCNF。 根据 BCNF 的要求，我们需要把仓库管理关系 warehouse_keeper 表拆分成下面这样：\n仓库表 ：（仓库名，管理员）\n库存表 ：（仓库名，物品名，数量）\n这样就不存在主属性对于候选键的部分依赖或传递依赖，上面数据表的设计就符合 BCNF。\n再举例：\n有一个 学生导师表 ，其中包含字段：学生ID，专业，导师，专业GPA，这其中学生ID和专业是联合主键。\n这个表的设计满足三范式，但是这里存在另一个依赖关系，“专业”依赖于“导师”，也就是说每个导师只做一个专业方面的导师，只要知道了是哪个导师，我们自然就知道是哪个专业的了。\n所以这个表的部分主键Major依赖于非主键属性Advisor，那么我们可以进行以下的调整，拆分成2个表：\n学生导师表：\n导师表：\n5. 第四范式 多值依赖的概念：\n多值依赖即属性之间的一对多关系，记为K—\u0026gt;—\u0026gt;A。 函数依赖事实上是单值依赖，所以不能表达属性值之间的一对多关系。 平凡的多值依赖：全集U=K+A，一个K可以对应于多个A，即K—\u0026gt;—\u0026gt;A。此时整个表就是一组一对多关系。 非平凡的多值依赖：全集U=K+A+B，一个K可以对应于多个A，也可以对应于多个B，A与B相互独立，即K—\u0026gt;—\u0026gt;A，K—\u0026gt;—\u0026gt;B。整个表有多组一对多关系，且有：\u0026ldquo;一\u0026quot;部分是相同的属性集合，“多”部分是相互独立的属性集合。 第四范式即在满足巴斯 - 科德范式（BCNF）的基础上，消除非平凡且非函数依赖的多值依赖（即把同一表的多对多关系删除）。\n**举例1：**职工表(职工编号，职工孩子姓名，职工选修课程)。\n在这个表中，同一个职工可能会有多个职工孩子姓名。同样，同一个职工也可能会有多个职工选修课程，即这里存在着多值事实，不符合第四范式。\n如果要符合第四范式，只需要将上表分为两个表，使它们只有一个多值事实，例如： 职工表一 (职工编 号，职工孩子姓名)， 职工表二(职工编号，职工选修课程)，两个表都只有一个多值事实，所以符合第四范式。\n举例2：\n比如我们建立课程、教师、教材的模型。我们规定，每门课程有对应的一组教师，每门课程也有对应的一组教材，一门课程使用的教材和教师没有关系。我们建立的关系表如下：\n课程ID，教师ID，教材ID；这三列作为联合主键。\n为了表述方便，我们用Name代替ID，这样更容易看懂：\n这个表除了主键，就没有其他字段了，所以肯定满足BC范式，但是却存在 多值依赖 导致的异常。\n假如我们下学期想采用一本新的英版高数教材，但是还没确定具体哪个老师来教，那么我们就无法在这 个表中维护Course高数和Book英版高数教材的的关系。\n解决办法是我们把这个多值依赖的表拆解成2个表，分别建立关系。这是我们拆分后的表：\n以及\n6. 第五范式、域键范式 除了第四范式外，我们还有更高级的第五范式（又称完美范式）和域键范式（DKNF）。\n在满足第四范式（4NF）的基础上，消除不是由候选键所蕴含的连接依赖。如果关系模式R中的每一个连 接依赖均由R的候选键所隐含，则称此关系模式符合第五范式。\n函数依赖是多值依赖的一种特殊的情况，而多值依赖实际上是连接依赖的一种特殊情况。但连接依赖不 像函数依赖和多值依赖可以由 语义直接导出 ，而是在 关系连接运算 时才反映出来。存在连接依赖的关系 模式仍可能遇到数据冗余及插入、修改、删除异常等问题。\n第五范式处理的是 无损连接问题 ，这个范式基本 没有实际意义 ，因为无损连接很少出现，而且难以察觉。而域键范式试图定义一个 终极范式 ，该范式考虑所有的依赖和约束类型，但是实用价值也是最小的，只存在理论研究中。\n7. 实战案例 商超进货系统中的进货单表进行剖析：\n进货单表：\n这个表中的字段很多，表里的数据量也很惊人。大量重复导致表变得庞大，效率极低。如何改造？\n在实际工作场景中，这种由于数据表结构设计不合理，而导致的数据重复的现象并不少见。往往是系统虽然能够运行，承载能力却很差，稍微有点流量，就会出现内存不足、CPU使用率飙升的情况，甚至会导致整个项目失败。\n7.1 迭代1次：考虑1NF 第一范式要求：所有的字段都是基本数据类型，不可进行拆分。这里需要确认，所有的列中，每个字段只包含一种数据。\n这张表里，我们把“property\u0026quot;这一字段，拆分成”specification (规格)\u0026rdquo; 和 \u0026ldquo;unit (单位)\u0026quot;，这两个字段如下：\n7.2 迭代2次：考虑2NF 第二范式要求，在满足第一范式的基础上，还要满足数据表里的每一条数据记录，都是可唯一标识的。而且所有字段，都必须完全依赖主键，不能只依赖主键的一部分。\n第1步，就是要确定这个表的主键。通过观察发现，字段“listnumber（单号）\u0026quot;+\u0026ldquo;barcode（条码）\u0026ldquo;可以唯一标识每一条记录，可以作为主键。\n第2步，确定好了主键以后，判断哪些字段完全依赖主键，哪些字段只依赖于主键的一部分。把只依赖于主键一部分的字段拆出去，形成新的数据表。\n首先，进货单明细表里面的\u0026quot;goodsname(名称)\u0026ldquo;\u0026ldquo;specification(规格)\u0026ldquo;\u0026ldquo;unit(单位)\u0026ldquo;这些信息是商品的属性，只依赖于\u0026quot;batcode(条码)\u0026quot;，不完全依赖主键，可以拆分出去。我们把这3个字段加上它们所依赖的字段\u0026quot;barcode(条码)\u0026quot;，拆分形成新的数据表\u0026quot;商品信息表\u0026rdquo;。\n这样一来，原来的数据表就被拆分成了两个表。\n商品信息表：\n进货单表：\n此外，字段\u0026quot;supplierid(供应商编号)\u0026ldquo;\u0026ldquo;suppliername(供应商名称)\u0026ldquo;\u0026ldquo;stock(仓库)“只依赖于\u0026quot;listnumber(单号)\u0026quot;，不完全依赖于主键，所以，我们可以把\u0026quot;supplierid\u0026quot;\u0026ldquo;suppliername\u0026quot;\u0026ldquo;stock\u0026quot;这3个字段拆出去，再加上它们依赖的字段\u0026quot;listnumber(单号)\u0026quot;，就形成了一个新的表\u0026quot;进货单头表\u0026rdquo;。剩下的字段，会组成新的表，我们叫它\u0026quot;进货单明细表\u0026rdquo;。\n原来的数据表就拆分成了3个表。\n进货单头表：\n进货单明细表：\n商品信息表：\n现在，我们再来分析一下拆分后的3个表，保证这3个表都满足第二范式的要求。\n第3步，在“商品信息表”中，字段“barcode\u0026quot;是有可能存在重复的，比如，用户门店可能有散装称重商品和自产商品，会存在条码共用的情况。所以，所有的字段都不能唯一标识表里的记录。这个时候，我们必须给这个表加上一个主键，比如说是自增字段\u0026quot;itemnumber\u0026quot;。\n7.3 迭代3次：考虑3NF 我们的进货单头表，还有数据冗余的可能。因为\u0026quot;suppliername\u0026quot;依赖\u0026quot;supplierid\u0026rdquo;，那么就可以按照第三范式的原则进行拆分了。我们就进一步拆分进货单头表，把它拆解陈供货商表和进货单头表。\n供货商表：\n进货单头表：\n这2个表都满足第三范式的要求了。\n7.4 反范式化：业务优先的原则 因此，最后我们可以把进货单表拆分成下面的4个表：\n供货商表：\n进货单头表：\n进货单明细表：\n商品信息表：\n这样一来，我们就避免了冗余数据，而且还能够满足业务的需求，这样的数据库设计，才是合格的设计。\n8. ER模型 8.1 ER模型包括哪些要素？ ER 模型中有三个要素，分别是实体、属性和关系。\n实体 ，可以看做是数据对象，往往对应于现实生活中的真实存在的个体。在 ER 模型中，用 矩形 来表 示。实体分为两类，分别是 强实体 和 弱实体 。强实体是指不依赖于其他实体的实体；弱实体是指对另一个实体有很强的依赖关系的实体。\n属性 ，则是指实体的特性。比如超市的地址、联系电话、员工数等。在 ER 模型中用 椭圆形 来表示。\n关系 ，则是指实体之间的联系。比如超市把商品卖给顾客，就是一种超市与顾客之间的联系。在 ER 模 型中用 菱形 来表示。\n注意：实体和属性不容易区分。这里提供一个原则：我们从系统整体的角度出发去看，可以独立存在的是实体，不可再分的是属性。也就是说，属性不能包含其他属性。\n8.2 关系的类型 在 ER 模型的 3 个要素中，关系又可以分为 3 种类型，分别是 一对一、一对多、多对多。\n一对一 ：指实体之间的关系是一一对应的，比如个人与身份证信息之间的关系就是一对一的关系。一个人只能有一个身份证信息，一个身份证信息也只属于一个人。\n一对多 ：指一边的实体通过关系，可以对应多个另外一边的实体。相反，另外一边的实体通过这个关系，则只能对应唯一的一边的实体。比如说，我们新建一个班级表，而每个班级都有多个学生，每个学 生则对应一个班级，班级对学生就是一对多的关系。\n多对多 ：指关系两边的实体都可以通过关系对应多个对方的实体。比如在进货模块中，供货商与超市之 间的关系就是多对多的关系，一个供货商可以给多个超市供货，一个超市也可以从多个供货商那里采购 商品。再比如一个选课表，有许多科目，每个科目有很多学生选，而每个学生又可以选择多个科目，这 就是多对多的关系。\n8.3 建模分析 ER 模型看起来比较麻烦，但是对我们把控项目整体非常重要。如果你只是开发一个小应用，或许简单设 计几个表够用了，一旦要设计有一定规模的应用，在项目的初始阶段，建立完整的 ER 模型就非常关键了。开发应用项目的实质，其实就是 建模 。\n我们设计的案例是 电商业务 ，由于电商业务太过庞大且复杂，所以我们做了业务简化，比如针对 SKU（StockKeepingUnit，库存量单位）和SPU（Standard Product Unit，标准化产品单元）的含义上，我 们直接使用了SKU，并没有提及SPU的概念。本次电商业务设计总共有8个实体，如下所示。\n地址实体 用户实体 购物车实体 评论实体 商品实体 商品分类实体 订单实体 订单详情实体 其中， 用户 和 商品分类 是强实体，因为它们不需要依赖其他任何实体。而其他属于弱实体，因为它们 虽然都可以独立存在，但是它们都依赖用户这个实体，因此都是弱实体。知道了这些要素，我们就可以 给电商业务创建 ER 模型了，如图：\n在这个图中，地址和用户之间的添加关系，是一对多的关系，而商品和商品详情示一对一的关系，商品和订单是多对多的关系。这个 ER 模型，包括了 8 个实体之间的 8 种关系。\n（1）用户可以在电商平台添加多个地址；\n（2）用户只能拥有一个购物车；\n（3）用户可以生成多个订单；\n（4）用户可以发表多条评论；\n（5）一件商品可以有多条评论；\n（6）每一个商品分类包含多种商品；\n（7）一个订单可以包含多个商品，一个商品可以在多个订单里。\n（8）订单中又包含多个订单详情，因为一个订单中可能包含不同种类的商品\n8.4 ER 模型的细化 有了这个 ER 模型，我们就可以从整体上 理解 电商的业务了。刚刚的 ER 模型展示了电商业务的框架， 但是只包括了订单，地址，用户，购物车，评论，商品，商品分类和订单详情这八个实体，以及它们之间的关系，还不能对应到具体的表，以及表与表之间的关联。我们需要把属性加上 ，用 椭圆 来表示， 这样我们得到的 ER 模型就更加完整了。\n因此，我们需要进一步去设计一下这个 ER 模型的各个局部，也就是细化下电商的具体业务流程，然后把它们综合到一起，形成一个完整的 ER 模型。这样可以帮助我们理清数据库的设计思路。\n接下来，我们再分析一下各个实体都有哪些属性，如下所示。\n（1） 地址实体 包括用户编号、省、市、地区、收件人、联系电话、是否是默认地址。\n（2） 用户实体 包括用户编号、用户名称、昵称、用户密码、手机号、邮箱、头像、用户级别。\n（3） 购物车实体 包括购物车编号、用户编号、商品编号、商品数量、图片文件url。\n（4） 订单实体 包括订单编号、收货人、收件人电话、总金额、用户编号、付款方式、送货地址、下单时间。\n（5） 订单详情实体 包括订单详情编号、订单编号、商品名称、商品编号、商品数量。\n（6） 商品实体 包括商品编号、价格、商品名称、分类编号、是否销售，规格、颜色。\n（7） 评论实体 包括评论id、评论内容、评论时间、用户编号、商品编号\n（8） 商品分类实体 包括类别编号、类别名称、父类别编号\n这样细分之后，我们就可以重新设计电商业务了，ER 模型如图：\n8.5 ER 模型图转换成数据表 通过绘制 ER 模型，我们已经理清了业务逻辑，现在，我们就要进行非常重要的一步了：把绘制好的 ER 模型，转换成具体的数据表，下面介绍下转换的原则：\n（1）一个实体通常转换成一个数据表；\n（2）一个多对多的关系，通常也转换成一个数据表；\n显然添加冗余字段就无法满足了。必须添加一个新的数据表用于描述这种多对多的关系。如果强行添加冗余字段来支持这种多对多的关系的话，会破坏第二范式里的“数据表里的每一行数据都是可唯一标识的”这一句话。\r常见的用户表和角色表来举例，一个用户可以对应有多个角色，同时一个角色也对应着多个用户。如果建表的时候，按照在用户表里面添加一个冗余字段 角色ID，我们会看到这样一种情况。\r用户表user（主键为id,user_id）\nid user_id user_name role_id 1 001 张三 01 2 002 李四 02 3 003 王五 03 4 001 张三 02 5 002 李四 03 角色表role(主键为id,role_id)\nid role_id role_name 1 01 管理员 2 02 测试 3 03 运维 显然，上面表的设计违反了第二范式。\n改进后如下：\n用户表user\nid user_id user_name 1 001 张三 2 002 李四 3 003 王五 角色表role\nid role_id role_name 1 01 管理员 2 02 测试 3 03 运维 用户角色关系表user_role\nid user_id role_id 1 001 01 2 001 02 3 002 02 4 002 03 5 003 03 这样增加了一个关系表，我们的每张表可读性与易维护性都变高了。关系表是两张实体表建立关系，关系独立出来就是关系表。\n（3）一个1对1，或者1对多的关系，往往通过表的 外键 来表达，而不是设计一个新的数据表；\n针对一对多、一对一的实体关系，往往采用添加冗余字段来维持关系，而不是新建一个关系表来表述两个实体之间的关系，虽然这样做行得通，但是成本更高\r下面正反举例解释：\n坏情况：\n用户和地址这两个实体的关系就是一对多的关系。如果设计一个新的数据表来刻画两个表的关系———数据表（主键id，用户表的用户编号，地址表的地址编号）。\r如果有一个业务：我要查询某一个用户的所有地址信息。针对这样的一个业务场景，如果使用建表的方式来刻画这种关系的话，会这样来完成业务：首先，根据用户表的唯一属性（id或编号）查到**关系表**上的对应的**所有满足要求的地址编号**；之后，再根据查到的地址编号，再去地址表里查询所有满足要求具体地址信息。\r好情况：\n针对一对多的这样实体关系，不使用建立数据表描述关系，而是使用针对“多”表附属一个“一”表里唯一标识字段，即在地址表里附加一个用户表的唯一标识字段即可。那么针对这样一个业务：“我要查询某一个用户的所有地址信息。”，就会这样做：直接就查询地址表里的对应用户的所有地址信息了。\r好坏对比：\n坏情况需要维护一张额外的表，使得内存更多地被占用、数据库还得维护又一个聚簇索引、并且为了增加这个关系的查询效率避免全表扫描，需要为用户编号添加唯一索引时，又多了索引维护还有回表查询的过程。\n好情况就优势突出了，只是在原来实体，模型上增加了一个冗余字段即可满足业务需求。成本的话，主要集中在可能的地址表里用户编号普通索引维护即回表查询，相较坏情况而言，省去了中间表的所有缺点。\n当然一对一和一对多的关系往往都被在ER图分析的时候基本都会被刻画成属性了\n（4）属性转换成表的字段。\n下面结合前面的ER模型，具体讲解一下怎么运用这些转换的原则，把 ER 模型转换成具体的数据表，从而把抽象出来的数据模型，落实到具体的数据库设计当中。\n1. 一个实体转换成一个数据库 先来看一下强实体转换成数据表:\n用户实体转换成用户表(user_info)的代码如下所示。\n上面这张表的设计谬误很常见：所以设计表的时候，一定要考虑数据表是否符合第三范式。\n下面我们再把弱实体转换成数据表：\n2. 一个多对多的关系转换成一个数据表 3. 通过外键来表达一对多的关系 4. 把属性转换成表的字段 9. 数据表的设计原则 综合以上内容，总结出数据表设计的一般原则：\u0026ldquo;三少一多\u0026rdquo;\n1. 数据表的个数越少越好\n2. 数据表中的字段个数越少越好\n3. 数据表中联合主键的字段个数越少越好\n4. 使用主键和外键越多越好\n10. 数据库对象编写建议 10.1 关于库 【强制】库的名称必须控制在32个字符以内，只能使用英文字母、数字和下划线，建议以英文字母开头。 【强制】库名中英文一律小写 ，不同单词采用 下划线 分割。须见名知意。 【强制】库的名称格式：业务系统名称_子系统名。 【强制】库名禁止使用关键字（如type,order等）。 【强制】创建数据库时必须显式指定字符集 ，并且字符集只能是utf8或者utf8mb4。 创建数据库SQL举例：CREATE DATABASE crm_fund DEFAULT CHARACTER SET \u0026lsquo;utf8\u0026rsquo; ; 【建议】对于程序连接数据库账号，遵循权限最小原则 使用数据库账号只能在一个DB下使用，不准跨库。程序使用的账号 原则上不准有drop权限 。 【建议】临时库以 tmp_ 为前缀，并以日期为后缀； 备份库以 bak_ 为前缀，并以日期为后缀。 10.2 关于表、列 【强制】表和列的名称必须控制在32个字符以内，表名只能使用英文字母、数字和下划线，建议以英文字母开头 。\n【强制】表名、列名一律小写，不同单词采用下划线分割。须见名知意。\n【强制】表名要求有模块名强相关，同一模块的表名尽量使用 统一前缀 。比如：crm_fund_item\n【强制】创建表时必须显式指定字符集 为utf8或utf8mb4。\n【强制】表名、列名禁止使用关键字（如type,order等）。\n【强制】创建表时必须显式指定表存储引擎类型。如无特殊需求，一律为InnoDB。\n【强制】建表必须有comment。\n【强制】字段命名应尽可能使用表达实际含义的英文单词或缩写 。如：公司 ID，不要使用 corporation_id, 而用corp_id 即可。\n【强制】布尔值类型的字段命名为is_描述 。如member表上表示是否为enabled的会员的字段命 名为 is_enabled。\n【强制】禁止在数据库中存储图片、文件等大的二进制数据通常文件很大，短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时。通常存储于文件服务器，数据库只存储文件地址信息。\n【建议】建表时关于主键： 表必须有主键\n(1)强制要求主键为id，类型为int或bigint，且为 auto_increment 建议使用unsigned无符号型。\n(2)标识表里每一行主体的字段不要设为主键，建议设为其他字段如user_id，order_id等，并建立unique key索引。因为如果设为主键且主键值为随机插入，则会导致innodb内部页分裂和大量随机I/O，性能下降。\n【建议】核心表（如用户表）必须有行数据的 创建时间字段（create_time）和 最后更新时间字段（update_time），便于查问题。\n【建议】表中所有字段尽量都是 NOT NULL 属性，业务可以根据需要定义 DEFAULT值。因为使用NULL值会存在每一行都会占用额外存储空间、数据迁移容易出错、聚合函数计算结果偏差等问题。\n【建议】所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）。\n【建议】中间表（或临时表）用于保留中间结果集，名称以 tmp_ 开头。 备份表用于备份或抓取源表快照，名称以 bak_ 开头。中间表和备份表定期清理。\n【示范】一个较为规范的建表语句：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 CREATE TABLE user_info ( `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT \u0026#39;自增主键\u0026#39;, `user_id` bigint(11) NOT NULL COMMENT \u0026#39;用户id\u0026#39;, `username` varchar(45) NOT NULL COMMENT \u0026#39;真实姓名\u0026#39;, `email` varchar(30) NOT NULL COMMENT \u0026#39;用户邮箱\u0026#39;, `nickname` varchar(45) NOT NULL COMMENT \u0026#39;昵称\u0026#39;, `birthday` date NOT NULL COMMENT \u0026#39;生日\u0026#39;, `sex` tinyint(4) DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;性别\u0026#39;, `short_introduce` varchar(150) DEFAULT NULL COMMENT \u0026#39;一句话介绍自己，最多50个汉字\u0026#39;, `user_resume` varchar(300) NOT NULL COMMENT \u0026#39;用户提交的简历存放地址\u0026#39;, `user_register_ip` int NOT NULL COMMENT \u0026#39;用户注册时的源ip\u0026#39;, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;创建时间\u0026#39;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;修改时间\u0026#39;, `user_review_status` tinyint NOT NULL COMMENT \u0026#39;用户资料审核状态，1为通过，2为审核中，3为未 通过，4为还未提交审核\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `uniq_user_id` (`user_id`), KEY `idx_username`(`username`), KEY `idx_create_time_status`(`create_time`,`user_review_status`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=\u0026#39;网站用户基本信息 【建议】创建表时，可以使用可视化工具。这样可以确保表、字段相关的约定都能设置上。 实际上，我们通常很少自己写 DDL 语句，可以使用一些可视化工具来创建和操作数据库和数据表。\n可视化工具除了方便，还能直接帮我们将数据库的结构定义转化成 SQL 语言，方便数据库和数据表结构的导出和导入。\n10.3 关于索引 【强制】InnoDB表必须主键为id int/bigint auto_increment，且主键值禁止被更新 。 【强制】InnoDB和MyISAM存储引擎表，索引类型必须为 BTREE 。 【建议】主键的名称以 pk_ 开头，唯一键以 uni_ 或 uk_ 开头，普通索引以 idx_ 开头，一律使用小写格式，以字段的名称或缩写作为后缀。 【建议】多单词组成的columnname，取前几个单词首字母，加末单词组成column_name。如: sample 表 member_id 上的索引：idx_sample_mid。 【建议】单个表上的索引个数不能超过6个 。 【建议】在建立索引时，多考虑建立联合索引，并把区分度最高的字段放在最前面。 【建议】在多表 JOIN 的SQL里，保证被驱动表的连接列上有索引，这样JOIN 执行效率最高。 【建议】建表或加索引时，保证表里互相不存在冗余索引。比如：如果表里已经存在key(a,b)，则key(a)为冗余索引，需要删除。 10.4 SQL编写 【强制】程序端SELECT语句必须指定具体字段名称，禁止写成 *。 【建议】程序端insert语句指定具体字段名称，不要写成INSERT INTO t1 VALUES(…)。 【建议】除静态表或小表（100行以内），DML语句必须有WHERE条件，且使用索引查找。 【建议】INSERT INTO…VALUES(XX),(XX),(XX).. 这里XX的值不要超过5000个。 值过多虽然上线很 快，但会引起主从同步延迟。 【建议】SELECT语句不要使用UNION，推荐使用UNION ALL，并且UNION子句个数限制在5个以 内。 【建议】线上环境，多表 JOIN 不要超过5个表。 【建议】减少使用ORDER BY，和业务沟通能不排序就不排序，或将排序放到程序端去做。ORDER BY、GROUP BY、DISTINCT 这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。 【建议】包含了ORDER BY、GROUP BY、DISTINCT 这些查询的语句，WHERE 条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。 【建议】对单表的多次alter操作必须合并为一次，对于超过100W行的大表进行 alter table ，必须经过DBA审核，并在业务低峰期执行，多个alter需整合在一起。 因为alter table会产生表锁，期间阻塞对于该表的所有写入，对于业务可能会产生极大影响。 【建议】批量操作数据时，需要控制事务处理间隔时间，进行必要的sleep。 【建议】事务里包含SQL不超过5个。因为过长的事务会导致锁数据较久，MySQL内部缓存、连接消耗过多等问题。 【建议】事务里更新语句尽量基于主键或UNIQUE KEY，如UPDATE… WHERE id=XX; 否则会产生间隙锁，内部扩大锁定范围，导致系统性能下降，产生死锁。 11. PowerDesigner的使用 PowerDesigner是一款开发人员常用的数据库建模工具，用户利用该软件可以方便地制作 数据流程图 、 概念数据模型 、 物理数据模型 ，它几乎包括了数据库模型设计的全过程，是Sybase公司为企业建模和设 计提供的一套完整的集成化企业级建模解决方案。\n11.1 开始界面 当前使用的PowerDesigner版本是16.5的。打开软件即是此页面，可选择Create Model,也可以选择Do Not Show page Again,自行在打开软件后创建也可以！完全看个人的喜好，在此我在后面的学习中不在显示此页面。\n“Create Model”的作用类似于普通的一个文件，该文件可以单独存放也可以归类存放。\n“Create Project”的作用类似于文件夹，负责把有关联关系的文件集中归类存放。\n11.2 概念数据模型 常用的模型有4种，分别是 概念模型(CDM Conceptual Data Model) ， 物理模型（PDM,Physical Data Model） ， 面向对象的模型（OOM Objcet Oriented Model） 和 业务模型（BPM Business Process Model） ，我们先创建概念数据模型。\n点击上面的ok，即可出现下图左边的概念模型1，可以自定义概念模型的名字，在概念模型中使用最多的 就是如图所示的Entity(实体),Relationship(关系)\nEntity实体\n选中右边框中Entity这个功能，即可出现下面这个方框，需要注意的是书写name的时候，code自行补全，name可以是英文的也可以是中文的，但是code必须是英文的。\n填充实体字段\nGeneral中的name和code填好后，就可以点击Attributes（属性）来设置name（名字），code(在数据库中 的字段名)，Data Type(数据类型) ，length(数据类型的长度)\nName: 实体名字一般为中文，如论坛用户 Code: 实体代号，一般用英文，如XXXUser Comment:注释，对此实体详细说明 Code属性：代号，一般用英文UID DataType Domain域，表示属性取值范围如可以创建10个字符的地址域 M:Mandatory强制属性，表示该属性必填。不能为空 P:Primary Identifer是否是主标识符，表示实体唯一标识符 D:Displayed显示出来，默认全部勾选 在此上图说明name和code的起名方法\n设置主标识符\n如果不希望系统自动生成标识符而是手动设置的话，那么切换到Identifiers选项卡，添加一行Identifier， 然后单击左上角的“属性”按钮，然后弹出的标识属性设置对话框中单击“添加行”按钮，选择该标识中使用的属性。例如将学号设置为学生实体的标识。\n放大模型\n创建好概念数据模型如图所示，但是创建好的字体很小，读者可以按着ctrl键同时滑动鼠标的可滑动按钮 即可放大缩写字体，同时也可以看到主标识符有一个*号的标志，同时也显示出来了，name,Data type和 length这些可见的属性\n实体关系\n同理创建一个班级的实体（需要特别注意的是，点击完右边功能的按钮后需要点击鼠标指针状态的按钮 或者右击鼠标即可，不然很容易乱操作，这点注意一下就可以了），然后使用Relationship（关系）这个 按钮可以连接学生和班级之间的关系，发生一对多（班级对学生）或者多对一（学生对班级）的关系。\n如图所示\n需要注意的是点击Relationship这个按钮，就把班级和学生联系起来了，就是一条线，然后双击这条线进 行编辑，在General这块起name和code\n上面的name和code起好后就可以在Cardinalities这块查看班级和学生的关系，可以看到班级的一端是一 条线，学生的一端是三条，代表班级对学生是一对多的关系即one对many的关系，点击应用，然后确定 即可\n一对多和多对一练习完还有多对多的练习，如下图操作所示，老师实体和上面介绍的一样，自己将 name，data type等等修改成自己需要的即可，满足项目开发需求即可。（comment是解释说明，自己可以写相关的介绍和说明）\n多对多需要注意的是自己可以手动点击按钮将关系调整称为多对多的关系many对many的关系，然后点击应用和确定即可\n综上即可完成最简单的学生，班级，教师这种概念数据模型的设计，需要考虑数据的类型和主标识码， 是否为空。关系是一对一还是一对多还是多对多的关系，自己需要先规划好再设计，然后就ok了。\n11.3 物理数据模型 上面是概念数据模型，下面介绍一下物理数据模型，以后经常使用 的就是物理数据模型。打开 PowerDesigner，然后点击File\u0026ndash;\u0026gt;New Model然后选择如下图所示的物理数据模型，物理数据模型的名字自己起，然后选择自己所使用的数据库即可。\n创建好主页面如图所示，但是右边的按钮和概念模型略有差别，物理模型最常用的三个是 table(表) ， view(视图)， reference(关系) ；\n鼠标先点击右边table这个按钮然后在新建的物理模型点一下，即可新建一个表，然后双击新建如下图所示，在General的name和code填上自己需要的，点击应用即可），如下图：\n然后点击Columns,如下图设置，非常简单，需要注意的就是P（primary主键） , F （foreign key外键） , M（mandatory强制性的，代表不可为空） 这三个。\n在此设置学号的自增（MYSQL里面的自增是这个AUTO_INCREMENT），班级编号同理，不多赘述！\n在下面的这个点上对号即可，就设置好了自增\n全部完成后如下图所示。\n班级物理模型同理如下图所示创建即可\n完成后如下图所示\n上面的设置好如上图所示，然后下面是关键的地方，点击右边按钮Reference这个按钮，因为是班级对学生是一对多的，所以鼠标从学生拉到班级如下图所示，学生表将发生变化，学生表里面增加了一行，这行是班级表的主键作为学生表的外键，将班级表和学生表联系起来。（仔细观察即可看到区别。）\n做完上面的操作，就可以双击中间的一条线，显示如下图，修改name和code即可\n但是需要注意的是，修改完毕后显示的结果却如下图所示，并没有办法直接像概念模型那样，修改过后 显示在中间的那条线上面，自己明白即可。\n学习了多对一或者一对多的关系，接下来学习多对对的关系，同理自己建好老师表，这里不在叙述，记得老师编号自增，建好如下图所示\n下面是多对多关系的关键，由于物理模型多对多的关系需要一个中间表来连接，如下图，只设置一个字段，主键，自增\n点击应用，然后设置Columns，只添加一个字段\n这是设置字段递增，前面已经叙述过好几次\n设置好后如下图所示，需要注意的是有箭头的一方是一，无箭头的一方是多，即一对多的多对一的关系 需要搞清楚，学生也可以有很多老师，老师也可以有很多学生，所以学生和老师都可以是主体；\n可以看到添加关系以后学生和教师的关系表前后发生的变化\n11.4 概念模型转为物理模型 1：如下图所示先打开概念模型图，然后点击Tool,如下图所示\n点开的页面如下所示，name和code已经从概念模型1改成物理模型1了\n完成后如下图所示，将自行打开修改的物理模型，需要注意的是这些表的数据类型已经自行改变了，而 且中间表出现两个主键，即双主键\n11.5 物理模型转为概念模型 上面介绍了概念模型转物理模型，下面介绍一下物理模型转概念模型（如下图点击操作即可）\n然后出现如下图所示界面，然后将物理修改为概念 ，点击应用确认即可\n点击确认后将自行打开如下图所示的页面，自己观察有何变化，如果转换为oracle的，数据类型会发生变 化，比如Varchar2等等）；\n11.6 物理模型导出SQL语句 打开之后如图所示，修改好存在sql语句的位置和生成文件的名称即可\n在Selection中选择需要导出的表，然后点击应用和确认即可\n完成以后出现如下图所示，可以点击Edit或者close按钮\n自此，就完成了导出sql语句，就可以到自己指定的位置查看导出的sql语句了；PowerDesigner在以后在 项目开发过程中用来做需求分析和数据库的设计非常的方便和快捷。\n第12章_数据库其它调优策略 1. 数据库调优的措施 1.1 调优的目标 尽可能节省系统资源 ，以便系统可以提供更大负荷的服务。（吞吐量更大） 合理的结构设计和参数调整，以提高用户操作响应的速度。（响应速度更快） 减少系统的瓶颈，提高MySQL数据库整体的性能。 1.2 如何定位调优问题 如何确定呢？一般情况下，有如下几种方式：\n1.3 调优的维度和步骤 我们需要调优的对象是整个数据库管理系统，它不仅包括 SQL 查询，还包括数据库的部署配置、架构等。从这个角度来说，我们思考的维度就不仅仅局限在 SQL 优化上了。通过如下的步骤我们进行梳理：\n第1步：选择适合的 DBMS 第2步：优化表设计 第3步：优化逻辑查询 第4步：优化物理查询 物理查询优化是在确定了逻辑查询优化之后，采用物理优化技术（比如索引等），通过计算代价模型对 各种可能的访问路径进行估算，从而找到执行方式中代价最小的作为执行计划。在这个部分中，我们需要掌握的重点是对索引的创建和使用。\n第5步：使用 Redis 或 Memcached 作为缓存 除了可以对 SQL 本身进行优化以外，我们还可以请外援提升查询的效率。\n因为数据都是存放到数据库中，我们需要从数据库层中取出数据放到内存中进行业务逻辑的操作，当用户量增大的时候，如果频繁地进行数据查询，会消耗数据库的很多资源。如果我们将常用的数据直接放到内存中，就会大幅提升查询的效率。\n键值存储数据库可以帮我们解决这个问题。\n常用的键值存储数据库有 Redis 和 Memcached，它们都可以将数据存放到内存中。\n第6步：库级优化 但需要注意的是，分拆在提升数据库性能的同时，也会增加维护和使用成本。\n2. 优化MySQL服务器 2.1 优化服务器硬件 服务器的硬件性能直接决定着MySQL数据库的性能。硬件的性能瓶颈直接决定MySQL数据库的运行速度 和效率。针对性能瓶颈提高硬件配置，可以提高MySQL数据库查询、更新的速度。\n（1） 配置较大的内存 。足够大的显存是提高MySQL数据库性能的方法之一。内存的速度比磁盘I/O快得多，可以通过增加系统的缓冲区容量使数据在内存中停留的时间更长，以减少磁盘I/O。\n（2） 配置高速磁盘系统 ，以减少读盘的等待时间，提高响应速度。磁盘的I/O能力，也就是它的寻道能力，目前的SCSI高速旋转的是7200转/分钟，这样的速度，一旦访问的用户量上去，磁盘的压力就会过大，如果是每天的网站pv (page view) 在150w，这样的一般的配置就无法满足这样的需求了。现在SSD盛行，在SSD上随机访问和顺序访问性能差不多，使用SSD可以减少随机IO带来的性能损耗。\n（3） 合理分布磁盘I/O，把磁盘I/O分散在多个设备，以减少资源竞争，提高冰箱操作能力。\n（4） 配置多处理器, MySQL是多线程的数据库，多处理器可同时执行多个线程。\n2.2 优化MySQL的参数 innodb_buffer_pool_size ：这个参数是Mysql数据库最重要的参数之一，表示InnoDB类型的 表 和索引的最大缓存 。它不仅仅缓存 索引数据 ，还会缓存 表的数据 。这个值越大，查询的速度就会越 快。但是这个值太大会影响操作系统的性能。\nkey_buffer_size ：表示 索引缓冲区的大小 。索引缓冲区是所有的 线程共享 。增加索引缓冲区可 以得到更好处理的索引（对所有读和多重写）。当然，这个值不是越大越好，它的大小取决于内存 的大小。如果这个值太大，就会导致操作系统频繁换页，也会降低系统性能。对于内存在 4GB 左右 的服务器该参数可设置为 256M 或 384M 。\ntable_cache ：表示 同时打开的表的个数 。这个值越大，能够同时打开的表的个数越多。物理内 存越大，设置就越大。默认为2402，调到512-1024最佳。这个值不是越大越好，因为同时打开的表 太多会影响操作系统的性能。\nquery_cache_size ：表示 查询缓冲区的大小 。可以通过在MySQL控制台观察，如果 Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，就要增加Query_cache_size 的值；如果Qcache_hits的值非常大，则表明查询缓冲使用非常频繁，如果该值较小反而会影响效 率，那么可以考虑不用查询缓存；Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很 多。MySQL8.0之后失效。该参数需要和query_cache_type配合使用。\nquery_cache_type 的值是0时，所有的查询都不使用查询缓存区。但是query_cache_type=0并不 会导致MySQL释放query_cache_size所配置的缓存区内存。\n当query_cache_type=1时，所有的查询都将使用查询缓存区，除非在查询语句中指定 SQL_NO_CACHE ，如SELECT SQL_NO_CACHE * FROM tbl_name。 当query_cache_type=2时，只有在查询语句中使用 SQL_CACHE 关键字，查询才会使用查询缓 存区。使用查询缓存区可以提高查询的速度，这种方式只适用于修改操作少且经常执行相同的 查询操作的情况。 sort_buffer_size：表示每个需要进行排序的线程分配的缓冲区的大小。增加这个参数的值可以提高 ORDER BY 或 GROUP BY 操作的速度。默认数值是2097144字节（约2MB）。对于内存在4GB 左右的服务器推荐设置为6-8M，如果有100个连接，那么实际分配的总共排序缓冲区大小为 100 × 6 ＝ 600MB 。\njoin_buffer_size = 8M：表示联合查询操作所能使用的缓冲区大小，和sort_buffer_size一样， 该参数对应的分配内存也是每个连接独享。\nread_buffer_size：表示 每个线程连续扫描时为扫描的每个表分配的缓冲区的大小（字节）。当线程从表中连续读取记录时需要用到这个缓冲区。SET SESSION read_buffer_size=n可以临时设置该参数的值。默认为64K，可以设置为4M。\ninnodb_flush_log_at_trx_commit：表示何时将缓冲区的数据写入日志文件，并且将日志文件写入磁盘中。该参数对于innoDB引擎非常重要。该参数有3个值，分别为0、1和2。该参数的默认值为1。\n值为 0 时，表示 每秒1次 的频率将数据写入日志文件并将日志文件写入磁盘。每个事务的 commit 并不会触发前面的任何操作。该模式速度最快，但不太安全，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。 值为 1 时，表示 每次提交事务时将数据写入日志文件并将日志文件写入磁盘进行同步。该模式是最安全的，但也是最慢的一种方式。因为每次事务提交或事务外的指令都需要把日志写入（flush）硬盘。 值为 2 时，表示 每次提交事务时将数据写入日志文件，每隔1秒将日志文件写入磁盘。该模式速度较快，也比0安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 innodb_log_buffer_size：这是 InnoDB 存储引擎的 事务日志所使用的缓冲区 。为了提高性能， 也是先将信息写入 Innodb Log Buffer 中，当满足 innodb_flush_log_trx_commit 参数所设置的相应条 件（或者日志缓冲区写满）之后，才会将日志写到文件（或者同步到磁盘）中。\nmax_connections ：表示 允许连接到MySQL数据库的最大数量 ，默认值是 151 。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大 max_connections 的值。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。这个连接数不是越大越好 ，因为这些连接会浪费内存的资源。过多的连接可能会导致MySQL服务器僵死。\nback_log：用于控制MySQL监听TCP端口时设置的积压请求栈大小 。如果MySql的连接数达到 max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即 back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默 认值为 50 ， 之后的版本默认为 50 +（max_connections / 5）， 对于Linux系统推荐设置为小于512 的整数，但最大不超过900。\n如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。\nthread_cache_size ： 线程池缓存线程数量的大小 ，当客户端断开连接后将当前线程缓存起来，当在接到新的连接请求时快速响应无需创建新的线程。这尤其对那些使用短连接的应用程序来说可以极大的提高创建连接的效率。那么为了提高性能可以增大该参数的值。默认为60，可以设置为 120。\n可以通过如下几个MySQL状态值来适当调整线程池的大小：\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; show global status like \u0026#39;Thread%\u0026#39;; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 2 | | Threads_connected | 1 | | Threads_created | 3 | | Threads_running | 2 | +-------------------+-------+ 4 rows in set (0.01 sec) 当 Threads_cached 越来越少，但 Threads_connected 始终不降，且 Threads_created 持续升高，可适当增加 thread_cache_size 的大小。\nwait_timeout ：指定一个请求的最大连接时间 ，对于4GB左右内存的服务器可以设置为5-10。\ninteractive_timeout ：表示服务器在关闭连接前等待行动的秒数。\n这里给出一份my.cnf的参考配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 mysqld] port = 3306 serverid = 1 socket = /tmp/mysql.sock skip-locking #避免MySQL的外部锁定，减少出错几率增强稳定性。 skip-name-resolve #禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ back_log = 384 key_buffer_size = 256M max_allowed_packet = 4M thread_stack = 256K table_cache = 128K sort_buffer_size = 6M read_buffer_size = 4M read_rnd_buffer_size=16M join_buffer_size = 8M myisam_sort_buffer_size =64M table_cache = 512 thread_cache_size = 64 query_cache_size = 64M tmp_table_size = 256M max_connections = 768 max_connect_errors = 10000000 wait_timeout = 10 thread_concurrency = 8 #该参数取值为服务器逻辑CPU数量*2，在本例中，服务器有2颗物理CPU，而每颗物理CPU又支持H.T超线程，所以实际取值为4*2=8 skip-networking #开启该选项可以彻底关闭MySQL的TCP/IP连接方式，如果WEB服务器是以远程连接的方式访问MySQL数据库服务器则不要开启该选项！否则将无法正常连接！ table_cache=1024 innodb_additional_mem_pool_size=4M #默认为2M innodb_flush_log_at_trx_commit=1 innodb_log_buffer_size=2M #默认为1M innodb_thread_concurrency=8 #你的服务器CPU有几个就设置为几。建议用默认一般为8 tmp_table_size=64M #默认为16M，调到64-256最挂 thread_cache_size=120 query_cache_size=32M 很多情况还需要具体情况具体分析！\n举例：\n(1) 调整系统参数 InnoDB_flush_log_at_trx_commit\n(2) 调整系统参数 InnoDB_buffer_pool_size\n(3) 调整系统参数 InnoDB_buffer_pool_instances\n3. 优化数据库结构 3.1 拆分表：冷热数据分离 举例1： 会员members表 存储会员登录认证信息，该表中有很多字段，如id、姓名、密码、地址、电话、个人描述字段。其中地址、电话、个人描述等字段并不常用，可以将这些不常用的字段分解出另一个表。将这个表取名叫members_detail，表中有member_id、address、telephone、description等字段。这样就把会员表分成了两个表，分别为 members表 和 members_detail表 。\n创建这两个表的SQL语句如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CREATE TABLE members ( id int(11) NOT NULL AUTO_INCREMENT, username varchar(50) DEFAULT NULL, password varchar(50) DEFAULT NULL, last_login_time datetime DEFAULT NULL, last_login_ip varchar(100) DEFAULT NULL, PRIMARY KEY(Id) ); CREATE TABLE members_detail ( Member_id int(11) NOT NULL DEFAULT 0, address varchar(255) DEFAULT NULL, telephone varchar(255) DEFAULT NULL, description text ); 如果需要查询会员的基本信息或详细信息，那么可以用会员的id来查询。如果需要将会员的基本信息和详细信息同时显示，那么可以将members表和members_detail表进行联合查询，查询语句如下：\n1 2 SELECT * FROM members LEFT JOIN members_detail on members.id = members_detail.member_id; 通过这种分解可以提高表的查询效率。对于字段很多且有些字段使用不频繁的表，可以通过这种分解的方式来优化数据库的性能。\n3.2 增加中间表 举例1： 学生信息表 和 班级表 的SQL语句如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CREATE TABLE `class` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `className` VARCHAR(30) DEFAULT NULL, `address` VARCHAR(40) DEFAULT NULL, `monitor` INT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL, `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 现在有一个模块需要经常查询带有学生名称（name）、学生所在班级名称（className）、学生班级班长（monitor）的学生信息。根据这种情况可以创建一个 temp_student 表。temp_student表中存储学生名称（stu_name）、学生所在班级名称（className）和学生班级班长（monitor）信息。创建表的语句如下：\n1 2 3 4 5 6 7 CREATE TABLE `temp_student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stu_name` INT NOT NULL , `className` VARCHAR(20) DEFAULT NULL, `monitor` INT(3) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 接下来，从学生信息表和班级表中查询相关信息存储到临时表中：\n1 2 3 4 insert into temp_student(stu_name,className,monitor) select s.name,c.className,c.monitor from student as s,class as c where s.classId = c.id 以后，可以直接从temp_student表中查询学生名称、班级名称和班级班长，而不用每次都进行联合查询。这样可以提高数据库的查询速度。\n3.3 增加冗余字段 设计数据库表时应尽量遵循范式理论的规约，尽可能减少冗余字段，让数据库设计看起来精致、优雅。 但是，合理地加入冗余字段可以提高查询速度。\n表的规范化程度越高，表与表之间的关系就越多，需要连接查询的情况也就越多。尤其在数据量大，而 且需要频繁进行连接的时候，为了提升效率，我们也可以考虑增加冗余字段来减少连接。\n这部分内容在《第11章_数据库的设计规范》章节中 反范式化小节中具体展开讲解了。这里省略。\n3.4 优化数据类型 情况1：对整数类型数据进行优化。\n遇到整数类型的字段可以用 INT 型 。这样做的理由是，INT 型数据有足够大的取值范围，不用担心数据超出取值范围的问题。刚开始做项目的时候，首先要保证系统的稳定性，这样设计字段类型是可以的。但在数据量很大的时候，数据类型的定义，在很大程度上会影响到系统整体的执行效率。\n对于 非负型 的数据（如自增ID、整型IP）来说，要优先使用无符号整型 UNSIGNED 来存储。因为无符号相对于有符号，同样的字节数，存储的数值范围更大。如tinyint有符号为-128-127，无符号为0-255，多出一倍的存储空间。\n情况2：既可以使用文本类型也可以使用整数类型的字段，要选择使用整数类型。\n跟文本类型数据相比，大整数往往占用更少的存储空间 ，因此，在存取和比对的时候，可以占用更少的 内存空间。所以，在二者皆可用的情况下，尽量使用整数类型，这样可以提高查询的效率。如：将IP地址转换成整型数据。\n情况3：避免使用TEXT、BLOB数据类型\n情况4：避免使用ENUM类型\n修改ENUM值需要使用ALTER语句。\nENUM类型的ORDER BY 操作效率低，需要额外操作。使用TINYINT来代替ENUM类型。\n情况5：使用TIMESTAMP存储时间\nTIMESTAMP存储的时间范围1970-01-01 00:00:01 ~ 2038-01_19-03:14:07。TIMESTAMP使用4字节，DATETIME使用8个字节，同时TIMESTAMP具有自动赋值以及自动更新的特性。\n情况6：用DECIMAL代替FLOAT和DOUBLE存储精确浮点数\n非精准浮点： float, double 精准浮点：decimal Decimal类型为精准浮点数，在计算时不会丢失精度，尤其是财务相关的金融类数据。占用空间由定义的宽度决定，每4个字节可以存储9位数字，并且小数点要占用一个字节。可用于存储比bigint更大的整型数据。\n总之，遇到数据量大的项目时，一定要在充分了解业务需求的前提下，合理优化数据类型，这样才能充分发挥资源的效率，使系统达到最优。\n3.5 优化插入记录的速度 插入记录时，影响插入速度的主要是索引、唯一性校验、一次插入记录条数等。根据这些情况可以分别进行优化。这里我们分为MyISAM引擎和InnoDB引擎来讲。\n1. MyISAM引擎的表：\n① 禁用索引\n② 禁用唯一性检查\n③ 使用批量插入\n插入多条记录时，可以使用一条INSERT语句插入一条数据，也可以使用一条INSERT语句插入多条数据。插入一条记录的INSERT语句情形如下：\n1 2 3 4 insert into student values(1,\u0026#39;zhangsan\u0026#39;,18,1); insert into student values(2,\u0026#39;lisi\u0026#39;,17,1); insert into student values(3,\u0026#39;wangwu\u0026#39;,17,1); insert into student values(4,\u0026#39;zhaoliu\u0026#39;,19,1); 使用一条INSERT语句插入多条记录的情形如下：\n1 2 3 4 5 insert into student values (1,\u0026#39;zhangsan\u0026#39;,18,1), (2,\u0026#39;lisi\u0026#39;,17,1), (3,\u0026#39;wangwu\u0026#39;,17,1), (4,\u0026#39;zhaoliu\u0026#39;,19,1); 第2种情形的插入速度要比第1种情形快。\n④ 使用LOAD DATA INFILE 批量导入\n当需要批量导入数据时，如果能用LOAD DATA INFILE语句，就尽量使用。因为LOAD DATA INFILE语句导入数据的速度比INSERT语句块。\n2. InnoDB引擎的表：\n① 禁用唯一性检查\n插入数据之前执行set unique_checks=0来禁止对唯一索引的检查，数据导入完成之后再运行set unique_check=1。这个和MyISAM引擎的使用方法一样。\n② 禁用外键检查\n③ 禁止自动提交\n3.6 使用非空约束 3.7 分析表、检查表与优化表 MySQL提供了分析表、检查表和优化表的语句。分析表主要是分析关键字的分布，检查表主要是检查表是否存在错误，优化表主要是消除删除或者更新造成的空间浪费。\n1. 分析表 MySQL中提供了ANALYZE TABLE语句分析表，ANALYZE TABLE语句的基本语法如下：\n1 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name[,tbl_name]… 默认的，MySQL服务会将 ANALYZE TABLE语句写到binlog中，以便在主从架构中，从服务能够同步数据。 可以添加参数LOCAL 或者 NO_WRITE_TO_BINLOG 取消将语句写到binlog中。\n使用 ANALYZE TABLE 分析表的过程中，数据库系统会自动对表加一个 只读锁 。在分析期间，只能读取表中的记录，不能更新和插入记录。ANALYZE TABLE语句能够分析InnoDB和MyISAM类型的表，但是不能作用于视图。\nANALYZE TABLE分析后的统计结果会反应到 cardinality 的值，该值统计了表中某一键所在的列不重复 的值的个数。该值越接近表中的总行数，则在表连接查询或者索引查询时，就越优先被优化器选择使用。也就是索引列的cardinality的值与表中数据的总条数差距越大，即使查询的时候使用了该索引作为查询条件，存储引擎实际查询的时候使用的概率就越小。下面通过例子来验证下。cardinality可以通过 SHOW INDEX FROM 表名查看。\n1 2 3 4 5 6 mysql\u0026gt; ANALYZE TABLE user; +--------------+---------+----------+---------+ | Table | Op | Msg_type |Msg_text | +--------------+---------+----------+---------+ | atguigu.user | analyze | status | Ok | +--------------+----------+---------+---------+ 上面结果显示的信息说明如下：\nTable: 表示分析的表的名称。 Op: 表示执行的操作。analyze表示进行分析操作。 Msg_type: 表示信息类型，其值通常是状态 (status) 、信息 (info) 、注意 (note) 、警告 (warning) 和 错误 (error) 之一。 Msg_text: 显示信息。 2. 检查表 MySQL中可以使用 CHECK TABLE 语句来检查表。CHECK TABLE语句能够检查InnoDB和MyISAM类型的表 是否存在错误。CHECK TABLE语句在执行过程中也会给表加上 只读锁 。\n对于MyISAM类型的表，CHECK TABLE语句还会更新关键字统计数据。而且，CHECK TABLE也可以检查视图是否有错误，比如在视图定义中被引用的表已不存在。该语句的基本语法如下：\n1 2 CHECK TABLE tbl_name [, tbl_name] ... [option] ... option = {QUICK | FAST | MEDIUM | EXTENDED | CHANGED} 其中，tbl_name是表名；option参数有5个取值，分别是QUICK、FAST、MEDIUM、EXTENDED和 CHANGED。各个选项的意义分别是：\nQUICK ：不扫描行，不检查错误的连接。 FAST ：只检查没有被正确关闭的表。 CHANGED ：只检查上次检查后被更改的表和没有被正确关闭的表。 MEDIUM ：扫描行，以验证被删除的连接是有效的。也可以计算各行的关键字校验和，并使用计算出的校验和验证这一点。 EXTENDED ：对每行的所有关键字进行一个全面的关键字查找。这可以确保表是100%一致的，但 是花的时间较长。 option只对MyISAM类型的表有效，对InnoDB类型的表无效。比如：\n该语句对于检查的表可能会产生多行信息。最后一行有一个状态的 Msg_type 值，Msg_text 通常为 OK。 如果得到的不是 OK，通常要对其进行修复；是 OK 说明表已经是最新的了。表已经是最新的，意味着存 储引擎对这张表不必进行检查。\n3. 优化表 方式1：OPTIMIZE TABLE\nMySQL中使用 OPTIMIZE TABLE 语句来优化表。但是，OPTILMIZE TABLE语句只能优化表中的 VARCHAR 、 BLOB 或 TEXT 类型的字段。一个表使用了这些字段的数据类型，若已经 删除 了表的一大部分数据，或者已经对含有可变长度行的表（含有VARCHAR、BLOB或TEXT列的表）进行了很多 更新 ，则应使用OPTIMIZE TABLE来重新利用未使用的空间，并整理数据文件的 碎片 。\nOPTIMIZE TABLE 语句对InnoDB和MyISAM类型的表都有效。该语句在执行过程中也会给表加上 只读锁 。\nOPTILMIZE TABLE语句的基本语法如下：\n1 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... LOCAL | NO_WRITE_TO_BINLOG关键字的意义和分析表相同，都是指定不写入二进制日志。\n执行完毕，Msg_text显示\n‘numysql.SYS_APP_USER’, ‘optimize’, ‘note’, ‘Table does not support optimize, doing recreate + analyze instead’\n原因是我服务器上的MySQL是InnoDB存储引擎。\n到底优化了没有呢？看官网！\nMySQL :: MySQL 8.0 Reference Manual :: 13.7.3.4 OPTIMIZE TABLE Statement\n在MyISAM中，是先分析这张表，然后会整理相关的MySQL datafile，之后回收未使用的空间；在InnoDB 中，回收空间是简单通过Alter table进行整理空间。在优化期间，MySQL会创建一个临时表，优化完成之 后会删除原始表，然后会将临时表rename成为原始表。\n说明： 在多数的设置中，根本不需要运行OPTIMIZE TABLE。即使对可变长度的行进行了大量的更新，也不需要经常运行， 每周一次 或 每月一次 即可，并且只需要对 特定的表 运行。\n方式二：使用mysqlcheck命令\n3.8 小结 上述这些方法都是有利有弊的。比如：\n修改数据类型，节省存储空间的同时，你要考虑到数据不能超过取值范围； 增加冗余字段的时候，不要忘了确保数据一致性； 把大表拆分，也意味着你的查询会增加新的连接，从而增加额外的开销和运维的成本。 因此，你一定要结合实际的业务需求进行权衡。\n4. 大表优化 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：\n4.1 限定查询的范围 禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；\n4.2 读/写分离 经典的数据库拆分方案，主库负责写，从库负责读。\n一主一从模式： 双主双从模式： 4.3 垂直拆分 当数据量级达到 千万级 以上时，有时候我们需要把一个数据库切成多份，放到不同的数据库服务器上， 减少对单一数据库服务器的访问压力。\n如果数据库的数据表过多，可以采用垂直分库的方式，将关联的数据库部署在同一个数据库上。 如果数据库中的列过多，可以采用垂直分表的方式，将一张数据表分拆成多张数据表，把经常一起使用的列放在同一张表里。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。\n垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起 JOIN 操作。此外，垂直拆分会让事务变得更加复杂。\n4.4 水平拆分 下面补充一下数据库分片的两种常见方案：\n客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 **中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。**我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 5. 其它调优策略 5.1 服务器语句超时处理 在MySQL 8.0中可以设置服务器语句超时的限制，单位可以达到毫秒级别。当中断的执行语句超过设置的毫秒数后，服务器将终止查询影响不大的事务或连接，然后将错误报给客户端。\n设置服务器语句超时的限制，可以通过设置系统变量 MAX_EXECUTION_TIME 来实现。默认情况下，MAX_EXECUTION_TIME的值为0，代表没有时间限制。例如：\n1 SET GLOBAL MAX_EXECUTION_TIME=2000; 1 SET SESSION MAX_EXECUTION_TIME=2000; #指定该会话中SELECT语句的超时时间 5.2 创建全局通用表空间 5.3 MySQL 8.0新特性：隐藏索引对调优的帮助 ","permalink":"https://cold-bin.github.io/post/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/","tags":["mysql索引及调优","数据库几大范式"],"title":"MySQL索引及调优篇"},{"categories":["mysql"],"contents":"[toc]\n第04章_逻辑架构 1. 逻辑架构剖析 1.1 服务器处理客户端请求 首先MySQL是典型的C/S架构，即Client/Server 架构，服务端程序使用的mysqld。\n不论客户端进程和服务器进程是采用哪种方式进行通信，最后实现的效果是：客户端进程向服务器进程发送一段文本（SQL语句），服务器进程处理后再向客户端进程发送一段文本（处理结果）。\n那服务器进程对客户端进程发送的请求做了什么处理，才能产生最后的处理结果呢？这里以查询请求为 例展示：\n下面具体展开如下：\n1.2 Connectors Connectors, 指的是不同语言中与SQL的交互。MySQL首先是一个网络程序，在TCP之上定义了自己的应用层协议。所以要使用MySQL，我们可以编写代码，跟MySQL Server建立TCP连接，之后按照其定义好的协议进行交互。或者比较方便的方法是调用SDK，比如Native C API、JDBC、PHP等各语言MySQL Connectors,或者通过ODBC。但通过SDK来访问MySQL，本质上还是在TCP连接上通过MySQL协议跟MySQL进行交互\n接下来的MySQL Server结构可以分为如下三层：\n1.3 第一层：连接层 系统（客户端）访问 MySQL 服务器前，做的第一件事就是建立 TCP 连接。经过三次握手建立连接成功后， MySQL 服务器对 TCP 传输过来的账号密码做身份认证、权限获取。\n用户名或密码不对，会收到一个Access denied for user错误，客户端程序结束执行 用户名密码认证通过，会从权限表查出账号拥有的权限与连接关联，之后的权限判断逻辑，都将依赖于此时读到的权限 TCP 连接收到请求后，必须要分配给一个线程专门与这个客户端的交互。所以还会有个线程池，去走后面的流程。每一个连接从线程池中获取线程，省去了创建和销毁线程的开销。\n所以连接管理的职责是负责认证、管理连接、获取权限信息。\n1.4 第二层：服务层 第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。\n在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化：如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。\n如果是SELECT语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。\nSQL Interface: SQL接口\n接收用户的SQL命令，并且返回用户需要查询的结果。比如SELECT ... FROM就是调用SQL Interface MySQL支持DML（数据操作语言）、DDL（数据定义语言）、存储过程、视图、触发器、自定义函数等多种SQL语言接口 Parser: 解析器\n在解析器中对 SQL 语句进行语法分析、语义分析。将SQL语句分解成数据结构，并将这个结构传递到后续步骤，以后SQL语句的传递和处理就是基于这个结构的。如果在分解构成中遇到错误，那么就说明这个SQL语句是不合理的。\n在SQL命令传递到解析器的时候会被解析器验证和解析，并为其创建语法树 ，并根据数据字典丰富查询语法树，会验证该客户端是否具有执行该查询的权限。创建好语法树后，MySQL还会对SQl查询进行语法上的优化，进行查询重写。\nps: 所以在连接数据库的时候权限一般不要给的太高，一般只是应用数据的话，只需要赋予用户不能对系统数据库进行操作的权限\nOptimizer: 查询优化器\nSQL语句在语法解析之后、查询之前会使用查询优化器确定 SQL 语句的执行路径，生成一个执行计划 。 这个执行计划表明应该使用哪些索引进行查询（全表检索还是使用索引检索），表之间的连接顺序如何，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。 它使用“ 选取-投影-连接 ”策略进行查询。例如： 1 SELECT id,name FROM student WHERE gender = \u0026#39;女\u0026#39;; 这个SELECT查询先根据WHERE语句进行选取 ，而不是将表全部查询出来以后再进行gender过滤。 这个SELECT查询先根据id和name进行属性投影，而不是将属性全部取出以后再进行过滤，将这两个查询条件连接起来生成最终查询结果。\nps: 编写查询语句时，应尽可能避免全表查询和表级锁，写SQL语句应该尽可能地书写性能更好的SQL语句\nCaches \u0026amp; Buffers： 查询缓存组件\nMySQL内部维持着一些Cache和Buffer，比如Query Cache用来缓存一条SELECT语句的执行结果，如果能够在其中找到对应的查询结果，那么就不必再进行查询解析、优化和执行的整个过程了，直接将结果反馈给客户端。 这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等。 这个查询缓存可以在不同客户端之间共享。 MySQL 8.0+ 不支持查询缓存，并且鼓励用户升级以使用服务器端查询重写或ProxySQL作为中间人缓存。 1.5 第三层：引擎层 插件式存储引擎层（Storage Engines），真正的负责了MySQL中数据的存储和提取，对物理服务器级别维护的底层数据执行操作，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样 我们可以根据自己的实际需要进行选取。\nMySQL 8.0.25默认支持的存储引擎如下：\n1.6 存储层 所有的数据，数据库、表的定义，表的每一行的内容，索引，都是存在文件系统上，以文件的方式存在的，并完成与存储引擎的交互。当然有些存储引擎比如InnoDB，也支持不使用文件系统直接管理裸设备，但现代文件系统的实现使得这样做没有必要了。在文件系统之下，可以使用本地磁盘，可以使用 DAS、NAS、SAN等各种存储系统。\n1.7 小结 MySQL架构图本节开篇所示。下面为了熟悉SQL执行流程方便，我们可以简化如下：\n简化为三层结构：\n连接层：客户端和服务器端建立连接，客户端发送 SQL 至服务器端； SQL 层（服务层）：对 SQL 语句进行查询处理；与数据库文件的存储方式无关； 存储引擎层：与数据库文件打交道，负责数据的存储和读取。 2. SQL执行流程 2.1 MySQL中的SQL执行流程 MySQL的查询流程：\n查询缓存：Server 如果在查询缓存中发现了这条 SQL 语句，就会直接将结果返回给客户端；如果没有，就进入到解析器阶段。需要说明的是，因为查询缓存往往效率不高，所以在 MySQL8.0 之后就抛弃了这个功能。 总之，因为查询缓存往往弊大于利，查询缓存的失效非常频繁。缓存本质就是将查询语句的字符串作为key，查询结果作为value进行缓存，瓶颈在于更新数据之后，再使用缓存取得数据虽然快但是显然不是新的数据，而且查询的语法发生改变、多个空格等都会导致缓存命中失败\n一般建议大家在静态表里使用查询缓存，什么叫静态表呢？就是一般我们极少更新的表。比如，一个系统配置表、字典表，这张表上的查询才适合使用查询缓存。好在MySQL也提供了这种“按需使用”的方式。你可以将 my.cnf 参数 query_cache_type 设置成 DEMAND，代表当 sql 语句中有 SQL_CACHE关键字时才缓存。比如：\n1 2 # query_cache_type 有3个值。 0代表关闭查询缓存OFF，1代表开启ON，2代表(DEMAND) query_cache_type = 2 这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以供SQL_CACHE显示指定，像下面这个语句一样：\n1 SELECT SQl_CACHE * FROM test WHERE ID=5; 查看当前 mysql 实例是否开启缓存机制\n1 2 # MySQL5.7中： show global variables like \u0026#34;%query_cache_type%\u0026#34;; 监控查询缓存的命中率：\n1 show status like \u0026#39;%Qcache%\u0026#39;; 运行结果解析：\nQcache_free_blocks: 表示查询缓存中海油多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内部碎片过多了，可能在一定的时间进行整理。\nQcache_free_memory: 查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，DBA可以根据实际情况做出调整。\nQcache_hits: 表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越理想。\nQcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行查询处理后把结果insert到查询缓存中。这样的情况的次数越多，表示查询缓存应用到的比较少，效果也就不理想。当然系统刚启动后，查询缓存是空的，这也正常。\nQcache_lowmem_prunes: 该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的调整缓存大小。\nQcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。\nQcache_queries_in_cache: 当前缓存中缓存的查询数量。\nQcache_total_blocks: 当前缓存的block数量。\n解析器：在解析器中对 SQL 语句进行语法分析、语义分析。 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。SQL语句的分析分为词法分析与语法分析。\n分析器先做词法分析。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。\nMySQL 从你输入的select这个关键字识别出来，这是一个查询语句。它也要把字符串T识别成表名T，把字符串ID识别成列ID。\n接着，要做语法分析。根据词法分析的结果，语法分析器（比如：Bison）会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法 。\nselect department_id,job_id, avg(salary) from employees group by department_id; 如果SQL语句正确，则会生成一个这样的语法树：\n下图是SQL分词分析的过程步骤:\n至此解析器的工作任务也基本圆满了。\n优化器：在优化器中会确定 SQL 语句的执行路径，比如是根据 全表检索 ，还是根据 索引检索 等。\n经过解释器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。一条查询可以有很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。\n比如：优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联 (join) 的时候，决定各个表的连接顺序，还有表达式简化、子查询转为连接、外连接转为内连接等。\n举例：如下语句是执行两个表的 join：\n1 2 select * from test1 join test2 using(ID) where test1.name=\u0026#39;zhangwei\u0026#39; and test2.name=\u0026#39;mysql高级课程\u0026#39;; 1 2 3 4 5 6 7 8 9 方案1：可以先从表 test1 里面取出 name=\u0026#39;zhangwei\u0026#39;的记录的 ID 值，再根据 ID 值关联到表 test2，再判 断 test2 里面 name的值是否等于 \u0026#39;mysql高级课程\u0026#39;。 方案2：可以先从表 test2 里面取出 name=\u0026#39;mysql高级课程\u0026#39; 的记录的 ID 值，再根据 ID 值关联到 test1， 再判断 test1 里面 name的值是否等于 zhangwei。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化 器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。 如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等。后面讲到索引我们再谈。 在查询优化器中，可以分为 逻辑查询 优化阶段和 物理查询 优化阶段。\n逻辑查询优化就是通过改变SQL语句的内容来使得SQL查询更高效，同时为物理查询优化提供更多的候选执行计划。通常采用的方式是对SQL语句进行等价变换，对查询进行重写，而查询重写的数学基础就是关系代数。对条件表达式进行等价谓词重写、条件简化，对视图进行重写，对子查询进行优化，对连接语义进行了外连接消除、嵌套连接消除等。\n物理查询优化是基于关系代数进行的查询重写，而关系代数的每一步都对应着物理计算，这些物理计算往往存在多种算法，因此需要计算各种物理路径的代价，从中选择代价最小的作为执行计划。在这个阶段里，对于单表和多表连接的操作，需要高效地使用索引，提升查询效率。\n执行器： 截止到现在，还没有真正去读写真实的表，仅仅只是产出了一个执行计划。于是就进入了执行器阶段 。\n在执行之前需要判断该用户是否 具备权限 。如果没有，就会返回权限错误。如果具备权限，就执行 SQL 查询并返回结果。在 MySQL8.0 以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存。\n1 select * from test where id=1; 比如：表 test 中，ID 字段没有索引，那么执行器的执行流程是这样的：\n1 2 3 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是1，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。\nSQL 语句在 MySQL 中的流程是： SQL语句→查询缓存→解析器→优化器→执行器 。\n2.2 MySQL8中SQL执行原理 1) 确认profiling是否开启 了解查询语句底层执行的过程：select @profiling 或者 show variables like '%profiling' 查看是否开启计划。开启它可以让MySQL收集在SQL\n执行时所使用的资源情况，命令如下：\n1 2 mysql\u0026gt; select @@profiling; mysql\u0026gt; show variables like \u0026#39;profiling\u0026#39;; profiling=0 代表关闭，我们需要把 profiling 打开，即设置为 1：\n1 mysql\u0026gt; set profiling=1; 2) 多次执行相同SQL查询 然后我们执行一个 SQL 查询（你可以执行任何一个 SQL 查询）：\n1 mysql\u0026gt; select * from employees; 3) 查看profiles 查看当前会话所产生的所有 profiles：\n1 mysql\u0026gt; show profiles; # 显示最近的几次查询 4) 查看profile 显示执行计划，查看程序的执行步骤：\n1 mysql\u0026gt; show profile; 当然你也可以查询指定的 Query ID，比如：\n1 mysql\u0026gt; show profile for query 7; 查询 SQL 的执行时间结果和上面是一样的。\n此外，还可以查询更丰富的内容：\n1 mysql\u0026gt; show profile cpu,block io for query 6; 继续：\n1 mysql\u0026gt; show profile cpu,block io for query 7; 1、除了查看cpu、io阻塞等参数情况，还可以查询下列参数的利用情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Syntax: SHOW PROFILE [type [, type] ... ] [FOR QUERY n] [LIMIT row_count [OFFSET offset]] type: { | ALL -- 显示所有参数的开销信息 | BLOCK IO -- 显示IO的相关开销 | CONTEXT SWITCHES -- 上下文切换相关开销 | CPU -- 显示CPU相关开销信息 | IPC -- 显示发送和接收相关开销信息 | MEMORY -- 显示内存相关开销信息 | PAGE FAULTS -- 显示页面错误相关开销信息 | SOURCE -- 显示和Source_function,Source_file,Source_line 相关的开销信息 | SWAPS -- 显示交换次数相关的开销信息 } 2、发现两次查询当前情况都一致，说明没有缓存。\n在 8.0 版本之后，MySQL 不再支持缓存的查询。一旦数据表有更新，缓存都将清空，因此只有数据表是静态的时候，或者数据表很少发生变化时，使用缓存查询才有价值，否则如果数据表经常更新，反而增加了 SQL 的查询时间。\n2.3 MySQL5.7中SQL执行原理 上述操作在MySQL5.7中测试，发现前后两次相同的sql语句，执行的查询过程仍然是相同的。不是会使用 缓存吗？这里我们需要 显式开启查询缓存模式 。在MySQL5.7中如下设置：\n1) 配置文件中开启查询缓存 在 /etc/my.cnf 中新增一行：\n1 query_cache_type=1 2) 重启mysql服务 1 systemctl restart mysqld 3) 开启查询执行计划 由于重启过服务，需要重新执行如下指令，开启profiling。\n1 mysql\u0026gt; set profiling=1; 4) 执行语句两次： 1 mysql\u0026gt; select * from locations; 5) 查看profiles 6) 查看profile 显示执行计划，查看程序的执行步骤：\n1 mysql\u0026gt; show profile for query 1; 1 mysql\u0026gt; show profile for query 2; 结论不言而喻。执行编号2时，比执行编号1时少了很多信息，从截图中可以看出查询语句直接从缓存中 获取数据。\n2.4 SQL语法顺序 随着Mysql版本的更新换代，其优化器也在不断的升级，优化器会分析不同执行顺序产生的性能消耗不同 而动态调整执行顺序。\n3. 数据库缓冲池（buffer pool） InnoDB 存储引擎是以页为单位来管理存储空间的，我们进行的增删改查操作其实本质上都是在访问页面（包括读页面、写页面、创建新页面等操作）。而磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS会申请占用内存来作为数据缓冲池，在真正访问页面之前，需要把在磁盘上的页缓存到内存中的 Buffer Pool 之后才可以访问。\n这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。\n3.1 缓冲池 vs 查询缓存 缓冲池和查询缓存是一个东西吗？不是。\n1) 缓冲池（Buffer Pool） 首先我们需要了解在 InnoDB 存储引擎中，缓冲池都包括了哪些。\n在 InnoDB 存储引擎中有一部分数据会放到内存中，缓冲池则占了这部分内存的大部分，它用来存储各种数据的缓存，如下图所示：\n从图中，你能看到 InnoDB 缓冲池包括了数据页、索引页、插入缓冲、锁信息、自适应 Hash 和数据字典信息等。\n缓存池的重要性：\n缓存原则：\n位置 * 频次，这个原则可以帮我们对 I/O 访问效率进行优化。\n首先，位置决定效率，提供缓冲池就是为了在内存中可以直接访问数据。\n其次，频次决定优先级顺序。因为缓冲池的大小是有限的，比如磁盘有200G，但是内存只有16G，缓冲池大小只有1G，就无法将所有数据都加载到缓冲池里，这时就涉及到优先级顺序，会优先对使用频次高的热数据进行加载。\n缓冲池的预读特性:\n缓冲池的作用就是提升 I/O 效率，而我们进行读取数据的时候存在一个“局部性原理”，也就是说我们使用了一些数据，大概率还会使用它周围的一些数据，因此采用“预读”的机制提前加载，可以减少未来可能的磁盘 I/O 操作。\n2) 查询缓存 那么什么是查询缓存呢？\n查询缓存是提前把查询结果缓存起来，这样下次不需要执行就可以直接拿到结果。需要说明的是，在 MySQL 中的查询缓存，不是缓存查询计划，而是查询对应的结果。因为命中条件苛刻，而且只要数据表发生变化，查询缓存就会失效，因此命中率低。\n3.2 缓冲池如何读取数据 缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。\n缓存在数据库中的结构和作用如下图所示：\n如果我们执行 SQL 语句的时候更新了缓存池中的数据，那么这些数据会马上同步到磁盘上吗？\n实际上，当我们对数据库中的记录进行修改的时候，首先会修改缓冲池中页里面的记录信息，然后数据库会以一定的频率刷新到磁盘中。注意并不是每次发生更新操作，都会立即进行磁盘回写。缓冲池会采用一种叫做 checkpoint 的机制 将数据回写到磁盘上，这样做的好处就是提升了数据库的整体性能。\n比如，当缓冲池不够用时，需要释放掉一些不常用的页，此时就可以强行采用checkpoint的方式，将不常用的脏页回写到磁盘上，然后再从缓存池中将这些页释放掉。这里的脏页 (dirty page) 指的是缓冲池中被修改过的页，与磁盘上的数据页不一致。\n3.3 查看/设置缓冲池的大小 如果你使用的是 MySQL MyISAM 存储引擎，它只缓存索引，不缓存数据，对应的键缓存参数为key_buffer_size，你可以用它进行查看。\n如果你使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小。命令如下：\n1 show variables like \u0026#39;innodb_buffer_pool_size\u0026#39;; 你能看到此时 InnoDB 的缓冲池大小只有 134217728/1024/1024=128MB。我们可以修改缓冲池大小，比如改为256MB，方法如下：\n1 set global innodb_buffer_pool_size = 268435456; 或者：\n1 2 [server] innodb_buffer_pool_size = 268435456 3.4 多个Buffer Pool实例 1 2 [server] innodb_buffer_pool_instances = 2 这样就表明我们要创建2个 Buffer Pool 实例。\n我们看下如何查看缓冲池的个数，使用命令：\n1 show variables like \u0026#39;innodb_buffer_pool_instances\u0026#39;; 那每个 Buffer Pool 实例实际占多少内存空间呢？其实使用这个公式算出来的：\n1 innodb_buffer_pool_size/innodb_buffer_pool_instances 也就是总共的大小除以实例的个数，结果就是每个 Buffer Pool 实例占用的大小。\n不过也不是说 Buffer Pool 实例创建的越多越好，分别管理各个 Buffer Pool 也是需要性能开销的，InnDB规定：当innodb_buffer_pool_size的值小于1G的时候设置多个实例是无效的，InnoDB会默认把innodb_buffer_pool_instances的值修改为1。而我们鼓励在 Buffer Pool 大于等于 1G 的时候设置多个 Buffer Pool 实例。\n3.5 引申问题 Buffer Pool是MySQL内存结构中十分核心的一个组成，你可以先把它想象成一个黑盒子。\n黑盒下的更新数据流程\n当我们查询数据的时候，会先去 Buffer Pool 中查询。如果 Buffer Pool 中不存在，存储引擎会先将数据从磁盘加载到 Buffer Pool 中，然后将数据返回给客户端；同理，当我们更新某个数据的时候，如果这个数据不存在于 Buffer Pool，同样会先数据加载进来，然后修改内存的数据。被修改的数据会在之后统一刷入磁盘。\n我更新到一半突然发生错误了，想要回滚到更新之前的版本，该怎么办？连数据持久化的保证、事务回滚都做不到还谈什么崩溃恢复？\n答案：Redo Log \u0026amp; Undo Log\n第05章_存储引擎 1. 查看存储引擎 查看mysql提供什么存储引擎 1 show engines; 2. 设置系统默认的存储引擎 查看默认的存储引擎 1 2 3 show variables like \u0026#39;%storage_engine%\u0026#39;; #或 SELECT @@default_storage_engine; 修改默认的存储引擎 如果在创建表的语句中没有显式指定表的存储引擎的话，那就会默认使用 InnoDB 作为表的存储引擎。 如果我们想改变表的默认存储引擎的话，可以这样写启动服务器的命令行：\n1 SET DEFAULT_STORAGE_ENGINE=MyISAM; 或者修改 my.cnf 文件：\n1 2 3 default-storage-engine=MyISAM # 重启服务 systemctl restart mysqld.service 3. 设置表的存储引擎 存储引擎是负责对表中的数据进行提取和写入工作的，我们可以为 不同的表设置不同的存储引擎 ，也就是 说不同的表可以有不同的物理存储结构，不同的提取和写入方式。\n3.1 创建表时指定存储引擎 我们之前创建表的语句都没有指定表的存储引擎，那就会使用默认的存储引擎 InnoDB 。如果我们想显 式的指定一下表的存储引擎，那可以这么写：\n1 2 3 CREATE TABLE 表名( 建表语句; ) ENGINE = 存储引擎名称; 3.2 修改表的存储引擎 如果表已经建好了，我们也可以使用下边这个语句来修改表的存储引擎：\n1 ALTER TABLE 表名 ENGINE = 存储引擎名称; 比如我们修改一下 engine_demo_table 表的存储引擎：\n1 mysql\u0026gt; ALTER TABLE engine_demo_table ENGINE = InnoDB; 这时我们再查看一下 engine_demo_table 的表结构：\n1 2 3 4 5 6 7 mysql\u0026gt; SHOW CREATE TABLE engine_demo_table\\G *************************** 1. row *************************** Table: engine_demo_table Create Table: CREATE TABLE `engine_demo_table` ( `i` int(11) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.01 sec) 4. 引擎介绍 4.1 InnoDB 引擎：具备外键支持功能的事务存储引擎 MySQL从3.23.34a开始就包含InnoDB存储引擎。 大于等于5.5之后，默认采用InnoDB引擎 。 InnoDB是MySQL的 默认事务型引擎 ，它被设计用来处理大量的短期(short-lived)事务。可以确保事务的完整提交(Commit)和回滚(Rollback)。 除了增加和查询外，还需要更新、删除操作，那么，应优先选择InnoDB存储引擎。 除非有非常特别的原因需要使用其他的存储引擎，否则应该优先考虑InnoDB引擎。 数据文件结构：（在《第02章_MySQL数据目录》章节已讲） 表名.frm 存储表结构（MySQL8.0时，合并在表名.ibd中） 表名.ibd 存储数据和索引 InnoDB是 为处理巨大数据量的最大性能设计 。 在以前的版本中，字典数据以元数据文件、非事务表等来存储。现在这些元数据文件被删除 了。比如： .frm ， .par ， .trn ， .isl ， .db.opt 等都在MySQL8.0中不存在了。 对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保存数据和索引。 MyISAM只缓存索引，不缓存真实数据；InnoDB不仅缓存索引还要缓存真实数据，对内存要求较高 ，而且内存大小对性能有决定性的影响。 4.2 MyISAM 引擎：主要的非事务处理存储引擎 MyISAM提供了大量的特性，包括全文索引、压缩、空间函数(GIS)等，但MyISAM不支持事务、行级 锁、外键 ，有一个毫无疑问的缺陷就是崩溃后无法安全恢复 。 5.5之前默认的存储引擎 优势是访问的速度快 ，对事务完整性没有要求或者以SELECT、INSERT为主的应用 针对数据统计有额外的常数存储。故而 count(*) 的查询效率很高 数据文件结构：（在《第02章_MySQL数据目录》章节已讲） 表名.frm 存储表结构 表名.MYD 存储数据 (MYData) 表名.MYI 存储索引 (MYIndex) 应用场景：只读应用或者以读为主的业务 4.3 Archive 引擎：用于数据存档 下表展示了ARCHIVE 存储引擎功能 4.4 Blackhole 引擎：丢弃写操作，读操作会返回空内容 没有实现任何存储机制，会丢弃所有插入的数据，不做任何保存 但服务器会记录该引擎对应表的日志，所以可以用于赋值数据到备库，或者简单地记录到日志，但这种应用方式会碰到很多问题，因此不推荐 4.5 CSV 引擎：存储数据时，以逗号分隔各个数据项 使用案例如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 mysql\u0026gt; CREATE TABLE test (i INT NOT NULL, c CHAR(10) NOT NULL) ENGINE = CSV; Query OK, 0 rows affected (0.06 sec) mysql\u0026gt; INSERT INTO test VALUES(1,\u0026#39;record one\u0026#39;),(2,\u0026#39;record two\u0026#39;); Query OK, 2 rows affected (0.05 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u0026gt; SELECT * FROM test; +---+------------+ | i | c | +---+------------+ | 1 | record one | | 2 | record two | +---+------------+ 2 rows in set (0.00 sec) 创建CSV表还会创建相应的元文件 ，用于 存储表的状态 和 表中存在的行数 。此文件的名称与表的名称相同，后缀为 CSM 。如图所示\n如果检查 test.CSV 通过执行上述语句创建的数据库目录中的文件，其内容使用Notepad++打开如下：\n1 2 \u0026#34;1\u0026#34;,\u0026#34;record one\u0026#34; \u0026#34;2\u0026#34;,\u0026#34;record two\u0026#34; 这种格式可以被 Microsoft Excel 等电子表格应用程序读取，甚至写入。使用Microsoft Excel打开如图所示\n4.6 Memory 引擎：置于内存的表 概述：\nMemory采用的逻辑介质是内存，响应速度很快 ，但是当mysqld守护进程崩溃的时候数据会丢失 。另外，要求存储的数据是数据长度不变的格式，比如，Blob和Text类型的数据不可用(长度不固定的)。\n主要特征：\nMemory同时 支持哈希（HASH）索引 和 B+树索引 。 Memory表至少比MyISAM表要快一个数量级 。 MEMORY 表的大小是受到限制 的。表的大小主要取决于两个参数，分别是 max_rows 和 max_heap_table_size 。其中，max_rows可以在创建表时指定；max_heap_table_size的大小默 认为16MB，可以按需要进行扩大。 数据文件与索引文件分开存储。 缺点：其数据易丢失，生命周期短。基于这个缺陷，选择MEMORY存储引擎时需要特别小心。 使用Memory存储引擎的场景：\n目标数据比较小 ，而且非常频繁的进行访问 ，在内存中存放数据，如果太大的数据会造成内存溢出 。可以通过参数 max_heap_table_size 控制Memory表的大小，限制Memory表的最大的大小。 如果数据是临时的 ，而且必须立即可用得到，那么就可以放在内存中。 存储在Memory表中的数据如果突然间丢失的话也没有太大的关系。 4.7 Federated 引擎：访问远程表 Federated引擎是访问其他MySQL服务器的一个 代理 ，尽管该引擎看起来提供了一种很好的 跨服务 器的灵活性 ，但也经常带来问题，因此 默认是禁用的 。\n4.8 Merge引擎：管理多个MyISAM表构成的表集合 4.9 NDB引擎：MySQL集群专用存储引擎 也叫做 NDB Cluster 存储引擎，主要用于 MySQL Cluster 分布式集群 环境，类似于 Oracle 的 RAC 集 群。\n4.10 引擎对比 MySQL中同一个数据库，不同的表可以选择不同的存储引擎。如下表对常用存储引擎做出了对比。\n其实这些东西大家没必要立即就给记住，列出来的目的就是想让大家明白不同的存储引擎支持不同的功能。\n其实我们最常用的就是 InnoDB 和 MyISAM ，有时会提一下 Memory 。其中 InnoDB 是 MySQL 默认的存储引擎。\n5. MyISAM和InnoDB 很多人对 InnoDB 和 MyISAM 的取舍存在疑问，到底选择哪个比较好呢？\nMySQL5.5之前的默认存储引擎是MyISAM，5.5之后改为了InnoDB。\n","permalink":"https://cold-bin.github.io/post/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/","tags":["mysql存储引擎","innodb引擎","sql执行过程"],"title":"MySQL架构篇"},{"categories":["golang"],"contents":"Go语言reflect包的使用 反射包使用 map and slice 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func MapAndSlice() { stringSlice := make([]string,0) stringMap := make(map[string]string) sliceType := reflect.TypeOf(stringSlice) mapType := reflect.TypeOf(stringMap) rMap := reflect.MakeMap(mapType) rSlice := reflect.MakeSlice(sliceType,0,0) k := \u0026#34;first\u0026#34; rMap.SetMapIndex(reflect.ValueOf(k),reflect.ValueOf(\u0026#34;test\u0026#34;)) i := rMap.Interface().(map[string]string) fmt.Println(i) reflect.AppendSlice(rSlice,reflect.ValueOf(\u0026#34;test slice\u0026#34;)) strings := rSlice.Interface().([]string) fmt.Println(strings) } function 1 2 3 4 5 6 7 8 9 10 11 func MakeFun()interface{} { f := timeMe vf := reflect.ValueOf(f) return reflect.MakeFunc(reflect.TypeOf(f),func(in []reflect.Value) []reflect.Value { start := time.Now() out := vf.Call(in) end := time.Now() fmt.Printf(\u0026#34;calling %s took %v\\n\u0026#34;, runtime.FuncForPC(vf.Pointer()).Name(), end.Sub(start)) return out }).Interface() } struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func MakeStruct(vals ...interface{}) interface{} { var sfs []reflect.StructField for k, v := range vals { t := reflect.TypeOf(v) sf := reflect.StructField{ Name: fmt.Sprintf(\u0026#34;F%d\u0026#34;, (k + 1)), Type: t, } sfs = append(sfs, sf) } st := reflect.StructOf(sfs) so := reflect.New(st) return so.Interface() } 获取type typeOf valueOf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //type和value m := MyStruct{\u0026#34;test\u0026#34;,10} t := reflect.TypeOf(m) fmt.Println(t) v := reflect.ValueOf(\u0026amp;m) fmt.Println(v) //读取 for i := 0; i \u0026lt; t.NumField() ;i++ { fmt.Printf(\u0026#34;name:%s,json_tag:%s\u0026#34;,t.Field(i).Name,t.Field(i).Tag.Get(\u0026#34;json\u0026#34;)) fmt.Println() } //设置 v.Elem().Field(0).SetString(\u0026#34;test1\u0026#34;) v.Elem().Field(1).SetInt(31) //读取 for i := 0; i \u0026lt; t.NumField() ;i++ { fmt.Printf(\u0026#34;name:%v\u0026#34;,v.Elem().Field(i)) fmt.Println() } 反射调用struct方法 1 2 3 4 5 6 //带参数调用方式 setNameMethod := v.MethodByName( \u0026#34;AddAge\u0026#34; ) args := []reflect.Value{ reflect.ValueOf(10) } //构造一个类型为reflect.Value的切片 setNameMethod.Call(args) //返回Value类型 fmt.Println(\u0026#34;User.Age = \u0026#34;,m.Age) 还有很多反射api就不逐一介绍了，翻翻源码就行 三大法则 运行时反射是程序在运行期间检查其自身结构的一种方式。反射带来的灵活性是一把双刃剑，反射作为一种元编程方式可以减少重复代码，但是过量的使用反射会使我们的程序逻辑变得难以理解并且运行缓慢。我们在这一节中会介绍 Go 语言反射的三大法则，其中包括：\n从 interface{} 变量可以反射出反射对象； 从反射对象可以获取 interface{} 变量； 要修改反射对象，其值必须可设置； 第一法则 反射的第一法则是我们能将 Go 语言的 interface{} 变量转换成反射对象。很多读者可能会对这以法则产生困惑 — 为什么是从 interface{} 变量到反射对象？当我们执行 reflect.ValueOf(1) 时，虽然看起来是获取了基本类型 int 对应的反射类型，但是由于 reflect.TypeOf、reflect.ValueOf 两个方法的入参都是 interface{} 类型，所以在方法执行的过程中发生了类型转换。\n因为Go 语言的函数调用都是值传递的，所以变量会在函数调用时进行类型转换。基本类型 int 会转换成 interface{} 类型，这也就是为什么第一条法则是从接口到反射对象。\n上面提到的 reflect.TypeOf 和 reflect.ValueOf 函数就能完成这里的转换，如果我们认为 Go 语言的类型和反射类型处于两个不同的世界，那么这两个函数就是连接这两个世界的桥梁。\n图 接口到反射对象\n我们可以通过以下例子简单介绍它们的作用，reflect.TypeOf 获取了变量 author 的类型，reflect.ValueOf 获取了变量的值 draven。如果我们知道了一个变量的类型和值，那么就意味着我们知道了这个变量的全部信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; ) func main() { author := \u0026#34;draven\u0026#34; fmt.Println(\u0026#34;TypeOf author:\u0026#34;, reflect.TypeOf(author)) fmt.Println(\u0026#34;ValueOf author:\u0026#34;, reflect.ValueOf(author)) } $ go run main.go TypeOf author: string ValueOf author: draven 有了变量的类型之后，我们可以通过 Method 方法获得类型实现的方法，通过 Field 获取类型包含的全部字段。对于不同的类型，我们也可以调用不同的方法获取相关信息：\n结构体：获取字段的数量并通过下标和字段名获取字段 StructField；\n哈希表：获取哈希表的 Key 类型；\n函数或方法：获取入参和返回值的类型；\n……\n总而言之，使用 reflect.TypeOf 和 reflect.ValueOf 能够获取 Go 语言中的变量对应的反射对象。一旦获取了反射对象，我们就能得到跟当前类型相关数据和操作，并可以使用这些运行时获取的结构执行方法。\n第二法则 反射的第二法则是我们可以从反射对象可以获取 interface{} 变量。既然能够将接口类型的变量转换成反射对象，那么一定需要其他方法将反射对象还原成接口类型的变量，reflect 中的 reflect.Value.Interface 就能完成这项工作：\n图 反射对象到接口\n不过调用 reflect.Value.Interface 方法只能获得 interface{} 类型的变量，如果想要将其还原成最原始的状态还需要经过如下所示的显式类型转换：\n1 2 v := reflect.ValueOf(1) v.Interface().(int) 从反射对象到接口值的过程是从接口值到反射对象的镜面过程，两个过程都需要经历两次转换\n从接口值到反射对象 从基本类型到接口类型的类型转换 从接口类型到反射对象的转换 从反射对象到接口值 反射对象转换成接口类型 通过显式类型转换变成原始类型 图 接口和反射对象的双向转换\n当然不是所有的变量都需要类型转换这一过程。如果变量本身就是 interface{} 类型的，那么它不需要类型转换，因为类型转换这一过程一般都是隐式的，所以我不太需要关心它，只有在我们需要将反射对象转换回基本类型时才需要显式的转换操作。\n第三法则 Go 语言反射的最后一条法则是与值是否可以被更改有关，如果我们想要更新一个 reflect.Value，那么它持有的值一定是可以被更新的，假设我们有以下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { i := 1 v := reflect.ValueOf(i) v.SetInt(10) fmt.Println(i) } $ go run reflect.go panic: reflect: reflect.flag.mustBeAssignable using unaddressable value goroutine 1 [running]: reflect.flag.mustBeAssignableSlow(0x82, 0x1014c0) /usr/local/go/src/reflect/value.go:247 +0x180 reflect.flag.mustBeAssignable(...) /usr/local/go/src/reflect/value.go:234 reflect.Value.SetInt(0x100dc0, 0x414020, 0x82, 0x1840, 0xa, 0x0) /usr/local/go/src/reflect/value.go:1606 +0x40 main.main() /tmp/sandbox590309925/prog.go:11 +0xe0 运行上述代码会导致程序崩溃并报出 “reflect: reflect.flag.mustBeAssignable using unaddressable value” 错误，仔细思考一下就能够发现出错的原因：由于 Go 语言的函数调用都是传值的，所以我们得到的反射对象跟最开始的变量没有任何关系，那么直接修改反射对象无法改变原始变量，程序为了防止错误就会崩溃。\n想要修改原变量只能使用如下的方法：\n1 2 3 4 5 6 7 8 9 func main() { i := 1 v := reflect.ValueOf(\u0026amp;i) v.Elem().SetInt(10) fmt.Println(i) } $ go run reflect.go 10 调用 reflect.ValueOf 获取变量指针； 调用 reflect.Value.Elem 获取指针指向的变量； 调用 reflect.Value.SetInt 更新变量的值： 由于 Go 语言的函数调用都是值传递的，所以我们只能只能用迂回的方式改变原变量：先获取指针对应的 reflect.Value，再通过 reflect.Value.Elem 方法得到可以被设置的变量，我们可以通过下面的代码理解这个过程：\n1 2 3 4 5 func main() { i := 1 v := \u0026amp;i *v = 10 } 如果不能直接操作 i 变量修改其持有的值，我们就只能获取 i 变量所在地址并使用 *v 修改所在地址中存储的整数。\n反射底层原理刨析 虽然在大多数的应用和服务中并不常见，但是很多框架都依赖 Go 语言的反射机制简化代码。因为 Go 语言的语法元素很少、设计简单，所以它没有特别强的表达能力，但是 Go 语言的 reflect 包能够弥补它在语法上reflect.Type的一些劣势。\nreflect 实现了运行时的反射能力，能够让程序操作不同类型的对象。反射包中有两对非常重要的函数和类型，两个函数分别是：\nreflect.TypeOf 能获取类型信息； reflect.ValueOf 能获取数据的运行时表示； 两个类型是 reflect.Type 和 reflect.Value，它们与函数是一一对应的关系：\n类型 reflect.Type 是反射包定义的一个接口，我们可以使用 reflect.TypeOf 函数获取任意变量的类型，reflect.Type 接口中定义了一些有趣的方法，MethodByName 可以获取当前类型对应方法的引用、Implements 可以判断当前类型是否实现了某个接口：\n1 2 3 4 5 6 7 8 9 10 type Type interface { Align() int FieldAlign() int Method(int) Method MethodByName(string) (Method, bool) NumMethod() int ... Implements(u Type) bool ... } 反射包中 reflect.Value 的类型与 reflect.Type 不同，它被声明成了结构体。这个结构体没有对外暴露的字段，但是提供了获取或者写入数据的方法：\n1 2 3 4 5 6 7 8 type Value struct { // 包含过滤的或者未导出的字段 } func (v Value) Addr() Value func (v Value) Bool() bool func (v Value) Bytes() []byte ... 反射包中的所有方法基本都是围绕着 reflect.Type 和 reflect.Value 两个类型设计的。我们通过 reflect.TypeOf、reflect.ValueOf 可以将一个普通的变量转换成反射包中提供的 reflect.Type 和 reflect.Value，随后就可以使用反射包中的方法对它们进行复杂的操作。\n类型和值 Go 语言的 interface{} 类型在语言内部是通过 reflect.emptyInterface 结体表示的，其中的 rtype 字段用于表示变量的类型，另一个 word 字段指向内部封装的数据：\n1 2 3 4 type emptyInterface struct { typ *rtype word unsafe.Pointer } 用于获取变量类型的 reflect.TypeOf 函数将传入的变量隐式转换成 reflect.emptyInterface 类型并获取其中存储的类型信息 reflect.rtype：\n1 2 3 4 5 6 7 8 9 10 11 func TypeOf(i interface{}) Type { eface := *(*emptyInterface)(unsafe.Pointer(\u0026amp;i)) return toType(eface.typ) } func toType(t *rtype) Type { if t == nil { return nil } return t } reflect.rtype 是一个实现了 reflect.Type 接口的结构体，该结构体实现的 reflect.rtype.String 方法可以帮助我们获取当前类型的名称：\n1 2 3 4 5 6 7 func (t *rtype) String() string { s := t.nameOff(t.str).name() if t.tflag\u0026amp;tflagExtraStar != 0 { return s[1:] } return s } reflect.TypeOf 的实现原理其实并不复杂，它只是将一个 interface{} 变量转换成了内部的 reflect.emptyInterface 表示，然后从中获取相应的类型信息。\n用于获取接口值 reflect.Value 的函数 reflect.ValueOf 实现也非常简单，在该函数中我们先调用了 reflect.escapes 保证当前值逃逸到堆上，然后通过 reflect.unpackEface 从接口中获取 reflect.Value 结构体：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func ValueOf(i interface{}) Value { if i == nil { return Value{} } escapes(i) return unpackEface(i) } func unpackEface(i interface{}) Value { e := (*emptyInterface)(unsafe.Pointer(\u0026amp;i)) t := e.typ if t == nil { return Value{} } f := flag(t.Kind()) if ifaceIndir(t) { f |= flagIndir } return Value{t, e.word, f} } reflect.unpackEface 会将传入的接口转换成 reflect.emptyInterface，然后将具体类型和指针包装成 reflect.Value 结构体后返回。\nreflect.TypeOf 和 reflect.ValueOf 的实现都很简单。我们已经分析了这两个函数的实现，现在需要了解编译器在调用函数之前做了哪些工作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import ( \u0026#34;reflect\u0026#34; ) func main() { i := 20 _ = reflect.TypeOf(i) } $ go build -gcflags=\u0026#34;-S -N\u0026#34; main.go ... MOVQ\t$20, \u0026#34;\u0026#34;..autotmp_20+56(SP) // autotmp = 20 LEAQ\ttype.int(SB), AX // AX = type.int(SB) MOVQ\tAX, \u0026#34;\u0026#34;..autotmp_19+280(SP) // autotmp_19+280(SP) = type.int(SB) LEAQ\t\u0026#34;\u0026#34;..autotmp_20+56(SP), CX // CX = 20 MOVQ\tCX, \u0026#34;\u0026#34;..autotmp_19+288(SP) // autotmp_19+288(SP) = 20 ... 从上面这段截取的汇编语言，我们可以发现在函数调用之前已经发生了类型转换，上述指令将 int 类型的变量转换成了占用 16 字节 autotmp_19+280(SP) ~ autotmp_19+288(SP) 的接口，两个 LEAQ 指令分别获取了类型的指针 type.int(SB) 以及变量 i 所在的地址。\n当我们想要将一个变量转换成反射对象时，Go 语言会在编译期间完成类型转换，将变量的类型和值转换成了 interface{} 并等待运行期间使用 reflect 包获取接口中存储的信息。\n更新变量 当我们想要更新 reflect.Value 时，就需要调用 reflect.Value.Set 更新反射对象，该方法会调用 reflect.flag.mustBeAssignable 和 reflect.flag.mustBeExported 分别检查当前反射对象是否是可以被设置的以及字段是否是对外公开的：\n1 2 3 4 5 6 7 8 9 10 func (v Value) Set(x Value) { v.mustBeAssignable() x.mustBeExported() var target unsafe.Pointer if v.kind() == Interface { target = v.ptr } x = x.assignTo(\u0026#34;reflect.Set\u0026#34;, v.typ, target) typedmemmove(v.typ, v.ptr, x.ptr) } reflect.Value.Set 会调用 reflect.Value.assignTo 并返回一个新的反射对象，这个返回的反射对象指针会直接覆盖原反射变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (v Value) assignTo(context string, dst *rtype, target unsafe.Pointer) Value { ... switch { case directlyAssignable(dst, v.typ): ... return Value{dst, v.ptr, fl} case implements(dst, v.typ): if v.Kind() == Interface \u0026amp;\u0026amp; v.IsNil() { return Value{dst, nil, flag(Interface)} } x := valueInterface(v, false) if dst.NumMethod() == 0 { *(*interface{})(target) = x } else { ifaceE2I(dst, x, target) } return Value{dst, target, flagIndir | flag(Interface)} } panic(context + \u0026#34;: value of type \u0026#34; + v.typ.String() + \u0026#34; is not assignable to type \u0026#34; + dst.String()) } reflect.Value.assignTo 会根据当前和被设置的反射对象类型创建一个新的 reflect.Value 结构体\n如果两个反射对象的类型是可以被直接替换，就会直接返回目标反射对象 如果当前反射对象是接口并且目标对象实现了接口，就会把目标对象简单包装成接口值 在变量更新的过程中，reflect.Value.assignTo 返回的 reflect.Value 中的指针会覆盖当前反射对象中的指针实现变量的更新。\n实现协议 reflect 包还为我们提供了 reflect.rtype.Implements 方法可以用于判断某些类型是否遵循特定的接口。在 Go 语言中获取结构体的反射类型 reflect.Type 还是比较容易的，但是想要获得接口类型需要通过以下方式：\n1 reflect.TypeOf((*\u0026lt;interface\u0026gt;)(nil)).Elem() Go\n我们通过一个例子在介绍如何判断一个类型是否实现了某个接口。假设我们需要判断如下代码中的 CustomError 是否实现了 Go 语言标准库中的 error 接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type CustomError struct{} func (*CustomError) Error() string { return \u0026#34;\u0026#34; } func main() { typeOfError := reflect.TypeOf((*error)(nil)).Elem() customErrorPtr := reflect.TypeOf(\u0026amp;CustomError{}) customError := reflect.TypeOf(CustomError{}) fmt.Println(customErrorPtr.Implements(typeOfError)) // #=\u0026gt; true fmt.Println(customError.Implements(typeOfError)) // #=\u0026gt; false } CustomError 类型并没有实现 error 接口； *CustomError 指针类型实现了 error 接口； 抛开上述的执行结果不谈，我们来分析一下 reflect.rtype.Implements 方法的工作原理：\n1 2 3 4 5 6 7 8 9 func (t *rtype) Implements(u Type) bool { if u == nil { panic(\u0026#34;reflect: nil type passed to Type.Implements\u0026#34;) } if u.Kind() != Interface { panic(\u0026#34;reflect: non-interface type passed to Type.Implements\u0026#34;) } return implements(u.(*rtype), t) } reflect.rtype.Implements 会检查传入的类型是不是接口，如果不是接口或者是空值就会直接崩溃并中止当前程序。在参数没有问题的情况下，上述方法会调用私有函数 reflect.implements 判断类型之间是否有实现关系：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func implements(T, V *rtype) bool { t := (*interfaceType)(unsafe.Pointer(T)) if len(t.methods) == 0 { return true } ... v := V.uncommon() i := 0 vmethods := v.methods() for j := 0; j \u0026lt; int(v.mcount); j++ { tm := \u0026amp;t.methods[i] tmName := t.nameOff(tm.name) vm := vmethods[j] vmName := V.nameOff(vm.name) if vmName.name() == tmName.name() \u0026amp;\u0026amp; V.typeOff(vm.mtyp) == t.typeOff(tm.typ) { if i++; i \u0026gt;= len(t.methods) { return true } } } return false } 如果接口中不包含任何方法，就意味着这是一个空的接口，任意类型都自动实现该接口，这时会直接返回 true。\n图 类型实现接口\n在其他情况下，由于方法都是按照字母序存储的，reflect.implements 会维护两个用于遍历接口和类型方法的索引 i 和 j 判断类型是否实现了接口，因为最多只会进行 n 次比较（类型的方法数量），所以整个过程的时间复杂度是 O(n)。\n方法调用 作为一门静态语言，如果我们想要通过 reflect 包利用反射在运行期间执行方法不是一件容易的事情，下面的十几行代码就使用反射来执行 Add(0, 1) 函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func Add(a, b int) int { return a + b } func main() { v := reflect.ValueOf(Add) if v.Kind() != reflect.Func { return } t := v.Type() argv := make([]reflect.Value, t.NumIn()) for i := range argv { if t.In(i).Kind() != reflect.Int { return } argv[i] = reflect.ValueOf(i) } result := v.Call(argv) if len(result) != 1 || result[0].Kind() != reflect.Int { return } fmt.Println(result[0].Int()) // #=\u0026gt; 1 } 通过 reflect.ValueOf 获取函数 Add 对应的反射对象； 调用 reflect.rtype.NumIn 获取函数的入参个数； 多次调用 reflect.ValueOf 函数逐一设置 argv 数组中的各个参数； 调用反射对象 Add 的 reflect.Value.Call 方法并传入参数列表； 获取返回值数组、验证数组的长度以及类型并打印其中的数据； 使用反射来调用方法非常复杂，原本只需要一行代码就能完成的工作，现在需要十几行代码才能完成，但这也是在静态语言中使用动态特性需要付出的成本。\n1 2 3 4 5 func (v Value) Call(in []Value) []Value { v.mustBe(Func) v.mustBeExported() return v.call(\u0026#34;Call\u0026#34;, in) } reflect.Value.Call 是运行时调用方法的入口，它通过两个 MustBe 开头的方法确定了当前反射对象的类型是函数以及可见性，随后调用 reflect.Value.call 完成方法调用，这个私有方法的执行过程会分成以下的几个部分\n检查输入参数以及类型的合法性 将传入的 reflect.Value 参数数组设置到栈上 通过函数指针和输入参数调用函数 从栈上获取函数的返回值 我们将按照上面的顺序分析使用 reflect 进行函数调用的几个过程。\n参数检查 参数检查是通过反射调用方法的第一步，在参数检查期间我们会从反射对象中取出当前的函数指针 unsafe.Pointer，如果该函数指针是方法，那么我们会通过 reflect.methodReceiver 获取方法的接收者和函数指针。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func (v Value) call(op string, in []Value) []Value { t := (*funcType)(unsafe.Pointer(v.typ)) ... if v.flag\u0026amp;flagMethod != 0 { rcvr = v rcvrtype, t, fn = methodReceiver(op, v, int(v.flag)\u0026gt;\u0026gt;flagMethodShift) } else { ... } n := t.NumIn() if len(in) \u0026lt; n { panic(\u0026#34;reflect: Call with too few input arguments\u0026#34;) } if len(in) \u0026gt; n { panic(\u0026#34;reflect: Call with too many input arguments\u0026#34;) } for i := 0; i \u0026lt; n; i++ { if xt, targ := in[i].Type(), t.In(i); !xt.AssignableTo(targ) { panic(\u0026#34;reflect: \u0026#34; + op + \u0026#34; using \u0026#34; + xt.String() + \u0026#34; as type \u0026#34; + targ.String()) } } 上述方法还会检查传入参数的个数以及参数的类型与函数签名中的类型是否可以匹配，任何参数的不匹配都会导致整个程序的崩溃中止。\n准备参数 当我们已经对当前方法的参数完成验证后，就会进入函数调用的下一个阶段，为函数调用准备参数，在前面函数调用一节中，我们已经介绍过 Go 语言的函数调用惯例，函数或者方法在调用时，所有的参数都会被依次放到栈上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 nout := t.NumOut() frametype, _, retOffset, _, framePool := funcLayout(t, rcvrtype) var args unsafe.Pointer if nout == 0 { args = framePool.Get().(unsafe.Pointer) } else { args = unsafe_New(frametype) } off := uintptr(0) if rcvrtype != nil { storeRcvr(rcvr, args) off = ptrSize } for i, v := range in { targ := t.In(i).(*rtype) a := uintptr(targ.align) off = (off + a - 1) \u0026amp;^ (a - 1) n := targ.size ... addr := add(args, off, \u0026#34;n \u0026gt; 0\u0026#34;) v = v.assignTo(\u0026#34;reflect.Value.Call\u0026#34;, targ, addr) *(*unsafe.Pointer)(addr) = v.ptr off += n } 通过 reflect.funcLayout 计算当前函数需要的参数和返回值的栈布局，也就是每一个参数和返回值所占的空间大小； 如果当前函数有返回值，需要为当前函数的参数和返回值分配一片内存空间 args； 如果当前函数是方法，需要向将方法的接收接收者者拷贝到 args 内存中； 将所有函数的参数按照顺序依次拷贝到对应 args 内存中 使用 reflect.funcLayout 返回的参数计算参数在内存中的位置； 将参数拷贝到内存空间中； 准备参数是计算各个参数和返回值占用的内存空间并将所有的参数都拷贝内存空间对应位置的过程，该过程会考虑函数和方法、返回值数量以及参数类型带来的差异。\n调用函数 准备好调用函数需要的全部参数后，就会通过下面的代码执行函数指针了。我们会向该函数传入栈类型、函数指针、参数和返回值的内存空间、栈的大小以及返回值的偏移量：\n1 call(frametype, fn, args, uint32(frametype.size), uint32(retOffset)) 上述函数实际上并不存在，它会在编译期间链接到 reflect.reflectcall 这个用汇编实现的函数上，我们在这里不会分析该函数的具体实现，感兴趣的读者可以自行了解其实现原理。\n处理返回值 当函数调用结束之后，就会开始处理函数的返回值\n如果函数没有任何返回值，会直接清空 args 中的全部内容来释放内存空间 如果当前函数有返回值 将 args 中与输入参数有关的内存空间清空 创建一个 nout 长度的切片用于保存由反射对象构成的返回值数组 从函数对象中获取返回值的类型和内存大小，将 args 内存中的数据转换成 reflect.Value 类型并存储到切片中 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 var ret []Value if nout == 0 { typedmemclr(frametype, args) framePool.Put(args) } else { typedmemclrpartial(frametype, args, 0, retOffset) ret = make([]Value, nout) off = retOffset for i := 0; i \u0026lt; nout; i++ { tv := t.Out(i) a := uintptr(tv.Align()) off = (off + a - 1) \u0026amp;^ (a - 1) if tv.Size() != 0 { fl := flagIndir | flag(tv.Kind()) ret[i] = Value{tv.common(), add(args, off, \u0026#34;tv.Size() != 0\u0026#34;), fl} } else { ret[i] = Zero(tv) } off += tv.Size() } } return ret } 由 reflect.Value 构成的 ret 数组会被返回到调用方，到这里为止使用反射实现函数调用的过程就结束了。\n小结 Go 语言的 reflect 包为我们提供了多种能力，包括如何使用反射来动态修改变量、判断类型是否实现了某些接口以及动态调用方法等功能，通过分析反射包中方法的原理能帮助我们理解之前看起来比较怪异、令人困惑的现象。\n","permalink":"https://cold-bin.github.io/post/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":["go反射"],"title":"Go语言reflect包的使用"},{"categories":["golang"],"contents":"sync.Pool 的使用场景 一句话总结：保存和复用临时对象，减少内存分配，降低GC压力。\n举个简单的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 type Student struct { Name string Age int32 Remark [1024]byte } var buf, _ = json.Marshal(Student{Name: \u0026#34;Geektutu\u0026#34;, Age: 25}) func unmarsh() { stu := \u0026amp;Student{} json.Unmarshal(buf, stu) } json的反序列化在文本解析和网络通信过程中非常常见，当程序并发度非常高的情况下，短时间内需要创建大量的临时对象。而这些对象是都是分配在堆上的，会给GC造成很大压力，严重影响程序的性能。\n注：json反序列化没有使用sync.Pool进行池化复用对象\n参考：垃圾回收(GC)的工作原理\nGo语言从1.3版本开始提供了对象重用的机制，即sync.Pool。sync.Pool是可伸缩的，同时也是并发安全的，其大小仅受限于内存的大小。sync.Pool用于存储那些被分配了但是没有被使用，而未来可能会使用的值。这样就可以不用再次经过内存分配，可直接复用已有对象，减轻GC的压力，从而提升系统的性能。\nsync.Pool的大小是可伸缩的，高负载时会动态扩容，存放在池中的对象如果不活跃了会被自动清理。\n如何使用 sync.Pool的使用方式非常简单\n声明对象池 只需要实现New函数即可。对象池中没有对象时，将会调用New函数创建。\n1 2 3 4 5 var studentPool = sync.Pool{ New: func() interface{} { return new(Student) }, } Get\u0026amp;Put 1 2 3 stu := studentPool.Get().(*Student) json.Unmarshal(buf, stu) studentPool.Put(stu) Get() 用于从对象池中获取对象，因为返回值是 interface{}，因此需要类型转换。 Put() 则是在对象使用完毕后，返回对象池。 性能测试 struct反序列化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func BenchmarkUnmarshal(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { stu := \u0026amp;Student{} json.Unmarshal(buf, stu) } } func BenchmarkUnmarshalWithPool(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { stu := studentPool.Get().(*Student) json.Unmarshal(buf, stu) studentPool.Put(stu) } } 测试结果如下：\n1 go test -bench . -benchmem 在这个例子中，因为Student结构体内存占用较小，内存分配几乎不耗时间。而标准库json反序列化时利用了反射，效率是比较低的，占据了大部分时间，因此两种方式最终的执行时间几乎没什么变化。但是内存占用差了一个数量级，使用了 sync.Pool 后，内存占用仅为未使用的 234/5096 = 1/22，对GC的影响就很大了。\nbytes.Buffer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 var bufferPool = sync.Pool{ New: func() interface{} { return \u0026amp;bytes.Buffer{} }, } var data = make([]byte, 10000) func BenchmarkBufferWithPool(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { buf := bufferPool.Get().(*bytes.Buffer) buf.Write(data) buf.Reset() bufferPool.Put(buf) } } func BenchmarkBuffer(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { var buf bytes.Buffer buf.Write(data) } } 测试结果如下：\n这个例子创建了一个bytes.Buffer对象池，而且每次只执行一个简单的Write操作，存粹的内存搬运工，耗时几乎可以忽略。而内存分配和回收的耗时占比较多，因此对程序整体的性能影响更大。\n标准库中的应用 fmt.Printf Go语言标准库也大量使用了sync.Pool，例如fmt和encoding/json。\n以下是fmt.Printf的源代码(go/src/fmt/print.go)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // go 1.13.6 // pp is used to store a printer\u0026#39;s state and is reused with sync.Pool to avoid allocations. type pp struct { buf buffer ... } var ppFree = sync.Pool{ New: func() interface{} { return new(pp) }, } // newPrinter allocates a new pp struct or grabs a cached one. func newPrinter() *pp { p := ppFree.Get().(*pp) p.panicking = false p.erroring = false p.wrapErrs = false p.fmt.init(\u0026amp;p.buf) return p } // free saves used pp structs in ppFree; avoids an allocation per invocation. func (p *pp) free() { if cap(p.buf) \u0026gt; 64\u0026lt;\u0026lt;10 { return } p.buf = p.buf[:0] p.arg = nil p.value = reflect.Value{} p.wrappedErr = nil ppFree.Put(p) } func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) { p := newPrinter() p.doPrintf(format, a) n, err = w.Write(p.buf) p.free() return } // Printf formats according to a format specifier and writes to standard output. // It returns the number of bytes written and any write error encountered. func Printf(format string, a ...interface{}) (n int, err error) { return Fprintf(os.Stdout, format, a...) } fmt.Printf 的调用是非常频繁的，利用 sync.Pool 复用 pp 对象能够极大地提升性能，减少内存占用，同时降低 GC 压力。\njson.Marshal 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func Marshal(v interface{}) ([]byte, error) { e := newEncodeState() err := e.marshal(v, encOpts{escapeHTML: true}) if err != nil { return nil, err } buf := append([]byte(nil), e.Bytes()...) // 放回对象池里 encodeStatePool.Put(e) return buf, nil } ... var encodeStatePool sync.Pool func newEncodeState() *encodeState { // 取出对象池里的 encodeState 对象，避免减少内存空间分配，从而降低GC的压力 if v := encodeStatePool.Get(); v != nil { e := v.(*encodeState) e.Reset() if len(e.ptrSeen) \u0026gt; 0 { panic(\u0026#34;ptrEncoder.encode should have emptied ptrSeen via defers\u0026#34;) } e.ptrLevel = 0 return e } return \u0026amp;encodeState{ptrSeen: make(map[interface{}]struct{})} } 在后端开发里json.Marshal的调用次数会很多很多，经常遇到需要将要发给前端的数据先调用json.Marshal方法序列化为json格式的数据，然后回传前端响应。显然，官方库里的池化复用json解码器encodeState对象，可以减少内存分配，减轻GC压力\n","permalink":"https://cold-bin.github.io/post/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","tags":["sync.pool"],"title":"sync.pool的使用场景"},{"categories":["golang"],"contents":"unsafe使用及底层 unsafe实现原理 在使用之前我们先来看一下unsafe包的源码部分，标准库unsafe包中只提供了3种方法，分别是:\n1 2 3 func Sizeof(x ArbitraryType) uintptr func Offsetof(x ArbitraryType) uintptr func Alignof(x ArbitraryType) uintptr Sizeof(x ArbitrayType)方法主要作用是用返回类型x所占据的字节数，但并不包含x所指向的内容的大小，与C语言标准库中的Sizeof()方法功能一样 Offsetof(x ArbitraryType)方法主要作用是返回结构体成员在内存中的位置离结构体起始处(结构体的第一个字段的偏移量都是0)的字节数，即偏移量，我们在注释中看一看到其入参必须是一个结构体，其返回值是一个常量。 Alignof(x ArbitratyType)的主要作用是返回一个类型的对齐值，也可以叫做对齐系数或者对齐倍数。对齐值是一个和内存对齐有关的值，合理的内存对齐可以提高内存读写的性能。一般对齐值是2^n^，最大不会超过8(受内存对齐影响). 获取对齐值还可以使用反射包的函数，也就是说：unsafe.Alignof(x)等价于reflect.TypeOf(x).Align()。对于任意类型的变量x，unsafe.Alignof(x)至少为1。对于struct结构体类型的变量x，计算x每一个字段f的unsafe.Alignof(x，f)，unsafe.Alignof(x)等于其中的最大值。对于数组类型的变量x，unsafe.Alignof(x)等于构成数组的元素类型的对齐倍数。没有任何字段的空结构体和没有任何元素的数组占据的内存空间大小为0，不同大小为0的变量可能指向同一块地址。 细心的朋友会发发现这三个方法返回的都是uintptr类型，这个目的就是可以和unsafe.poniter类型相互转换，因为*T是不能计算偏移量的，也不能进行计算，但是uintptr是可以的，所以可以使用uintptr类型进行计算，这样就可以可以访问特定的内存了，达到对不同的内存读写的目的。三个方法的入参都是ArbitraryType类型，代表着任意类型的意思，同时还提供了一个Pointer指针类型，即像void *一样的通用型指针。\n1 2 3 4 type ArbitraryType int type Pointer *ArbitraryType // uintptr 是一个整数类型，它足够大，可以存储指针的值，当然已经不是指针了，不是地址 type uintptr uintptr 上面说了这么多，可能会有点懵，在这里对三种指针类型做一个总结：\n*T：普通类型指针类型，用于传递对象地址，不能进行指针运算。 unsafe.poniter：通用指针类型，用于转换不同类型的指针，不能进行指针运算，不能读取内存存储的值(需转换到某一类型的普通指针) uintptr：用于指针运算，GC不把uintptr当指针，uintptr无法持有对象。uintptr类型的目标会被回收，所以一般不要独立出一个uintptr类型的变量，可能莫名其妙就被GC掉了。 三者关系就是：unsafe.Pointer是桥梁，可以让任意类型的指针实现相互转换，也可以将任意类型的指针转换为uintptr进行指针运算，也就说uintptr是用来与unsafe.Pointer打配合，用于指针运算。画个图表示一下：\n基本原理就说到这里啦，接下来我们一起来看看如何使用~\nunsafe.Pointer基本使用 在atomic.Value源码里，看到atomic/value.go中定义了一个ifaceWords结构，其中typ和data字段类型就是unsafe.Poniter，这里使用unsafe.Poniter类型的原因是传入的值就是interface{}类型，使用unsafe.Pointer强转成ifaceWords类型，这样可以把类型和值都保存了下来，方便后面的写入类型检查。截取部分代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // ifaceWords is interface{} internal representation. type ifaceWords struct { typ unsafe.Pointer data unsafe.Pointer } // Load returns the value set by the most recent Store. // It returns nil if there has been no call to Store for this Value. func (v *Value) Load() (x interface{}) { vp := (*ifaceWords)(unsafe.Pointer(v)) for { typ := LoadPointer(\u0026amp;vp.typ) // 读取已经存在值的类型 /** ..... 中间省略 **/ // First store completed. Check type and overwrite data. if typ != xp.typ { //当前类型与要存入的类型做对比 panic(\u0026#34;sync/atomic: store of inconsistently typed value into Value\u0026#34;) } } 上面就是源码中使用unsafe.Pointer的一个例子，有一天当你准备读源码时，unsafe.pointer的使用到处可见。好啦，接下来我们写一个简单的例子，看看unsafe.Pointer是如何使用的。\n1 2 3 4 5 6 7 8 9 10 func main() { number := 5 pointer := \u0026amp;number fmt.Printf(\u0026#34;number:addr:%p, value:%d\\n\u0026#34;,pointer,*pointer) float32Number := (*float32)(unsafe.Pointer(pointer)) *float32Number = *float32Number + 3 fmt.Printf(\u0026#34;float64:addr:%p, value:%f\\n\u0026#34;,float32Number,*float32Number) } 运行结果：\n1 2 number:addr:0xc000018090, value:5 float64:addr:0xc000018090, value:3.000000 由运行可知使用unsafe.Pointer强制类型转换后指针指向的地址是没有改变，只是类型发生了改变。这个例子本身没什么意义，正常项目中也不会这样使用。\n总结一下基本使用：先把*T类型转换成unsafe.Pointer类型，然后在进行强制转换转成你需要的指针类型即可。\nSizeof、Alignof、Offsetof三个函数的基本使用 先看一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 type User struct { Name string Age uint32 Gender bool // 男:true 女：false } func func_example() { // sizeof fmt.Println(unsafe.Sizeof(true)) fmt.Println(unsafe.Sizeof(int8(0))) fmt.Println(unsafe.Sizeof(int16(10))) fmt.Println(unsafe.Sizeof(int(10))) fmt.Println(unsafe.Sizeof(int32(190))) fmt.Println(unsafe.Sizeof(\u0026#34;asong\u0026#34;)) fmt.Println(unsafe.Sizeof([]int{1,3,4})) // Offsetof user := User{Name: \u0026#34;Asong\u0026#34;, Age: 23,Gender: true} userNamePointer := unsafe.Pointer(\u0026amp;user) nNamePointer := (*string)(unsafe.Pointer(userNamePointer)) *nNamePointer = \u0026#34;Golang梦工厂\u0026#34; nAgePointer := (*uint32)(unsafe.Pointer(uintptr(userNamePointer) + unsafe.Offsetof(user.Age))) *nAgePointer = 25 nGender := (*bool)(unsafe.Pointer(uintptr(userNamePointer)+unsafe.Offsetof(user.Gender))) *nGender = false fmt.Printf(\u0026#34;u.Name: %s, u.Age: %d, u.Gender: %v\\n\u0026#34;, user.Name, user.Age,user.Gender) // Alignof var b bool var i8 int8 var i16 int16 var i64 int64 var f32 float32 var s string var m map[string]string var p *int32 fmt.Println(unsafe.Alignof(b)) fmt.Println(unsafe.Alignof(i8)) fmt.Println(unsafe.Alignof(i16)) fmt.Println(unsafe.Alignof(i64)) fmt.Println(unsafe.Alignof(f32)) fmt.Println(unsafe.Alignof(s)) fmt.Println(unsafe.Alignof(m)) fmt.Println(unsafe.Alignof(p)) } 为了省事，把三个函数的使用示例放到了一起。\n首先看sizeof方法\nSizeof 采用任何类型的表达式 x，并返回假设变量 v 的大小（以字节为单位），就好像 v 是通过 var v = x 声明的一样。该大小不包括 x 可能引用的任何内存。例如，如果 x 是切片，则 Sizeof 返回切片描述符的大小，而不是切片引用的内存的大小。\n我们可以知道基本类型所占字节大小。这里重点说一下int、string、[]byte类型。\nGo语言中的int类型的具体大小是跟机器的CPU位数相关的。如果CPU是32位的，那么int就占4字节，如果CPU是64位的，那么int就占8字节，这里我的电脑是64位的，所以结果就是8字节。\nGo语言里的string类型，其实是不可变的字符串结构。字符串运行时的结构如下\n1 2 3 4 type StringHeader struct { Data uintptr // 示操作系统cpu而定，64位 --\u0026gt; 64位 Len int\t// 示操作系统cpu而定，64位 --\u0026gt; 64位 } 因此，使用sizeof取string的大小时，其实是对一个结构体的sizeof，始终都是8+8=16个字节。\nGo语言里的[]byte类型。切片运行时的结构如下\n1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } 因此，使用sizeof取[]byte的大小时，同string理，始终都是8+8+8=24个字节。当然，数组就不会了。\nOffsetof函数\n我想要修改结构体中成员变量，第一个成员变量是不需要进行偏移量计算的，直接取出指针后转换为unsafe.pointer，再强制给他转换成字符串类型的指针值即可。如果要修改其他成员变量，需要进行偏移量计算，才可以对其内存地址修改，所以Offsetof方法就可返回成员变量在结构体中的偏移量，也就是返回结构体初始位置到成员变量之间的字节数。\n看代码时大家应该要住uintptr的使用，不可以用一个临时变量存储uintptr类型，前面我们提到过用于指针运算，GC不把uintptr当指针，uintptr无法持有对象。uintptr类型的目标会被回收，所以你不知道他什么时候会被GC掉，那样接下来的内存操作会发生什么样的错误，咱也不知道。比如这样一个例子：\n1 2 3 // 切记不要这样使用 p1 := uintptr(userNamePointer) nAgePointer := (*uint32)(unsafe.Pointer(p1 + unsafe.Offsetof(user.Age))) 最后看一下Alignof函数，主要是获取变量的对齐值.\n除了int、uintptr这些依赖CPU位数的类型，基本类型的对齐值都是固定的，结构体中对齐值取他的成员对齐值的最大值，结构体的对齐涉及到内存对齐。\n经典应用：string与[]byte的相互转换 实现string与byte的转换，正常情况下，我们可能会写出这样的标准转换：\n1 2 3 4 5 6 // string to []byte str1 := \u0026#34;Golang梦工厂\u0026#34; by := []byte(s1) // []byte to string str2 := string(by) 使用这种方式进行转换都会涉及底层数值的拷贝，所以想要实现零拷贝，我们可以使用unsafe.Pointer来实现，通过强转换直接完成指针的指向，从而使string和[]byte指向同一个底层数据。在reflect包中有string和slice对应的结构体，他们的分别是：\n1 2 3 4 5 6 7 8 9 10 type StringHeader struct { Data uintptr Len int } type SliceHeader struct { Data uintptr Len int Cap int } StringHeader代表的是string运行时的表现形式(SliceHeader同理)，通过对比string和slice运行时的表达可以看出，他们只有一个Cap字段不同，所以他们的内存布局是对齐的，所以可以通过unsafe.Pointer进行转换，因为可以写出如下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func stringToByte(s string) []byte { header := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) newHeader := reflect.SliceHeader{ Data: header.Data, Len: header.Len, Cap: header.Len, } return *(*[]byte)(unsafe.Pointer(\u0026amp;newHeader)) } func bytesToString(b []byte) string{ header := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) newHeader := reflect.StringHeader{ Data: header.Data, Len: header.Len, } return *(*string)(unsafe.Pointer(\u0026amp;newHeader)) } 上面的代码我们通过重新构造slice header和string header完成了类型转换，其实[]byte转换成string可以省略掉自己构造StringHeader的方式，直接使用强转就可以，因为string的底层也是[]byte，强转会自动构造，省略后的代码如下：\n1 2 3 func bytesToString(b []byte) string { return *(* string)(unsafe.Pointer(\u0026amp;b)) } 虽然这种方式更高效率，但是不推荐大家使用，前面也提高到了，这要是不安全的，使用当不当会出现极大的隐患，一些严重的情况recover也不能捕获。\n","permalink":"https://cold-bin.github.io/post/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":["unsafe","零拷贝"],"title":"Go语言unsafe包的使用"},{"categories":["mysql","gorm","数据库"],"contents":"本文主要补充一些gorm的使用技巧，完整的gorm使用移步官方文档gorm官方文档\n总结 配置单数表名, 再也不用写TableName\n1 2 3 db, err := gorm.Open(mysql.Open(fmt.Sprintf(dsn, un, pwd, host, port, database)), \u0026amp;gorm.Config{ NamingStrategy: schema.NamingStrategy{SingularTable: true}, }) 当然也可以传入其他参数定制命名策略\n创建模型迁移表时，针对string类型一定要给出gorm的type约束，否则默认是创建mysql的字符串最大数据类型longtext较为浪费资源\n使用DB前先db.model, 万无一失\n创建模型时使用基本类型的指针类型，可以使得零值保存到数据库\nCreatedAt, UpdatedAt, mysql类型用datetime(3)(毫秒)\n支持db.Create创建和批量创建，创建成功时回填结构体里主键字段或结构体切片每个单元的主键字段\nSave方法没有查找到，就会创建记录；查找到就会以Save里的参数替换整行数据，即使是零值也会替换\nBeforeCreate创建前自动填写id\nAfterUpdate更新后自动创建record\nWhere用结构体查询, 自动忽略零值, 不用手打列名\nErrRecordNotFound只会出现在Take, First,Last方法中，如果发生了多个错误，你可以通过 errors.Is 判断错误是否为 ErrRecordNotFound\nFind结构体时: 等同于Take, 但没有ErrRecordNotFound, RowsAffected为0或1\n行锁的写法DB.Clauses(clause.Locking{Strength: \u0026quot;UPDATE\u0026quot;)\nUpdates支持结构体更新, 自动忽略零值, 不用手打列名. 强行更新零值用Select\nUpdate支持gorm.ExprSQL表达式更新\n事务直接用db.Transaction方法即可\nUpdate更新时，如果更新失败，但是返回不一定报错。需要综合检测 RowsAffected\n零值创建、更新、作为条件等时，使用struct会失效，应该使用map[string]interface{}来指定。\n使用map[string]interface{}还可以使用SQL表达式\ngorm外键多表联查时可以考虑使用Preload预加载或嵌套预加载\nPluck 用于从数据库查询单个列，并将结果扫描到切片。如果您想要查询多列，您应该使用 Select 和 Scan。某些业务场景下，这个Pluck方法很有用\ngorm支持查询创建更新删除等前后的钩子函数\ngorm不仅支持创建多条记录，还支持分批创建多条记录（CreateInBatches）或分批查询多条记录（FindInBatches）\nScopes 允许你指定常用的查询，可以在调用方法时引用这些查询。可以根据业务需要集成某些常用的业务查询条件。\n作用域允许你复用通用的逻辑，这种共享逻辑需要定义为类型func(*gorm.DB) *gorm.DB。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func AmountGreaterThan1000(db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;amount \u0026gt; ?\u0026#34;, 1000) } func PaidWithCreditCard(db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;pay_mode_sign = ?\u0026#34;, \u0026#34;C\u0026#34;) } func PaidWithCod(db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;pay_mode_sign = ?\u0026#34;, \u0026#34;C\u0026#34;) } func OrderStatus(status []string) func (db *gorm.DB) *gorm.DB { return func (db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;status IN (?)\u0026#34;, status) } } db.Scopes(AmountGreaterThan1000, PaidWithCreditCard).Find(\u0026amp;orders) // 查找所有金额大于 1000 的信用卡订单 db.Scopes(AmountGreaterThan1000, PaidWithCod).Find(\u0026amp;orders) // 查找所有金额大于 1000 的 COD 订单 db.Scopes(AmountGreaterThan1000, OrderStatus([]string{\u0026#34;paid\u0026#34;, \u0026#34;shipped\u0026#34;})).Find(\u0026amp;orders) // 查找所有金额大于1000 的已付款或已发货订单 Save 用来更新，会保存所有的字段，即使字段是零值。单列更新推荐使用Update或Updates,多列更新但又不是所有列推荐使用Updates，所有列更新可以使用Save\nSelect选择字段或表、Omit跳过字段或表。使用 Struct 进行 Select（会 select 零值的字段）\n1 2 db.Model(\u0026amp;user).Select(\u0026#34;Name\u0026#34;, \u0026#34;Age\u0026#34;).Updates(User{Name: \u0026#34;new_name\u0026#34;, Age: 0}) // UPDATE users SET name=\u0026#39;new_name\u0026#39;, age=0 WHERE id=111; 如果在没有任何条件的情况下执行批量更新，默认情况下，GORM 不会执行该操作，并返回 ErrMissingWhereClause 错误.对此，你必须加一些条件，或者使用原生 SQL，或者启用 AllowGlobalUpdate 模式，例如：\n1 2 3 4 5 6 7 8 9 10 db.Model(\u0026amp;User{}).Update(\u0026#34;name\u0026#34;, \u0026#34;jinzhu\u0026#34;).Error // gorm.ErrMissingWhereClause db.Model(\u0026amp;User{}).Where(\u0026#34;1 = 1\u0026#34;).Update(\u0026#34;name\u0026#34;, \u0026#34;jinzhu\u0026#34;) // UPDATE users SET `name` = \u0026#34;jinzhu\u0026#34; WHERE 1=1 db.Exec(\u0026#34;UPDATE users SET name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;) // UPDATE users SET name = \u0026#34;jinzhu\u0026#34; db.Session(\u0026amp;gorm.Session{AllowGlobalUpdate: true}).Model(\u0026amp;User{}).Update(\u0026#34;name\u0026#34;, \u0026#34;jinzhu\u0026#34;) // UPDATE users SET `name` = \u0026#34;jinzhu\u0026#34; 如果您想在更新时跳过 Hook 方法且不追踪更新时间，可以使用 UpdateColumn、UpdateColumns，其用法类似于 Update、Updates\n删除一条记录时，删除对象需要指定主键，否则会触发 批量 Delete\n如果在没有任何条件的情况下执行批量删除，GORM 不会执行该操作，并返回 ErrMissingWhereClause 错误.对此，你必须加一些条件，或者使用原生 SQL，或者启用 AllowGlobalUpdate 模式\n返回被删除的数据，仅适用于支持 Returning 的数据库\n1 2 3 4 5 // 返回所有列 var users []User DB.Clauses(clause.Returning{}).Where(\u0026#34;role = ?\u0026#34;, \u0026#34;admin\u0026#34;).Delete(\u0026amp;users) // DELETE FROM `users` WHERE role = \u0026#34;admin\u0026#34; RETURNING * // users =\u0026gt; []User{{ID: 1, Name: \u0026#34;jinzhu\u0026#34;, Role: \u0026#34;admin\u0026#34;, Salary: 100}, {ID: 2, Name: \u0026#34;jinzhu.2\u0026#34;, Role: \u0026#34;admin\u0026#34;, Salary: 1000}} 如果您的模型包含了一个 gorm.deletedat 字段（gorm.Model 已经包含了该字段)，它将自动获得软删除的能力！拥有软删除能力的模型调用 Delete 时，记录不会从数据库中被真正删除。但 GORM 会将 DeletedAt 置为当前时间， 并且你不能再通过普通的查询方法找到该记录。值得注意的是：软删除不是真真的删除，如果数据表再设计的时候加入太多约束，可能会引发软删除与约束相互矛盾，例如给数据表某个字段加上unique的约束，显然软删除后再添加相同的数据会出问题。\n有一种折中的方案：放弃这个唯一，删除的时候还是采用软删除，但是查询的时候需要使用First来查询最新的一条记录，这样来看就是可以的。缺点是，无法保证数据表这个字段的唯一性。\n您可以使用 Unscoped 找到被软删除的记录，您也可以使用 Unscoped 永久删除匹配的记录\n原生DQL使用Raw，扫描结果使用Scan；原生DML使用Exec\n关联关系很好用\u0026ndash;\u0026gt; 例子详解：https://juejin.cn/post/7067138794788487181\nbelongs to\nbelongs to 会与另一个模型建立了一对一的连接。这种模型的每一个实例都“属于”另一个模型的一个实例。\n例如，您的应用包含 user 和 company，并且每个 user 能且只能被分配给一个 company。下面的类型就表示这种关系。 注意，在 User 对象中，有一个和 Company 一样的 CompanyID。 默认情况下， CompanyID 被隐含地用来在 User 和 Company 之间创建一个外键关系， 因此必须包含在 User 结构体中才能填充 Company 内部结构体。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type User struct { gorm.Model Name string CompanyID int Company Company `gorm:\u0026#34;foreignKey:CompanyID\u0026#34;` // 不指定引用的名字时，默认为主键。也就是引用的Company的ID字段； // 不指定外键的名字时，也默认为主键。也就是User的ID字段 } type Company struct { ID int Code string Name string } GORM 可以通过 Preload、Joins 预加载 belongs to 关联的记录，查看 预加载 获取详情\nhas one\nhas one 与另一个模型建立一对一的关联，但它和一对一关系有些许不同。 这种关联表明一个模型的每个实例都包含或拥有另一个模型的一个实例。\n例如，您的应用包含 user 和 credit card 模型，且每个 user 只能有一张 credit card。\n1 2 3 4 5 6 7 8 9 10 11 12 // User 有一张 CreditCard，默认 CreditCard 的 UserID 是外键，没有USerID字段时，需要使用foreignKey来指定 type User struct { gorm.Model Name string CreditCard CreditCard `gorm:\u0026#34;foreignKey:UserName;references:name\u0026#34;` } type CreditCard struct { gorm.Model Number string UserName string } Has Many\nhas many 与另一个模型建立了一对多的连接。 不同于 has one，拥有者可以有零或多个关联模型。\n例如，您的应用包含 user 和 credit card 模型，且每个 user 可以有多张 credit card。\n1 2 3 4 5 6 7 8 9 10 11 12 type User struct { gorm.Model MemberNumber string // 重写默认外键和引用 CreditCards []CreditCard `gorm:\u0026#34;foreignKey:UserNumber;references:MemberNumber\u0026#34;` } type CreditCard struct { gorm.Model Number string UserNumber string } Many To Many\nMany to Many 会在两个 model 中添加一张连接表。一般来说原生SQL建表，针对多对多关系都是需要建立中间表。\n例如，您的应用包含了 user 和 language，且一个 user 可以说多种 language，多个 user 也可以说一种 language。\n1 2 3 4 5 6 7 8 9 10 // User 拥有并属于多种 language，`user_languages` 是连接表 type User struct { gorm.Model Languages []Language `gorm:\u0026#34;many2many:user_languages;\u0026#34;` } type Language struct { gorm.Model Name string } 当使用 GORM 的 AutoMigrate 为 User 创建表时，GORM 会自动创建连接表\n强大的实体关联\n在创建、更新记录时，GORM 会通过 Upsert 自动保存关联及其引用记录。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 user := User{ Name: \u0026#34;jinzhu\u0026#34;, BillingAddress: Address{Address1: \u0026#34;Billing Address - Address 1\u0026#34;}, ShippingAddress: Address{Address1: \u0026#34;Shipping Address - Address 1\u0026#34;}, Emails: []Email{ {Email: \u0026#34;jinzhu@example.com\u0026#34;}, {Email: \u0026#34;jinzhu-2@example.com\u0026#34;}, }, Languages: []Language{ {Name: \u0026#34;ZH\u0026#34;}, {Name: \u0026#34;EN\u0026#34;}, }, } // create时会自动创建所有关联的数据 db.Create(\u0026amp;user) // BEGIN TRANSACTION; // INSERT INTO \u0026#34;addresses\u0026#34; (address1) VALUES (\u0026#34;Billing Address - Address 1\u0026#34;), (\u0026#34;Shipping Address - Address 1\u0026#34;) ON DUPLICATE KEY DO NOTHING; // INSERT INTO \u0026#34;users\u0026#34; (name,billing_address_id,shipping_address_id) VALUES (\u0026#34;jinzhu\u0026#34;, 1, 2); // INSERT INTO \u0026#34;emails\u0026#34; (user_id,email) VALUES (111, \u0026#34;jinzhu@example.com\u0026#34;), (111, \u0026#34;jinzhu-2@example.com\u0026#34;) ON DUPLICATE KEY DO NOTHING; // INSERT INTO \u0026#34;languages\u0026#34; (\u0026#34;name\u0026#34;) VALUES (\u0026#39;ZH\u0026#39;), (\u0026#39;EN\u0026#39;) ON DUPLICATE KEY DO NOTHING; // INSERT INTO \u0026#34;user_languages\u0026#34; (\u0026#34;user_id\u0026#34;,\u0026#34;language_id\u0026#34;) VALUES (111, 1), (111, 2) ON DUPLICATE KEY DO NOTHING; // COMMIT; // 下面的save会自动更新或创建所有关联的数据（有就替换，没有就 db.Save(\u0026amp;user) 若要在创建、更新时跳过自动保存，您可以使用 Select 或 Omit，例如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 user := User{ Name: \u0026#34;jinzhu\u0026#34;, BillingAddress: Address{Address1: \u0026#34;Billing Address - Address 1\u0026#34;}, ShippingAddress: Address{Address1: \u0026#34;Shipping Address - Address 1\u0026#34;}, Emails: []Email{ {Email: \u0026#34;jinzhu@example.com\u0026#34;}, {Email: \u0026#34;jinzhu-2@example.com\u0026#34;}, }, Languages: []Language{ {Name: \u0026#34;ZH\u0026#34;}, {Name: \u0026#34;EN\u0026#34;}, }, } db.Select(\u0026#34;Name\u0026#34;).Create(\u0026amp;user) // INSERT INTO \u0026#34;users\u0026#34; (name) VALUES (\u0026#34;jinzhu\u0026#34;, 1, 2); db.Omit(\u0026#34;BillingAddress\u0026#34;).Create(\u0026amp;user) // Skip create BillingAddress when creating a user db.Omit(clause.Associations).Create(\u0026amp;user) // Skip all associations when creating a user Select/Omit 关联字段\n1 2 3 4 5 6 7 8 9 10 11 user := User{ Name: \u0026#34;jinzhu\u0026#34;, BillingAddress: Address{Address1: \u0026#34;Billing Address - Address 1\u0026#34;, Address2: \u0026#34;addr2\u0026#34;}, ShippingAddress: Address{Address1: \u0026#34;Shipping Address - Address 1\u0026#34;, Address2: \u0026#34;addr2\u0026#34;}, } // 创建 user 及其 BillingAddress、ShippingAddress // 在创建 BillingAddress 时，仅使用其 address1、address2 字段，忽略其它字段 db.Select(\u0026#34;BillingAddress.Address1\u0026#34;, \u0026#34;BillingAddress.Address2\u0026#34;).Create(\u0026amp;user) db.Omit(\u0026#34;BillingAddress.Address2\u0026#34;, \u0026#34;BillingAddress.CreatedAt\u0026#34;).Create(\u0026amp;user) 关联模式包含一些在处理关系时有用的方法\n1 2 3 4 5 6 7 // 开始关联模式 var user User db.Model(\u0026amp;user).Association(\u0026#34;Languages\u0026#34;) // `user` 是源模型，它的主键不能为空 // 关系的字段名是 `Languages`，关系需要是 belongs to、has one、has many、many to many // 如果匹配了上面两个要求，会开始关联模式，否则会返回错误 db.Model(\u0026amp;user).Association(\u0026#34;Languages\u0026#34;).Error 查找所有匹配的关联记录\n1 db.Model(\u0026amp;user).Association(\u0026#34;Languages\u0026#34;).Find(\u0026amp;languages) 查找带条件的关联\n1 2 3 4 codes := []string{\u0026#34;zh-CN\u0026#34;, \u0026#34;en-US\u0026#34;, \u0026#34;ja-JP\u0026#34;} db.Model(\u0026amp;user).Where(\u0026#34;code IN ?\u0026#34;, codes).Association(\u0026#34;Languages\u0026#34;).Find(\u0026amp;languages) db.Model(\u0026amp;user).Where(\u0026#34;code IN ?\u0026#34;, codes).Order(\u0026#34;code desc\u0026#34;).Association(\u0026#34;Languages\u0026#34;).Find(\u0026amp;languages) 返回当前关联的计数\n1 2 3 4 5 db.Model(\u0026amp;user).Association(\u0026#34;Languages\u0026#34;).Count() // 条件计数 codes := []string{\u0026#34;zh-CN\u0026#34;, \u0026#34;en-US\u0026#34;, \u0026#34;ja-JP\u0026#34;} db.Model(\u0026amp;user).Where(\u0026#34;code IN ?\u0026#34;, codes).Association(\u0026#34;Languages\u0026#34;).Count() 关联模式也支持批量处理，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 查询所有用户的所有角色 db.Model(\u0026amp;users).Association(\u0026#34;Role\u0026#34;).Find(\u0026amp;roles) // 从所有 team 中删除 user A db.Model(\u0026amp;users).Association(\u0026#34;Team\u0026#34;).Delete(\u0026amp;userA) // 获取去重的用户所属 team 数量 db.Model(\u0026amp;users).Association(\u0026#34;Team\u0026#34;).Count() // 对于批量数据的 `Append`、`Replace`，参数的长度必须与数据的长度相同，否则会返回 error var users = []User{user1, user2, user3} // 例如：现在有三个 user，Append userA 到 user1 的 team，Append userB 到 user2 的 team，Append userA、userB 和 userC 到 user3 的 team db.Model(\u0026amp;users).Association(\u0026#34;Team\u0026#34;).Append(\u0026amp;userA, \u0026amp;userB, \u0026amp;[]User{userA, userB, userC}) // 重置 user1 team 为 userA，重置 user2 的 team 为 userB，重置 user3 的 team 为 userA、 userB 和 userC db.Model(\u0026amp;users).Association(\u0026#34;Team\u0026#34;).Replace(\u0026amp;userA, \u0026amp;userB, \u0026amp;[]User{userA, userB, userC}) 你可以在删除记录时通过 Select 来删除具有 has one、has many、many2many 关系的记录，例如：\n1 2 3 4 5 6 7 8 9 10 11 // 删除 user 时，也删除 user 的 account db.Select(\u0026#34;Account\u0026#34;).Delete(\u0026amp;user) // 删除 user 时，也删除 user 的 Orders、CreditCards 记录 db.Select(\u0026#34;Orders\u0026#34;, \u0026#34;CreditCards\u0026#34;).Delete(\u0026amp;user) // 删除 user 时，也删除用户所有 has one/many、many2many 记录 db.Select(clause.Associations).Delete(\u0026amp;user) // 删除 users 时，也删除每一个 user 的 account db.Select(\u0026#34;Account\u0026#34;).Delete(\u0026amp;users) 注意： 只有当记录的主键不为空时，关联才会被删除，GORM 会使用这些主键作为条件来删除关联记录. 而且也这个Select关联删除只能删除当前表的关联不能删除嵌套的关联。例如user表里的Orders还可能与其他有关联，但是不能把他们也删掉\nGORM 允许在 Preload 的其它 SQL 中直接加载关系（belong to\\has one\\has many\\many to many），例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type User struct { gorm.Model Username string Orders []Order `gorm:foreignKey:UserID` } type Order struct { gorm.Model UserID uint Price float64 } // 查找 user 时预加载相关 Order db.Preload(\u0026#34;Orders\u0026#34;).Find(\u0026amp;users) // SELECT * FROM users; // SELECT * FROM orders WHERE user_id IN (1,2,3,4); # 前面查找好的所有user_id db.Preload(\u0026#34;Orders\u0026#34;).Preload(\u0026#34;Profile\u0026#34;).Preload(\u0026#34;Role\u0026#34;).Find(\u0026amp;users) // SELECT * FROM users; // SELECT * FROM orders WHERE user_id IN (1,2,3,4); // has many // SELECT * FROM profiles WHERE user_id IN (1,2,3,4); // has one // SELECT * FROM roles WHERE id IN (4,5,6); // belongs to 与创建、更新时使用 Select 类似，clause.Associations 也可以和 Preload 一起使用，它可以用来 预加载 全部关联，例如：\n1 2 3 4 5 6 7 8 9 10 type User struct { gorm.Model Name string CompanyID uint Company Company Role Role Orders []Order } db.Preload(clause.Associations).Find(\u0026amp;users) clause.Associations 不会预加载嵌套的关联，但你可以使用嵌套预加载 例如：\n1 2 3 4 db.Preload(\u0026#34;Orders.OrderItems.Product\u0026#34;).Preload(clause.Associations).Find(\u0026amp;users) // 假设有A B C 三个表，这三个表依次依赖，这样就形成了嵌套的关系。可以使用Preload的嵌套预加载来实现简洁的关联查询 // 或者使用连接查询也可以 // 或者使用 Assosiation来关联表也可以做查询 GORM 允许带条件的 Preload 关联，类似于内联条件\n1 2 3 4 5 6 7 8 // 带条件的预加载 Order db.Preload(\u0026#34;Orders\u0026#34;, \u0026#34;state NOT IN (?)\u0026#34;, \u0026#34;cancelled\u0026#34;).Find(\u0026amp;users) // SELECT * FROM users; // SELECT * FROM orders WHERE user_id IN (1,2,3,4) AND state NOT IN (\u0026#39;cancelled\u0026#39;); db.Where(\u0026#34;state = ?\u0026#34;, \u0026#34;active\u0026#34;).Preload(\u0026#34;Orders\u0026#34;, \u0026#34;state NOT IN (?)\u0026#34;, \u0026#34;cancelled\u0026#34;).Find(\u0026amp;users) // SELECT * FROM users WHERE state = \u0026#39;active\u0026#39;; // SELECT * FROM orders WHERE user_id IN (1,2) AND state NOT IN (\u0026#39;cancelled\u0026#39;); Preload预加载时需要条件的话，只能使用内联条件或者直接改成连接查询\n链式方法是将 Clauses 修改或添加到当前 Statement 的方法，例如：\nWhere, Select, Omit, Joins, Scopes, Preload, Raw (Raw can’t be used with other chainable methods to build SQL)…\n终结（方法） 是会立即执行注册回调的方法，然后生成并执行 SQL，比如这些方法：\nCreate, First, Find, Take, Save, Update, Delete, Scan, Row, Rows…\nGORM 定义了 Session、WithContext、Debug 方法做为 新建会话方法，查看会话 获取详情.\n在 链式方法, Finisher 方法之后, GORM 返回一个初始化的 *gorm.DB 实例，不能安全地再使用。您应该使用 新建会话方法 来标记 *gorm.DB 为可共享。调用已经调用终结方法的实例会被上个实例污染，可以使用会话方式新建会话避免前面实例调用终结方法导致的条件污染\n让我们用实例来解释它：\n示例 1：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 db, err := gorm.Open(sqlite.Open(\u0026#34;test.db\u0026#34;), \u0026amp;gorm.Config{}) // db is a new initialized `*gorm.DB`, which is safe to reuse db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Where(\u0026#34;age = ?\u0026#34;, 18).Find(\u0026amp;users) // `Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;)` is the first chain method call, it will create an initialized `*gorm.DB` instance, aka `*gorm.Statement` // `Where(\u0026#34;age = ?\u0026#34;, 18)` is the second chain method call, it reuses the above `*gorm.Statement`, adds new condition `age = 18` to it // `Find(\u0026amp;users)` is a finisher method, it executes registered Query Callbacks, which generates and runs the following SQL: // SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; AND age = 18; db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu2\u0026#34;).Where(\u0026#34;age = ?\u0026#34;, 20).Find(\u0026amp;users) // `Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu2\u0026#34;)` is also the first chain method call, it creates a new `*gorm.Statement` // `Where(\u0026#34;age = ?\u0026#34;, 20)` reuses the above `Statement`, and add conditions to it // `Find(\u0026amp;users)` is a finisher method, it executes registered Query Callbacks, generates and runs the following SQL: // SELECT * FROM users WHERE name = \u0026#39;jinzhu2\u0026#39; AND age = 20; db.Find(\u0026amp;users) // `Find(\u0026amp;users)` is a finisher method call, it also creates a new `Statement` and executes registered Query Callbacks, generates and runs the following SQL: // SELECT * FROM users; (错误的) 示例2：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 db, err := gorm.Open(sqlite.Open(\u0026#34;test.db\u0026#34;), \u0026amp;gorm.Config{}) // db is a new initialized *gorm.DB, which is safe to reuse tx := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;) // `Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;)` returns an initialized `*gorm.Statement` instance after chain method `Where`, which is NOT safe to reuse // good case tx.Where(\u0026#34;age = ?\u0026#34;, 18).Find(\u0026amp;users) // `tx.Where(\u0026#34;age = ?\u0026#34;, 18)` use the above `*gorm.Statement`, adds new condition to it // `Find(\u0026amp;users)` is a finisher method call, it executes registered Query Callbacks, generates and runs the following SQL: // SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; AND age = 18 // 会被上个实例污染，可以使用会话方式新建会话避免前面实例调用终结方法导致的条件污染 // bad case tx.Where(\u0026#34;age = ?\u0026#34;, 28).Find(\u0026amp;users) // `tx.Where(\u0026#34;age = ?\u0026#34;, 18)` also use the above `*gorm.Statement`, and keep adding conditions to it // So the following generated SQL is polluted by the previous conditions: // SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; AND age = 18 AND age = 28; 示例 3：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 db, err := gorm.Open(sqlite.Open(\u0026#34;test.db\u0026#34;), \u0026amp;gorm.Config{}) // db is a new initialized *gorm.DB, which is safe to reuse tx := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Session(\u0026amp;gorm.Session{}) tx := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).WithContext(context.Background()) tx := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Debug() // `Session`, `WithContext`, `Debug` returns `*gorm.DB` marked as safe to reuse, newly initialized `*gorm.Statement` based on it keeps current conditions // good case tx.Where(\u0026#34;age = ?\u0026#34;, 18).Find(\u0026amp;users) // SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; AND age = 18 // good case tx.Where(\u0026#34;age = ?\u0026#34;, 28).Find(\u0026amp;users) // SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; AND age = 28; PreparedStmt 在执行任何 SQL 时都会创建一个 prepared statement 并将其缓存，以提高后续的效率。这在业务里还是很有用的，一次SQL加载就可以提升后续执行效率\n通过 NewDB 选项创建一个不带之前条件的新 DB，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 tx := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Session(\u0026amp;gorm.Session{NewDB: true}) tx.First(\u0026amp;user) // SELECT * FROM users ORDER BY id LIMIT 1 tx.First(\u0026amp;user, \u0026#34;id = ?\u0026#34;, 10) // SELECT * FROM users WHERE id = 10 ORDER BY id // 不带 `NewDB` 选项 tx2 := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Session(\u0026amp;gorm.Session{}) tx2.First(\u0026amp;user) // SELECT * FROM users WHERE name = \u0026#34;jinzhu\u0026#34; ORDER BY id 在一个 DB 事务中使用 Transaction 方法，GORM 会使用 SavePoint(savedPointName)，RollbackTo(savedPointName) 为你提供嵌套事务支持。 你可以通过 DisableNestedTransaction 选项关闭它，例如：\n1 2 3 db.Session(\u0026amp;gorm.Session{ DisableNestedTransaction: true, }).CreateInBatches(\u0026amp;users, 100) GORM 默认不允许进行全局 update/delete，该操作会返回 ErrMissingWhereClause 错误。 您可以通过将一个选项设置为 true 来启用它，例如：\n1 2 3 4 db.Session(\u0026amp;gorm.Session{ AllowGlobalUpdate: true, }).Model(\u0026amp;User{}).Update(\u0026#34;name\u0026#34;, \u0026#34;jinzhu\u0026#34;) // UPDATE users SET `name` = \u0026#34;jinzhu\u0026#34; 在创建、更新记录时，GORM 会通过 Upsert 自动保存关联及其引用记录。 如果您想要更新关联的数据，您应该使用 FullSaveAssociations 模式，例如：(重要)\n1 2 3 4 5 6 db.Session(\u0026amp;gorm.Session{FullSaveAssociations: true}).Updates(\u0026amp;user) // ... // INSERT INTO \u0026#34;addresses\u0026#34; (address1) VALUES (\u0026#34;Billing Address - Address 1\u0026#34;), (\u0026#34;Shipping Address - Address 1\u0026#34;) ON DUPLICATE KEY SET address1=VALUES(address1); // INSERT INTO \u0026#34;users\u0026#34; (name,billing_address_id,shipping_address_id) VALUES (\u0026#34;jinzhu\u0026#34;, 1, 2); // INSERT INTO \u0026#34;emails\u0026#34; (user_id,email) VALUES (111, \u0026#34;jinzhu@example.com\u0026#34;), (111, \u0026#34;jinzhu-2@example.com\u0026#34;) ON DUPLICATE KEY SET email=VALUES(email); // ... 声明查询字段，一种优化手段，避免*再次解析成表的行字段的时间消耗\n1 2 3 db.Session(\u0026amp;gorm.Session{QueryFields: true}).Find(\u0026amp;user) // SELECT `users`.`name`, `users`.`age`, ... FROM `users` // 有该选项 // SELECT * FROM `users` // 没有该选项 事务Begin、Commit和Rollback不是很方便(手动事务)，推荐使用下面的方式。GORM 提供了 SavePoint、Rollbackto 方法，来提供保存点以及回滚至保存点功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 db.Transaction(func(tx *gorm.DB) error { // 在事务中执行一些 db 操作（从这里开始，您应该使用 \u0026#39;tx\u0026#39; 而不是 \u0026#39;db\u0026#39;） if err := tx.Create(\u0026amp;Animal{Name: \u0026#34;Giraffe\u0026#34;}).Error; err != nil { // 返回任何错误都会回滚事务 return err } if err := tx.Create(\u0026amp;Animal{Name: \u0026#34;Lion\u0026#34;}).Error; err != nil { return err } // 返回 nil 提交事务 return nil }) GORM 支持嵌套事务，您可以回滚较大事务内执行的一部分操作，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 db.Transaction(func(tx *gorm.DB) error { tx.Create(\u0026amp;user1) tx.Transaction(func(tx2 *gorm.DB) error { tx2.Create(\u0026amp;user2) return errors.New(\u0026#34;rollback user2\u0026#34;) // Rollback user2 }) tx.Transaction(func(tx2 *gorm.DB) error { tx2.Create(\u0026amp;user3) return nil }) return nil }) // Commit user1, user3 Migrator 接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 为 `User` 创建表 db.Migrator().CreateTable(\u0026amp;User{}) // 将 \u0026#34;ENGINE=InnoDB\u0026#34; 添加到创建 `User` 的 SQL 里去 db.Set(\u0026#34;gorm:table_options\u0026#34;, \u0026#34;ENGINE=InnoDB\u0026#34;).Migrator().CreateTable(\u0026amp;User{}) // 检查 `User` 对应的表是否存在 db.Migrator().HasTable(\u0026amp;User{}) db.Migrator().HasTable(\u0026#34;users\u0026#34;) // 如果存在表则删除（删除时会忽略、删除外键约束) db.Migrator().DropTable(\u0026amp;User{}) db.Migrator().DropTable(\u0026#34;users\u0026#34;) // 重命名表 db.Migrator().RenameTable(\u0026amp;User{}, \u0026amp;UserInfo{}) db.Migrator().RenameTable(\u0026#34;users\u0026#34;, \u0026#34;user_infos\u0026#34;) 连接池\n1 2 3 4 5 6 7 8 9 10 11 // 获取通用数据库对象 sql.DB ，然后使用其提供的功能 sqlDB, err := db.DB() // SetMaxIdleConns 用于设置连接池中空闲连接的最大数量。 sqlDB.SetMaxIdleConns(10) // SetMaxOpenConns 设置打开数据库连接的最大数量。 sqlDB.SetMaxOpenConns(100) // SetConnMaxLifetime 设置了连接可复用的最大时间。 sqlDB.SetConnMaxLifetime(time.Hour) 执行任何 SQL 时都创建并缓存预编译语句，可以提高后续的调用速度\n1 2 3 4 5 6 7 8 9 10 // 全局模式 db, err := gorm.Open(sqlite.Open(\u0026#34;gorm.db\u0026#34;), \u0026amp;gorm.Config{ PrepareStmt: true, }) // 会话模式 tx := db.Session(\u0026amp;Session{PrepareStmt: true}) tx.First(\u0026amp;user, 1) tx.Find(\u0026amp;users) tx.Model(\u0026amp;user).Update(\u0026#34;Age\u0026#34;, 18) 您可以使用 Table 方法临时指定表名(和Model()方法类似)，例如：\n1 2 3 4 5 6 7 8 9 10 // 根据 User 的字段创建 `deleted_users` 表 db.Table(\u0026#34;deleted_users\u0026#34;).AutoMigrate(\u0026amp;User{}) // 从另一张表查询数据 var deletedUsers []User db.Table(\u0026#34;deleted_users\u0026#34;).Find(\u0026amp;deletedUsers) // SELECT * FROM deleted_users; db.Table(\u0026#34;deleted_users\u0026#34;).Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Delete(\u0026amp;User{}) // DELETE FROM deleted_users WHERE name = \u0026#39;jinzhu\u0026#39;; 字段的序列化\nSerializer 是一个可扩展的接口，允许自定义如何使用数据库对数据进行序列化和反序列化\nGORM 提供了一些默认的序列化器：json、gob、unixtime，这里有一个如何使用它的快速示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type User struct { Name []byte `gorm:\u0026#34;serializer:json\u0026#34;` Roles Roles `gorm:\u0026#34;serializer:json\u0026#34;` Contracts map[string]interface{} `gorm:\u0026#34;serializer:json\u0026#34;` JobInfo Job `gorm:\u0026#34;type:bytes;serializer:gob\u0026#34;` CreatedTime int64 `gorm:\u0026#34;serializer:unixtime;type:time\u0026#34;` // 将 int 作为日期时间存储到数据库中 } type Roles []string type Job struct { Title string Location string IsIntern bool } 复合索引列的顺序会影响其性能，因此必须仔细考虑\u0026ndash;\u0026gt;后续的条件尽量符合覆盖索引原则\n您可以使用 priority 指定顺序，默认优先级值是 10，如果优先级值相同，则顺序取决于模型结构体字段的顺序()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type User struct { Name string `gorm:\u0026#34;index:idx_member\u0026#34;` Number string `gorm:\u0026#34;index:idx_member\u0026#34;` } // column order: name, number type User struct { Name string `gorm:\u0026#34;index:idx_member,priority:2\u0026#34;` Number string `gorm:\u0026#34;index:idx_member,priority:1\u0026#34;` } // column order: number, name type User struct { Name string `gorm:\u0026#34;index:idx_member,priority:12\u0026#34;` Number string `gorm:\u0026#34;index:idx_member\u0026#34;` } // column order: number, name 复合主键\n通过将多个字段设为主键，以创建复合主键，例如：\n1 2 3 4 5 6 type Product struct { ID string `gorm:\u0026#34;primaryKey\u0026#34;` LanguageCode string `gorm:\u0026#34;primaryKey\u0026#34;` Code string Name string } **注意：**默认情况下，整型 PrioritizedPrimaryField 启用了 AutoIncrement，要禁用它，您需要为整型字段关闭 autoIncrement：\n1 2 3 4 type Product struct { CategoryID uint64 `gorm:\u0026#34;primaryKey;autoIncrement:false\u0026#34;` TypeID uint64 `gorm:\u0026#34;primaryKey;autoIncrement:false\u0026#34;` } gorm与go的类型映射\n下面是gorm结构体模型与mysql数据库类型的具体映射关系\ngorm model field mysql table field type note int8 tinyint int16 smallint int32 int int64 bigint uint8 tinyint unsigned uint16 smallint unsigned uint32 int unsigned uint64 bigint unsigned float32 float float64 double complex64 没有对应类型 不支持 complex128 没有对应类型 不支持 byte tinyint unsigned 等效于go的uint8 rune int rune等效于go里的int32 bool tinyint(1) 都占一个bit位 time.Time datetime(3) string longtext 不添加gorm的约束，默认为longtext（mysql的字符串最大数据类型） field string `gorm:\u0026ldquo;type:char(1)\u0026rdquo;` char 定长字符串设置为1字节不生效 field string `gorm:\u0026ldquo;type:char(5)\u0026rdquo;` char(5) 定长 field string `gorm:\u0026ldquo;type:varchar(1)\u0026rdquo;` varchar(1) 可变长度，最多1个字节 field string `gorm:\u0026ldquo;type:varchar(1000)\u0026rdquo;` varcahr(1000) 可变长度，最多1000个字节 field string `gorm:\u0026ldquo;type:tinyblob\u0026rdquo;` tinyblob 不可以添加长度，会报错 field string `gorm:\u0026ldquo;type:tinytext\u0026rdquo;` tinytext 不可以添加长度，会报错 field string `gorm:\u0026ldquo;type:blob\u0026rdquo;` blob 不可以添加长度，会报错 field string `gorm:\u0026ldquo;type:text\u0026rdquo;` text 不可以添加长度，会报错 可以看出：go的数据类型通过gorm与mysql的数据类型是完全对应起来的。go的int8的1字节对mysql的tinyint的1字节，有无符号也是对应起来的，但是针对go的string可以添加gorm的类型约束来限制对应mysql的类型，不屑约束的话默认是longtext（太浪费空间了!!!）\n其次，非string类型添加长度约束会失效。而且文本或二进制类型不管大中小都无法添加长度，添加会报错\n从表名开始 如何定义表名? gorm的默认表名策略是模型名字的复数, 比如:\n1 type User Struct{} 默认表名为users, 但是生产习惯常用单数表名, 而且加es的复数或者sheep单复同形容易让人迷惑.\n当然, gorm实现了复杂的单数变复数逻辑, 我们可以从 http://github.com/jinzhu/inflection/inflections.go 一探究竟, 下面复习下单复同型/不可数单词, 以及特殊变复数的单词:\n1 2 3 4 5 6 7 8 9 10 var uncountableInflections = []string{\u0026#34;equipment\u0026#34;, \u0026#34;information\u0026#34;, \u0026#34;rice\u0026#34;, \u0026#34;money\u0026#34;, \u0026#34;species\u0026#34;, \u0026#34;series\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;sheep\u0026#34;, \u0026#34;jeans\u0026#34;, \u0026#34;police\u0026#34;} var irregularInflections = IrregularSlice{ {\u0026#34;person\u0026#34;, \u0026#34;people\u0026#34;}, {\u0026#34;man\u0026#34;, \u0026#34;men\u0026#34;}, {\u0026#34;child\u0026#34;, \u0026#34;children\u0026#34;}, {\u0026#34;sex\u0026#34;, \u0026#34;sexes\u0026#34;}, {\u0026#34;move\u0026#34;, \u0026#34;moves\u0026#34;}, {\u0026#34;mombie\u0026#34;, \u0026#34;mombies\u0026#34;}, } 好吧, 写个增删改查还得过专八.\n我们可以实现gorm.schema.scheme.go.Tabler.TableName()方法, 重写表名.\n1 func (*User) TableName() string {return \u0026#34;user\u0026#34;} OK!\n除了这种奇奇怪怪的写法, 在gormV2中, 我们可以使用一个配置改为单数表名, 谢天谢地.\n1 NamingStrategy: \u0026amp;schema.NamingStrategy{SingularTable: true} schema.NamingStrategy实现了gorm.schema.naming.go.Namer接口, 我们也可以自己实现这个接口,替换gorm的命名策略, 接口简单易懂.\n1 2 3 4 5 6 7 8 type Namer interface { TableName(table string) string ColumnName(table, column string) string JoinTableName(joinTable string) string RelationshipFKName(Relationship) string CheckerName(table, column string) string IndexName(table, column string) string } 列名 列名无可非议, 简简单单的下划线模式. 而且gorm会自动将ID转换成id而不是i_d, 那么, 就遵循golang的语言规范, 放心的在代码中使用ID吧~\n让我们来看下这个特殊变小写数组.\n列名变小写?\ngorm.schema.naming.go\n1 2 // https://github.com/golang/lint/blob/master/lint.go#L770 commonInitialisms = []string{\u0026#34;API\u0026#34;, \u0026#34;ASCII\u0026#34;, \u0026#34;CPU\u0026#34;, \u0026#34;CSS\u0026#34;, \u0026#34;DNS\u0026#34;, \u0026#34;EOF\u0026#34;, \u0026#34;GUID\u0026#34;, \u0026#34;HTML\u0026#34;, \u0026#34;HTTP\u0026#34;, \u0026#34;HTTPS\u0026#34;, \u0026#34;ID\u0026#34;, \u0026#34;IP\u0026#34;, \u0026#34;JSON\u0026#34;, \u0026#34;LHS\u0026#34;, \u0026#34;QPS\u0026#34;, \u0026#34;RAM\u0026#34;, \u0026#34;RHS\u0026#34;, \u0026#34;RPC\u0026#34;, \u0026#34;SLA\u0026#34;, \u0026#34;SMTP\u0026#34;, \u0026#34;SSH\u0026#34;, \u0026#34;TLS\u0026#34;, \u0026#34;TTL\u0026#34;, \u0026#34;UID\u0026#34;, \u0026#34;UI\u0026#34;, \u0026#34;UUID\u0026#34;, \u0026#34;URI\u0026#34;, \u0026#34;URL\u0026#34;, \u0026#34;UTF8\u0026#34;, \u0026#34;VM\u0026#34;, \u0026#34;XML\u0026#34;, \u0026#34;XSRF\u0026#34;, \u0026#34;XSS\u0026#34;} CreatedAt, UpdatedAt\n更加有争议和难以理解的列名,应该是这两个.\n最原始的方法 ,mysql自动添加\n如果想让mysql来做这件事, 可以这样写:\n1 2 3 4 5 6 7 8 9 10 11 12 create table user2 ( .... create_time datetime(3) not null DEFAULT CURRENT_TIMESTAMP(3) comment \u0026#39;创建时间\u0026#39;, update_time datetime(3) not null DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) comment \u0026#39;修改时间\u0026#39;, .... ); type User struct { ID int64 `gorm:\u0026#34;autoIncrement\u0026#34;` CreatedAt time.Time `gorm:default` UpdatedAt time.Time `gorm:default` } 加上gorm的default标签, 这个标签的意思是: gorm.Create时, 如果该列值为零, 使用建表时的default值, sql语句中也就没有这一列.\ngorm.Updates如果Update一个map,就是强制更新. 如果Update一个结构体, 不赋值也就是用mysql的默认值, 如果赋值就直接Update, 这一点很巧妙.\ngorm对默认值的处理:\ngorm.callbacks.create.go:285\n1 2 3 4 5 6 7 8 9 10 11 defaultValueFieldsHavingValue := map[*schema.Field][]interface{}{} for _, field := range stmt.Schema.FieldsWithDefaultDBValue { if v, ok := selectColumns[field.DBName]; (ok \u0026amp;\u0026amp; v) || (!ok \u0026amp;\u0026amp; !restricted) { if v, isZero := field.ValueOf(rv); !isZero { if len(defaultValueFieldsHavingValue[field]) == 0 { defaultValueFieldsHavingValue[field] = make([]interface{}, stmt.ReflectValue.Len()) } defaultValueFieldsHavingValue[field][i] = v } } } 你要说我能看懂, 那我肯定看不懂, 但你要说看不懂, 我还知道它能干啥, 不错不错\u0026hellip;\n更好的方法，gorm自动添加\n如果想让gorm来添加, sql就可以不用写default\n1 2 3 4 5 type User struct { ID int64 `gorm:\u0026#34;autoIncrement\u0026#34;` CreatedAt time.Time UpdatedAt time.Time } 只要列名叫CreatedAt和UpdatedAt即可.\ngorm.Create即可自动更新CreatedAt和UpdatedAt.\n但是gorm.Updates时可要注意了, 如果没有使用db.Model指定model结构体, 就不能更新UpdatedAt.\n所以如果这样:\n1 db.Table(tableName).Updates(map[string]interface{}) 是无法更新更新时间的.\n正确做法是:\n1 db.Model(\u0026amp;User{}).Updates(map[string]interface{}) 如果傲娇的你, 偏偏不喜欢这样的列名, 非要用CreateTime, 那好吧, 你可以这样: 不告诉你, 自己去查文档\u0026hellip; https://gorm.io/zh_CN/docs/models.html\n小建议: 建议建表时使用datetime(3)秒类型\nCreate https://gorm.io/zh_CN/docs/create.html\ngormV2支持创建和批量创建, 你可以这样写:\n1 2 3 4 5 6 7 //创建 db.Create(\u0026amp;User{}) //批量创建 users := []*User{{},{},{}} db.Create(\u0026amp;users) // 创建成功会往结构体或结构体切片回填主键的值。\nHooks 我们可以灵活的使用hooks, 以减少重复代码在logic层对业务逻辑的侵入, 使代码更简洁. 钩子命名好像Vue啊! 哈哈哈\nWhere gorm中最常用的语句, gormv1中最常用的方法是:\n1 db.Where(\u0026#34;uid = ?\u0026#34;, 1) gormV2支持两种新的where, Struct和map条件, 本文介绍struct条件:\n1 2 3 4 db.Where(\u0026amp;User{phone:\u0026#34;123\u0026#34;, Age:0}) //Select * from User where phone = 123 //Age没有被查询 结构体查询非常好用, 你不用手动写列名, 避免了因为列名写错而导致的错误.\n注意: 当使用结构作为条件查询时，GORM 只会查询非零值字段。这意味着如果您的字段值为0 、'' 、false 或其他零值，该字段不会被用于构建查询条件\nFind, Take, First, Last ErrRecordNotFound\n这个东西可是实在太恶心了\u0026hellip; 但是golang的反射又没法对ptr赋值为nil, 所以只能通过error返回. 这种处理方式也是可以理解的.\n1 2 3 err := db.Where(...).First(...).Error // 检查 ErrRecordNotFound 错误 errors.Is(err, gorm.ErrRecordNotFound) 注意: 只有First,Last,Take方法会产生gorm.ErrRecordNotFound错误哦\nFind\n为了躲避ErrRecordNotFound, 我们来看下这个可爱的Find方法.\nFind方法可以接受两种参数, 一种是结构体指针, 一种是数组指针.\n接受数组指针很好理解, 我们可以通过数组长度来判断是否查到数据:\n1 2 res := []*model.User{} ret := db.Model(\u0026amp;model.User{}).Where(`user_id \u0026gt; ?`, 0).Find(res) 接受结构体指针时呢? 我们只能通过ret.RowsAffected == 0来判断是否查到数据, 所以ErrRecordNotFound还有点可爱?\n注意:db.Find(\u0026amp;User{}).RowsAffected只会是0或1\n1 2 3 res := \u0026amp;model.User{} ret := db.Model(\u0026amp;model.User{}).Where(`user_id \u0026gt; ?`, 0).Find(res) ret.RowsAffected 从源码来看, Find结构体和Find数组的差别如下:\ngorm.scan.go:214\n1 2 3 4 5 6 7 8 switch(){ case reflect.Slice, reflect.Array: for initialized || rows.Next() { } case reflect.Struct: if initialized || rows.Next() { } } OMG! 仅仅是for和if的差别\u0026hellip;\n那么Find和Take有什么差别呢? 你猜的没错, 只是Take会返回ErrRecordNotFound\ngorm.scan.go:239\n1 2 3 4 5 if db.RowsAffected == 0 \u0026amp;\u0026amp; db.Statement.RaiseErrorOnNotFound { db.AddError(ErrRecordNotFound) } //Find RaiseErrorOnNotFound=false //Take RaiseErrorOnNotFound=true 好家伙! 我直接好家伙! 果然是大佬的代码\u0026hellip;\n行锁\n哪里的代码有version乐观锁和share锁, 我去参观只见过UPDATE锁.\ngormV2和V1这里确实不一样, 写法如下:\n1 DB.Clauses(clause.Locking{Strength: \u0026#34;UPDATE\u0026#34;}).Find(\u0026amp;users) mysql行锁只在事务中有用.\nUpdate gormV2的更新是我最喜欢的一部分, 非常的有趣\u0026hellip;\n零值\n我们不再需要这样, 我讨厌的方式:\n1 2 3 4 5 var updateMap := make(map[string]interface{}) if phone != \u0026#34;\u0026#34; { updateMap[\u0026#34;phone\u0026#34;] = phone } db.Updates(updateMap) 我们只需要这样:\n1 db.Updates(\u0026amp;User{Phone:phone}) Updates方法接受一个Model结构体, 如果更新的列为 \u0026quot;\u0026quot;,0,false,time.Time{} 就会不更新该列. (如果使用gorm-curd, 连数组长度都帮你判断了,真的是太好用了!)\n那你要问, 如果我非要设置这个用户的phone为\u0026quot;\u0026quot;呢?\n你可以这样写:\n1 2 3 db.Select(\u0026#34;phone\u0026#34;).Updates(\u0026amp;User{Phone:\u0026#34;\u0026#34;}) //或者 db.Updates(map[string]interface{}{\u0026#34;phone\u0026#34;:\u0026#34;\u0026#34;}) 又回去map了\u0026hellip; 梅开二度\u0026hellip;\nSQL 表达式\n如果需要商品库存 - 1, gormV2不再需要用事务取出, 再-1 , 再存入\u0026hellip;\n可以这样:\n1 DB.Model(\u0026amp;product).Where(\u0026#34;goods_id = ?\u0026#34;, 10086).Update(\u0026#34;quantity\u0026#34;, gorm.Expr(\u0026#34;quantity - ?\u0026#34;, 1)) 事务 要在事务中执行一系列操作，一般流程如下, 这个事务写法真的是太棒了!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 db.Transaction(func(tx *gorm.DB) error { // 在事务中执行一些 db 操作（从这里开始，您应该使用 \u0026#39;tx\u0026#39; 而不是 \u0026#39;db\u0026#39;） if err := tx.Create(\u0026amp;Animal{Name: \u0026#34;Giraffe\u0026#34;}).Error; err != nil { // 返回任何错误都会回滚事务 return err } if err := tx.Create(\u0026amp;Animal{Name: \u0026#34;Lion\u0026#34;}).Error; err != nil { return err } // 返回 nil 提交事务 return nil }) 嵌套事务\nGORM 支持嵌套事务，可以回滚事务中的事务而不用担心外层事务回滚.\n","permalink":"https://cold-bin.github.io/post/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/","tags":[],"title":"Gorm使用补充点"},{"categories":["分布式系统"],"contents":"微服务架构项目开发要点 随着系统用户逐日增长，原来的单机架构集群部署的方式，虽然能够顶得住流量顶峰地倾泄，但是，系统越来越复杂。集群方式部署的单机架构，终究是单机架构，当系统设计得很复杂时，我们需要把整个单机系统都停掉，然后修bug、添加新功能、回归测试、上线发布。试想一下：在这个过程里，如果出现了一个小小的bug，你就不得不为了修复bug而把所有服务都停掉，这样的作法显然对用户得体验不好。归根结底，是因为项目是单机架构的。如果采用分布式、微服务架构来进行开发并部署项目，那么，原来单机架构项目的诸多功能被拆分成多个服务分别开发、分别部署，每个独立的服务都分布在不同的进程空间、甚至不同的主机上。这样，一个服务挂掉或者修复bug测试等，显然不会再影响其他服务的运行。将原来粒度高的单价架构项目演变成粒度更小的微服务架构，这样就方便开发、部署和维护了。\n要点一：API 网关 在实施微服务的过程中，不免要面临服务的聚合与拆分，当后端服务的拆分相对比较频繁的时候，往往需要一个统一的入口，将不同的请求路由到不同的服务，这就不得不提到API网关，\nAPI网关优势：\n简单的数据聚合可以在网关层完成，避免后台复杂调用 进行统一的认证和鉴权，尽管服务之间的相互调用比较复杂，接口也会比较多，API 网关往往只暴露必须的对外接口，并且对接口进行统一的认证和鉴权，使得内部的服务相互访问的时候，不用再进行认证和鉴权，效率会比较高。 设定一定的策略，进行 A/B 测试，蓝绿发布，预发环境导流等等。API 网关往往是无状态的，可以横向扩展，从而不会成为性能瓶颈。\n要点二：无状态化 影响应用迁移和横向扩展的重要因素就是应用的状态，无状态服务，是要把这个状态往外移，将 Session 数据，文件数据，结构化数据保存在后端统一的存储中，从而应用仅仅包含商务逻辑。 状态是不可避免的，例如 ZooKeeper, DB，Cache 等，把这些所有有状态的东西收敛在一个非常集中的集群里面，因此整个业务就分两部分，一个是无状态的部分，一个是有状态的部分。 无状态的部分能实现两点，一是跨机房随意地部署，也即迁移性，一是弹性伸缩，很容易地进行扩容。 有状态的部分，如 DB，Cache，ZooKeeper 有自己的高可用机制，要利用到他们自己高可用的机制来实现这个状态的集群。 虽说无状态化，但是当前处理的数据，还是会在内存里面的，当前的进程挂掉数据，肯定也是有一部分丢失的，为了实现这一点，服务要有重试的机制，接口要有幂等的机制，通过服务发现机制，重新调用一次后端服务的另一个实例就可以了。\n要点三：数据库的横向扩展 数据库保存状态，是最重要的也是最容易出现瓶颈的。有了分布式数据库可以使数据库的性能可以随着节点增加线性地增加。 分布式数据库最最下面是 RDS，是主备的，通过 MySql 的内核开发能力，我们能够实现主备切换数据零丢失，所以数据落在这个 RDS 里面，是非常放心的，哪怕是挂了一个节点，切换完了以后，你的数据也是不会丢的。 双机房的部署，DDB 开发了一个数据运河 NDC 的组件，可以使得不同的 DDB 之间在不同的机房里面进行同步，这时候不但在一个数据中心里面是分布式的，在多个数据中心里面也会有一个类似双活的一个备份，高可用性有非常好的保证。\n要点四：缓存 在高并发场景下缓存是非常重要的。要有层次的缓存，使得数据尽量靠近用户。数据越靠近用户能承载的并发量也越大，响应时间越短。尤其对于静态数据，可以过一段时间去取一次，而且也没必要到数据中心去取，可以通过 CDN，将数据缓存在距离客户端最近的节点上，进行就近下载。有时候 CDN 里面没有，还是要回到数据中心去下载，称为回源，在数据中心的最外层，我们称为接入层，可以设置一层缓存，将大部分的请求拦截，从而不会对后台的数据库造成压力。 如果是动态数据，还是需要访问应用，通过应用中的商务逻辑生成，或者去数据库读取，为了减轻数据库的压力，应用可以使用本地的缓存，也可以使用分布式缓存，如 Memcached 或者 Redis，使得大部分请求读取缓存即可，不必访问数据库。 当然动态数据还可以做一定的静态化，也即降级成静态数据，从而减少后端的压力。\n要点五：服务拆分和服务发现 当应用变化快的时候，往往要考虑将比较大的服务拆分为一系列小的服务。每个微服务之间是独立的，不耦合，例如一般不会出现多个服务对同一个MySQL数据库进行操作。 这样第一个好处就是开发比较独立，当非常多的人在维护同一个代码仓库的时候，往往对代码的修改就会相互影响，常常会出现我没改什么测试就不通过了，而且代码提交的时候，经常会出现冲突，需要进行代码合并，大大降低了开发的效率。 另一个好处就是上线独立，物流模块对接了一家新的快递公司，需要连同下单一起上线，这是非常不合理的行为，我没改还要我重启，我没改还让我发布，我没改还要我开会，都是应该拆分的时机。 另外再就是高并发时段的扩容，往往只有最关键的下单和支付流程是核心，只要将关键的交易链路进行扩容即可，如果这时候附带很多其他的服务，扩容即是不经济的，也是很有风险的。 再就是容灾和降级，在大促的时候，可能需要牺牲一部分的边角功能，但是如果所有的代码耦合在一起，很难将边角的部分功能进行降级。 当然拆分完毕以后，应用之间的关系就更加复杂了，因而需要服务发现的机制，来管理应用相互的关系，实现自动的修复，自动的关联，自动的负载均衡，自动的容错切换。\n要点六：服务编排与弹性伸缩 服务拆分了，进程就会非常的多，因而需要服务编排来管理服务之间的依赖关系，以及将服务的部署代码化，也就是我们常说的基础设施即代码。这样对于服务的发布，更新，回滚，扩容，缩容，都可以通过修改编排文件来实现，从而增加了可追溯性，易管理性，和自动化的能力。 既然编排文件也可以用代码仓库进行管理，就可以实现一百个服务中，更新其中五个服务，只要修改编排文件中的五个服务的配置就可以，当编排文件提交的时候，代码仓库自动触发自动部署升级脚本，从而更新线上的环境，当发现新的环境有问题时，当然希望将这五个服务原子性地回滚，如果没有编排文件，需要人工记录这次升级了哪五个服务。有了编排文件，只要在代码仓库里面 revert，就回滚到上一个版本了。所有的操作在代码仓库里都是可以看到的。\n要点七：统一配置中心 服务拆分以后，服务的数量非常多，如果所有的配置都以配置文件的方式放在应用本地的话，非常难以管理，可以想象当有几百上千个进程中有一个配置出现了问题，是很难将它找出来的，因而需要有统一的配置中心，来管理所有的配置，进行统一的配置下发。 在微服务中，配置往往分为几类，一类是几乎不变的配置，这种配置可以直接打在容器镜像里面，第二类是启动时就会确定的配置，这种配置往往通过环境变量，在容器启动的时候传进去，第三类就是统一的配置，需要通过配置中心进行下发，例如在大促的情况下，有些功能需要降级，哪些功能可以降级，哪些功能不能降级，都可以在配置文件中统一配置。\n要点八：统一的日志中心 同样是进程数目非常多的时候，很难对成千上百个容器，一个一个登录进去查看日志，所以需要统一的日志中心来收集日志，为了使收集到的日志容易分析，对于日志的规范，需要有一定的要求，当所有的服务都遵守统一的日志规范的时候，在日志中心就可以对一个交易流程进行统一的追溯。例如在最后的日志搜索引擎中，搜索交易号，就能够看到在哪个过程出现了错误或者异常\n要点九：熔断，限流，降级 服务要有熔断，限流，降级的能力，当一个服务调用另一个服务，出现超时的时候，应及时返回，而非阻塞在那个地方，从而影响其他用户的交易，可以返回默认的托底数据。 当一个服务发现被调用的服务，因为过于繁忙，线程池满，连接池满，或者总是出错，则应该及时熔断，防止因为下一个服务的错误或繁忙，导致本服务的不正常，从而逐渐往前传导，导致整个应用的雪崩。 当发现整个系统的确负载过高的时候，可以选择降级某些功能或某些调用，保证最重要的交易流程的通过，以及最重要的资源全部用于保证最核心的流程。 还有一种手段就是限流，当既设置了熔断策略，又设置了降级策略，通过全链路的压力测试，应该能够知道整个系统的支撑能力，因而就需要制定限流策略，保证系统在测试过的支撑能力范围内进行服务，超出支撑能力范围的，可拒绝服务。当你下单的时候，系统弹出对话框说 “系统忙，请重试”，并不代表系统挂了，而是说明系统是正常工作的，只不过限流策略起到了作用。\n要点十：全方位的监控 当系统非常复杂的时候，要有统一的监控，主要有两个方面，一个是是否健康，一个是性能瓶颈在哪里。当系统出现异常的时候，监控系统可以配合告警系统，及时地发现，通知，干预，从而保障系统的顺利运行。 当压力测试的时候，往往会遭遇瓶颈，也需要有全方位的监控来找出瓶颈点，同时能够保留现场，从而可以追溯和分析，进行全方位的优化。\n示例架构 ","permalink":"https://cold-bin.github.io/post/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%A6%81%E7%82%B9/","tags":["微服务"],"title":"微服务架构项目开发要点"},{"categories":["git"],"contents":"Git 每次提交代码，都要写 Commit message（提交说明），否则就不允许提交。\n1 $ git commit -m \u0026#34;hello world\u0026#34; 上面代码的-m参数，就是用来指定 commit mesage 的。\n如果一行不够，可以只执行git commit，就会跳出文本编辑器，让你写多行。\n1 $ git commit 基本上，你写什么都行（这里，这里和这里）。\n但是，一般来说，commit message 应该清晰明了，说明本次提交的目的。\n目前，社区有多种 Commit message 的写法规范。本文介绍Angular 规范（见上图），这是目前使用最广的写法，比较合理和系统化，并且有配套的工具。\nCommit message 的作用 格式化的Commit message，有几个好处。\n（1）提供更多的历史信息，方便快速浏览。\n比如，下面的命令显示上次发布后的变动，每个commit占据一行。你只看行首，就知道某次 commit 的目的。\n1 $ git log \u0026lt;last tag\u0026gt; HEAD --pretty=format:%s （2）可以过滤某些commit（比如文档改动），便于快速查找信息。\n比如，下面的命令仅仅显示本次发布新增加的功能。\n1 $ git log \u0026lt;last release\u0026gt; HEAD --grep feature （3）可以直接从commit生成Change log。\nChange Log 是发布新版本时，用来说明与上一个版本差异的文档，详见后文。\nCommit message 的格式 每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。\n1 2 3 4 5 \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; // 空一行 \u0026lt;body\u0026gt; // 空一行 \u0026lt;footer\u0026gt; 其中，Header 是必需的，Body 和 Footer 可以省略。\n不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。\nHeader Header部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）。\n（1）type\ntype用于说明 commit 的类别，只允许使用下面7个标识。\nfeat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 perf：性能优化 build：构建工具或外部依赖包的修改，比如更新依赖包的版本等 ci：持续集成的配置文件或脚本的修改 如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要。\n（2）scope\nscope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。\n（3）subject\nsubject是 commit 目的的简短描述，不超过50个字符。\n以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号（.） Body Body 部分是对本次 commit 的详细描述，可以分成多行。下面是一个范例。\n1 2 3 4 5 6 7 More detailed explanatory text, if necessary. Wrap it to about 72 characters or so. Further paragraphs come after blank lines. - Bullet points are okay, too - Use a hanging indent 有两个注意点。\n（1）使用第一人称现在时，比如使用change而不是changed或changes。\n（2）应该说明代码变动的动机，以及与以前行为的对比。\nFooter Footer 部分只用于两种情况。\n（1）不兼容变动\n如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 BREAKING CHANGE: isolate scope bindings definition has changed. To migrate the code follow the example below: Before: scope: { myAttr: \u0026#39;attribute\u0026#39;, } After: scope: { myAttr: \u0026#39;@\u0026#39;, } The removed `inject` wasn\u0026#39;t generaly useful for directives so there should be no code using it. （2）关闭 Issue\n如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。\n1 Closes #234 也可以一次关闭多个 issue 。\n1 Closes #123, #245, #992 Revert 还有一种特殊情况，如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。\n1 2 3 revert: feat(pencil): add \u0026#39;graphiteWidth\u0026#39; option This reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit \u0026lt;hash\u0026gt;.，其中的hash是被撤销 commit 的 SHA 标识符。\n如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。\nCommit规范示例 https://github.com/go-redis/redis\nhttps://github.com/gomodule/redigo\n","permalink":"https://cold-bin.github.io/post/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/","tags":["gitのcommit规范"],"title":"Git之commit规范指南"},{"categories":["git"],"contents":"关于版本控制 什么是版本控制？我为什么要关心它呢？\n版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。\n在本书所展示的例子中，我们仅对保存着软件源代码的文本文件作版本控制管理，但实际上，你可以对任何类型的文件进行版本控制。\n如果你是位图形或网页设计师，可能会需要保存某一幅图片或页面布局文件的所有修订版本（这或许是你非常渴望拥有的功能）。采用版本控制系统（VCS）是个明智的选择。有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态。你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。使用版本控制系统通常还意味着，就算你乱来一气把整个项目中的文件改的改删的删，你也照样可以轻松恢复到原先的样子。但额外增加的工作量却微乎其微。\n本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异（见图）。\n其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。甚至在流行的 Mac OS X 系统上安装了开发者工具包之后，也可以使用 rcs 命令。它的工作原理基本上就是保存并管理文件补丁（patch）。文件补丁是一种特定格式的文本文件，记录着对应文件修订前后的内容变化。所以，根据每次修订后的补丁，rcs 可以通过不断打补丁，计算出各个版本的文件内容。\n集中化的版本控制系统 接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？于是，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这已成为版本控制系统的标准做法（见图 1-2）。\n这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。\n事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，而被客户端偶然提取出来的保存在本地的某些快照数据就成了恢复数据的希望。但这样的话依然是个问题，你不能保证所有的数据都已经有人事先完整提取出来过。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。\n分布式版本控制系统 于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份（见图）。\n更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。\nGit 基础 那么，简单地说，Git 究竟是怎样的一个系统呢？请注意，接下来的内容非常重要，若是理解了 Git 的思想和基本工作原理，用起来就会知其所以然，游刃有余。在开始学习 Git 的时候，请不要尝试把各种概念和其他版本控制系统（诸如 Subversion 和 Perforce 等）相比拟，否则容易混淆每个操作的实际意义。Git 在保存和处理各种信息的时候，虽然操作起来的命令形式非常相近，但它与其他版本控制系统的做法颇为不同。理解这些差异将有助于你准确地使用 Git 提供的各种工具。\n直接记录快照，而非差异比较 Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统（CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容，请看图。\nGit 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式就像图 1-5 所示。\n这是 Git 同其他系统的重要区别。它完全颠覆了传统版本控制的套路，并对各个环节的实现方式作了新的设计。Git 更像是个小型的文件系统，但它同时还提供了许多以此为基础的超强工具，而不只是一个简单的 VCS。稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。\n近乎所有操作都是本地执行 在 Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。但如果用 CVCS 的话，差不多所有操作都需要连接网络。因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。\n举个例子，如果要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。所以任何时候你都可以马上翻阅，无需等待。如果想要看当前版本的文件和一个月前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算，而不用请求远程服务器来做这件事，或是把老版本的文件拉到本地来作比较。\n用 CVCS 的话，没有网络或者断开 VPN 你就无法做任何事情。但用 Git 的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程仓库。同样，在回家的路上，不用连接 VPN 你也可以继续工作。换作其他版本控制系统，这么做几乎不可能，抑或非常麻烦。比如 Perforce，如果不连到服务器，几乎什么都做不了（译注：默认无法发出命令 p4 edit file 开始编辑文件，因为 Perforce 需要联网通知系统声明该文件正在被谁修订。但实际上手工修改文件权限可以绕过这个限制，只是完成后还是无法提交更新。）；如果是 Subversion 或 CVS，虽然可以编辑文件，但无法提交更新，因为数据库在网络上。看上去好像这些都不是什么大问题，但实际体验过之后，你就会惊喜地发现，这其实是会带来很大不同的。\n时刻保持数据完整性 在保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。换句话说，不可能在你修改了文件或目录之后，Git 一无所知。这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致文件数据缺失，Git 都能立即察觉。\nGit 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是：24b9da6552252987aa493b52f8696cd6d3b00373\nGit 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。\n多数操作仅添加数据 常用的 Git 操作大多仅仅是把数据添加到数据库。因为任何一种不可逆的操作，比如删除数据，都会使回退或重现历史版本变得困难重重。在别的 VCS 中，若还未提交更新，就有可能丢失或者混淆一些修改的内容，但在 Git 里，一旦提交快照之后就完全不用担心丢失数据，特别是养成定期推送到其他仓库的习惯的话。\n这种高可靠性令我们的开发工作安心不少，尽管去做各种试验性的尝试好了，再怎样也不会弄丢数据。至于 Git 内部究竟是如何保存和恢复数据的，我们会在第九章讨论 Git 内部原理时再作详述。\n文件的三种状态 好，现在请注意，接下来要讲的概念非常重要。对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）。已提交表示该文件已经被安全地保存在本地数据库中了；已修改表示修改了某个文件，但还没有提交保存；已暂存表示把已修改的文件放在下次提交时要保存的清单中。\n由此我们看到 Git 管理项目时，文件流转的三个工作区域：Git 的工作目录，暂存区域，以及本地仓库。\n每个项目都有一个 Git 目录（译注：如果 git clone 出来的话，就是其中 .git 的目录；如果 git clone \u0026ndash;bare 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。\n从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。\n所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。\n基本的 Git 工作流程如下 在工作目录中修改某些文件。 对修改后的文件进行快照，然后保存到暂存区域。 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。 所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。到第二章的时候，我们会进一步了解其中细节，并学会如何根据文件状态实施后续操作，以及怎样跳过暂存直接提交。\nGit 常用命令 git add 把要提交的文件的信息添加到暂存区中。当使用 git commit 时，将依据暂存区中的内容提交至本地库。\n它通常将现有路径的当前内容作为一个整体添加，但是通过一些选项，它也可以用于添加内容，只对所应用的工作树文件进行一些更改，或删除工作树中不存在的路径了。\n“索引”保存工作树内容的快照，并且将该快照作为下一个提交的内容。 因此，在对工作树进行任何更改之后，并且在运行 git commit 命令之前，必须使用 git add 命令将任何新的或修改的文件添加到索引。\n该命令可以在提交之前多次执行。它只在运行 git add 命令时添加指定文件的内容; 如果希望随后的更改包含在下一个提交中，那么必须再次运行 git add 将新的内容添加到索引。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 把指定的文件添加到暂存区中 $ git add \u0026lt;文件路径\u0026gt; # 添加所有修改、已删除的文件到暂存区中 $ git add -u [\u0026lt;文件路径\u0026gt;] $ git add --update [\u0026lt;文件路径\u0026gt;] # 添加所有修改、已删除、新增的文件到暂存区中，省略 \u0026lt;文件路径\u0026gt; 即为当前目录 $ git add -A [\u0026lt;文件路径\u0026gt;] $ git add --all [\u0026lt;文件路径\u0026gt;] # 查看所有修改、已删除但没有提交的文件，进入一个子命令系统 $ git add -i [\u0026lt;文件路径\u0026gt;] $ git add --interactive [\u0026lt;文件路径\u0026gt;] git branch 操作 Git 的分支命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 列出本地的所有分支，当前所在分支以 \u0026#34;*\u0026#34; 标出 $ git branch # 列出本地的所有分支并显示最后一次提交，当前所在分支以 \u0026#34;*\u0026#34; 标出 $ git branch -v # 创建新分支，新的分支基于上一次提交建立 $ git branch \u0026lt;分支名\u0026gt; # 修改分支名称 # 如果不指定原分支名称则为当前所在分支 $ git branch -m [\u0026lt;原分支名称\u0026gt;] \u0026lt;新的分支名称\u0026gt; # 强制修改分支名称 $ git branch -M [\u0026lt;原分支名称\u0026gt;] \u0026lt;新的分支名称\u0026gt; # 删除指定的本地分支 $ git branch -d \u0026lt;分支名称\u0026gt; # 强制删除指定的本地分支 $ git branch -D \u0026lt;分支名称\u0026gt; git checkout 更新工作树中的文件以匹配索引或指定树中的版本。如果没有给出路径 - git checkout 还会更新 HEAD ，将指定的分支设置为当前分支。\n1 2 3 4 5 6 7 8 9 10 11 12 # 切换到已存在的指定分支 $ git checkout \u0026lt;分支名称\u0026gt; # 创建并切换到指定的分支，保留所有的提交记录 # 等同于 \u0026#34;git branch\u0026#34; 和 \u0026#34;git checkout\u0026#34; 两个命令合并 $ git checkout -b \u0026lt;分支名称\u0026gt; # 创建并切换到指定的分支，删除所有的提交记录 $ git checkout --orphan \u0026lt;分支名称\u0026gt; # 替换掉本地的改动，新增的文件和已经添加到暂存区的内容不受影响 $ git checkout \u0026lt;文件路径\u0026gt; git checkout 是 git 最常用的命令之一，同时也是一个很危险的命令，因为这条命令会重写工作区。\ngit clone 将存储库克隆到新创建的目录中，为克隆的存储库中的每个分支创建远程跟踪分支(使用 git branch -r 可见)，并从克隆检出的存储库作为当前活动分支的初始分支。\n1 2 3 4 5 6 7 8 # 默认在当前目录下创建和版本库名相同的文件夹并下载版本到该文件夹下 $ git clone \u0026lt;远程仓库的网址\u0026gt; # 指定本地仓库的目录 $ git clone \u0026lt;远程仓库的网址\u0026gt; \u0026lt;本地目录\u0026gt; # -b 指定要克隆的分支，默认是master分支 $ git clone \u0026lt;远程仓库的网址\u0026gt; -b \u0026lt;分支名称\u0026gt; \u0026lt;本地目录\u0026gt; git stash 暂存当前修改\ngit stash会把所有未提交的修改（包括暂存的和非暂存的）都保存起来，用于后续恢复当前工作目录。 比如下面的中间状态，通过git stash命令推送一个新的储藏，当前的工作目录就干净了。\n注意：stash是本地的，不会通过git push命令上传到git server上。\n实际应用中推荐给每个stash加一个message，用于记录版本，使用git stash save取代git stash命令。示例如下：\n1 git stash save \u0026#34;test-cmd-stash\u0026#34; 重新应用缓存的stash\n可以通过git stash pop命令恢复之前缓存的工作目录\n1 git stash pop 这个指令将缓存堆栈中的第一个stash删除，并将对应修改应用到当前的工作目录下。\n你也可以使用git stash apply命令，将缓存堆栈中的stash多次应用到工作目录中，但并不删除stash拷贝。\n1 git stash apply 在使用git stash apply命令时可以通过名字指定使用哪个stash，默认使用最近的stash（即stash@{0}）。\n查看现有stash\n可以使用git stash list命令，一个典型的输出如下：\n1 2 3 4 git stash list stash@{0}: WIP on master: 049d078 added the index file stash@{1}: WIP on master: c264051 Revert \u0026#34;added file_size\u0026#34; stash@{2}: WIP on master: 21d80a5 added number to log 移除stash\n可以使用git stash drop命令，后面可以跟着stash名字。下面是一个示例：\n1 2 3 4 5 6 $ git stash list stash@{0}: WIP on master: 049d078 added the index file stash@{1}: WIP on master: c264051 Revert \u0026#34;added file_size\u0026#34; stash@{2}: WIP on master: 21d80a5 added number to log $ git stash drop stash@{0} Dropped stash@{0} (364e91f3f268f0900bc3ee613f9f733e82aaed43) 或者使用git stash clear命令，删除所有缓存的stash。\n暂存未跟踪或忽略的文件\n默认情况下，git stash会缓存下列文件：\n添加到暂存区的修改（staged changes） Git跟踪的但并未添加到暂存区的修改（unstaged changes） 但不会缓存一下文件：\n在工作目录中新的文件（untracked files） 被忽略的文件（ignored files） git stash命令提供了参数用于缓存上面两种类型的文件。使用-u或者--include-untracked可以stash untracked文件。使用-a或者--all命令可以stash当前目录下的所有修改。\ngit commit 将索引的当前内容与描述更改的用户和日志消息一起存储在新的提交中。\n1 2 3 4 5 6 7 8 9 10 11 12 # 把暂存区中的文件提交到本地仓库，调用文本编辑器输入该次提交的描述信息 $ git commit # 把暂存区中的文件提交到本地仓库中并添加描述信息 $ git commit -m \u0026#34;\u0026lt;提交的描述信息\u0026gt;\u0026#34; # 把所有修改、已删除的文件提交到本地仓库中 # 不包括未被版本库跟踪的文件，等同于先调用了 \u0026#34;git add -u\u0026#34; $ git commit -a -m \u0026#34;\u0026lt;提交的描述信息\u0026gt;\u0026#34; # 修改上次提交的描述信息 $ git commit --amend git config 主要是用来配置 Git 的相关参数，其主要操作有：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 查看配置信息 # --local：仓库级，--global：全局级，--system：系统级 $ git config \u0026lt; --local | --global | --system \u0026gt; -l # 查看当前生效的配置信息 $ git config -l # 编辑配置文件 # --local：仓库级，--global：全局级，--system：系统级 $ git config \u0026lt; --local | --global | --system \u0026gt; -e # 添加配置项 # --local：仓库级，--global：全局级，--system：系统级 $ git config \u0026lt; --local | --global | --system \u0026gt; --add \u0026lt;name\u0026gt; \u0026lt;value\u0026gt; # 获取配置项 $ git config \u0026lt; --local | --global | --system \u0026gt; --get \u0026lt;name\u0026gt; # 删除配置项 $ git config \u0026lt; --local | --global | --system \u0026gt; --unset \u0026lt;name\u0026gt; # 配置提交记录中的用户信息 $ git config --global user.name \u0026lt;用户名\u0026gt; $ git config --global user.email \u0026lt;邮箱地址\u0026gt; # 更改Git缓存区的大小 # 如果提交的内容较大，默认缓存较小，提交会失败 # 缓存大小单位：B，例如：524288000（500MB） $ git config --global http.postBuffer \u0026lt;缓存大小\u0026gt; # 调用 git status/git diff 命令时以高亮或彩色方式显示改动状态 $ git config --global color.ui true # 配置可以缓存密码，默认缓存时间15分钟 $ git config --global credential.helper cache # 配置密码的缓存时间 # 缓存时间单位：秒 $ git config --global credential.helper \u0026#39;cache --timeout=\u0026lt;缓存时间\u0026gt;\u0026#39; # 配置长期存储密码 $ git config --global credential.helper store Git 一共有3个配置文件：\n仓库级的配置文件：在仓库的 .git/.gitconfig，该配置文件只对所在的仓库有效。 全局配置文件：Mac 系统在 ~/.gitconfig，Windows 系统在 C:\\Users\\\u0026lt;用户名\u0026gt;\\.gitconfig。 系统级的配置文件：在 Git 的安装目录下（Mac 系统下安装目录在 /usr/local/git）的 etc 文件夹中的 gitconfig。 git diff 用于显示提交和工作树等之间的更改。\n此命令比较的是工作目录中当前文件和暂存区域快照之间的差异,也就是修改之后还没有暂存起来的变化内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 比较当前文件和暂存区中文件的差异，显示没有暂存起来的更改 $ git diff # 比较暂存区中的文件和上次提交时的差异 $ git diff --cached $ git diff --staged # 比较当前文件和上次提交时的差异 $ git diff HEAD # 查看从指定的版本之后改动的内容 $ git diff \u0026lt;commit ID\u0026gt; # 比较两个分支之间的差异 $ git diff \u0026lt;分支名称\u0026gt; \u0026lt;分支名称\u0026gt; # 查看两个分支分开后各自的改动内容 $ git diff \u0026lt;分支名称\u0026gt;...\u0026lt;分支名称\u0026gt; git fetch 从远程仓库获取最新的版本到本地的 tmp 分支上。\n1 2 3 4 5 # 将远程仓库所有分支的最新版本全部取回到本地 $ git fetch \u0026lt;远程仓库的别名\u0026gt; # 将远程仓库指定分支的最新版本取回到本地 $ git fetch \u0026lt;远程主机名\u0026gt; \u0026lt;分支名\u0026gt; git init 初始化项目所在目录，初始化后会在当前目录下出现一个名为 .git 的目录。\n1 2 # 初始化本地仓库，在当前目录下生成 .git 文件夹 $ git init git log 显示提交的记录。\n1 2 3 4 5 6 7 8 # 打印所有的提交记录 $ git log # 打印从第一次提交到指定的提交的记录 $ git log \u0026lt;commit ID\u0026gt; # 打印指定数量的最新提交的记录 $ git log -\u0026lt;指定的数量\u0026gt; git merge 用于将两个或两个以上的开发历史加入(合并)一起。\n1 2 3 4 5 # 把指定的分支合并到当前所在的分支下，并自动进行新的提交 $ git merge \u0026lt;分支名称\u0026gt; # 把指定的分支合并到当前所在的分支下，不进行新的提交 $ git merge --no-commit \u0026lt;分支名称\u0026gt; git mv 重命名文件或者文件夹。\n1 2 # 重命名指定的文件或者文件夹 $ git mv \u0026lt;源文件/文件夹\u0026gt; \u0026lt;目标文件/文件夹\u0026gt; git pull 从远程仓库获取最新版本并合并到本地。 首先会执行 git fetch，然后执行 git merge，把获取的分支的 HEAD 合并到当前分支。\n1 2 3 4 5 6 # 从远程仓库获取最新版本。 $ git pull # 从origin远程仓库的main分支拉取代码到本地，一般需要加上 --allow-unrelated-histories 参数忽略提交历史的差异。然后在本地合并冲突 $ git remote add origin 连接 #（http or ssh） $ git pull origin main --allow-unrelated-histories git push 把本地仓库的提交推送到远程仓库。\n1 2 3 4 5 6 # 把本地仓库的分支推送到远程仓库的指定分支 $ git push \u0026lt;远程仓库的别名\u0026gt; \u0026lt;本地分支名\u0026gt;:\u0026lt;远程分支名\u0026gt; # 删除指定的远程仓库的分支 $ git push \u0026lt;远程仓库的别名\u0026gt; :\u0026lt;远程分支名\u0026gt; $ git push \u0026lt;远程仓库的别名\u0026gt; --delete \u0026lt;远程分支名\u0026gt; git remote 操作远程库。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 列出已经存在的远程仓库 $ git remote # 列出远程仓库的详细信息，在别名后面列出URL地址 $ git remote -v $ git remote --verbose # 添加远程仓库 $ git remote add \u0026lt;远程仓库的别名\u0026gt; \u0026lt;远程仓库的URL地址\u0026gt; # 修改远程仓库的别名 $ git remote rename \u0026lt;原远程仓库的别名\u0026gt; \u0026lt;新的别名\u0026gt; # 删除指定名称的远程仓库 $ git remote remove \u0026lt;远程仓库的别名\u0026gt; # 修改远程仓库的 URL 地址 $ git remote set-url \u0026lt;远程仓库的别名\u0026gt; \u0026lt;新的远程仓库URL地址\u0026gt; git reset 还原提交记录，版本回退\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 重置暂存区，但文件不受影响 # 相当于将用 \u0026#34;git add\u0026#34; 命令更新到暂存区的内容撤出暂存区，可以指定文件 # 没有指定 commit ID 则默认为当前 HEAD $ git reset [\u0026lt;文件路径\u0026gt;] $ git reset --mixed [\u0026lt;文件路径\u0026gt;] # 将 HEAD 的指向改变，撤销到指定的提交记录，文件未修改 $ git reset \u0026lt;commit ID\u0026gt; $ git reset --mixed \u0026lt;commit ID\u0026gt; # 将 HEAD 的指向改变，撤销到指定的提交记录，文件未修改 # 相当于调用 \u0026#34;git reset --mixed\u0026#34; 命令后又做了一次 \u0026#34;git add\u0026#34; $ git reset --soft \u0026lt;commit ID\u0026gt; # 将 HEAD 的指向改变，撤销到指定的提交记录，文件也修改了 $ git reset --hard \u0026lt;commit ID\u0026gt; git revert 生成一个新的提交来撤销某次提交，此次提交之前的所有提交都会被保留。\n1 2 # 生成一个新的提交来撤销某次提交 $ git revert \u0026lt;commit ID\u0026gt; git rm 删除文件或者文件夹。\n1 2 3 4 5 6 7 8 # 移除跟踪指定的文件，并从本地仓库的文件夹中删除 $ git rm \u0026lt;文件路径\u0026gt; # 移除跟踪指定的文件夹，并从本地仓库的文件夹中删除 $ git rm -r \u0026lt;文件夹路径\u0026gt; # 移除跟踪指定的文件，在本地仓库的文件夹中保留该文件 $ git rm --cached git status 用于显示工作目录和暂存区的状态。使用此命令能看到那些修改被暂存到了, 哪些没有, 哪些文件没有被 Git tracked 到。\n1 2 # 查看本地仓库的状态 $ git status git status 不显示已经 commit 到项目历史中去的信息。 看项目历史的信息要使用 git log。\ngit tag 操作标签的命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 打印所有的标签 $ git tag # 添加轻量标签，指向提交对象的引用，可以指定之前的提交记录 $ git tag \u0026lt;标签名称\u0026gt; [\u0026lt;commit ID\u0026gt;] # 添加带有描述信息的附注标签，可以指定之前的提交记录 $ git tag -a \u0026lt;标签名称\u0026gt; -m \u0026lt;标签描述信息\u0026gt; [\u0026lt;commit ID\u0026gt;] # 切换到指定的标签 $ git checkout \u0026lt;标签名称\u0026gt; # 查看标签的信息 $ git show \u0026lt;标签名称\u0026gt; # 删除指定的标签 $ git tag -d \u0026lt;标签名称\u0026gt; # 将指定的标签提交到远程仓库 $ git push \u0026lt;远程仓库的别名\u0026gt; \u0026lt;标签名称\u0026gt; # 将本地所有的标签全部提交到远程仓库 $ git push \u0026lt;远程仓库的别名\u0026gt; –tags 分支合并冲突 由于多人同时进行开发，有时候会同时修改一个文件，或者多分支开发，合并的时候就很容易引发冲突，下面是一个制造冲突和解决冲突的例子。\n制造冲突：\n同学 A 新建码云仓库,同时添加同学 B 为开发者(其实一个人也可以制造冲突的)\n同学 A 新建文件 main.js 并提交推送到远程仓库,同学 B 把仓库同步到本地,这时两位同学都有一个 main.js 的空文件\n同学 A 在 main.js 的第一行添加\u0026quot;我是同学 A\u0026quot;,然后添加,提交并推送到远程仓库\n同学 B 在 main.js 的第一行添加\u0026quot;我是同学 B\u0026quot;,\n然后执行以下命令\n1 2 3 git add . git commit -m\u0026#34;修改main.js\u0026#34; git push origin master 出现以下提示\n意思是被拒绝了,要先执行 git pull 命令\n因为两位同学同时修改了同一行代码,所有 git 不知如何取舍(如果同学 a 修改了第一行,同学 b 修改了第二行,那么 git 会智能的合并),只好把合并代码这个事情交给开发者去处理,上图中\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 到========的代码是 B 同学的代码,======到\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; a248f68a5fcbcbe4cc887bee3dfc3cfd1cf7147b 的代码是同学a的代码,括号的 Incoming Change 的意思是从外面来的修改,a248f68a5fcbcbe4cc887bee3dfc3cfd1cf7147b 是仓库是冲突的版本号,你可以通过 git log 看到详细的信息\n你可以根据具体情况去合并代码,取 a 的代码或者取 b 的代码,或者 a 取一点,b 取一点,具体情况具体分析. 手动删除这些特殊符号，并选择某个修改内容进行提交，就可以解决冲突。\n分支管理策略 核心：因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。\nbug分支 软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后合并分支，然后将临时分支删除。\n当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，当前正在dev上进行的工作还没有提交。\n并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？于是，你就现在要停下并暂存手头的工作，转而去修复bug。\n幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等修改BUG以后恢复现场继续工作：\n将当前工作现场“储藏”起来，等以后恢复现场继续工作 git stash 现在用git status查看工作区，工作区就是干净的（除非没有被Git管理的文件） 查看“储藏”的工作现场 git stash list 恢复最近一次“储藏”的工作现场 git stash apply 恢复指定工作现场，通过git stash list查看工作现场的序号，将（stash@{序号}）追加到命令后面 注意，序号越靠后，说明储藏的时间越长 这样恢复后，stash内容并不会删除，需要手动删除 删除“储藏”的工作现场 git stash drop 恢复并删除“储藏”的工作现场 git stash pop master分支出现bug，说明dev同样也存在，Git专门提供了命令，让我们复制一个特定的提交到当前分支 git cherry-pick 特定的提交id Feature分支 软件开发中，总有无穷无尽的新的功能要不断添加进来。\n添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。\n如果在feature分支还没有合并前，需要取消这个功能，删除时会销毁失败，需要使用大写的-D参数强制删除，git branch -D 分支\n多人协作 当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认别名是origin。\n查看远程库的信息 git remote -v ，-v表示详细信息\n推送分支 git push 远程库别名 本地分支名称\n抓取分支 git clone 远程仓库的URL，默认只能看到本地的master分支 如果要在dev分支上开发，就必须创建远程origin的dev分支到本地 git checkout -b dev origin/dev，创建并且切换到dev分支，并将远程库origin/dev弄下来 如果最新提交和你推送的提交有冲突，先用git pull把最新提交抓下来，然后在本地合并，解决冲突，再提交。\n1 2 3 4 5 6 7 8 9 # git pull 失败 $ git pull There is no tracking information for the current branch. Please specify which branch you want to merge with. See git-pull(1) for details. git pull \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev # 原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接 设置本地分支dev与远程分支origin/dev的链接 git branch --set-upstream-to=origin/dev dev\n多人协作工作模式 尽量将冲突在本地解决\npull远程库内容到本地先进行合并 如合并冲突，则解决冲突 如没有冲突或解决后，再commit 并push到远程库 ","permalink":"https://cold-bin.github.io/post/git%E5%B7%A5%E4%BD%9C%E6%B5%81/","tags":["git"],"title":"Git工作流"},{"categories":["mysql"],"contents":"[toc]\n导入表的问题 导入数据时外键约束问题\n数据导入指令：\n1 source d:\\xxx.sql 通过FOREIGN_KEY_CHECKS解决，用法如下：\n1 2 set FOREIGN_KEY_CHECKS=0; #在导入前设置为不检查外键约束 set FOREIGN_KEY_CHECKS=1; #在导入后恢复检查外键约束 第三章_最基本的SELECT语句 1. SQL语言的规则和规范 1) 基本规则 SQL 可以写在一行或者多行。为了提高可读性，各子句分行写，必要时使用缩进 每条命令以 ; 或 \\g 或 \\G 结束 关键字不能被缩写也不能分行 关于标点符号 必须保证所有的小括号、单引号、双引号是成对结束的 必须使用英文状态下的半角输入方式 字符串型和日期时间类型的数据可以使用单引号（\u0026rsquo; \u0026lsquo;）表示 列的别名，尽量使用双引号（\u0026quot; \u0026ldquo;），而且不建议省略as 2) SQL大小写规范（建议遵守） MySQL 在Windows环境下是大小写不敏感的 MySQL 在Linux环境下是大小写敏感的 数据库名、表名、表的别名、变量名是严格区分大小写的 关键字、函数名、列名(或字段名)、列的别名(字段的别名) 是忽略大小写的。 推荐采用统一的书写规范： 数据库名、表名、表别名、字段名、字段别名等都小写 SQL关键字、函数名、绑定变量等都大写 3) 注释 1 2 3 单行注释：#注释文字(MySQL特有的方式) 单行注释：-- 注释文字(--后面必须包含一个空格。) 多行注释：/* 注释文字 */ 4) 命名规则 数据库、表名不得超过30个字符，变量名限制为29个 必须只能包含 A–Z, a–z, 0–9, _ 共63个字符 数据库名、表名、字段名等对象名中间不要包含空格 同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名； 同一个表中，字段不能重名 必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使 用`（着重号）引起来 保持字段名和类型的一致性，在命名字段并为其指定数据类型的时候一定要保证一致性。假如数据 类型在一个表里是整数，那在另一个表里可就别变成字符型了 2. 基本的SELECT语句 1) SELECT \u0026hellip; FROM 语法 1 2 SELECT 标识选择哪些列 FROM 标识从哪个表中选择 选择全部列 1 2 SELECT * FROM departments; 选择特定的列： 1 2 SELECT department_id, location_id FROM departments; 2) 列的别名 重命名一个列 便于计算 紧跟列名，也可以在列名和别名之间加入关键字AS，别名使用双引号，以便在别名中包含空格或特 殊的字符并区分大小写。 AS可以省略，最好不要省略 建议别名简短，见名知意 举例： 1 2 SELECT `last_name` AS `name`, `commission_pct` `comm` FROM `employees`; 3) 去除重复行 DISTINCT关键字\n1 SELECT DISTINCT `department_id` FROM `employees`; 4) 空值参与运算 空值：null ( 不等同于0, ’ ‘, ’null‘ )\n实际问题的解决方案：引入IFNULL\n1 SELECT employee_id, salary \u0026#34;月工资\u0026#34;, salary * (1 + IFNULL(commission_pct, 0)) * 12 \u0026#34;年工资\u0026#34; FROM employees; 这里你一定要注意，在MySQL里面，值不等于空字符串。一个空字符串的长度是0，而一个空值的长度是空。而且，在MySQL里面，空值是占用空间的。\n5) 着重号 `` 必须保证你的字段没有和保留字、数据库系统或常见方法冲突。\n如果坚持使用，在SQL语句中使用 ` ` 引起来。\n1 SELECT * FROM `order`; 6) 查询常数 1 SELECT \u0026#39;小张科技\u0026#39; as \u0026#34;公司名\u0026#34;, employee_id, last_name FROM employees; 3. 显示表结构 显示表中字段的详细信息\n1 2 3 DESCRIBE employees; 或 DESC employees; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mysql\u0026gt; desc employees; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | employee_id | int(6) | NO | PRI | 0 | | | first_name | varchar(20) | YES | | NULL | | | last_name | varchar(25) | NO | | NULL | | | email | varchar(25) | NO | UNI | NULL | | | phone_number | varchar(20) | YES | | NULL | | | hire_date | date | NO | | NULL | | | job_id | varchar(10) | NO | MUL | NULL | | | salary | double(8,2) | YES | | NULL | | | commission_pct | double(2,2) | YES | | NULL | | | manager_id | int(6) | YES | MUL | NULL | | | department_id | int(4) | YES | MUL | NULL | | +----------------+-------------+------+-----+---------+-------+ 11 rows in set (0.00 sec) 其中，各个字段的含义分别解释如下：\nField：表示字段名称。 Type：表示字段类型。 Null：表示该列是否可以存储NULL值。 Key：表示该列是否已编制索引。 PRI表示该列是表主键的一部分； UNI表示该列是UNIQUE索引的一部分； MUL表示在列中某个给定值允许出现多次。 Default：表示该列是否有默认值，如果有，那么值是多少。 Extra：表示可以获取的与给定列有关的附加信息，例如AUTO_INCREMENT等。 4. 过滤数据 语法： 1 2 3 SELECT 字段1,字段2 FROM 表名 WHERE 过滤条件 使用WHERE子句，将不满足条件的行过滤掉。WHERE子句紧随FROM子句。当然，WHERE条件里不能使用聚合函数\n举例： 1 2 3 SELECT employee_id, last_name, job_id, department_id FROM employees WHERE department_id = 90; 第四章_运算符 DUAL \u0026ndash;\u0026gt; 伪表\n1. 算术运算符 1 SELECT 100 + 0, 100 + 50 * 30, 100 - 35.5 FROM `DUAL`; 一个整数类型的值对整数进行加法和减法操作，结果还是一个整数； 一个整数类型的值对浮点数进行加法和减法操作，结果是一个浮点数； 在Java中，+的左右两边如果有字符串，那么表示字符串的拼接。但是在MySQL中+只表示数值相加。如果遇到非数值类型，先尝试转成数值，如果转失败，就按0计算。（注：MySQL 中字符串拼接要使用字符串函数CONCAT()实现） 在数学运算中，0不能用作除数，在MySQL中，一个数除以0为NULL。 取余是MOD(x,y),x除以y的余数 2. 比较运算符 1) 等号运算符 比较运算符用来对表达式左边的操作数和右边的操作数进行比较，比较的结果为真则返回1，比较的结果为假则返回0，其他情况则返回NULL。\n比较运算符经常被用来作为SELECT查询语句的条件来使用，返回符合条件的结果记录。\n如果等号两边的值、字符串或表达式中有一个为NULL，则比较结果为NULL。\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT 1 = 1, 1 = \u0026#39;1\u0026#39;, 1 = 0, \u0026#39;a\u0026#39; = \u0026#39;a\u0026#39;, (5 + 3) = (2 + 6), \u0026#39;\u0026#39; = NULL , NULL = NULL; +-------+---------+-------+-----------+-------------------+-----------+-------------+ | 1 = 1 | 1 = \u0026#39;1\u0026#39; | 1 = 0 | \u0026#39;a\u0026#39; = \u0026#39;a\u0026#39; | (5 + 3) = (2 + 6) | \u0026#39;\u0026#39; = NULL | NULL = NULL | +-------+---------+-------+-----------+-------------------+-----------+-------------+ | 1 | 1 | 0 | 1 | 1 | NULL | NULL | +-------+---------+-------+-----------+-------------------+-----------+-------------+ 1 row in set (0.00 sec) 1 2 3 4 5 6 7 mysql\u0026gt; SELECT 1 = 2, 0 = \u0026#39;abc\u0026#39;, 1 = \u0026#39;abc\u0026#39; FROM `DUAL`; +-------+-----------+-----------+ | 1 = 2 | 0 = \u0026#39;abc\u0026#39; | 1 = \u0026#39;abc\u0026#39; | +-------+-----------+-----------+ | 0 | 1 | 0 | +-------+-----------+-----------+ 1 row in set, 2 warnings (0.00 sec) 如果等号两边的值都是字符串或表达式都为字符串，则MySQL会按照字符串进行比较，其比较的是每个字符串中字符的ANSI编码是否相等。 如果等号两边的值都是整数，则MySQL会按照整数来比较两个值的大小。 如果等号两边的值一个是整数，另一个是字符串，则MySQL会将字符串转化为数字进行比较。 如果等号两边的值、字符串或表达式中有一个为NULL，则比较结果为NULL。 1 2 3 4 5 6 7 mysql\u0026gt; SELECT 1 \u0026lt;=\u0026gt; \u0026#39;1\u0026#39;, 1 \u0026lt;=\u0026gt; 0, \u0026#39;a\u0026#39; \u0026lt;=\u0026gt; \u0026#39;a\u0026#39;, (5 + 3) \u0026lt;=\u0026gt; (2 + 6), \u0026#39;\u0026#39; \u0026lt;=\u0026gt; NULL,NULL \u0026lt;=\u0026gt; NULL FROM dual; +-----------+---------+-------------+---------------------+-------------+---------------+ | 1 \u0026lt;=\u0026gt; \u0026#39;1\u0026#39; | 1 \u0026lt;=\u0026gt; 0 | \u0026#39;a\u0026#39; \u0026lt;=\u0026gt; \u0026#39;a\u0026#39; | (5 + 3) \u0026lt;=\u0026gt; (2 + 6) | \u0026#39;\u0026#39; \u0026lt;=\u0026gt; NULL | NULL \u0026lt;=\u0026gt; NULL | +-----------+---------+-------------+---------------------+-------------+---------------+ | 1 | 0 | 1 | 1 | 0 | 1 | +-----------+---------+-------------+---------------------+-------------+---------------+ 1 row in set (0.00 sec) 可以看到，使用安全等于运算符时，两边的操作数的值都为NULL时，返回的结果为1而不是NULL，其他返回结果与等于运算符相同。\n2) 不等于运算符 不等于运算符（\u0026lt;\u0026gt;和!=）用于判断两边的数字、字符串或者表达式的值是否不相等， 如果不相等则返回1，相等则返回0。不等于运算符不能判断NULL值。如果两边的值有任意一个为NULL， 或两边都为NULL，则结果为NULL。 SQL语句示例如下：\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT 1 \u0026lt;\u0026gt; 1, 1 != 2, \u0026#39;a\u0026#39; != \u0026#39;b\u0026#39;, (3+4) \u0026lt;\u0026gt; (2+6), \u0026#39;a\u0026#39; != NULL, NULL \u0026lt;\u0026gt; NULL; +--------+--------+------------+----------------+-------------+--------------+ | 1 \u0026lt;\u0026gt; 1 | 1 != 2 | \u0026#39;a\u0026#39; != \u0026#39;b\u0026#39; | (3+4) \u0026lt;\u0026gt; (2+6) | \u0026#39;a\u0026#39; != NULL | NULL \u0026lt;\u0026gt; NULL | +--------+--------+------------+----------------+-------------+--------------+ | 0 | 1 | 1 | 1 | NULL | NULL | +--------+--------+------------+----------------+-------------+--------------+ 1 row in set (0.00 sec) 此外，还有非符号类型的运算符：\n3) 空运算符 空运算符 (IS NULL 或者 ISNULL) 判断一个值是否为NULL，如果为NULL则返回1，否则返回0。\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT NULL IS NULL, ISNULL(NULL), ISNULL(\u0026#39;a\u0026#39;), 1 IS NULL; +--------------+--------------+-------------+-----------+ | NULL IS NULL | ISNULL(NULL) | ISNULL(\u0026#39;a\u0026#39;) | 1 IS NULL | +--------------+--------------+-------------+-----------+ | 1 | 1 | 0 | 0 | +--------------+--------------+-------------+-----------+ 1 row in set (0.00 sec) 4) 非空运算符 非空运算符（IS NOT NULL）判断一个值是否不为NULL，如果不为NULL则返回1，否则返回0。\n5) 最小值运算符 语法格式为：LEAST(值1，值2，...，值n)。其中，“值n”表示参数列表中有n个值。在有两个或多个参数的情况下，返回最小值。\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT LEAST (1,0,2), LEAST(\u0026#39;b\u0026#39;,\u0026#39;a\u0026#39;,\u0026#39;c\u0026#39;), LEAST(1,NULL,2); +---------------+--------------------+-----------------+ | LEAST (1,0,2) | LEAST(\u0026#39;b\u0026#39;,\u0026#39;a\u0026#39;,\u0026#39;c\u0026#39;) | LEAST(1,NULL,2) | +---------------+--------------------+-----------------+ | 0 | a | NULL | +---------------+--------------------+-----------------+ 1 row in set (0.00 sec) 由结果可以看到，当参数是整数或者浮点数时，LEAST将返回其中最小的值；当参数为字符串时，返回字母表中顺序最靠前的字符；当比较值列表中有NULL时，不能判断大小，返回值为NULL。\n6) 最大值运算符 语法格式为：GREATEST(值1，值2，...，值n)。其中，n表示参数列表中有n个值。当有两个或多个参数时，返回值为最大值。假如任意一个自变量为NULL，则GREATEST()的返回值为NULL。\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT GREATEST(1,0,2), GREATEST(\u0026#39;b\u0026#39;,\u0026#39;a\u0026#39;,\u0026#39;c\u0026#39;), GREATEST(1,NULL,2); +-----------------+-----------------------+--------------------+ | GREATEST(1,0,2) | GREATEST(\u0026#39;b\u0026#39;,\u0026#39;a\u0026#39;,\u0026#39;c\u0026#39;) | GREATEST(1,NULL,2) | +-----------------+-----------------------+--------------------+ | 2 | c | NULL | +-----------------+-----------------------+--------------------+ 1 row in set (0.00 sec) 由结果可以看到，当参数中是整数或者浮点数时，GREATEST将返回其中最大的值；当参数为字符串时， 返回字母表中顺序最靠后的字符；当比较值列表中有NULL时，不能判断大小，返回值为NULL。\n7) BETWEEN AND运算符 BETWEEN运算符使用的格式通常为SELECT D FROM TABLE WHERE C BETWEEN A AND B，此时，当C大于或等于A，并且C小于或等于B时，结果为1，否则结果为0。\n8) IN运算符 IN运算符用于判断给定的值是否是IN列表中的一个值，如果是则返回1，否则返回0。如果给定的值为NULL，或者IN列表中存在NULL，则结果为NULL。\n1 2 3 4 5 6 mysql\u0026gt; SELECT \u0026#39;a\u0026#39; IN (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;), 1 IN (2,3), NULL IN (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;), \u0026#39;a\u0026#39; IN (\u0026#39;a\u0026#39;, NULL); +----------------------+------------+-------------------+--------------------+ | \u0026#39;a\u0026#39; IN (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;) | 1 IN (2,3) | NULL IN (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;) | \u0026#39;a\u0026#39; IN (\u0026#39;a\u0026#39;, NULL) | +----------------------+------------+-------------------+--------------------+ | 1 | 0 | NULL | 1 | +----------------------+------------+-------------------+--------------------+ 9) NOT IN运算符 NOT IN运算符用于判断给定的值是否不是IN列表中的一个值，如果不是IN列表中的一 个值，则返回1，否则返回0。\n10) LIKE运算符 LIKE运算符主要用来匹配字符串，通常用于模糊匹配，如果满足条件则返回1，否则返回0。如果给定的值或者匹配条件为NULL，则返回结果为NULL。\n1 2 “%”：匹配0个或多个字符。 “_”：只能匹配一个字符。 11) ESCAPE 回避特殊符号的：使用转义符。就是声明另外一个特殊的转义字符，作用和\\一样\n例如：查找某个字段含有IT_的前缀时，需要对其中的特殊字符_进行转义，前面加个\\。\n1 2 3 SELECT job_id FROM jobs WHERE job_id LIKE ‘IT\\_%‘; 如果不是\\，则要加上ESCAPE。\n1 2 3 SELECT job_id FROM jobs WHERE job_id LIKE ‘IT$_%‘ escape ‘$‘; 12) REGEXP运算符(×) REGEXP运算符用来匹配字符串，语法格式为：expr REGEXP匹配条件 。\n（1）‘^’匹配以该字符后面的字符开头的字符串。\n（2）‘$’匹配以该字符前面的字符结尾的字符串。\n（3）‘.’匹配任何一个单字符。\n（4）“[\u0026hellip;]”匹配在方括号内的任何字符。例如，“[abc]”匹配“a”或“b”或“c”。为了命名字符的范围，使用一 个‘-’。“[a-z]”匹配任何字母，而“[0-9]”匹配任何数字。\n（5）*匹配零个或多个在它前面的字符。例如，x*匹配任何数量的x字符，[0-9]*匹配任何数量的数字， 而*匹配任何数量的任何字符。\n3. 逻辑运算符 逻辑运算符主要用来判断表达式的真假，在MySQL中，逻辑运算符的返回结果为1、0或者NULL。\nMySQL中支持4种逻辑运算符如下：\nPS: 一般不会采用符号，直接使用英文单词具有更好的语义\n4. 位运算(×) 位运算符是在二进制数上进行计算的运算符。位运算符会先将操作数变成二进制数，然后进行位运算， 最后将计算结果从二进制变回十进制数。\nMySQL支持的位运算符如下：\n5. 运算符的优先级 数字编号越大，优先级越高，优先级高的运算符先进行计算。\n扩展：使用正则表达式查询(×) 第五章_排序与分页 1. 排序规则 使用ORDER BY子句排序\nASC（ascend）: 升序 DESC（descend）:降序 ORDER BY子句在SELECT语句的结尾。\n1) 单列排序 1 2 3 SELECT last_name, job_id, department_id, hire_date FROM employees ORDER BY hire_date DESC;# 降序查找 2) 多列排序 可以使用不在SELECT列表中的列排序。\n在对多列进行排序的时候，首先排序的第一列必须有相同的列值，才会对第二列进行排序。如果第一列数据中所有值都是唯一的，将不再对第二列进行排序。\n为什么呢？因为，多列排序的原理是：当第一列有多个值相同时，就按照第二列的排序规则排序；如果所有值都是唯一，那么第二列排序就排了个寂寞。\n2. 分页 格式： 1 LIMIT [位置偏移量,] 行数 # 位置偏移量从0开始 举例： 1 2 3 4 5 6 7 8 -- 前10条记录： SELECT * FROM 表名 LIMIT 0,10; # 或者 SELECT * FROM 表名 LIMIT 10; -- 第11至20条记录： SELECT * FROM 表名 LIMIT 10,10; -- 第21至30条记录： SELECT * FROM 表名 LIMIT 20,10; MySQL 8.0中可以使用LIMIT 3 OFFSET 4，意思是获取从第5条记录开始后面的3条记录，和LIMIT 4,3;返回的结果相同。\n分页显式公式：（当前页数-1）* 每页条数，每页条数 1 2 SELECT * FROM table LIMIT(PageNo - 1) * PageSize, PageSize; 注意：LIMIT子句必须放在整个SELECT语句的最后！\n使用LIMIT的好处\n约束返回结果的数量可以减少数据表的网络传输量 ，也可以提升查询效率。如果我们知道返回结果只有1条，就可以使用LIMIT 1 ，告诉SELECT语句只需要返回一条记录即可。这样的好处就是SELECT不需要扫描完整的表，只需要检索到一条符合条件的记录即可返回。而且，当数据库表中数据量过大时（上亿的数据），显然为了避免查找效率过低、减少网络传输量、加快传输速度，分页查询是业务中必须做的。\n第六章_多表查询 1. 多表查询分类讲解 Join 是“连接”的意思，顾名思义，SQL JOIN 子句用于将两个或者多个表联合起来进行查询。\n联合表时需要在每个表中选择一个字段，并对这些字段的值进行比较，值相同的两条记录将合并为一条。联合表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。\n数据库中的表可以通过键将彼此联合起来，一个典型的例子是，将一个表的主键和另一个表的外键进行匹配。在表中，每个主键的值都是唯一的，这样做的目的是在不重复每个表中所有记录的情况下，将表之间的数据交叉捆绑在一起。\n1) 自连接 题目：查询employees表，返回 \u0026lt;员工 works for 老板\u0026gt;\n1 2 3 SELECT CONCAT(worker.last_name , \u0026#39; works for \u0026#39;, manager.last_name) FROM employees AS worker, employees AS manager WHERE worker.manager_id = manager.employee_id; 2) 内连接与外连接 内连接: （默认连接方式）只有当两个表都存在满足on条件的记录时才会返回行。\nSQL92语法:(×)\n1 2 3 SELECT emp.employee_id, dep.department_name FROM employee emp, department dep WHERE emp.`department_id` = dep.`department_id`; SQL99语法:\n1 2 3 SELECT emp.employee_id, dep.department_name FROM employee emp JOIN department dep ON emp.`department_id` = dep.`department_id`; 外连接: 返回左（右）表中的所有行，即使右（左）表中没有满足条件的行也是如此。没有匹配的行时, 结果表中相应的列为空(NULL)。\n如果是左外连接，则连接条件中左边的表也称为主表 ，右边的表称为从表。\nLEFT OUTER JOIN（OUTER可以省略）：返回左表中的所有行，即使右表中没有满足条件的行也是如此。右表满足条件的行拼接在左表上，不满足条件的填充null字段值\n1 2 3 SELECT last_name, department_name FROM employees emp LEFT OUTER JOIN department dep ON emp.`department_id` = dep.`department_id`; 如果是右外连接，则连接条件中右边的表也称为主表 ，左边的表称为从表。\nRIGHT OUTER JOIN（OUTER可以省略）：返回右表中的所有行，即使左表中没有满足条件的行也是如此。左表满足条件的行拼接在右表上，不满足条件的填充null字段值\n1 2 3 SELECT last_name, department_name FROM employees emp RIGHT JOIN department dep ON emp.`department_id` = dep.`department_id`; # 没有匹配主表的行数据，会显示null，可以通过where进行筛选 2. UNION的使用 合并查询结果\n利用UNION关键字，可以给出多条SELECT语句，并将它们的结果组合成单个结果集。合并时，两个表对应的列数和数据类型必须相同，并且相互对应。各个SELECT语句之间使用UNION或UNION ALL关键字分隔。\n语法格式：\n1 2 3 SELECT column,... FROM table1 UNION [ALL] SELECT column,... FROM table2 UNION操作符\nUNION操作符返回两个查询的结果集的并集，去除重复记录。\nUNION ALL操作符\nUNION ALL操作符返回两个查询的结果集的并集。对于两个结果集的重复部分，不去重。\n注意：执行UNION ALL语句时所需要的资源比UNION语句少。如果明确知道合并数据后的结果数据不存在重复数据，或者不需要去除重复的数据，则尽量使用UNION ALL语句，以提高数据查询的效率。\n举例：查询部门编号\u0026gt;90或邮箱包含a的员工信息\n1 2 #方式1 SELECT * FROM employees WHERE email LIKE \u0026#39;%a%\u0026#39; OR department_id\u0026gt;90; 1 2 3 4 #方式2 SELECT * FROM employees WHERE email LIKE \u0026#39;%a%\u0026#39; UNION SELECT * FROM employees WHERE department_id\u0026gt;90; 举例：查询中国用户中男性的信息以及美国用户中年男性的用户信息\n1 2 3 SELECT id,cname FROM t_chinamale WHERE csex=\u0026#39;男\u0026#39; UNION ALL SELECT id,tname FROM t_usmale WHERE tGender=\u0026#39;male\u0026#39;; 3.七种SQL JOINS的实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # 中图：内连接 SELECT employee_id,department_name FROM employees e JOIN departments d ON e.`department_id` = d.`department_id`; # 左上图：左外连接 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id`; # 右上图：右外连接 SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id`; # 左中图： SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL; # 右中图： SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; # 左下图：满外连接 # 方式1：左上图 UNION ALL 右中图 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` UNION ALL SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; # 方式2：左中图 UNION ALL 右上图 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL UNION ALL SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id`; # 右下图：左中图 UNION ALL 右中图 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL UNION ALL SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; 4. SQL99语法的新特性 1) 自然连接 SQL99在SQL92的基础上提供了一些特殊语法，比如NATURAL JOIN用来表示自然连接。我们可以把自然连接理解为SQL92中的等值连接。它会帮你自动查询两张连接表中所有相同的字段，然后进行等值连接。\n在SQL92标准中：\n1 2 3 4 SELECT employee_id,last_name,department_name FROM employees e JOIN departments d ON e.`department_id` = d.`department_id` AND e.`manager_id` = d.`manager_id`; 在 SQL99 中你可以写成：\n1 2 SELECT employee_id,last_name,department_name FROM employees e NATURAL JOIN departments d; 2) USING连接 当我们进行连接的时候，SQL99还支持使用USING指定数据表里的同名字段进行等值连接。但是只能配合JOIN一起使用。比如：\n1 2 3 SELECT employee_id,last_name,department_name FROM employees e JOIN departments d USING (department_id); 你能看出与自然连接NATURAL JOIN同的是，USING指定了具体的相同的字段名称，你需要在USING的括号 () 中填入要指定的同名字段。同时使用JOIN...USING可以简化 JOIN ON 的等值连接。它与下面的SQL查询结果是相同的：\n1 2 3 SELECT employee_id,last_name,department_name FROM employees e ,departments d WHERE e.department_id = d.department_id; 5. 小结 表连接的约束条件可以有三种方式：WHERE, ON, USING\nWHERE：适用于所有关联查询，where是针对临时表生成以后再对临时表的数据进行过滤 ON ：join是连接表的条件，决定了连接表的生成。虽然关联条件可以并到WHERE中和其他条件一起写，但使用join分开写可读性更好。 USING：只能和JOIN一起使用，而且要求两个关联字段在关联表中名称一致，而且只能表示关联字段值相等 我们要控制连接表的数量。\n多表连接就相当于嵌套for循环一样，非常消耗资源，会让SQL查询性能下降得很严重，因此不要连接不必要的表。\n在许多DBMS中，也都会有最大连接表的限制。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 习题巩固 # 注意：当两个表外连接之后，组成主表和从表，主表的连接字段是不为空的，从表的连接字段可能为空，因此从表的关键字段用来判断是否为空。 # 1.查询哪些部门没有员工 # 方式一 SELECT d.department_id FROM departments d LEFT JOIN employees e ON d.`department_id` = e.`department_id` WHERE e.`department_id` IS NULL; # 方式二 SELECT department_id FROM departments d WHERE NOT EXISTS ( SELECT * FROM employees e WHERE e.`department_id` = d.`department_id` ); # 2.查询哪个城市没有部门 SELECT l.location_id, l.city FROM locations l LEFT JOIN departments d ON l.`location_id` = d.`location_id` WHERE d.`location_id` IS NULL; # 3.查询部门名为 Sales 或 IT 的员工信息 SELECT e.employee_id, e.last_name, e.department_id FROM employees e JOIN department d ON e.`department_id` = d.`department_id` WHERE d.`department_name` IN (\u0026#39;Sales\u0026#39;, \u0026#39;IT\u0026#39;); 第七章_单行函数 1. 数值函数 1) 基本函数 函数 用法 ABS(x) 返回x的绝对值 SIGN(X) 单元格 PI() 返回圆周率的值 CEIL(x)，CEILING(x) 返回大于或等于某个值的最小整数 FLOOR(x) 返回小于或等于某个值的最大整数 LEAST(e1,e2,e3…) 返回列表中的最小值 GREATEST(e1,e2,e3…) 返回列表中的最大值 MOD(x,y) 返回X除以Y后的余数 RAND() 返回0~1的随机值 RAND(x) 返回0~1的随机值，其中x的值用作种子值，相同的X值会产生相同的随机数 ROUND(x) 返回一个对x的值进行四舍五入后，最接近于X的整数 ROUND(x,y) 返回一个对x的值进行四舍五入后最接近X的值，并保留到小数点后面Y位 TRUNCATE(x,y) 返回数字x截断为y位小数的结果 SQRT(x) 返回x的平方根。当X的值为负数时，返回NULL 2) 角度与弧度互换函数 函数 用法 RADIANS(x) 将角度转化为弧度，其中，参数x为角度值 DEGREES(x) 将弧度转化为角度，其中，参数x为弧度值 3) 三角函数 函数 用法 SIN(x) 将角度转化为弧度，其中，参数x为角度值 ASIN(x) 将弧度转化为角度，其中，参数x为弧度值 COS(x) 返回x的余弦值，其中，参数x为弧度值 ACOS(x) 返回x的反余弦值，即获取余弦为x的值。如果x的值不在-1到1之间，则返回NULL TAN(x) 返回x的正切值，其中，参数x为弧度值 ATAN(x) 返回x的反正切值，即返回正切值为x的值 ATAN2(m,n) 返回两个参数的反正切值 COT(x) 返回x的余切值，其中，X为弧度值 4) 指数与对数函数 函数 用法 POW(x,y)，POWER(X,Y) 返回x的y次方 EXP(X) 返回e的X次方，其中e是一个常数，2.718281828459045 LN(X)，LOG(X) 返回以e为底的X的对数，当X \u0026lt;= 0 时，返回的结果为NULL LOG10(X) 返回以10为底的X的对数，当X \u0026lt;= 0 时，返回的结果为NULL LOG2(X) 返回以2为底的X的对数，当X \u0026lt;= 0 时，返回NULL 5) 进制间的转换 函数 用法 BIN(x) 返回x的二进制编码 HEX(x) 返回x的十六进制编码 OCT(x) 返回x的八进制编码 CONV(x,f1,f2) 返回f1进制数变成f2进制数 2. 字符串函数 函数 用法 ASCII(S) 返回字符串S中的第一个字符的ASCII码值 CHAR_LENGTH(s) 返回字符串s的字符数。作用与CHARACTER_LENGTH(s)相同 LENGTH(s) 返回字符串s的字节数，和字符集有关 CONCAT(s1,s2,......,sn) 连接s1,s2,......,sn为一个字符串 CONCAT_WS(x, s1,s2,......,sn) 同CONCAT(s1,s2,...)函数，但是每个字符串之间要加上x INSERT(str, idx, len, replacestr) 将字符串str从第idx位置开始，len个字符长的子串替换为字符串replacestr REPLACE(str, a, b) 用字符串b替换字符串str中所有出现的字符串a UPPER(s) 或 UCASE(s) 将字符串s的所有字母转成大写字母 ``LOWER(s)或LCASE(s)` 将字符串s的所有字母转成小写字母 LEFT(str,n) 返回字符串str最左边的n个字符 RIGHT(str,n) 返回字符串str最右边的n个字符 LPAD(str, len, pad) 用字符串pad对str最左边进行填充，直到str的长度为len个字符 RPAD(str ,len, pad) 用字符串pad对str最右边进行填充，直到str的长度为len个字符 LTRIM(s) 去掉字符串s左侧的空格 RTRIM(s) 去掉字符串s右侧的空格 TRIM(s) 去掉字符串s开始与结尾的空格 TRIM(s1 FROM s) 去掉字符串s开始与结尾的s1 TRIM(LEADING s1 FROM s) 去掉字符串s开始处的s1 TRIM(TRAILING s1 FROM s) 去掉字符串s结尾处的s1 REPEAT(str, n) 返回str重复n次的结果 SPACE(n) 返回n个空格 STRCMP(s1,s2) 比较字符串s1,s2的ASCII码值的大小 SUBSTR(s,index,len) 返回从字符串s的index位置其len个字符，作用与SUBSTRING(s,n,len)、MID(s,n,len)相同 LOCATE(substr,str) 返回字符串substr在字符串str中首次出现的位置，作用于POSITION(substr IN str)、INSTR(str,substr)相同。未找到，返回0 ELT(m,s1,s2,…,sn) 返回指定位置的字符串，如果m=1，则返回s1，如果m=2，则返回s2，如果m=n，则返回sn FIELD(s,s1,s2,…,sn) 返回字符串s在字符串列表中第一次出现的位置 FIND_IN_SET(s1,s2) 返回字符串s1在字符串s2中出现的位置。其中，字符串s2是一个以逗号分隔的字符串 REVERSE(s) 返回s反转后的字符串 NULLIF(value1,value2) 比较两个字符串，如果value1与value2相等，则返回NULL，否则返回 value1 注意：MySQL中，字符串的位置是从1开始的。\n3. 日期和时间函数 1) 获取日期、时间 函数 用法 CURDATE() ，CURRENT_DATE() 返回当前日期，只包含年、 月、日 CURTIME() ， CURRENT_TIME() 返回当前时间，只包含时、 分、秒 NOW() / SYSDATE() / CURRENT_TIMESTAMP() / LOCALTIME() / LOCALTIMESTAMP() 返回当前系统日期和时间 UTC_DATE() 返回UTC（世界标准时间） 日期 UTC_TIME() 返回UTC（世界标准时间） 时间 2) 日期与时间戳的转换 函数 用法 UNIX_TIMESTAMP() 以UNIX时间戳的形式返回当前时间。SELECT UNIX_TIMESTAMP() - \u0026gt;1634348884 UNIX_TIMESTAMP(date) 将时间date以UNIX时间戳的形式返回。 FROM_UNIXTIME(timestamp) 将UNIX时间戳的时间转换为普通格式的时间 3) 获取月份、星期、星期数、天数等函数 函数 用法 YEAR(date) / MONTH(date) / DAY(date) 返回具体的日期值 HOUR(time) / MINUTE(time) / SECOND(time) 返回具体的时间值 FROM_UNIXTIME(timestamp) 将UNIX时间戳的时间转换为普通格式的时间 MONTHNAME(date) 返回月份：January，\u0026hellip; DAYNAME(date) 返回星期几：MONDAY，TUESDAY\u0026hellip;..SUNDAY WEEKDAY(date) 返回周几，注意，周1是0，周2是1，。。。周日是6 QUARTER(date) 返回日期对应的季度，范围为1～4 WEEK(date) ， WEEKOFYEAR(date) 返回一年中的第几周 DAYOFYEAR(date) 返回日期是一年中的第几天 DAYOFMONTH(date) 返回日期位于所在月份的第几天 DAYOFWEEK(date) 返回周几，注意：周日是1，周一是2，。。。周六是 7 4) 日期的操作函数 函数 用法 EXTRACT(type FROM date) 返回指定日期中特定的部分，type指定返回的值 EXTRACT(type FROM date)函数中type的取值与含义：\n5) 时间和秒钟转换的函数 函数 用法 TIME_TO_SEC(time) 将 time 转化为秒并返回结果值。转化的公式为： 小时*3600+分钟 *60+秒 SEC_TO_TIME(seconds) 将 seconds 描述转化为包含小时、分钟和秒的时间 6) 计算日期和时间的函数 函数 用法 DATE_ADD(datetime, INTERVAL expr type)，ADDDATE(date,INTERVAL expr type) 返回与给定日期时间相差INTERVAL时间段的日期时间 DATE_SUB(date,INTERVAL expr type)， SUBDATE(date,INTERVAL expr type) 返回与date相差INTERVAL时间间隔的日期 上述函数中type的取值：\n函数 用法 ADDTIME(time1,time2) 返回time1加上time2的时间。当time2为一个数字时，代表的是 秒 ，可以为负数 SUBTIME(time1,time2) 返回time1减去time2后的时间。当time2为一个数字时，代表的 是 秒 ，可以为负数 DATEDIFF(date1,date2) 返回date1 - date2的日期间隔天数 TIMEDIFF(time1, time2) 返回time1 - time2的时间间隔 FROM_DAYS(N) 返回从0000年1月1日起，N天以后的日期 TO_DAYS(date) 返回日期date距离0000年1月1日的天数 LAST_DAY(date) 返回date所在月份的最后一天的日期 MAKEDATE(year,n) 针对给定年份与所在年份中的天数返回一个日期 MAKETIME(hour,minute,second) 将给定的小时、分钟和秒组合成时间并返回 PERIOD_ADD(time,n) 返回time加上n后的时间 7) 日期的格式化与解析 函数 用法 DATE_FORMAT(date,fmt) 按照字符串fmt格式化日期date值 TIME_FORMAT(time,fmt) 按照字符串fmt格式化时间time值 GET_FORMAT(date_type,format_type) 返回日期字符串的显示格式 STR_TO_DATE(str, fmt) 按照字符串fmt对str进行解析，解析为一个日期 上述 非GET_FORMAT 函数中fmt参数常用的格式符：\n格式符 说明 格式符 说明 %Y 4位数字表示年份 %y 表示两位数字表示年份 %M 月名表示月份（January,\u0026hellip;.） %m 两位数字表示月份 （01,02,03。。。） %b 缩写的月名（Jan.，Feb.，\u0026hellip;.） %c 数字表示月份（1,2,3,\u0026hellip;） %D 英文后缀表示月中的天数 （1st,2nd,3rd,\u0026hellip;） %d 两位数字表示月中的天数(01,02\u0026hellip;) %e 数字形式表示月中的天数 （1,2,3,4,5\u0026hellip;..） %H 两位数字表示小数，24小时制 （01,02..） %h 和%I 两位数字表示小时，12小时制 （01,02..） %k 数字形式的小时，24小时制(1,2,3) %l 数字形式表示小时，12小时制 （1,2,3,4\u0026hellip;.） %i 两位数字表示分钟（00,01,02） %S 和%s 两位数字表示秒(00,01,02\u0026hellip;) %W 一周中的星期名称（Sunday\u0026hellip;） %a 一周中的星期缩写（Sun.， Mon.,Tues.，..） %w 以数字表示周中的天数 (0=Sunday,1=Monday\u0026hellip;.) %j 以3位数字表示年中的天数(001,002\u0026hellip;) %U 以数字表示年中的第几周， （1,2,3。。）其中Sunday为周中第一 天 %u 以数字表示年中的第几周， （1,2,3。。）其中Monday为周中第一 天 %T 24小时制 %r 12小时制 %p AM或PM %% 表示% 4. 流程控制函数 流程处理函数可以根据不同的条件，执行不同的处理流程，可以在SQL语句中实现不同的条件选择。 MySQL中的流程处理函数主要包括IF()、IFNULL()和CASE()函数。\n函数 用法 IF(value,value1,value2) 如果value的值为TRUE，返回value1， 否则返回value2 IFNULL(value1, value2) 如果value1不为NULL，返回value1，否则返回value2 CASE WHEN 条件1 THEN 结果1 WHEN 条件2 THEN 结果2 .... [ELSE resultn] END 相当于Java的if\u0026hellip;else if\u0026hellip;else\u0026hellip; CASE expr WHEN 常量值1 THEN 值1 WHEN 常量值1 THEN 值1 .... [ELSE 值n] END 相当于Java的switch\u0026hellip;case\u0026hellip; 5. 加密与解密函数 加密与解密函数主要用于对数据库中的数据进行加密和解密处理，以防止数据被他人窃取。这些函数在保证数据库安全时非常有用。\n函数 用法 PASSWORD(str) 返回字符串str的加密版本，41位长的字符串。加密结果不可逆 ，常用于用户的密码加密 MD5(str) 返回字符串str的md5加密后的值，也是一种加密方式。若参数为 NULL，则会返回NULL SHA(str) 从原明文密码str计算并返回加密后的密码字符串，当参数为 NULL时，返回NULL。 SHA加密算法比MD5更加安全 。 ENCODE(value,password_seed) 返回使用password_seed作为加密密码加密value DECODE(value,password_seed) 返回使用password_seed作为加密密码解密value 6. MySQL信息函数 MySQL中内置了一些可以查询MySQL信息的函数，这些函数主要用于帮助数据库开发或运维人员更好地 对数据库进行维护工作。\n函数 用法 VERSION() 返回当前MySQL的版本号 CONNECTION_ID() 返回当前MySQL服务器的连接数 DATABASE()，SCHEMA() 返回MySQL命令行当前所在的数据库 USER()，CURRENT_USER()、SYSTEM_USER()， SESSION_USER() 返回当前连接MySQL的用户名，返回结果格式为 “主机名@用户名” CHARSET(value) 返回字符串value自变量的字符集 COLLATION(value) 返回字符串value的比较规则 MySQL中有些函数无法对其进行具体的分类，但是这些函数在MySQL的开发和运维过程中也是不容忽视 的。\n函数 用法 FORMAT(value,n) 返回对数字value进行格式化后的结果数据。n表示四舍五入后保留到小数点后n位 CONV(value,from,to) 将value的值进行不同进制之间的转换 INET_ATON(ipvalue) 将以点分隔的IP地址转化为一个数字 INET_NTOA(value) 将数字形式的IP地址转化为以点分隔的IP地址 BENCHMARK(n,expr) 将表达式expr重复执行n次。用于测试MySQL处理expr表达式所耗费的时间 CONVERT(value USING char_code) 将value所使用的字符编码修改为char_code 第八章_聚合函数 1. 聚合函数介绍 什么是聚合函数\n聚合函数作用于一列数据，并对一列数据返回一个值。\n聚合函数类型\nAVG() SUM() MAX() MIN() COUNT() 1) AVG和SUM函数 1 2 3 SELECT AVG(salary), MAX(salary),MIN(salary), SUM(salary) FROM employees WHERE job_id LIKE \u0026#39;%REP%\u0026#39;; 2) MIN和MAX函数 可以对任意数据类型的数据使用 MIN 和 MAX 函数。\n1 2 SELECT MIN(hire_date), MAX(hire_date) FROM employees; 3) COUNT函数 COUNT(*)返回表中记录总数，适用于任意数据类型。\n1 2 3 SELECT COUNT(*) FROM employees WHERE department_id = 50; COUNT(expr) 返回expr不为空的记录总数。\n1 2 3 SELECT COUNT(commission_pct) FROM employees WHERE department_id = 50; 问题：用count(*)，count(1)，count(列名)谁好呢?\n其实，对于MyISAM引擎的表是没有区别的。这种引擎内部有一计数器在维护着行数。 InnoDB引擎的表用count(*),count(1)直接读行数，复杂度是O(n)，因为InnoDB真的要去数一遍。但好于具体的count(列名)。\n问题：能不能使用count(列名)替换count(*)?\n不要使用count(列名)来替代count(*)，count(*)是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。说明：count(*)会统计值为NULL 的行，而count(列名)不会统计此列为NULL值的行。\n2. GROUP BY 1) 基本使用 可以使用GROUP BY子句将表中的数据分成若干组\n1 2 3 4 5 SELECT column, group_function(column) FROM table [WHERE condition] [GROUP BY group_by_expression] [ORDER BY column]; 结论1：\nSELECT中出现的非组函数的字段必须声明在GROUP BY中。\n例如，SELECT查询的字段出现*或其他不是分组字段的字段（当然也不是聚合函数）会报错。如果使用group by分组查询时，SELECT查询的字段，要么包含在Group By语句的后面，作为分组的依据；要么就要被包含在聚合函数中，分组的字段可以不出现在查询字段里\n结论2：\nGROUP BY声明在FROM后面、WHERE后面、ORDER BY前面、LIMIT前面。(先分组后排序)\n2) 使用WITH ROLLUP 1 2 3 4 SELECT department_id,AVG(salary) FROM employees WHERE department_id \u0026gt; 80 GROUP BY department_id WITH ROLLUP; 注意： 当使用ROLLUP时，不能同时使用ORDER BY子句进行结果排序，即ROLLUP和ORDER BY是互相排斥的。\n3. HAVING 1) 基本使用 过滤分组：HAVING子句\n行已经被分组。 使用了聚合函数。 满足HAVING子句中条件的分组将被显示。 HAVING不能单独使用，必须要跟GROUP BY一起使用。 1 2 3 4 SELECT department_id, MAX(salary) FROM employees GROUP BY department_id HAVING MAX(salary)\u0026gt;10000 ; 要求\n如果过滤条件中使用了聚合函数，则必须使用HAVING来替换WHERE。否则，报错。 当过滤条件中没有聚合函数时，则此过滤条件声明在WHERE中或HAVING中都可以。但是，建议声明在WHERE中的执行效率高。 HAVING必须声明在GROUP BY的后面。 开发中，我们使用HAVING的前提是SQL中使用了GROUP BY。 2) WHERE和HAVING的对比 区别1：WHERE 可以直接使用表中的字段作为筛选条件，但不能使用分组中的计算函数作为筛选条件； HAVING 必须要与 GROUP BY 配合使用，可以把分组计算的函数和分组字段作为筛选条件。\n这决定了，在需要对数据进行分组统计的时候，HAVING 可以完成 WHERE 不能完成的任务。这是因为，在查询语法结构中，WHERE 在 GROUP BY 之前，所以无法对分组结果进行筛选。HAVING 在 GROUP BY 之 后，可以使用分组字段和分组中的计算函数，对分组的结果集进行筛选，这个功能是 WHERE 无法完成 的。另外，WHERE排除的记录不再包括在分组中。\n区别2：如果需要通过连接从关联表中获取需要的数据，WHERE 是先筛选后连接，而 HAVING 是先连接后筛选。\n这一点，就决定了在关联查询中，WHERE 比 HAVING 更高效。因为 WHERE 可以先筛选，用一个筛选后的较小数据集和关联表进行连接，这样占用的资源比较少，执行效率也比较高。HAVING 则需要 先把结果集准备好，也就是用未被筛选的数据集进行关联，然后对这个大的数据集进行筛选，这样占用 的资源就比较多，执行效率也较低。\n小结如下：\n关键字 用法 缺点 WHERE 先筛选数据再关联，执行效率高 不能使用分组中的计算函数进行筛选 HAVING 可以使用分组中的计算函数 在最后的结果集中进行筛选，执行效率较低 开发中的选择：\nWHERE 和 HAVING 也不是互相排斥的，我们可以在一个查询里面同时使用 WHERE 和 HAVING。包含分组统计函数的条件用 HAVING，普通条件用 WHERE。这样，我们就既利用了 WHERE 条件的高效快速，又发挥了 HAVING 可以使用包含分组统计函数的查询条件的优点。当数据量特别大的时候，运行效率会有很大的差别。\n4. SELECT的执行过程 1) 查询的结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #方式1：一般不采用，可读性不好 SELECT ...,....,... FROM ...,...,.... WHERE 多表的连接条件 AND 不包含组函数的过滤条件 GROUP BY ...,... HAVING 包含组函数的过滤条件 ORDER BY ... ASC/DESC LIMIT ...,... #方式2：采用，可读性良好 SELECT ...,....,... FROM ... JOIN ... ON 多表的连接条件 JOIN ... ON ... WHERE 不包含组函数的过滤条件 [AND/OR] 不包含组函数的过滤条件 GROUP BY ...,... HAVING 包含组函数的过滤条件 ORDER BY ... ASC/DESC LIMIT ...,... #其中： #（1）from：从哪些表中筛选 #（2）on：关联多表查询时，去除笛卡尔积 #（3）where：从表中筛选的条件 #（4）group by：分组依据 #（5）having：在统计结果中再次筛选 #（6）order by：排序 #（7）limit：分页 需要记住 SELECT 查询时的两个顺序：\n1. 关键字的顺序是不能颠倒的：\n1 SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... LIMIT... 1. SELECT 语句的执行顺序（在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同）：\n1 FROM -\u0026gt; WHERE -\u0026gt; GROUP BY -\u0026gt; HAVING -\u0026gt; SELECT 的字段 -\u0026gt; DISTINCT -\u0026gt; ORDER BY -\u0026gt; LIMIT 比如你写了一个 SQL 语句，那么它的关键字顺序和执行顺序是下面这样的：\n1 2 3 4 5 6 7 SELECT DISTINCT player_id, player_name, count(*) as num # 顺序 5 FROM player JOIN team ON player.team_id = team.team_id # 顺序 1：找到表并连接表，形成新的虚拟表 WHERE height \u0026gt; 1.80 # 顺序 2 GROUP BY player.team_id # 顺序 3 HAVING num \u0026gt; 2 # 顺序 4 ORDER BY num DESC # 顺序 6 LIMIT 2 # 顺序 7 在 SELECT 语句执行这些步骤的时候，每个步骤都会产生一个虚拟表 ，然后将这个虚拟表传入下一个步骤中作为输入。需要注意的是，这些步骤隐含在 SQL 的执行过程中，对于我们来说是不可见的。\n2) SQL的执行原理 SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤：\n首先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1； 通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2； 添加外部行。如果我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟 表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。 当然如果我们操作的是两张以上的表，还会重复上面的步骤，直到所有表都被处理完为止。这个过程得 到是我们的原始数据。\n然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段 。在这个阶段中，实际上是在虚拟表 vt2 的 基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4 。\n当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段 。\n首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2 。\n当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段 ，得到 虚拟表 vt6 。\n最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段 ，得到最终的结果，对应的是虚拟表 vt7 。\n当然我们在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。\n同时因为 SQL 是一门类似英语的结构化查询语言，所以我们在写 SELECT 语句的时候，还要注意相应的关键字顺序，所谓底层运行的原理，就是我们刚才讲到的执行顺序。\n第九章_子查询 1. 基本使用 子查询的基本语法结构： 子查询（内查询）在主查询之前一次执行完成。 子查询的结果被主查询（外查询）使用 。 注意事项 子查询要包含在括号内 将子查询放在比较条件的右侧 单行操作符对应单行子查询，多行操作符对应多行子查询 2. 子查询的分类 分类方式1：\n我们按内查询的结果返回一条还是多条记录，将子查询分为 单行子查询 、 多行子查询 。\n单行子查询 多行子查询 分类方式2：\n我们按内查询是否被执行多次，将子查询划分为 相关(或关联)子查询 和 不相关(或非关联)子查询 。\n子查询从数据表中查询了数据结果，如果这个数据结果只执行一次，然后这个数据结果作为主查询的条件进行执行，那么这样的子查询叫做不相关子查询。\n同样，如果子查询需要执行多次，即采用循环的方式，先从外部查询开始，每次都传入子查询进行查询，然后再将结果反馈给外部，这种嵌套的执行方式就称为相关子查询。\n3. 单行子查询 1) 单行比较操作符 操作符 含义 = equal to \u0026gt; greater than \u0026gt;= greater than or equal to \u0026lt; less than \u0026lt;= less than or equal to \u0026lt;\u0026gt; not equal to 2) 代码示例 题目：返回job_id与141号员工相同，salary比143号员工多的员工姓名，job_id和工资 1 2 3 4 5 6 7 8 9 10 11 12 SELECT last_name, job_id, salary FROM eployees WHERE job_id = ( SELECT job_id FROM eployees WHERE employee_id = 141 ) AND salary \u0026gt; ( SELECT salary FROM eployees WHERE employee_id = 143 ); 题目：查询与141号或174号员工的manager_id和department_id相同的其他员工的employee_id，manager_id，department_id 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 实现方式一：不成对比较 SELECT employee_id, manager_id, department_id FROM employees WHERE manager_id IN (SELECT manager_id FROM employees WHERE employee_id IN (174,141)) AND department_id IN (SELECT department_id FROM employees WHERE employee_id IN (174,141)) AND employee_id NOT IN(174,141); # 实现方式二：成对比较 SELECT employee_id, manager_id, department_id FROM employees WHERE (manager_id, department_id) IN (SELECT manager_id, department_id FROM employees WHERE employee_id IN (141,174)) AND employee_id NOT IN (141,174); 题目：查询最低工资大于50号部门最低工资的部门id和其最低工资 1 2 3 4 5 6 7 SELECT department_id, MIN(salary) FROM employees GROUP BY department_id HAVING MIN(salary) \u0026gt; (SELECT MIN(salary) FROM employees WHERE department_id = 50); 3) CASE中的子查询 题目：显式员工的employee_id,last_name和location。其中，若员工department_id与location_id为1800 的department_id相同，则location为’Canada’，其余则为’USA’。\n1 2 3 4 5 6 7 SELECT employee_id, last_name, (CASE department_id WHEN (SELECT department_id FROM departments WHERE location_id = 1800) THEN \u0026#39;Canada\u0026#39; ELSE \u0026#39;USA\u0026#39; END) location FROM employees; 4) 子查询中的空值问题 1 2 3 4 5 6 SELECT last_name, job_id FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE last_name = \u0026#39;Haas\u0026#39;); 子查询不返回任何行\n5) 非法使用子查询(×) 1 2 3 4 5 6 SELECT employee_id, last_name FROM employees WHERE salary = (SELECT MIN(salary) FROM employees GROUP BY department_id); 多行子查询使用单行比较符\n4. 多行子查询 也称为集合比较子查询 内查询返回多行 使用多行比较操作符 1) 多行比较操作符 操作符 含义 IN 等于列表中的任意一个 ANY 需要和单行比较操作符一起使用，和子查询返回的某一个值比较 ALL 需要和单行比较操作符一起使用，和子查询返回的所有值比较 SOME 实际上是ANY的别名，作用相同，一般常使用ANY 2) 代码示例 题目：返回其它job_id中比job_id为‘IT_PROG’部门任一工资低的员工的员工号、姓名、job_id 以及salary 1 2 3 4 5 6 7 8 SELECT employee_id, last_name, job_id, salary FROM employees WHERE job_id \u0026lt;\u0026gt; \u0026#39;IT_PROG\u0026#39; AND salary \u0026lt; ANY( SELECT salary FROM emplyees WHERE job_id = \u0026#39;IT_PROG\u0026#39; ); 题目：查询平均工资最低的部门id 1 2 3 4 5 6 7 8 9 10 11 12 #方式1： SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT MIN(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) dept_avg_sal ); 1 2 3 4 5 6 7 8 9 #方式2： SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) \u0026lt;= ALL ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ); 3) 空值问题 1 2 3 4 5 6 7 SELECT last_name FROM employees WHERE employee_id NOT IN ( SELECT manager_id FROM employees WHERE manager_id IS NOT NULL ); 5. 关联子查询 如果子查询的执行依赖于外部查询，通常情况下都是因为子查询中的表用到了外部的表，并进行了条件关联，因此每执行一次外部查询，子查询都要重新计算一次，这样的子查询就称之为关联子查询 。\n关联子查询按照一行接一行的顺序执行，主查询的每一行都执行一次子查询。\n说明：子查询中使用主查询中的列\n1) 代码示例 题目：查询员工中工资大于本部门平均工资的员工的last_name,salary和其department_id 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 方式一：使用相关子查询 SELECT last_name, salary, department_id FROM employees e1 WHERE salary \u0026gt; ( SELECT AVG(salary) FROM employees e2 WHERE department_id = e1.`department_id` ); # 方式二：在FROM中声明子查询 SELECT e.last_name, e.salary, e.department_id FROM employees e, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id) t_dept_avg_salary WHERE e.department_id = t_dept_avg_salary.department_id AND e.salary \u0026gt; t_dept_avg_salary.avg_sal; 在ORDER BY中使用子查询：\n查询员工的id,salary,按照department_name排序 1 2 3 4 5 6 7 SELECT employee_id, salary FROM employees e ORDER BY ( SELECT department_name FROM departments d WHERE e.`department_id` = d.`department_id` ); 题目：若employees表中employee_id与job_history表中employee_id相同的数目不小于2，输出这些相同id的员工的employee_id,last_name和其job_id 1 2 3 4 5 6 SELECT e.employee_id, last_name,e.job_id FROM employees e WHERE 2 \u0026lt;= (SELECT COUNT(*) FROM job_history WHERE employee_id = e.employee_id ); 2) EXISTS 与 NOT EXISTS 关键字 关联子查询通常也会和EXISTS操作符一起来使用，用来检查在子查询中是否存在满足条件的行。 如果在子查询中不存在满足条件的行： 条件返回FALSE 继续在子查询中查找 如果在子查询中存在满足条件的行： 不在子查询中继续查找 条件返回TRUE NOT EXISTS关键字表示如果不存在某种条件，则返回TRUE，否则返回FALSE。 题目：查询公司管理者的employee_id，last_name，job_id，department_id信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 方式一：EXISTS SELECT employee_id, last_name, job_id, department_id FROM employees e1 WHERE EXISTS ( SELECT * FROM employees e2 WHERE e2.manager_id = e1.employee_id ); # 方式二：自连接 SELECT DISTINCT e1.employee_id, e1.last_name, e1.job_id, e1.department_id FROM employees e1 JOIN employees e2 ON e1.employee_id = e2.manager_id; # 方式三：IN SELECT employee_id, last_name, job_id, department_id WHERE employee_id IN ( SELECT DISTINCT manager_id FROM employees ); 题目：查询departments表中，不存在于employees表中的部门的department_id和department_name\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 方式一： SELECT d.department_id, d.department_name FROM departments e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; # 方式二： SELECT department_id, department_name FROM departments d WHERE NOT EXISTS ( SELECT * FROM employees e WHERE d.`department_id` = e.`department_id` ); 3) 相关更新 1 2 3 4 UPDATE table1 alias1 SET column = (SELECT expression FROM table2 alias2 WHERE alias1.column = alias2.column); 使用相关子查询依据一个表中的数据更新另一个表的数据。\n题目：在employees中增加一个department_name字段，数据为员工对应的部门名称\n1 2 3 4 5 6 7 8 9 # 1） ALTER TABLE employees ADD(`department_name` VARCHAR(14)); # 2） UPDATE employees e SET department_name = (SELECT department_name FROM departments d WHERE e.department_id = d.department_id); 4) 相关删除 1 2 3 4 DELETE FROM table1 alias1 WHERE column operator (SELECT expression FROM table2 alias2 WHERE alias1.column = alias2.column); 使用相关子查询依据一个表中的数据删除另一个表的数据。\n题目：删除表employees中，其与emp_history表皆有的数据\n1 2 3 4 5 6 DELETE FROM employees e WHERE employee_id in( SELECT employee_id FROM emp_history WHERE employee_id = e.employee_id ); 6. 思考题 问题：谁的工资比Abel的高？ 解答：\n1 2 3 4 5 #方式1：自连接 SELECT e2.last_name,e2.salary FROM employees e1,employees e2 WHERE e1.last_name = \u0026#39;Abel\u0026#39; AND e1.`salary` \u0026lt; e2.`salary`; 1 2 3 4 5 6 7 8 #方式2：子查询 SELECT last_name,salary FROM employees WHERE salary \u0026gt; ( SELECT salary FROM employees WHERE last_name = \u0026#39;Abel\u0026#39; ); 问题：以上两种方式有好坏之分吗？\n解答：自连接方式好！\n题目中可以使用子查询，也可以使用自连接。一般情况建议你使用自连接，因为在许多 DBMS 的处理过程中，对于自连接的处理速度要比子查询快得多。 可以这样理解：子查询实际上是通过未知表进行查询后的条件判断，而自连接是通过已知的自身数据表 进行条件判断，因此在大部分 DBMS 中都对自连接处理进行了优化。\n7. 课后练习 查询和Zlotkey相同部门的员工姓名和工资 1 2 3 4 5 6 7 8 # 单行子查询，确定查询结果一定只有一行 SELECT last_name, salary FROM employees WHERE department_id = ( SELECT department_id FROM employees WHERE last_name = \u0026#39;Zlotkey\u0026#39; ); 查询工资比公司平均工资高的员工的员工号，姓名和工资。 1 2 3 4 5 6 SELECT employee_id, last_name, salary FROM employees WHERE salary \u0026gt; ( SELECT AVG(salary) FROM employee ); 选择工资大于所有JOB_ID = 'SA_MAN' 的员工的工资的员工的last_name, job_id, salary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 使用多行子查询：all，比较每一行 SELECT last_name, job_id, salary FROM employees WHERE salary \u0026gt; ALL ( SELECT salary FROM employees WHERE job_id = \u0026#39;SA_MAN\u0026#39; ); # 单行子查询：只比较最大值即可 SELECT last_name, job_id, salary FROM employees WHERE salary \u0026gt; ( SELECT MAX(salary) FROM employees WHERE job_id = \u0026#39;SA_MAN\u0026#39; ); 查询姓名中包含字母u的员工在相同部门员工的员工号和姓名 1 2 3 4 5 6 7 SELECT employee_id, last_name FROM eployees WHERE department_id IN ( SELECT DISTINCT department_id FROM employees WHERE last_name LIKE \u0026#39;%u%\u0026#39; ); 查询在部门的department_id为1700的部门员工的员工号 1 2 3 4 5 6 7 SELECT employee_id FROM employees WHERE department_id IN ( SELECT department_id FROM departments WHERE location_id = 1700 ); 查询管理者是King的员工姓名和工资 1 2 3 4 5 6 7 SELECT last_name, salary FROM employees WHERE manage_id IN ( SELECT employee_id FROM employees WHERE last_name = \u0026#39;King\u0026#39; ); 查询工资最低的员工信息 (last_name, salary) 1 2 3 4 5 6 SELECT last_name, salary FROM employees WHERE salary = ( SELECT MIN(salary) FROM employees ); 查询平均工资最低的部门信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # 方式一 SELECT * FROM departments WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT MIN(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) t_dept_avg_sal ) ); # 方式二 SELECT * FROM departments WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) \u0026lt;= ALL ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) ); # 方式三: LIMIT SELECT * FROM departments WHERE department_id IN ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal ASC LIMIT 1 ) ); # 方式四 SELECT d.* FROM departments d, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal ASC LIMIT 0,1 ) t_dept_avg_sal WHERE d.`department_id` = t_dept_avg_sal.`department_id`; 查询平均工资最低的部门信息和该部门的平均工资 (相关子查询) 1 2 3 4 5 6 7 8 9 SELECT d.*, (SELECT AVG(salary) FROM employees WHERE department_id = d.`department_id`) avg_sal FROM departments d, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal ASC LIMIT 0,1 ) t_dept_avg_sal WHERE d.`department_id` = t_dept_avg_sal.`department_id`; 查询平均工资最高的job信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT * FROM jobs WHERE job_id = ( SELECT job_id FROM employees GROUP BY job_id HAVING AVG(salary) = ( SELECT MAX(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY job_id ) t_job_avg_sal ) ); 查询平均工资高于公司平均工资的部门有哪些？ 1 2 3 4 5 6 7 8 SELECT depatment_id FROM employees WHERE department_id IS NOT NULL GROUP BY department_id HAVING AVG(salary) \u0026gt; ( SELECT AVG(salary) FROM eployees ); 查询出公司中所有manager的详细信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 方式1：自连接 SELECT DISTINCT * FROM employees emp, employees manager WHERE emp.`manager_id` = manager.`employee_id`; SELECT DISTINCT * FROM employees emp JOIN employees manager ON emp.`manager_id` = manager.`employee_id`; # 方式2：子查询 SELECT * FROM employees WHERE employee_id IN ( SELECT manager_id FROM employees ); # 方式3：EXISTS SELECT * FROM employees manager WHERE EXISTS ( SELECT * FROM employees emp WHERE manager.`employee_id` = emp.`manager_id` ); 各个部门中，最高工资中最低的那个部门的最低工资是多少？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # 方式一： SELECT MIN(salary) FROM employees WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING MAX(salary) = ( SELECT MIN(max_sal) FROM ( SELECT MAX(salary) max_sal FROM employees GROUP BY department_id ) t_dept_max_sal ) ); # 方式二： SELECT MIN(salary) FROM employees WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING MAX(salary) \u0026lt;= ALL ( SELECT MAX(salary) FROM employees GROUP BY department_id ) ); # 方式三： SELECT MIN(salary) FROM employees WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING MAX(salary) = ( SELECT MAX(salary) max_sal FROM employees GROUP BY department_id ORDER BY max_sal ASC LIMIT 0,1 ) ); # 方式四： FROM employees e, ( SELECT department_id, MAX(salary) max_sal FROM employees GROUP BY department_id ORDER BY max_sal ASC LIMIT 0,1 ) t_dept_max_sal WHERE e.`department_id` = t_dept_max_sal.`department_id`; 查询平均工资最高的部门的manager的详细信息：last_name, department_id, email, salary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 SELECT last_name, department_id, email, salary FROM employees WHERE employee_id IN ( SELECT DISTINCT manager_id FROM employees WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT MAX(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) t_dept_avg_sal ) ) ); SELECT last_name, department_id, email, salary FROM employees WHERE employee_id IN ( SELECT DISTINCT manager_id FROM employees e, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal DESC LIMIT 0,1 ) t_dept_avg_sal WHERE e.`department_id` = t_dept_avg_sal.`department_id` ); 查询部门的部门号，其中不包括job_id=\u0026quot;ST_CLERK\u0026quot;的部门号 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT department_id FROM departments WHERE department_id NOT IN ( SELECT DISTINCT department_id FROM employees WHERE job_id = `ST_CLERK` ); SELECT department_id FROM department d WHERE NOT EXISTS ( SELECT * FROM employees e WHERE d.`department_id` = e.`department_id` AND e.`job_id` = \u0026#39;ST_CLERK\u0026#39; ); 选择所有没有管理者的员工的last_name 1 2 3 4 5 6 7 SELECT last_name FROM employees emp WHERE NOT EXISTS ( SELECT * FROM employees mgr WHERE emp.`manager_id` = mgr.`employee_id` ); 查询员工号、姓名、雇用时间、工资，其中员工的管理者为 ‘De Haan' 1 2 3 4 5 6 7 SELECT employee_id, last_name, hire_date, salary FROM employee WHERE manager_id IN ( SELECT manager_id FROM employee WHERE last_name = \u0026#39;De Haan\u0026#39; ); 查询各部门中工资比本部门平均工资高的员工的员工号，姓名和工资（相关子查询） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT department_id, last_name, salary FROM employees e1 WHERE salary \u0026gt; ( SELECT AVG(salary) FROM employees e2 WHERE e2.`department_id` = e1.`department_id` ); SELECT e.last_name, e.salary, e.department_id FROM employees e, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id ) t_dept_avg_sal WHERE e.`department_id` = t_dept_avg_sal.`department_id` AND e.`salary` \u0026gt; t_dept_avg_sal.`avg_sal`; 查询每个部门下的部门人数大于5的部门名称（相关子查询） 1 2 3 4 5 6 7 SELECT department_name FROM departments d WHERE 5 \u0026lt; ( SELECT COUNT(*) FROM employees e WHERE d.`department_id` = e.`department_id` ); 查询每个国家下的部门个数大于2的国家编号（相关子查询） 1 2 3 4 5 6 7 SELECT country_id FROM locations l WHERE 2 \u0026lt; ( SELECT COUNT(*) FROM department d WHERE l.`location_id` = d.`location_id` ); 第十章_创建和管理表 1. 基础知识 1) 标识符命名规则 数据库名、表名不得超过30个字符，变量名限制为29个 必须只能包含 A–Z, a–z, 0–9, _共63个字符 数据库名、表名、字段名等对象名中间不要包含空格 同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名 必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使 用`（着重号）引起来 保持字段名和类型的一致性：在命名字段并为其指定数据类型的时候一定要保证一致性，假如数据 类型在一个表里是整数，那在另一个表里可就别变成字符型了 2) MySQL中的数据类型 类型 数据变量 整数类型 TINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT 浮点类型 FLOAT、DOUBLE 定点数类型 DECIMAL 位类型 BIT 日期时间类型 YEAR、TIME、DATE、DATETIME、TIMESTAMP 文本字符串类型 CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT 枚举类型 ENUM 集合类型 SET 二进制字符串类型 BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB JSON类型 JSON对象、JSON数组 空间数据类型 单值：GEOMETRY、POINT、LINESTRING、POLYGON； 集合：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、 GEOMETRYCOLLECTION 其中，常用的几类类型介绍如下：\n① 数值类型 出现“数值类型(n)”的表示，例如: TINYINT(1)、INT(2)表示数据库显示时的最小长度，而不是数据大小\n类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 Bytes (-128，127) (0，255) 小整数值 SMALLINT 2 Bytes (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 Bytes (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 Bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 Bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 Bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 Bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M\u0026gt;D，为M+2,否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 ② 日期类型 每个时间类型有一个有效值范围和一个\u0026quot;零\u0026quot;值，当指定不合法的MySQL不能表示的值时使用\u0026quot;零\u0026quot;值。\n类型 大小 (bytes) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 \u0026lsquo;-838:59:59\u0026rsquo;/\u0026lsquo;838:59:59\u0026rsquo; HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 \u0026lsquo;1000-01-01 00:00:00\u0026rsquo; 到 \u0026lsquo;9999-12-31 23:59:59\u0026rsquo; YYYY-MM-DD hh:mm:ss 混合日期和时间值 TIMESTAMP 4 \u0026lsquo;1970-01-01 00:00:01\u0026rsquo; UTC 到 \u0026lsquo;2038-01-19 03:14:07\u0026rsquo; UTC结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYY-MM-DD hh:mm:ss 混合日期和时间值，时间戳 ③ 字符串类型 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 TEXT 0-65 535 bytes 长文本数据 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 注意：\nchar(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符，有时有些特殊字符占有字节数大于一个byte. CHAR(n)定义的列的长度为固定的，n取值可以为0～255之间. 当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉。在存储或检索过程中不进行大小写转换。CHAR存储定长数据很方便，CHAR字段上的索引效率级高，比如定义 char(10)，那么不论你存储的数据是否达到了10个字节，都要占去10个字节的空间,不足的自动用空格填充。但是如果使用的是Innodb引擎的话，推荐使用varchar代替char BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR，不同的是它们包含二进制字符串而不要非二进制字符串。也就是说，它们包含字节字符串而不是字符字符串。这说明它们没有字符集，并且排序和比较基于列值字节的数值值。 BLOB 是一个二进制大对象，可以容纳可变数量的数据。有 4 种 BLOB 类型：TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB。它们区别在于可容纳存储范围不同。 有 4 种 TEXT 类型：TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT。对应的这 4 种 BLOB 类型，可存储的最大长度不同，可根据实际情况选择。 2. 创建和管理数据库 1) 创建数据库 方式1：创建数据库\n1 CREATE DATABASE 数据库名; 方式2：创建数据库并指定字符集\n1 CREATE DATABASE 数据库名 CHARACTER SET 字符集; 方式3：判断数据库是否已经存在，不存在则创建数据库（ 推荐 ）\n1 CREATE DATABASE IF NOT EXISTS 数据库名; 如果MySQL中已经存在相关的数据库，则忽略创建语句，不再创建数据库。\n注意：DATABASE 不能改名。一些可视化工具可以改名，它是建新库，把所有表复制到新库，再删旧库完成的。\n2) 使用数据库 查看当前所有的数据库\n1 SHOW DATABASES; #有一个S，代表多个数据库 查看当前正在使用的数据库\n1 SELECT DATABASE(); #使用的一个 mysql 中的全局函数 查看指定库下所有的表\n1 SHOW TABLES FROM 数据库名 查看数据库的创建信息\n1 2 3 SHOW CREATE DATABASE 数据库名; # 或者： SHOW CREATE DATABASE 数据库名\\G 使用use切换数据库\n1 USE 数据库名; 注意：要操作表格和数据之前必须先说明是对哪个数据库进行操作，否则就要对所有对象加上“use 据库名.”。\n3) 修改数据库 更改数据库字符集\n1 ALTER DATABASE 数据库名 CHARACTER SET 字符集; #比如：gbk、utf8等 方式1：删除指定的数据库\n1 DROP DATABASE 数据库名; 方式2：删除指定的数据库（ 推荐 ）\n1 DROP DATABASE IF EXISTS 数据库名; 3. 创建表 1) 创建方式1 语法格式： 1 2 3 4 5 6 7 CREATE TABLE [IF NOT EXISTS] 表名( 字段1, 数据类型 [约束条件] [默认值], 字段2, 数据类型 [约束条件] [默认值], 字段3, 数据类型 [约束条件] [默认值], …… [表约束条件] ); 加上了IF NOT EXISTS关键字，则表示：如果当前数据库中不存在要创建的数据表，则创建数据表； 如果当前数据库中已经存在要创建的数据表，则忽略建表语句，不再创建数据表。\n2) 创建方式2 使用 AS subquery 选项，将创建表和插入数据结合起来 1 2 3 CREATE TABLE 表名 [(column, column, ...)] AS subquery; 指定的列和子查询中的列要一一对应 通过列名和默认值定义列 1 2 3 4 5 CREATE TABLE dept80 AS SELECT employee_id, last_name, salary*12 ANNSAL, hire_date FROM employees WHERE department_id = 80; 3) 查看数据表结构 在MySQL中创建好数据表之后，可以查看数据表的结构。MySQL支持使用 DESCRIBE/DESC 语句查看数据 表结构，也支持使用 SHOW CREATE TABLE 语句查看数据表结构。\n语法格式如下：\n1 SHOW CREATE TABLE 表名\\G 使用SHOW CREATE TABLE语句不仅可以查看表创建时的详细语句，还可以查看存储引擎和字符编码。\n4. 修改表 修改表指的是修改数据库中已经存在的数据表的结构。\n使用 ALTER TABLE 语句可以实现：\n向已有的表中添加列 修改现有表中的列 删除现有表中的列 重命名现有表中的列 1) 追加一个列 语法格式如下：\n1 ALTER TABLE 表名 ADD [COLUMN] 字段名 字段类型 [FIRST]|[AFTER 字段名]; 举例：\n1 2 ALTER TABLE dept80 ADD job_id varchar(15); 2) 修改一个列 可以修改列的数据类型，长度、默认值和位置 修改字段数据类型、长度、默认值、位置的语法格式如下： 1 ALTER TABLE 表名 MODIFY [COLUMN] 字段名1 字段类型 [DEFAULT 默认值] [FIRST]|[AFTER 字段名2]; 举例： 1 2 ALTER TABLE dept80 MODIFY salary double(9,2) default 1000; 对默认值的修改只影响今后对表的修改 此外，还可以通过此种方式修改列的约束。 3) 重命名一个列 使用 CHANGE old_column new_column dataType子句重命名列。语法格式如下：\n1 ALTER TABLE 表名 CHANGE [column] 列名 新列名 新数据类型; 举例：\n1 2 ALTER TABLE dept80 CHANGE department_name dept_name varchar(15); 4) 删除一个列 删除表中某个字段的语法格式如下：\n1 ALTER TABLE 表名 DROP [COLUMN] 字段名 5) 更改表名 方式一：使用RENAME 1 2 RENAME TABLE emp TO myemp; 方式二： 1 2 ALTER table dept RENAME [TO] detail_dept; -- [TO]可以省略 必须是对象的拥有者 5. 删除表 在MySQL中，当一张数据表没有与其他任何数据表形成关联关系时，可以将当前数据表直接删除。 数据和结构都被删除 所有正在运行的相关事务被提交 所有相关索引被删除 语法格式： 1 DROP TABLE [IF EXISTS] 数据表1 [, 数据表2, …, 数据表n]; IF EXISTS 的含义为：如果当前数据库中存在相应的数据表，则删除数据表；如果当前数据库中不存在相应的数据表，则忽略删除语句，不再执行删除数据表的操作。\n举例：\n1 DROP TABLE dept80; DROP TABLE 语句不能回滚 6. 清空表 TRUNCATE TABLE语句： 删除表中所有的数据 释放表的存储空间 举例： 1 TRUNCATE TABLE detail_dept; TRUNCATE语句不能回滚，而使用 DELETE 语句删除数据，可以回滚 阿里开发规范： 【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但 TRUNCATE 无事务且不触发 TRIGGER，有可能造成事故，故不建议在开发代码中使用此语句。 说明：TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同。\n7. 内容扩展 拓展1：阿里巴巴《Java开发手册》之MySQL字段命名 【 强制 】表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\n正例：aliyun_admin，rdc_config，level3_name 反例：AliyunAdmin，rdcConfig，level_3_name 【 强制 】禁用保留字，如 desc、range、match、delayed 等，请参考 MySQL 官方保留字。\n【 强制 】表必备三字段：id, gmt_create, gmt_modified。\n说明：其中 id 必为主键，类型为BIGINT UNSIGNED、单表时自增、步长为1。gmt_create, gmt_modified 的类型均为 DATETIME类型，前者现在时表示主动式创建，后者过去分词表示被动式更新 【 推荐 】表的命名最好是遵循 “业务名称_表的作用”。\n正例：alipay_task 、 force_project、 trade_config 【 推荐 】库名与应用名称尽量一致。\n【参考】合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。\n正例：无符号值可以避免误存负数，且扩大了表示范围。 扩展2：操作注意要求 表删除操作将把表的定义和表中的数据一起删除，并且MySQL在执行删除操作时，不会有任何的确认信息提示，因此执行删除操时应当慎重。在删除表前，最好对表中的数据进行 备份，这样当操作失误时可以对数据进行恢复，以免造成无法挽回的后果。 同样的，在使用 ALTER TABLE 进行表的基本修改操作时，在执行操作过程之前，也应该确保对数据进行完整的备份，因为数据库的改变是无法撤销的，如果添加了一个不需要的字段，可以将其删除；相同的，如果删除了一个需要的列，该列下面的所有数据都将会丢失。 扩展3：MySQL8新特性—DDL的原子化 在MySQL 8.0版本中，InnoDB表的DDL支持事务完整性，即DDL操作要么成功要么回滚。DDL操作回滚日志写入到data dictionary数据字典表mysql.innodb_ddl_log（该表是隐藏的表，通过show tables无法看到）中，用于回滚操作。通过设置参数，可将DDL操作日志打印输出到MySQL错误日志中。\n第11章_数据处理之增删改 1. 插入数据 1) 方式1：VALUES的方式添加 使用这种语法一次只能向表中插入一条数据。\n情况1：为表的所有字段按默认顺序插入数据\n1 2 INSERT INTO 表名 VALUES (value1,value2,....); 值列表中需要为表的每一个字段指定值，并且值的顺序必须和数据表中字段定义时的顺序相同。\n举例：\n1 2 INSERT INTO departments VALUES (70, \u0026#39;Pub\u0026#39;, 100, 1700); 情况2: 指定字段名插入数据\n为表的指定字段插入数据，就是在INSERT语句中只向部分字段中插入值，而其他字段的值为表定义时的默认值或null。 在 INSERT 子句中随意列出列名，但是一旦列出，VALUES中要插入的value1,....valuen需要与 column1,...columnn列一一对应。如果类型不同，将无法插入，并且MySQL会产生错误。\n举例：\n1 2 INSERT INTO departments(department_id, department_name) VALUES (80, \u0026#39;IT\u0026#39;); 情况3：同时插入多条记录\nINSERT语句可以同时向数据表中插入多条记录，插入时指定多个值列表，每个值列表之间用逗号分隔开，基本语法格式如下：\n1 2 3 4 5 6 INSERT INTO table_name VALUES (value1 [,value2, …, valuen]), (value1 [,value2, …, valuen]), …… (value1 [,value2, …, valuen]); 或者\n1 2 3 4 5 6 INSERT INTO table_name(column1 [, column2, …, columnn]) VALUES (value1 [,value2, …, valuen]), (value1 [,value2, …, valuen]), …… (value1 [,value2, …, valuen]); 使用INSERT同时插入多条记录时，MySQL会返回一些在执行单行插入时没有的额外信息，这些信息的含义如下：\nRecords：表明插入的记录条数。 Duplicates：表明插入时被忽略的记录，原因可能是这些记录包含了重复的主键值。 Warnings：表明有问题的数据值，例如发生数据类型转换。 一个同时插入多行记录的INSERT语句等同于多个单行插入的INSERT语句，但是多行的INSERT语句在处理过程中效率更高 。因为MySQL执行单条INSERT语句插入多行数据比使用多条INSERT语句快，所以在插入多条记录时最好选择使用单条INSERT语句的方式插入.\n2) 方式2：将查询结果插入到表中 INSERT还可以将SELECT语句查询的结果插入到表中，此时不需要把每一条记录的值一个一个输入，只需要使用一条INSERT语句和一条SELECT语句组成的组合语句即可快速地从一个或多个表中向一个表中插入多行\n1 2 3 4 5 6 INSET INTO 目标表名 (tar_column1 [, tar_column2, ..., tar_columnn]) SELECT (src_column1 [, src_column2, …, src_columnn]) FROM 源表名 [WHERE condition] 在 INSERT 语句中加入子查询。 不必书写 VALUES 子句。 子查询中的值列表应与 INSERT 子句中的列名对应。 1 2 3 4 INSERT INTO emp2 SELECT * FROM employees WHERE department_id = 90; 1 2 3 4 INSERT INTO sales_reps(id, name, salary, commission_pct) SELECT employee_id, last_name, salary, commission_pct FROM employees WHERE job_id LIKE \u0026#39;%REP%\u0026#39;; 2. 更新数据 使用 UPDATE 语句更新数据。语法如下： 1 2 3 UPDATE table_name SET column1=value1, column2=value2, ..., column=valuen [WHERE condition] 可以一次更新多条数据。\n如果需要回滚数据，需要保证在DML前，进行设置：SET AUTOCOMMIT = FALSE;\n使用 WHERE 子句指定需要更新的数据。\n1 2 3 UPDATE employees SET department_id = 70 WHERE employee_id = 113; 如果省略 WHERE 子句，则表中的所有数据都将被更新。 3. 删除数据 1 DELETE FROM table_name [WHERE \u0026lt;condition\u0026gt;]; table_name指定要执行删除操作的表；“[WHERE ]”为可选参数，指定删除条件，如果没有WHERE子句， DELETE语句将删除表中的所有记录。\n4. MySQL8新特性：计算列 什么叫计算列呢？简单来说就是某一列的值是通过别的列计算得来的。例如，a列值为1、b列值为2，c列 不需要手动插入，定义a+b的结果为c的值，那么c就是计算列，是通过别的列计算得来的。\n在MySQL 8.0中，CREATE TABLE 和 ALTER TABLE 中都支持增加计算列。下面以CREATE TABLE为例进行讲解。\n举例：定义数据表tb1，然后定义字段id、字段a、字段b和字段c，其中字段c为计算列，用于计算a+b的 值。 首先创建测试表tb1，语句如下：\n1 2 3 4 5 6 CREATE TABLE tb1( id INT, a INT, b INT, c INT GENERATED ALWAYS AS (a + b) VIRTUAL ); 第12章_MySQL数据类型精讲 1. MySQL中的数据类型 类型 举例 整数类型 TINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT 浮点类型 FLOAT、DOUBLE 定点数类型 DECIMAL 位类型 BIT 日期时间类型 YEAR、TIME、DATE、DATETIME、TIMESTAMP 文本字符串类型 CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT 枚举类型 ENUM 集合类型 SET 二进制字符串类型 BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB JSON类型 JSON对象、JSON数组 空间数据类型 单值类型：GEOMETRY、POINT、LINESTRING、POLYGON； 集合类型：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、 GEOMETRYCOLLECTION 常见数据类型的属性，如下：\nMySQL关键字 含义 NULL 空 NOT NULL 非空 DEFAULT 默认值 PRIMARY KEY 主键约束 AUTO_INCREMENT 自增约束 UNSIGNED 无符号 CHARACTER SET name 设置字符集 2. 整数类型 1) 类型介绍 整数类型一共有 5 种，包括 TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）和 BIGINT。\n它们的区别如下表所示：\n整数类型 字节 有符号数取值范围 无符号数取值范围 TINYINT 1 -128~127 0~255 SMALLINT 2 -32768~32767 0~65535 MEDIUMINT 3 -8388608~8388607 0~16777215 INT、INTEGER 4 -2147483648~2147483647 0~4294967295 BIGINT 8 -9223372036854775808~9223372036854775807 0~18446744073709551615 2) 可选属性 整数类型的可选属性有三个：\nM M : 表示显示宽度，M的取值范围是(0, 255)。例如，int(5)：当数据宽度小于5位的时候在数字前面需要用\u0026rsquo; \u0026lsquo;字符填满宽度。该项功能需要配合“ ZEROFILL ”使用，表示用“0”填满宽度，否则指定显示宽度无效。如果设置了显示宽度，那么插入的数据宽度超过显示宽度限制，会不会截断或插入失败？\n答案：不会对插入的数据有任何影响，还是按照类型的实际宽度进行保存，即显示宽度与类型可以存储的值范围无关 。从MySQL 8.0.17开始，整数数据类型不推荐使用显示宽度属性。 整型数据类型可以在定义表结构时指定所需要的显示宽度，如果不指定，则系统为每一种类型指定默认的宽度值。\n举例：\n1 CREATE TABLE test_int1 ( x TINYINT, y SMALLINT, z MEDIUMINT, m INT, n BIGINT ); 查看表结构 （MySQL5.7中显式如下，MySQL8中不再显式范围）\n1 2 3 4 5 6 7 8 9 10 11 mysql\u0026gt; desc test_int1; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | x | tinyint(4) | YES | | NULL | | | y | smallint(6) | YES | | NULL | | | z | mediumint(9) | YES | | NULL | | | m | int(11) | YES | | NULL | | | n | bigint(20) | YES | | NULL | | +-------+--------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) TINYINT有符号数和无符号数的取值范围分别为-128~127和0~255，由于负号占了一个数字位，因此TINYINT默认的显示宽度为4。同理，其他整数类型的默认显示宽度与其有符号数的最小值的宽度相同。\nUNSIGNED UNSIGNED : 无符号类型（非负），所有的整数类型都有一个可选的属性UNSIGNED（无符号属性），无符号整数类型的最小取值为0。所以，如果需要在MySQL数据库中保存非负整数值时，可以将整数类型设置为无符号类型。 int类型默认显示宽度为int(11)，无符号int类型默认显示宽度为int(10)。\nZEROFILL ZEROFILL : 0填充,（如果某列是ZEROFILL，那么MySQL会自动为当前列添加UNSIGNED属性），如果指定了ZEROFILL只是表示不够M位时，用0在左边填充，如果超过M位，只要不超过数据存储范围即可。\n原来，在 int(M) 中，M 的值跟 int(M) 所占多少存储空间并无任何关系。 **int(3)、int(4)、int(8) 在磁盘上都 是占用 4 bytes 的存储空间。**也就是说，int(M)，必须和UNSIGNED、 ZEROFILL一起使用才有意义。如果整数值超过M位，就按照实际位数存储。只是无须再用字符 0 进行填充。\n3) 适用场景 TINYINT ：一般用于枚举数据，比如系统设定取值范围很小且固定的场景。\nSMALLINT ：可以用于较小范围的统计数据，比如统计工厂的固定资产库存数量等。\nMEDIUMINT ：用于较大整数的计算，比如车站每日的客流量等。\nINT、INTEGER ：取值范围足够大，一般情况下不用考虑超限问题，用得最多。比如商品编号。\nBIGINT ：只有当你处理特别巨大的整数时才会用到。比如双十一的交易量、大型门户网站点击量、证券公司衍生产品持仓等。\n4) 如何选择？ 在评估用哪种整数类型的时候，你需要考虑 存储空间 和 可靠性 的平衡问题：一方面，用占用字节数少的整数类型可以节省存储空间；另一方面，要是为了节省存储空间， 使用的整数类型取值范围太小，一旦遇到超出取值范围的情况，就可能引起系统错误，影响可靠性。\n举个例子，商品编号采用的数据类型是 INT。原因就在于，客户门店中流通的商品种类较多，而且，每天都有旧商品下架，新商品上架，这样不断迭代，日积月累。\n如果使用 SMALLINT 类型，虽然占用字节数比 INT 类型的整数少，但是却不能保证数据不会超出范围 65535。相反，使用 INT，就能确保有足够大的取值范围，不用担心数据超出范围影响可靠性的问题。\n你要注意的是，在实际工作中，系统故障产生的成本远远超过增加几个字段存储空间所产生的成本。因此，我建议你首先确保数据不会超过取值范围，在这个前提之下，再去考虑如何节省存储空间。\n3. 浮点类型 1) 类型介绍 浮点数和定点数类型的特点是可以 处理小数 ，你可以把整数看成小数的一个特例。因此，浮点数和定点数的使用场景，比整数大多了。 MySQL支持的浮点数类型，分别是FLOAT、DOUBLE、REAL。\nFLOAT 表示单精度浮点数；\nDOUBLE 表示双精度浮点数；\nREAL默认就是 DOUBLE。如果你把 SQL 模式设定为启用“ REAL_AS_FLOAT ”，那 么，MySQL 就认为REAL 是 FLOAT。如果要启用“REAL_AS_FLOAT”，可以通过以下 SQL 语句实现：\n1 SET sql_mode = “REAL_AS_FLOAT”; 问题：为什么浮点数类型的无符号数取值范围，只相当于有符号数取值范围的一半，也就是只相当于有符号数取值范围大于等于零的部分呢？\nMySQL 存储浮点数的格式为： 符号(S) 、 尾数(M) 和 阶码(E) 。因此，无论有没有符号，MySQL 的浮点数都会存储表示符号的部分。因此，所谓的无符号数取值范围，其实就是有符号数取值范围大于等于零的部分。\n2) 数据精度说明 对于浮点类型，在MySQL中单精度值使用 4 个字节，双精度值使用 8 个字节。\nMySQL允许使用非标准语法 （其他数据库未必支持，因此如果涉及到数据迁移，则最好不要这么用）： FLOAT(M,D) 或 DOUBLE(M,D) 。这里，M称为 精度 ，D称为 标度 。(M,D)中 M=整数位+小数位，D=小数位。 D\u0026lt;=M\u0026lt;=255，0\u0026lt;=D\u0026lt;=30。\n例如，定义为FLOAT(5,2)的一个列可以显示为-999.99-999.99。如果超过这个范围会报错。\nFLOAT和DOUBLE类型在不指定(M,D)时，默认会按照实际的精度（由实际的硬件和操作系统决定） 来显示。\n说明：浮点类型，也可以加 UNSIGNED ，但是不会改变数据范围，例如：FLOAT(3,2) UNSIGNED仍然 只能表示0-9.99的范围。\n不管是否显式设置了精度(M,D)，这里MySQL的处理方案如下：\n如果存储时，整数部分超出了范围，MySQL就会报错，不允许存这样的值 如果存储时，小数点部分若超出范围，就分以下情况： 若四舍五入后，整数部分没有超出范围，则只警告，但能成功操作并四舍五入删除多余的小数位后保存。例如在FLOAT(5,2)列内插入999.009，近似结果是999.01。 若四舍五入后，整数部分超出范围，则MySQL报错，并拒绝处理。如FLOAT(5,2)列内插入 999.995和-999.995都会报错。 从MySQL 8.0.17开始，FLOAT(M,D) 和DOUBLE(M,D)用法在官方文档中已经明确不推荐使用，将来可能被移除。另外，关于浮点型FLOAT和DOUBLE的UNSIGNED也不推荐使用了，将来也可能被移除。\n3) 精度误差说明 浮点数类型有个缺陷，就是不精准。下面我来重点解释一下为什么 MySQL 的浮点数不够精准。比如，我们设计一个表，有f1这个字段，插入值分别为0.47,0.44,0.19，我们期待的运行结果是：0.47 + 0.44 + 0.19 = 1.1。而使用sum之后查询：\n1 2 3 4 5 CREATE TABLE test_double2( f1 DOUBLE ); INSERT INTO test_double2 VALUES(0.47),(0.44),(0.19); 1 2 3 4 5 6 7 8 mysql\u0026gt; SELECT SUM(f1) -\u0026gt; FROM test_double2; +--------------------+ | SUM(f1) | +--------------------+ | 1.0999999999999999 | +--------------------+ 1 row in set (0.00 sec) 查询结果是 1.0999999999999999。看到了吗？虽然误差很小，但确实有误差。你也可以尝试把数据类型 改成 FLOAT，然后运行求和查询，得到的是， 1.0999999940395355。显然，误差更大了。\n那么，为什么会存在这样的误差呢？问题还是出在 MySQL 对浮点类型数据的存储方式上。\nMySQL 用 4 个字节存储 FLOAT 类型数据，用 8 个字节来存储 DOUBLE 类型数据。无论哪个，都是采用二进制的方式来进行存储的。比如 9.625，用二进制来表达，就是 1001.101，或者表达成 1.001101×2^3。如 果尾数不是 0 或 5（比如 9.624），你就无法用一个二进制数来精确表达。进而，就只好在取值允许的范围内进行四舍五入。\n在编程中，如果用到浮点数，要特别注意误差问题，因为浮点数是不准确的，所以我们要避免使用“=”来 判断两个数是否相等。同时，在一些对精确度要求较高的项目中，千万不要使用浮点数，不然会导致结果错误，甚至是造成不可挽回的损失。那么，MySQL 有没有精准的数据类型呢？当然有，这就是定点数 类型：DECIMAL 。\n4. 定点数类型 1) 类型介绍 MySQL中的定点数类型只有DECIMAL一种类型。 类型 字节 有符号数取值范围 DECIMAL(M,D),DEC,NUMERIC M+2字节 有效范围由M和D决定 使用 DECIMAL(M,D) 的方式表示高精度小数。其中，M被称为精度，D被称为标度。0\u0026lt;=M\u0026lt;=65， 0\u0026lt;=D\u0026lt;=30，D\nDECIMAL(M,D)的最大取值范围与DOUBLE类型一样，但是有效的数据范围是由M和D决定的。 DECIMAL 的存储空间并不是固定的，由精度值M决定，总共占用的存储空间为M+2个字节。也就是说，在一些对精度要求不高的场景下，比起占用同样字节长度的定点数，浮点数表达的数值范围可以更大一些。 定点数在MySQL内部是以字符串的形式进行存储，这就决定了它一定是精准的。 当DECIMAL类型不指定精度和标度时，其默认为DECIMAL(10,0)。当数据的精度超出了定点数类型的精度范围时，则MySQL同样会进行四舍五入处理。 浮点数 vs 定点数 浮点数相对于定点数的优点是在长度一定的情况下，浮点类型取值范围大，但是不精准，适用于需要取值范围大，又可以容忍微小误差的科学计算场景（比如计算化学、分子建模、流体动 力学等） 定点数类型取值范围相对小，但是精准，没有误差，适合于对精度要求极高的场景（比如涉及金额计算的场景） 2) 开发中的经验 “由于DECIMAL数据类型的精准性，在我们的项目中，除了极少数（比如商品编号）用到整数类型外，其他的数值都用的是 DECIMAL，原因就是这个项目所处的零售行业，要求精准，一分钱也不能差。 ” ——来自某项目经理\n5. 位类型：BIT BIT类型中存储的是二进制值，类似010110。\n二进制字符串类型 长度 长度范围 占用空间 BIT(M) M 1 \u0026lt;= M \u0026lt;= 64 约为(M + 7)/8个字节 BIT类型，如果没有指定(M)，默认是1位。这个1位，表示只能存1位的二进制值。这里(M)是表示二进制的位数，位数最小值为1，最大值为64。\n6. 日期与时间类型 日期与时间是重要的信息，在我们的系统中，几乎所有的数据表都用得到。原因是客户需要知道数据的 时间标签，从而进行数据查询、统计和处理。\nMySQL有多种表示日期和时间的数据类型，不同的版本可能有所差异，MySQL8.0版本支持的日期和时间 类型主要有：YEAR类型、TIME类型、DATE类型、DATETIME类型和TIMESTAMP类型。\nYEAR 类型通常用来表示年 DATE 类型通常用来表示年、月、日 TIME 类型通常用来表示时、分、秒 DATETIME 类型通常用来表示年、月、日、时、分、秒 TIMESTAMP 类型通常用来表示带时区的年、月、日、时、分、秒 类型 名称 字节 日期格式 最小值 最大值 YEAR 年 1 YYYY或YY 1901 2155 TIME 时间 3 HH:MM:SS -838:59:59 838:59:59 DATE 日期 3 YYYY-MM-DD 1000-01-01 9999-12-03 DATETIME 日期时间 8 YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00 9999-12-31 23:59:59 TIMESTAMP 日期时间 4 YYYY-MM-DD HH:MM:SS 1970-01-01 00:00:00 UTC 2038-01-19 03:14:07UTC 可以看到，不同数据类型表示的时间内容不同、取值范围不同，而且占用的字节数也不一样，你要根据 实际需要灵活选取。\n为什么时间类型 TIME 的取值范围不是 -23:59:59～23:59:59 呢？原因是 MySQL 设计的 TIME 类型，不光表示一天之内的时间，而且可以用来表示一个时间间隔，这个时间间隔可以超过 24 小时。\n7. 文本字符串类型 MySQL中，文本字符串总体上分为 CHAR 、 VARCHAR 、 TINYTEXT 、 TEXT 、 MEDIUMTEXT 、 LONGTEXT 、 ENUM 、 SET 等类型。\n8. ENUM类型 ENUM类型也叫作枚举类型，ENUM类型的取值范围需要在定义字段时进行指定。设置字段值时，ENUM 类型只允许从成员中选取单个值，不能一次选取多个值。 其所需要的存储空间由定义ENUM类型时指定的成员个数决定。\n文本字符串类型 长度 长度范围 占用的存储空间 ENUM L 1 \u0026lt;= L \u0026lt;= 65535 1或2个字节 当ENUM类型包含1～255个成员时，需要1个字节的存储空间； 当ENUM类型包含256～65535个成员时，需要2个字节的存储空间。 ENUM类型的成员个数的上限为65535个。 9. SET类型 当SET类型包含的成员个数不同时，其所占用的存储空间也是不同的，具体如下：\n成员个数范围（L表示实际成员个数） 占用的存储空间 1 \u0026lt;= L \u0026lt;= 8 1个字节 9 \u0026lt;= L \u0026lt;= 16 2个字节 17 \u0026lt;= L \u0026lt;= 24 3个字节 25 \u0026lt;= L \u0026lt;= 32 4个字节 33 \u0026lt;= L \u0026lt;= 64 8个字节 SET类型在存储数据时成员个数越多，其占用的存储空间越大。注意：SET类型在选取成员时，可以一次选择多个成员，这一点与ENUM类型不同。\n13. 小结及选择建议 在定义数据类型时，如果确定是整数，就用 INT ； 如果是小数 ，一定用定点数类型 DECIMAL(M,D) ； 如果是日期与时间，就用 DATETIME 。 这样做的好处是，首先确保你的系统不会因为数据类型定义出错。不过，凡事都是有两面的，可靠性好，并不意味着高效。比如，TEXT 虽然使用方便，但是效率不如 CHAR(M) 和 VARCHAR(M)。\n阿里巴巴《Java开发手册》之MySQL数据库：\n任何字段如果为非负数，必须是 UNSIGNED\n【 强制 】小数类型为 DECIMAL，禁止使用 FLOAT 和 DOUBLE。\n说明：在存储的时候，FLOAT 和 DOUBLE 都存在精度损失的问题，很可能在比较值的时候，得到不正确的结果。如果存储的数据范围超过 DECIMAL 的范围，建议将数据拆成整数和小数并分开存储。\n【 强制 】如果存储的字符串长度几乎相等，使用 CHAR 定长字符串类型。\n【 强制 】VARCHAR 是可变长字符串，不预先分配存储空间，长度不要超过 5000。如果存储长度大于此值，定义字段类型为 TEXT，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\n第13章_约束 1. 约束的分类 根据约束数据列的限制，约束可分为： 单列约束：每个约束只约束一列 多列约束：每个约束可约束多列数据 根据约束的作用范围，约束可分为： 列级约束：只能作用在一个列上，跟在列的定义后面 表级约束：可以作用在多个列上，不与列一起，而是单独定义 根据约束起的作用，约束可分为： NOT NULL 非空约束，规定某个字段不能为空 UNIQUE 唯一约束，规定某个字段在整个表中是唯一的 PRIMARY KEY 主键(非空且唯一)约束 FOREIGN KEY 外键约束 CHECK 检查约束 DEFAULT 默认值约束 注意： MySQL不支持check约束，但可以使用check约束，而没有任何效果\n如何添加/删除约束？ CREATE TABLE时添加约束\nALTER TABLE时增加约束、删除约束\n查看某个表已有的约束 1 2 3 4 #information_schema数据库名（系统库） #table_constraints表名称（专门存储各个表的约束） SELECT * FROM information_schema.table_constraints WHERE table_name = \u0026#39;表名称\u0026#39;; 2. 非空约束 1) 作用 限定某个字段/ 某列的值不允许为空\n2) 关键字 NOT NULL\n3) 特点 默认，所有的类型的值都可以是NULL，包括INT、FLOAT等数据类型 非空约束只能出现在表对象的列上，只能某个列单独限定非空，不能组合非空 一个表可以有很多列都分别限定了非空 空字符串''不等于NULL，0也不等于NULL 4) 添加非空约束 1. 建表时\n1 2 3 4 5 CREATE TABLE 表名称( 字段名 数据类型, 字段名 数据类型 NOT NULL, 字段名 数据类型 NOT NULL ); 2. 建表后\n1 alter table 表名称 modify 字段名 数据类型 not null; 5) 删除非空约束 1 2 3 alter table 表名称 modify 字段名 数据类型 NULL;#去掉not null，相当于修改某个非注解字段，该字段允许为空 或 alter table 表名称 modify 字段名 数据类型;#去掉not null，相当于修改某个非注解字段，该字段允许为空 3. 唯一性约束 1) 作用 用来限制某个字段/某列的值不能重复。\n2) 关键字 UNIQUE\n3) 特点 同一个表可以有多个唯一约束。 唯一约束可以是某一个列的值唯一，也可以多个列组合的值唯一。 唯一性约束允许列值为空。 在创建唯一约束的时候，如果不给唯一约束命名，就默认和列名相同。 MySQL会给唯一约束的列上默认创建一个唯一索引。 4) 添加唯一约束 1. 建表时\n1 2 3 4 5 6 7 8 9 10 11 12 13 create table 表名称( 字段名 数据类型, 字段名 数据类型 unique, 字段名 数据类型 unique key, 字段名 数据类型 ); create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] unique key(字段名) ); 举例：\n1 2 3 4 5 6 7 CREATE TABLE USER( id INT NOT NULL, NAME VARCHAR(25), PASSWORD VARCHAR(16), -- 使用表级约束语法 CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD) ); 表示用户名和密码组合不能重复\n2. 建表后指定唯一键约束\n1 2 3 4 5 6 #字段列表中如果是一个字段，表示该列的值唯一。如果是两个或更多个字段，那么复合唯一，即多个字段的组合是唯 一的 #方式1： alter table 表名称 add unique key(字段列表); #方式2： alter table 表名称 modify 字段名 字段类型 unique; 5) 关于复合唯一约束 1 2 3 4 5 6 create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, unique key(字段列表) #字段列表中写的是多个字段名，多个字段名用逗号分隔，表示那么是复合唯一，即多个字段的组合是唯一的 ); 6) 删除唯一约束 添加唯一性约束的列上也会自动创建唯一索引。 删除唯一约束只能通过删除唯一索引的方式删除。 删除时需要指定唯一索引名，唯一索引名就和唯一约束名一样。 如果创建唯一约束时未指定名称，如果是单列，就默认和列名相同； 如果是组合列，那么默认和()中排在第一个的列名相同。也可以自定义唯一性约束名。 1 SELECT * FROM information_schema.table_constraints WHERE table_name = \u0026#39;表名\u0026#39;; #查看都有哪些约束 1 2 ALTER TABLE `USER` DROP INDEX uk_name_pwd; 注意：可以通过 show index from 表名称; 查看表的索引\n4. PRIMARY KEY 约束 1) 作用 用来唯一标识表中的一行记录。\n2) 关键字 primary key\n3) 特点 主键约束相当于 [唯一约束+非空约束] 的组合，主键约束列不允许重复，也不允许出现空值。\n一个表最多只能有一个主键约束，建立主键约束可以在列级别创建，也可以在表级别上创建。 主键约束对应着表中的一列或者多列（复合主键） 如果是多列组合的复合主键约束，那么这些列都不允许为空值，并且组合的值不允许重复。 MySQL的主键名总是PRIMARY，就算自己命名了主键约束名也没用。 当创建主键约束时，系统默认会在所在的列或列组合上建立对应的主键索引（能够根据主键查询的，就根据主键查询，效率更高。如果删除主键约束了，主键约束对应的索引就自动删除了。 需要注意的一点是，不要修改主键字段的值。因为主键是数据记录的唯一标识，如果修改了主键的值，就有可能会破坏数据的完整性。 4) 添加主键约束 1. 建表时指定主键约束\n1 2 3 4 5 6 7 8 9 10 11 12 create table 表名称( 字段名 数据类型 primary key, #列级模式 字段名 数据类型, 字段名 数据类型 ); create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] primary key(字段名) #表级模式 ); 2. 建表后增加主键约束\n1 ALTER TABLE 表名称 ADD PRIMARY KEY(字段列表); #字段列表可以是一个字段，也可以是多个字段，如果是多个字段的话，是复合主键 5) 关于复合主键 1 2 3 4 5 6 create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, primary key(字段名1,字段名2) #表示字段1和字段2的组合是唯一的，也可以有更多个字段 ); 6) 删除主键约束 1 alter table 表名称 drop primary key 说明：删除主键约束，不需要指定主键名，因为一个表只有一个主键，删除主键约束后，非空还存在。\n5. 自增列：AUTO_INCREMENT 1) 作用 某个字段的值自增\n2) 关键字 auto_increment\n3) 特点 （1）一个表最多只能有一个自增长列\n（2）当需要产生唯一标识符或顺序值时，可设置自增长\n（3）自增长列约束的列必须是键列（主键列，唯一键列）\n（4）自增约束的列的数据类型必须是整数类型\n（5）如果自增列指定了 0 和 null，会在当前最大值的基础上自增；如果自增列手动指定了具体值，直接赋值为具体值。\n4) 如何指定自增约束 1. 建表时\n1 2 3 4 5 6 7 8 9 10 11 12 create table 表名称( 字段名 数据类型 primary key auto_increment, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值, ); create table 表名称( 字段名 数据类型 default 默认值 , 字段名 数据类型 unique key auto_increment,# 字段类型必须为整数型 字段名 数据类型 not null default 默认值, primary key(字段名) ); 2. 建表后\n1 alter table 表名称 modify 字段名 数据类型 auto_increment; 5) 删除自增约束 1 2 #alter table 表名称 modify 字段名 数据类型 auto_increment;#给这个字段增加自增约束 alter table 表名称 modify 字段名 数据类型; #去掉auto_increment相当于删除 6) MySQL 8.0新特性—自增变量的持久化 在MySQL 8.0之前，自增主键AUTO_INCREMENT的值如果大于max(primary key)+1，在MySQL重启后，会重置AUTO_INCREMENT=max(primary key)+1，这种现象在某些情况下会导致业务主键冲突或者其他难以发现的问题。下面通过案例来对比不同的版本中自增变量是否持久化。 在MySQL 5.7版本中，测试步骤如下：创建的数据表中包含自增主键的id字段，语句如下：\n1 2 3 CREATE TABLE test1( id INT PRIMARY KEY AUTO_INCREMENT, ); 在MySQL 5.7系统中，对于自增主键的分配规则，是由InnoDB数据字典内部一个计数器来决定的，而该计数器只在内存中维护，并不会持久化到磁盘中。当数据库重启时，该计数器会被初始化。\n在MySQL 8.0将自增主键的计数器持久化到重做日志中。每次计数器发生改变，都会将其写入重做日志中。如果数据库重启，InnoDB会根据重做日志中的信息来初始化计数器的内存值。\n6. FOREIGN KEY 约束 1) 作用 限定某个表的某个字段的引用完整性。\n2) 关键字 FOREIGN KEY\n3) 主表和从表/父表和子表 主表（父表）：被引用的表，被参考的表\n从表（子表）：引用别人的表，参考别人的表\n4) 特点 （1）从表的外键列，必须引用/参考主表的主键或唯一约束的列为什么？因为被依赖/被参考的值必须是唯一的\n（2）在创建外键约束时，如果不给外键约束命名，默认名不是列名，而是自动产生一个外键名（例如 student_ibfk_1;），也可以指定外键约束名。\n（3）创建(CREATE)表时就指定外键约束的话，先创建主表，再创建从表\n（4）删表时，先删从表（或先删除外键约束），再删除主表\n（5）当主表的记录被从表参照时，主表的记录将不允许删除，如果要删除数据，需要先删除从表中依赖该记录的数据，然后才可以删除主表的数据\n（6）在“从表”中指定外键约束，并且一个表可以建立多个外键约束\n（7）从表的外键列与主表被参照的列名字可以不相同，但是数据类型必须一样，逻辑意义一致。如果类型不一样，创建子表时，就会出现错误“ERROR 1005 (HY000): Can't create table'database.tablename'(errno: 150)”。 例如：都是表示部门编号，都是int类型。\n（8）当创建外键约束时，系统默认会在所在的列上建立对应的普通索引。但是索引名是外键的约束名。（根据外键查询效率很高）\n（9）删除外键约束后，必须手动删除对应的索引\n5) 添加外键约束 1. 建表时\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 create table 主表名称( 字段1 数据类型 primary key, 字段2 数据类型 ); create table 从表名称( 字段1 数据类型 primary key, 字段2 数据类型, [CONSTRAINT \u0026lt;外键约束名称\u0026gt;] FOREIGN KEY（从表的某个字段) references 主表名(被参考字段) ); #(从表的某个字段)的数据类型必须与主表名(被参考字段)的数据类型一致，逻辑意义也一样 #(从表的某个字段)的字段名可以与主表名(被参考字段)的字段名一样，也可以不一样 -- FOREIGN KEY: 在表级指定子表中的列 -- REFERENCES: 标示在父表中的列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 create table dept( #主表 did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp(#从表 eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) #在从表中指定外键约束 #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号 ); 说明： （1）主表dept必须先创建成功，然后才能创建emp表，指定外键成功。 （2）删除表时，先删除从表emp，再删除主表dept 2. 建表后\n一般情况下，表与表的关联都是提前设计好了的，因此，会在创建表的时候就把外键约束定义好。不过，如果需要修改表的设计（比如添加新的字段，增加新的关联关系），但没有预先定义外键约束，那么，就要用修改表的方式来补充定义。\n格式：\n1 ALTER TABLE 从表名 ADD [CONSTRAINT 约束名] FOREIGN KEY (从表的字段) REFERENCES 主表名(被引用字段) [on update xx][on delete xx]; 举例：\n1 2 ALTER TABLE emp1 ADD [CONSTRAINT emp_dept_id_fk] FOREIGN KEY(dept_id) REFERENCES dept(dept_id); 6) 约束等级 Cascade方式 ：在父表上update/delete记录时，同步update/delete掉子表的匹配记录 Set null方式 ：在父表上update/delete记录时，将子表上匹配记录的列设为null，但是要注意子表的外键列不能为not null No action方式 ：如果子表中有匹配的记录，则不允许对父表对应候选键进行update/delete操作 Restrict方式 ：同no action，都是立即检查外键约束 Set default方式 （在可视化工具SQLyog中可能显示空白）：父表有变更时，子表将外键列设置成一个默认的值，但Innodb不能识别x 如果没有指定等级，就相当于Restrict方式。 对于外键约束，最好是采用: ON UPDATE CASCADE; ON DELETE RESTRICT 的方式。\n7) 删除外键约束 流程如下：\n1 2 3 4 5 6 7 (1)第一步先查看约束名和删除外键约束 SELECT * FROM information_schema.table_constraints WHERE table_name = \u0026#39;表名称\u0026#39;; #查看某个表的约束名 ALTER TABLE 从表名 DROP FOREIGN KEY 外键约束名; (2)第二步查看索引名和删除索引。（注意，只能手动删除） SHOW INDEX FROM 表名称; #查看某个表的索引名 ALTER TABLE 从表名 DROP INDEX 索引名; 8) 开发场景 问题1：如果两个表之间有关系（一对一、一对多），比如：员工表和部门表（一对多），它们之间是否一定要建外键约束？\n答：不是的\n问题2：建和不建外键约束有什么区别？\n答：建外键约束，你的操作（创建表、删除表、添加、修改、删除）会受到限制，从语法层面受到限制。例如：在员工表中不可能添加一个员工信息，它的部门的值在部门表中找不到。\n不建外键约束，你的操作（创建表、删除表、添加、修改、删除）不受限制，要保证数据的引用完整性 ，只能依靠程序员的自觉 ，或者是在Java程序中进行限定。例如：在员工表中，可以添加一个员工的信息，它的部门指定为一个完全不存在的部门。\n问题3：那么建和不建外键约束和查询有没有关系？\n答：没有\n在 MySQL 里，外键约束是有成本的，需要消耗系统资源。对于大并发的 SQL 操作，有可能会不适合。比如大型网站的中央数据库，可能会因为外键约束的系统开销而变得非常慢。所以， MySQL 允许你不使用系统自带的外键约束，在应用层面完成检查数据一致性的逻辑。也就是说，即使你不用外键约束，也要想办法通过应用层面的附加逻辑，来实现外键约束的功能，确保数据的一致性。\n9) 阿里开发规范 【 强制 】不得使用外键与级联，一切外键概念必须在应用层解决。\n说明：（概念解释）学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群 ；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度 。\n7. CHECK 约束 1) 作用 检查某个字段的值是否符号xx要求，一般指的是值的范围\n2) 关键字 CHECK\n3) 说明 MySQL5.7 可以使用check约束，但check约束对数据验证没有任何作用。添加数据时，没有任何错误或警告\n但是MySQL 8.0中可以使用check约束了。\n1 2 3 4 5 create table employee( eid int primary key, ename varchar(5), gender char check (\u0026#39;男\u0026#39; or \u0026#39;女\u0026#39;) ); 8. DEFAULT约束 1) 作用 给某个字段/某列指定默认值，一旦设置默认值，在插入数据时，如果此字段没有显式赋值，则赋值为默认值。\n2) 关键字 DEFAULT\n3) 添加默认值 1. 建表时\n1 2 3 4 5 6 create table 表名称( 字段名 数据类型 primary key, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值, ); 2. 建表后\n1 2 3 4 alter table 表名称 modify 字段名 数据类型 default 默认值; #如果这个字段原来有非空约束，你还保留非空约束，那么在加默认值约束时，还得保留非空约束，否则非空约束就被删除了 #同理，在给某个字段加非空约束也一样，如果这个字段原来有默认值约束，你想保留，也要在modify语句中保留默认值约束，否则就删除了 alter table 表名称 modify 字段名 数据类型 default 默认值 not null; 删除默认值\n1 2 alter table 表名称 modify 字段名 数据类型; #删除默认值约束，也不保留非空约束 alter table 表名称 modify 字段名 数据类型 not null; #删除默认值约束，保留非空约束 9. 面试 面试1、为什么建表时，加 not null default '' 或 default 0\n答：不想让表中出现null值。\n面试2、为什么不想要 null 的值\n答:\n（1）不好比较。null是一种特殊值，比较时只能用专门的is null 和 is not null来比较。碰到运算符，通常返回null。\n（2）效率不高。影响提高索引效果。因此，我们往往在建表时 not null default '' 或 default 0\n面试3、带AUTO_INCREMENT约束的字段值是从1开始的吗？\n在MySQL中，默认AUTO_INCREMENT的初始值是1，每新增一条记录，字段值自动加1。设置自增属性（AUTO_INCREMENT）的时候，还可以指定第一条插入记录的自增字段的值，这样新插入的记录的自增字段值从初始值开始递增，如在表中插入第一条记录，同时指定id值为5，则以后插入的记录的id值就会从6开始往上增加。添加主键约束时，往往需要设置字段自动增加属性。\n面试4、并不是每个表都可以任意选择存储引擎？\n外键约束（FOREIGN KEY）不能跨引擎使用。\nMySQL支持多种存储引擎，每一个表都可以指定一个不同的存储引擎，需要注意的是：外键约束是用来保证数据的参照完整性的，如果表之间需要关联外键，却指定了不同的存储引擎，那么这些表之间是不能创建外键约束的。所以说，存储引擎的选择也不完全是随意的。\n第14章_视图(×) 1. 常见的数据库对象 对象 描述 表(TABLE) 表是存储数据的逻辑单元，以行和列的形式存在，列就是字段，行就是记录 数据字典 就是系统表，存放数据库相关信息的表。系统表的数据通常由数据库系统维护， 程序员通常不应该修改，只可查看 约束 (CONSTRAINT) 执行数据校验的规则，用于保证数据完整性的规则 视图(VIEW) 一个或者多个数据表里的数据的逻辑显示，视图并不存储数据 索引(INDEX) 用于提高查询性能，相当于书的目录 存储过程 (PROCEDURE) 用于完成一次完整的业务处理，没有返回值，但可通过传出参数将多个值传给调用环境 存储函数 (FUNCTION) 用于完成一次特定的计算，具有一个返回值 触发器 (TRIGGER) 相当于一个事件监听器，当数据库发生特定事件后，触发器被触发，完成相应的处理 2. 视图概述 视图是一种虚拟表 ，本身是不具有数据的，占用很少的内存空间，它是 SQL 中的一个重要概念。 视图建立在已有表的基础上, 视图赖以建立的这些表称为基表。 视图的创建和删除只影响视图本身，不影响对应的基表。但是当对视图中的数据进行增加、删除和修改操作时，数据表中的数据会相应地发生变化，反之亦然。 视图提供数据内容的语句为 SELECT 语句, 可以将视图理解为存储起来的 SELECT 语句 在数据库中，视图不会保存数据，数据真正保存在数据表中。当对视图中的数据进行增加、删 除和修改操作时，数据表中的数据会相应地发生变化；反之亦然。 视图，是向用户提供基表数据的另一种表现形式。通常情况下，小型项目的数据库可以不使用视 图，但是在大型项目中，以及数据表比较复杂的情况下，视图的价值就凸显出来了，它可以帮助我 们把经常查询的结果集放到虚拟表中，提升使用效率。理解和使用起来都非常方便。 3. 创建视图 在 CREATE VIEW 语句中嵌入子查询 1 2 3 4 5 CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW 视图名称 [(字段列表)] AS 查询语句 [WITH [CASCADED|LOCAL] CHECK OPTION] 精简版 1 2 CREATE VIEW 视图名称 AS 查询语句 1) 创建单表视图 举例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 方式一： CREATE VIEW empvu80 AS SELECT employee_id, last_name, salary FROM employees WHERE department_id = 80; # 方式二： CREATE VIEW empsalary8000(emp_id, NAME, monthly_sal) # 小括号内字段个数与SELECT中字段个数相同 AS SELECT employee_id, last_name, salary FROM employees WHERE salary \u0026gt; 8000; 查询视图：\n1 2 SELECT * FROM salvu80; 2) 创建多表联合视图 举例：\n1 2 3 4 5 CREATE VIEW empview AS SELECT employee_id emp_id,last_name NAME,department_name FROM employees e,departments d WHERE e.department_id = d.department_id; 1 2 3 4 5 6 7 CREATE VIEW dept_sum_vu (name, minsal, maxsal, avgsal) AS SELECT d.department_name, MIN(e.salary), MAX(e.salary),AVG(e.salary) FROM employees e, departments d WHERE e.department_id = d.department_id GROUP BY d.department_name; 利用视图对数据进行格式化 常需要输出某个格式的内容，比如我们想输出员工姓名和对应的部门名，对应格式为 emp_name(department_name)，就可以使用视图来完成数据格式化的操作：\n1 2 3 4 5 CREATE VIEW emp_depart AS SELECT CONCAT(last_name,\u0026#39;(\u0026#39;,department_name,\u0026#39;)\u0026#39;) AS emp_dept FROM employees e JOIN departments d WHERE e.department_id = d.department_id; 3) 基于视图创建视图 当我们创建好一张视图之后，还可以在它的基础上继续创建视图。\n举例：联合“emp_dept”视图和“emp_year_salary”视图查询员工姓名、部门名称、年薪信息创建 “emp_dept_ysalary”视图。\n1 2 3 4 5 CREATE VIEW emp_dept_ysalary AS SELECT emp_dept.ename,dname,year_salary FROM emp_dept INNER JOIN emp_year_salary ON emp_dept.ename = emp_year_salary.ename; 4. 查看视图 语法1：查看数据库的表对象、视图对象\n1 SHOW TABLES; 语法2：查看视图的结构\n1 DESC / DESCRIBE 视图名称; 语法3：查看视图的属性信息\n1 2 # 查看视图信息（显示数据表的存储引擎、版本、数据行数和数据大小等） SHOW TABLE STATUS LIKE \u0026#39;视图名称\u0026#39;\\G 执行结果显示，注释Comment为VIEW，说明该表为视图，其他的信息为NULL，说明这是一个虚表。 语法4：查看视图的详细定义信息\n1 SHOW CREATE VIEW 视图名称; 5. 更新视图的数据 1) 一般情况 MySQL支持使用INSERT、UPDATE和DELETE语句对视图中的数据进行插入、更新和删除操作。当视图中的 数据发生变化时，数据表中的数据也会发生变化，反之亦然。\n举例：UPDATE操作\n1 UPDATE emp_tel SET tel = \u0026#39;13789091234\u0026#39; WHERE ename = \u0026#39;孙洪亮\u0026#39;; 举例：DELETE操作\n1 DELETE FROM emp_tel WHERE ename = \u0026#39;孙洪亮\u0026#39;; 2) 不可更新的视图 要使视图可更新，视图中的行和底层基本表中的行之间必须存在 一对一 的关系。另外当视图定义出现如下情况时，视图不支持更新操作：\n在定义视图的时候指定了“ALGORITHM = TEMPTABLE”，视图将不支持INSERT和DELETE操作； 视图中不包含基表中所有被定义为非空又未指定默认值的列，视图将不支持INSERT操作； 在定义视图的SELECT语句中使用了 JOIN联合查询 ，视图将不支持INSERT和DELETE操作； 在定义视图的SELECT语句后的字段列表中使用了 数学表达式 或 子查询 ，视图将不支持INSERT，也 不支持UPDATE使用了数学表达式、子查询的字段值； 在定义视图的SELECT语句后的字段列表中使用 DISTINCT 、 聚合函数 、 GROUP BY 、 HAVING 、 UNION 等，视图将不支持INSERT、UPDATE、DELETE； 在定义视图的SELECT语句中包含了子查询，而子查询中引用了FROM后面的表，视图将不支持 INSERT、UPDATE、DELETE； 视图定义基于一个 不可更新视图 ； 常量视图。 虽然可以更新视图数据，但总的来说，视图作为虚拟表 ，主要用于方便查询 ，不建议更新视图的数据。对视图数据的更改，都是通过对实际数据表里数据的操作来完成的。\n6. 修改、删除视图 1) 修改视图 方式1：使用CREATE OR REPLACE VIEW 子句修改视图\n1 2 3 4 5 6 CREATE OR REPLACE VIEW empvu80 (id_number, name, sal, department_id) AS SELECT employee_id, first_name || \u0026#39; \u0026#39; || last_name, salary, department_id FROM employees WHERE department_id = 80; 说明：CREATE VIEW 子句中各列的别名应和子查询中各列相对应。\n方式2：ALTER VIEW\n修改视图的语法是：\n1 2 3 ALTER VIEW 视图名称 AS 查询语句 2) 删除视图 删除视图只是删除视图的定义，并不会删除基表的数据。 删除视图的语法是： 1 DROP VIEW IF EXISTS 视图名称; 举例： 1 DROP VIEW empvu80; 说明：基于视图a、b创建了新的视图c，如果将视图a或者视图b删除，会导致视图c的查询失败。这 样的视图c需要手动删除或修改，否则影响使用。 7. 总结 1) 优点 1. 操作简单\n将经常使用的查询操作定义为视图，可以使开发人员不需要关心视图对应的数据表的结构、表与表之间的关联关系，也不需要关心数据表之间的业务逻辑和查询条件，而只需要简单地操作视图即可，极大简化了开发人员对数据库的操作。\n2. 减少数据冗余\n视图跟实际数据表不一样，它存储的是查询语句。所以，在使用的时候，我们要通过定义视图的查询语句来获取结果集。而视图本身不存储数据，不占用数据存储的资源，减少了数据冗余。\n3. 数据安全\nMySQL将用户对数据的访问限制在某些数据的结果集上，而这些数据的结果集可以使用视图来实现。用户不必直接查询或操作数据表。这也可以理解为视图具有隔离性 。视图相当于在用户和实际的数据表之间加了一层虚拟表。\n同时，MySQL可以根据权限将用户对数据的访问限制在某些视图上，用户不需要查询数据表，可以直接通过视图获取数据表中的信息。这在一定程度上保障了数据表中数据的安全性。\n4. 适应灵活多变的需求\n当业务系统的需求发生变化后，如果需要改动数据表的结构，则工作量相对较大，可以使用视图来减少改动的工作量。这种方式在实际工作中使用得比较多。\n5. 能够分解复杂的查询逻辑\n数据库中如果存在复杂的查询逻辑，则可以将问题进行分解，创建多个视图获取数据，再将创建的多个视图结合起来，完成复杂的查询逻辑。\n2) 不足 如果我们在实际数据表的基础上创建了视图，那么，如果实际数据表的结构变更了，我们就需要及时对相关的视图进行相应的维护。特别是嵌套的视图（就是在视图的基础上创建视图），维护会变得比较复杂，可读性不好，容易变成系统的潜在隐患。因为创建视图的 SQL 查询可能会对字段重命名，也可能包含复杂的逻辑，这些都会增加维护的成本。\n实际项目中，如果视图过多，会导致数据库维护成本的问题。\n所以，在创建视图的时候，你要结合实际项目需求，综合考虑视图的优点和不足，这样才能正确使用视图，使系统整体达到最优。\n第15章_存储过程与函数(×) MySQL从5.0版本开始支持存储过程和函数。存储过程和函数能够将复杂的SQL逻辑封装在一起，应用程序无须关注存储过程和函数内部复杂的SQL逻辑，而只需要简单地调用存储过程和函数即可。\n1. 存储过程概述 1) 理解 **含义：**存储过程的英文是 Stored Procedure 。它的思想很简单，就是一组经过 预先编译的 SQL 语句 的封装。\n执行过程：存储过程预先存储在 MySQL 服务器上，需要执行的时候，客户端只需要向服务器端发出调用存储过程的命令，服务器端就可以把预先存储好的这一系列 SQL 语句全部执行。\n好处：\n1、简化操作，提高了sql语句的重用性，减少了开发程序员的压力。 2、减少操作过程中的失误，提高效率。 3、减少网络传输量（客户端不需要把所有的 SQL 语句通过网络发给服务器）。 4、减少了 SQL 语句暴露在网上的风险，也提高了数据查询的安全性。 和视图、函数的对比：\n它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。不过它和视图不同，视图是虚拟表，通常不对底层数据表直接操作，而存储过程是程序化的 SQL，可以 直接操作底层数据表 ，相比于面向集合的操作方式，能够实现一些更复杂的数据处理。\n一旦存储过程被创建出来，使用它就像使用函数一样简单，我们直接通过调用存储过程名即可。相较于函数，存储过程是 没有返回值 的。\n2) 分类 存储过程的参数类型可以是IN、OUT和INOUT。根据这点分类如下：\n1、没有参数（无参数无返回）\n2、仅仅带 IN 类型（有参数无返回）\n3、仅仅带 OUT 类型（无参数有返回）\n4、既带 IN 又带 OUT（有参数有返回）\n5、带 INOUT（有参数有返回）\n注意：IN、OUT、INOUT 都可以在一个存储过程中带多个。\n2. 创建存储过程 1) 语法分析 语法：\n1 2 3 4 5 CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...) [characteristics ...] BEGIN 存储过程体 END 说明：\n1、参数前面的符号的意思\nIN ：当前参数为输入参数，也就是表示入参；\n存储过程只是读取这个参数的值。如果没有定义参数种类， 默认就是 IN ，表示输入参数。\nOUT ：当前参数为输出参数，也就是表示出参；\n执行完成之后，调用这个存储过程的客户端或者应用程序就可以读取这个参数返回的值了。\nINOUT ：当前参数既可以为输入参数，也可以为输出参数。\n2、形参类型可以是 MySQL数据库中的任意类型。\n3、characteristics 表示创建存储过程时指定的对存储过程的约束条件，其取值信息如下：\n1 2 3 4 5 LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT \u0026#39;string\u0026#39; LANGUAGE SQL ：说明存储过程执行体是由SQL语句组成的，当前系统支持的语言为SQL。\n[NOT] DETERMINISTIC ：指明存储过程执行的结果是否确定。DETERMINISTIC表示结果是确定 的。每次执行存储过程时，相同的输入会得到相同的输出。NOT DETERMINISTIC表示结果是不确定 的，相同的输入可能得到不同的输出。如果没有指定任意一个值，默认为NOT DETERMINISTIC。\n{ CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } ：指明子程序使 用SQL语句的限制。\nCONTAINS SQL表示当前存储过程的子程序包含SQL语句，但是并不包含读写数据的SQL语句； NO SQL表示当前存储过程的子程序中不包含任何SQL语句； READS SQL DATA表示当前存储过程的子程序中包含读数据的SQL语句； MODIFIES SQL DATA表示当前存储过程的子程序中包含写数据的SQL语句。 默认情况下，系统会指定为CONTAINS SQL。 SQL SECURITY { DEFINER | INVOKER } ：执行当前存储过程的权限，即指明哪些用户能够执行当前存储过程。\nDEFINER 表示只有当前存储过程的创建者或者定义者才能执行当前存储过程； INVOKER 表示拥有当前存储过程的访问权限的用户能够执行当前存储过程。 COMMENT \u0026lsquo;string\u0026rsquo; ：注释信息，可以用来描述存储过程。\n4、存储过程体中可以有多条 SQL 语句，如果仅仅一条SQL 语句，则可以省略 BEGIN 和 END\n1 2 3 4 5 1. BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。 2. DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进 行变量的声明。 3. SET：赋值语句，用于对变量进行赋值。 4. SELECT… INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。 5、需要设置新的结束标记\n1 DELIMITER 新的结束标记 因为MySQL默认的语句结束符号为分号‘;’。为了避免与存储过程中SQL语句结束符相冲突，需要使用 DELIMITER改变存储过程的结束符。\n比如：“DELIMITER //”语句的作用是将MySQL的结束符设置为//，并以“END //”结束存储过程。存储过程定 义完毕之后再使用“DELIMITER ;”恢复默认结束符。DELIMITER也可以指定其他符号作为结束符。\n当使用DELIMITER命令时，应该避免使用反斜杠（‘\\’）字符，因为反斜线是MySQL的转义字符。\n示例：\n1 2 3 4 5 6 7 DELIMITER $ CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...) [characteristics ...] BEGIN sql语句1; sql语句2; END $ 2) 代码举例 举例1：创建存储过程select_all_data()，查看 emps 表的所有数据\n1 2 3 4 5 6 DELIMITER $ CREATE PROCEDURE select_all_data() BEGIN SELECT * FROM emps; END $ DELIMITER ; 举例2：创建存储过程avg_employee_salary()，返回所有员工的平均工资\n1 2 3 4 5 6 DELIMITER // CREATE PROCEDURE avg_employee_salary () BEGIN SELECT AVG(salary) AS avg_salary FROM emps; END // DELIMITER ; 3. 调用存储过程 1) 调用格式 存储过程有多种调用方法。存储过程必须使用CALL语句调用，并且存储过程和数据库相关，如果要执行其他数据库中的存储过程，需要指定数据库名称，例如CALL dbname.procname。\n1 CALL 存储过程名(实参列表) 格式：\n1、调用in模式的参数：\n1 CALL sp1(\u0026#39;值\u0026#39;); 2、调用out模式的参数：\n1 2 3 SET @name; CALL sp1(@name); SELECT @name; 3、调用inout模式的参数：\n1 2 3 SET @name=值; CALL sp1(@name); SELECT @name; 2) 代码举例 举例1：\n1 2 3 4 5 6 7 DELIMITER // CREATE PROCEDURE CountProc(IN sid INT,OUT num INT) BEGIN SELECT COUNT(*) INTO num FROM fruits WHERE s_id = sid; END // DELIMITER ; 调用存储过程：\n1 CALL CountProc (101, @num); 查看返回结果：\n1 SELECT @num; **举例2：**创建存储过程，实现累加运算，计算 1+2+…+n 等于多少。具体的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 DELIMITER // CREATE PROCEDURE `add_num`(IN n INT) BEGIN DECLARE i INT; DECLARE sum INT; SET i = 1; SET sum = 0; WHILE i \u0026lt;= n DO SET sum = sum + i; SET i = i +1; END WHILE; SELECT sum; END // DELIMITER ; 直接使用 CALL add_num(50); 即可。这里我传入的参数为 50，也就是统计 1+2+…+50 的积累之和。\n3) 如何调试 在 MySQL 中，存储过程不像普通的编程语言（比如 VC++、Java 等）那样有专门的集成开发环境。因此，你可以通过 SELECT 语句，把程序执行的中间结果查询出来，来调试一个 SQL 语句的正确性。调试成功之后，把 SELECT 语句后移到下一个 SQL 语句之后，再调试下一个 SQL 语句。这样 逐步推进 ，就可以完成对存储过程中所有操作的调试了。当然，你也可以把存储过程中的 SQL 语句复制出来，逐段单独调试。\n4. 存储函数的使用 1) 语法分析 学过的函数：LENGTH、SUBSTR、CONCAT等\n语法格式：\n1 2 3 4 5 6 CREATE FUNCTION 函数名(参数名 参数类型,...) RETURNS 返回值类型 [characteristics ...] BEGIN 函数体 #函数体中肯定有 RETURN 语句 END 说明：\n1、参数列表：指定参数为IN、OUT或INOUT只对PROCEDURE是合法的，FUNCTION中总是默认为IN参数。\n2、RETURNS type 语句表示函数返回数据的类型； RETURNS子句只能对FUNCTION做指定，对函数而言这是 强制 的。它用来指定函数的返回类型，而且函 数体必须包含一个 RETURN value 语句。\n3、characteristic 创建函数时指定的对函数的约束。取值与创建存储过程时相同，这里不再赘述。\n4、函数体也可以用BEGIN…END来表示SQL代码的开始和结束。如果函数体只有一条语句，也可以省略 BEGIN…END。\n2) 调用存储函数 在MySQL中，存储函数的使用方法与MySQL内部函数的使用方法是一样的。换言之，用户自己定义的存储函数与MySQL内部函数是一个性质的。区别在于，存储函数是 用户自己定义 的，而内部函数是MySQL 的 开发者定义 的。\n1 SELECT 函数名(实参列表) 3) 代码举例 举例1：\n创建存储函数，名称为email_by_name()，参数定义为空，该函数查询Abel的email，并返回，数据类型为字符串型。\n1 2 3 4 5 6 7 8 9 DELIMITER // CREATE FUNCTION email_by_name() RETURNS VARCHAR(25) DETERMINISTIC CONTAINS SQL BEGIN RETURN (SELECT email FROM employees WHERE last_name = \u0026#39;Abel\u0026#39;); END // DELIMITER ; 调用：\n1 SELECT email_by_name(); 举例2：\n创建存储函数，名称为email_by_id()，参数传入emp_id，该函数查询emp_id的email，并返回，数据类型 为字符串型。\n1 2 3 4 5 6 7 8 9 DELIMITER // CREATE FUNCTION email_by_id(emp_id INT) RETURNS VARCHAR(25) DETERMINISTIC CONTAINS SQL BEGIN RETURN (SELECT email FROM employees WHERE employee_id = emp_id); END // DELIMITER ; 调用：\n1 2 SET @emp_id = 102; SELECT email_by_id(@emp_id); 注意：\n若在创建存储函数中报错“ you might want to use the less safe log_bin_trust_function_creators variable ”，有两种处理方法：\n方式1：\n加上必要的函数特性“[NOT] DETERMINISTIC”和“{CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA}”\n方式2：\n1 SET GLOBAL log_bin_trust_function_creators = 1; 4) 对比存储函数与存储过程 关键字 调用语法 返回值 应用场景 存储过程 PROCEDURE CALL 存储过程() 理解为有0个或多个 一般用于更新 存储函数 FUNCTION SELECT 函数 () 只能是一个 一般用于查询结果为一个值并返回时 此外，存储函数可以放在查询语句中使用，存储过程不行。反之，存储过程的功能更加强大，包括能够 执行对表的操作（比如创建表，删除表等）和事务操作，这些功能是存储函数不具备的。\n5. 存储过程和函数的查看、修改、删除 1) 查看 创建完之后，怎么知道我们创建的存储过程、存储函数是否成功了呢？\nMySQL存储了存储过程和函数的状态信息，用户可以使用SHOW STATUS语句或SHOW CREATE语句来查 看，也可直接从系统的information_schema数据库中查询。这里介绍3种方法。\n使用SHOW CREATE语句查看存储过程和函数的创建信息 1 SHOW CREATE {PROCEDURE | FUNCTION} 存储过程名或函数名 使用SHOW STATUS语句查看存储过程和函数的状态信息 1 SHOW {PROCEDURE | FUNCTION} STATUS [LIKE \u0026#39;pattern\u0026#39;] 从information_schema.Routines表中查看存储过程和函数的信息 MySQL中存储过程和函数的信息存储在information_schema数据库下的Routines表中。可以通过查询该表的记录来查询存储过程和函数的信息。其基本语法形式如下：\n1 2 SELECT * FROM information_schema.Routines WHERE ROUTINE_NAME=\u0026#39;存储过程或函数的名\u0026#39; [AND ROUTINE_TYPE = {\u0026#39;PROCEDURE|FUNCTION\u0026#39;}]; 说明：如果在MySQL数据库中存在存储过程和函数名称相同的情况，最好指定ROUTINE_TYPE查询条件来 指明查询的是存储过程还是函数。\n2) 修改 修改存储过程或函数，不影响存储过程或函数功能，只是修改相关特性。使用ALTER语句实现。\n1 ALTER {PROCEDURE | FUNCTION} 存储过程或函数的名 [characteristic ...] 其中，characteristic指定存储过程或函数的特性，其取值信息与创建存储过程、函数时的取值信息略有不同。\n1 2 3 { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT \u0026#39;string\u0026#39; CONTAINS SQL ，表示子程序包含SQL语句，但不包含读或写数据的语句。\nNO SQL ，表示子程序中不包含SQL语句。\nREADS SQL DATA ，表示子程序中包含读数据的语句。\nMODIFIES SQL DATA ，表示子程序中包含写数据的语句。\nSQL SECURITY { DEFINER | INVOKER } ，指明谁有权限来执行。\nDEFINER ，表示只有定义者自己才能够执行。 INVOKER ，表示调用者可以执行。 COMMENT \u0026lsquo;string\u0026rsquo; ，表示注释信息。\n修改存储过程使用ALTER PROCEDURE语句，修改存储函数使用ALTER FUNCTION语句。但是，这两 个语句的结构是一样的，语句中的所有参数也是一样的。\n3) 删除 删除存储过程和函数，可以使用DROP语句，其语法结构如下：\n1 DROP {PROCEDURE | FUNCTION} [IF EXISTS] 存储过程或函数的名 6. 关于存储过程使用的争议 1) 优点 1、存储过程可以一次编译多次使用。存储过程只在创建时进行编译，之后的使用都不需要重新编译， 这就提升了 SQL 的执行效率。\n2、可以减少开发工作量。将代码 封装 成模块，实际上是编程的核心思想之一，这样可以把复杂的问题拆解成不同的模块，然后模块之间可以重复使用 ，在减少开发工作量的同时，还能保证代码的结构清晰。\n3、存储过程的安全性强。我们在设定存储过程的时候可以 设置对用户的使用权限 ，这样就和视图一样具有较强的安全性。\n4、可以减少网络传输量。因为代码封装到存储过程中，每次使用只需要调用存储过程即可，这样就减少了网络传输量。\n5、良好的封装性。在进行相对复杂的数据库操作时，原本需要使用一条一条的 SQL 语句，可能要连接多次数据库才能完成的操作，现在变成了一次存储过程，只需要连接一次即可 。\n2) 缺点 阿里开发规范 【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\n1、可移植性差。存储过程不能跨数据库移植，比如在 MySQL、Oracle 和 SQL Server 里编写的存储过程，在换成其他数据库时都需要重新编写。\n2、调试困难。只有少数 DBMS 支持存储过程的调试。对于复杂的存储过程来说，开发和维护都不容易。虽然也有一些第三方工具可以对存储过程进行调试，但要收费。\n3、存储过程的版本管理很困难。比如数据表索引发生变化了，可能会导致存储过程失效。我们在开发软件的时候往往需要进行版本管理，但是存储过程本身没有版本控制，版本迭代更新的时候很麻烦。\n4、它不适合高并发的场景。高并发的场景需要减少数据库的压力，有时数据库会采用分库分表的方式，而且对可扩展性要求很高，在这种情况下，存储过程会变得难以维护， 增加数据库的压力 ，显然就不适用了。\n3) 小结 存储过程既方便，又有局限性。尽管不同的公司对存储过程的态度不一，但是对于我们开发人员来说，不论怎样，掌握存储过程都是必备的技能之一。\n第16章_变量、流程控制与游标 在MySQL数据库的存储过程和函数中，可以使用变量来存储查询或计算的中间结果数据，或者输出最终的结果数据。\n1. 变量 在MySQL数据库的存储过程和函数中，可以使用变量来存储查询或计算的中间结果数据，或者输出最终 的结果数据。\n在 MySQL 数据库中，变量分为 系统变量 以及 用户自定义变量 。\n1) 系统变量 系统变量分类\n变量由系统定义，不是用户定义，属于 服务器 层面。启动MySQL服务，生成MySQL服务实例期间， MySQL将为MySQL服务器内存中的系统变量赋值，这些系统变量定义了当前MySQL服务实例的属性、特 征。这些系统变量的值要么是 编译MySQL时参数 的默认值，要么是 配置文件 （例如my.ini等）中的参数 值。大家可以通过网址 https://dev.mysql.com/doc/refman/8.0/en/server-systemvariables.html 查看MySQL文档的系统变量。\n系统变量分为全局系统变量（需要添加 global 关键字）以及会话系统变量（需要添加 session 关键字），有时也把全局系统变量简称为全局变量，有时也把会话系统变量称为local变量。如果不写，默认会话级别。静态变量（在 MySQL 服务实例运行期间它们的值不能使用 set 动态修改）属于特殊的全局系统变量。\n每一个MySQL客户机成功连接MySQL服务器后，都会产生与之对应的会话。会话期间，MySQL服务实例会在MySQL服务器内存中生成与该会话对应的会话系统变量，这些会话系统变量的初始值是全局系统变量值的复制。如下图：\n全局系统变量针对于所有会话（连接）有效，但 不能跨重启 会话系统变量仅针对于当前会话（连接）有效。会话期间，当前会话对某个会话系统变量值的修改，不会影响其他会话同一个会话系统变量的值。 会话1对某个全局系统变量值的修改会导致会话2中同一个全局系统变量值的修改。 在MySQL中有些系统变量只能是全局的，例如 max_connections 用于限制服务器的最大连接数；有些系 统变量作用域既可以是全局又可以是会话，例如 character_set_client 用于设置客户端的字符集；有些系 统变量的作用域只能是当前会话，例如 pseudo_thread_id 用于标记当前会话的 MySQL 连接 ID。\n查看系统变量\n查看所有或部分系统变量 1 2 3 4 5 6 #查看所有全局变量 SHOW GLOBAL VARIABLES; #查看所有会话变量 SHOW SESSION VARIABLES; # 或 SHOW VARIABLES; 1 2 3 4 #查看满足条件的部分系统变量。 SHOW GLOBAL VARIABLES LIKE \u0026#39;%标识符%\u0026#39;; #查看满足条件的部分会话变量 SHOW SESSION VARIABLES LIKE \u0026#39;%标识符%\u0026#39;; 查看指定系统变量\n作为 MySQL 编码规范，MySQL 中的系统变量以 两个“@” 开头，其中“@@global”仅用于标记全局系统变量，“@@session”仅用于标记会话系统变量。“@@”首先标记会话系统变量，如果会话系统变量不存在， 则标记全局系统变量。\n1 2 3 4 5 6 #查看指定的系统变量的值 SELECT @@global.变量名; #查看指定的会话变量的值 SELECT @@session.变量名; # 或 SELECT @@变量名; 修改系统变量的值\n有些时候，数据库管理员需要修改系统变量的默认值，以便修改当前会话或者MySQL服务实例的属性、 特征。具体方法：\n方式1：修改MySQL 配置文件 ，继而修改MySQL系统变量的值（该方法需要重启MySQL服务）\n方式2：在MySQL服务运行期间，使用“set”命令重新设置系统变量的值\n1 2 3 4 5 6 7 8 9 10 #为某个系统变量赋值 #方式1： SET @@global.变量名=变量值; #方式2： SET GLOBAL 变量名=变量值; #为某个会话变量赋值 #方式1： SET @@session.变量名=变量值; #方式2： SET SESSION 变量名=变量值; 2) 用户变量 用户变量分类\n用户变量是用户自己定义的，作为 MySQL 编码规范，MySQL 中的用户变量以一个“@” 开头。根据作用范围不同，又分为 会话用户变量 和 局部变量 。\n会话用户变量：作用域和会话变量一样，只对 当前连接 会话有效。 局部变量：只在 BEGIN 和 END 语句块中有效。局部变量只能在 存储过程和函数 中使用。 会话用户变量\n变量的定义 1 2 3 4 5 6 #方式1：“=”或“:=” SET @用户变量 = 值; SET @用户变量 := 值; #方式2：“:=” 或 INTO关键字 SELECT @用户变量 := 表达式 [FROM 等子句]; SELECT 表达式 INTO @用户变量 [FROM 等子句]; 查看用户变量的值 (查看、比较、运算等) 1 SELECT @用户变量 局部变量\n定义：可以使用 DECLARE 语句定义一个局部变量\n作用域：仅仅在定义它的 BEGIN \u0026hellip; END 中有效\n位置：只能放在 BEGIN \u0026hellip; END 中，而且只能放在第一句\n1 2 3 4 5 6 7 8 9 10 BEGIN #声明局部变量 DECLARE 变量名1 变量数据类型 [DEFAULT 变量默认值]; DECLARE 变量名2,变量名3,... 变量数据类型 [DEFAULT 变量默认值]; #为局部变量赋值 SET 变量名1 = 值; SELECT 值 INTO 变量名2 [FROM 子句]; #查看局部变量的值 SELECT 变量1,变量2,变量3; END 定义变量 1 DECLARE 变量名 类型 [default 值]; # 如果没有DEFAULT子句，初始值为NULL 变量赋值 方式1：一般用于赋简单的值\n1 2 SET 变量名=值; SET 变量名:=值; 方式2：一般用于赋表中的字段值\n1 SELECT 字段名或表达式 INTO 变量名 FROM 表; 使用变量 (查看、比较、运算等) 1 SELECT 局部变量名; 举例1：声明局部变量，并分别赋值为employees表中employee_id为102的last_name和salary\n1 2 3 4 5 6 7 8 9 10 11 DELIMITER // CREATE PROCEDURE set_value() BEGIN DECLARE emp_name VARCHAR(25); DECLARE sal DOUBLE(10,2); SELECT last_name, salary INTO emp_name,sal FROM employees WHERE employee_id = 102; SELECT emp_name, sal; END // DELIMITER ; 举例2：声明两个变量，求和并打印 （分别使用会话用户变量、局部变量的方式实现）\n1 2 3 4 5 #方式1：使用用户变量 SET @m=1; SET @n=1; SET @sum=@m+@n; SELECT @sum; 1 2 3 4 5 6 7 8 9 10 11 12 #方式2：使用局部变量 DELIMITER // CREATE PROCEDURE add_value() BEGIN #局部变量 DECLARE m INT DEFAULT 1; DECLARE n INT DEFAULT 3; DECLARE SUM INT; SET SUM = m+n; SELECT SUM; END // DELIMITER ; 对比会话用户变量与局部变量\n作用域 定义位置 语法 会话用户变量 当前会话 会话的任何地方 加@符号，不用指定类型 局部变量 定义它的BEGIN END中 BEGIN END的第一句话 一般不用加@,需要指定类型 2. 定义条件与处理程序 定义条件 是事先定义程序执行过程中可能遇到的问题， 处理程序 定义了在遇到问题时应当采取的处理方式，并且保证存储过程或函数在遇到警告或错误时能继续执行。这样可以增强存储程序处理问题的能力，避免程序异常停止运行。\n说明：定义条件和处理程序在存储过程、存储函数中都是支持的。\n1) 案例分析 案例分析：创建一个名称为“UpdateDataNoCondition”的存储过程。代码如下：\n1 2 3 4 5 6 7 8 9 10 DELIMITER // CREATE PROCEDURE UpdateDataNoCondition() BEGIN SET @x = 1; UPDATE employees SET email = NULL WHERE last_name = \u0026#39;Abel\u0026#39;; SET @x = 2; UPDATE employees SET email = \u0026#39;aabbel\u0026#39; WHERE last_name = \u0026#39;Abel\u0026#39;; SET @x = 3; END // DELIMITER ; 调用存储过程：\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; CALL UpdateDataNoCondition(); ERROR 1048 (23000): Column \u0026#39;email\u0026#39; cannot be null mysql\u0026gt; SELECT @x; +------+ | @x | +------+ | 1 | +------+ 1 row in set (0.00 sec) 可以看到，此时@x变量的值为1。结合创建存储过程的SQL语句代码可以得出：在存储过程中未定义条件 和处理程序，且当存储过程中执行的SQL语句报错时，MySQL数据库会抛出错误，并退出当前SQL逻辑， 不再向下继续执行。\n2) 定义条件 定义条件就是给MySQL中的错误码命名，这有助于存储的程序代码更清晰。它将一个 错误名字 和 指定的 错误条件 关联起来。这个名字可以随后被用在定义处理程序的 DECLARE HANDLER 语句中。\n定义条件使用DECLARE语句，语法格式如下：\n1 DECLARE 错误名称 CONDITION FOR 错误码（或错误条件） 错误码的说明：\nMySQL_error_code 和 sqlstate_value 都可以表示MySQL的错误。 MySQL_error_code是数值类型错误代码。 sqlstate_value是长度为5的字符串类型错误代码。 例如，在ERROR 1418 (HY000)中，1418是MySQL_error_code，\u0026lsquo;HY000\u0026rsquo;是sqlstate_value。\n例如，在ERROR 1142（42000）中，1142是MySQL_error_code，\u0026lsquo;42000\u0026rsquo;是sqlstate_value。\n举例1：定义“Field_Not_Be_NULL”错误名与MySQL中违反非空约束的错误类型是“ERROR 1048 (23000)”对应。\n1 2 3 4 #使用MySQL_error_code DECLARE Field_Not_Be_NULL CONDITION FOR 1048; #使用sqlstate_value DECLARE Field_Not_Be_NULL CONDITION FOR SQLSTATE \u0026#39;23000\u0026#39;; 3) 定义处理程序 可以为SQL执行过程中发生的某种类型的错误定义特殊的处理程序。定义处理程序时，使用DECLARE语句 的语法如下：\n1 DECLARE 处理方式 HANDLER FOR 错误类型 处理语句 处理方式：处理方式有3个取值：CONTINUE、EXIT、UNDO。\nCONTINUE ：表示遇到错误不处理，继续执行。 EXIT ：表示遇到错误马上退出。 UNDO ：表示遇到错误后撤回之前的操作。MySQL中暂时不支持这样的操作。 错误类型（即条件）可以有如下取值：\nSQLSTATE \u0026lsquo;字符串错误码\u0026rsquo; ：表示长度为5的sqlstate_value类型的错误代码； MySQL_error_code ：匹配数值类型错误代码； 错误名称 ：表示DECLARE \u0026hellip; CONDITION定义的错误条件名称。 SQLWARNING ：匹配所有以01开头的SQLSTATE错误代码； NOT FOUND ：匹配所有以02开头的SQLSTATE错误代码； SQLEXCEPTION ：匹配所有没有被SQLWARNING或NOT FOUND捕获的SQLSTATE错误代码； 处理语句：如果出现上述条件之一，则采用对应的处理方式，并执行指定的处理语句。语句可以是 像“ SET 变量 = 值 ”这样的简单语句，也可以是使用 BEGIN \u0026hellip; END 编写的复合语句。\n定义处理程序的几种方式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 #方法1：捕获sqlstate_value DECLARE CONTINUE HANDLER FOR SQLSTATE \u0026#39;42S02\u0026#39; SET @info = \u0026#39;NO_SUCH_TABLE\u0026#39;; #方法2：捕获mysql_error_value DECLARE CONTINUE HANDLER FOR 1146 SET @info = \u0026#39;NO_SUCH_TABLE\u0026#39;; #方法3：先定义条件，再调用 DECLARE no_such_table CONDITION FOR 1146; DECLARE CONTINUE HANDLER FOR NO_SUCH_TABLE SET @info = \u0026#39;NO_SUCH_TABLE\u0026#39;; #方法4：使用SQLWARNING DECLARE EXIT HANDLER FOR SQLWARNING SET @info = \u0026#39;ERROR\u0026#39;; #方法5：使用NOT FOUND DECLARE EXIT HANDLER FOR NOT FOUND SET @info = \u0026#39;NO_SUCH_TABLE\u0026#39;; #方法6：使用SQLEXCEPTION DECLARE EXIT HANDLER FOR SQLEXCEPTION SET @info = \u0026#39;ERROR\u0026#39;; 4) 案例解决 在存储过程中，定义处理程序，捕获sqlstate_value值，当遇到MySQL_error_code值为1048时，执行 CONTINUE操作，并且将@proc_value的值设置为-1。\n1 2 3 4 5 6 7 8 9 10 11 12 DELIMITER // CREATE PROCEDURE UpdateDataNoCondition() BEGIN #定义处理程序 DECLARE CONTINUE HANDLER FOR 1048 SET @proc_value = -1; SET @x = 1; UPDATE employees SET email = NULL WHERE last_name = \u0026#39;Abel\u0026#39;; SET @x = 2; UPDATE employees SET email = \u0026#39;aabbel\u0026#39; WHERE last_name = \u0026#39;Abel\u0026#39;; SET @x = 3; END // DELIMITER ; 3. 流程控制 解决复杂问题不可能通过一个 SQL 语句完成，我们需要执行多个 SQL 操作。流程控制语句的作用就是控 制存储过程中 SQL 语句的执行顺序，是我们完成复杂操作必不可少的一部分。只要是执行的程序，流程就分为三大类：\n顺序结构 ：程序从上往下依次执行 分支结构 ：程序按条件进行选择执行，从两条或多条路径中选择一条执行 循环结构 ：程序满足一定条件下，重复执行一组语句 针对于MySQL 的流程控制语句主要有 3 类。注意：只能用于存储程序。\n条件判断语句 ：IF 语句和 CASE 语句 循环语句 ：LOOP、WHILE 和 REPEAT 语句 跳转语句 ：ITERATE 和 LEAVE 语句 1) 分支结构之 IF IF 语句的语法结构是： 1 2 3 4 IF 表达式1 THEN 操作1 [ELSEIF 表达式2 THEN 操作2]…… [ELSE 操作N] END IF 根据表达式的结果为TRUE或FALSE执行相应的语句。这里“[]”中的内容是可选的。\n特点：① 不同的表达式对应不同的操作 ② 使用在begin end中\n举例1：\n1 2 3 4 IF val IS NULL THEN SELECT \u0026#39;val is null\u0026#39;; ELSE SELECT \u0026#39;val is not null\u0026#39;; END IF; 举例2：声明存储过程“update_salary_by_eid1”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于8000元并且入职时间超过5年，就涨薪500元；否则就不变。 1 2 3 4 5 6 7 8 9 10 11 12 13 DELIMITER // CREATE PROCEDURE update_salary_by_eid1(IN emp_id INT) BEGIN DECLARE emp_salary DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id; SELECT DATEDIFF(CURDATE(),hire_date)/365 INTO hire_year FROM employees WHERE employee_id = emp_id; IF emp_salary \u0026lt; 8000 AND hire_year \u0026gt; 5 THEN UPDATE employees SET salary = salary + 500 WHERE employee_id = emp_id; END IF; END // DELIMITER ; 2) 分支结构之 CASE CASE 语句的语法结构1： 1 2 3 4 5 6 7 #情况一：类似于switch CASE 表达式 WHEN 值1 THEN 结果1或语句1(如果是语句，需要加分号) WHEN 值2 THEN 结果2或语句2(如果是语句，需要加分号) ... ELSE 结果n或语句n(如果是语句，需要加分号) END [case]（如果是放在begin end中需要加上case，如果放在select后面不需要） CASE 语句的语法结构2： 1 2 3 4 5 6 7 #情况二：类似于多重if CASE WHEN 条件1 THEN 结果1或语句1(如果是语句，需要加分号) WHEN 条件2 THEN 结果2或语句2(如果是语句，需要加分号) ... ELSE 结果n或语句n(如果是语句，需要加分号) END [case]（如果是放在begin end中需要加上case，如果放在select后面不需要） 举例1：使用CASE流程控制语句的第1种格式，判断val值等于1、等于2，或者两者都不等。 1 2 3 4 5 CASE val WHEN 1 THEN SELECT \u0026#39;val is 1\u0026#39;; WHEN 2 THEN SELECT \u0026#39;val is 2\u0026#39;; ELSE SELECT \u0026#39;val is not 1 or 2\u0026#39;; END CASE; 举例2：声明存储过程“update_salary_by_eid4”，定义IN参数emp_id，输入员工编号。判断该员工 薪资如果低于9000元，就更新薪资为9000元；薪资大于等于9000元且低于10000的，但是奖金比例 为NULL的，就更新奖金比例为0.01；其他的涨薪100元。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DELIMITER // CREATE PROCEDURE update_salary_by_eid4(IN emp_id INT) BEGIN DECLARE emp_sal DOUBLE; DECLARE bonus DECIMAL(3,2); SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT commission_pct INTO bonus FROM employees WHERE employee_id = emp_id; CASE WHEN emp_sal\u0026lt;9000 THEN UPDATE employees SET salary=9000 WHERE employee_id = emp_id; WHEN emp_sal\u0026lt;10000 AND bonus IS NULL THEN UPDATE employees SET commission_pct=0.01 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id; END CASE; END // DELIMITER ; 举例3：声明存储过程update_salary_by_eid5，定义IN参数emp_id，输入员工编号。判断该员工的 入职年限，如果是0年，薪资涨50；如果是1年，薪资涨100；如果是2年，薪资涨200；如果是3年， 薪资涨300；如果是4年，薪资涨400；其他的涨薪500。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 DELIMITER // CREATE PROCEDURE update_salary_by_eid5(IN emp_id INT) BEGIN DECLARE emp_sal DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT ROUND(DATEDIFF(CURDATE(),hire_date)/365) INTO hire_year FROM employees WHERE employee_id = emp_id; CASE hire_year WHEN 0 THEN UPDATE employees SET salary=salary+50 WHERE employee_id = emp_id; WHEN 1 THEN UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id; WHEN 2 THEN UPDATE employees SET salary=salary+200 WHERE employee_id = emp_id; WHEN 3 THEN UPDATE employees SET salary=salary+300 WHERE employee_id = emp_id; WHEN 4 THEN UPDATE employees SET salary=salary+400 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary=salary+500 WHERE employee_id = emp_id; END CASE; END // DELIMITER ; 3) 循环结构之LOOP LOOP循环语句用来重复执行某些语句。LOOP内的语句一直重复执行直到循环被退出（使用LEAVE子 句），跳出循环过程。\nLOOP语句的基本格式如下：\n1 2 3 [loop_label:] LOOP 循环执行的语句 END LOOP [loop_label] 其中，loop_label表示LOOP语句的标注名称，该参数可以省略。\n举例1：使用LOOP语句进行循环操作，id值小于10时将重复执行循环过程。\n1 2 3 4 5 6 DECLARE id INT DEFAULT 0; add_loop:LOOP SET id = id +1; IF id \u0026gt;= 10 THEN LEAVE add_loop; END IF; END LOOP add_loop; 举例2：当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程 “update_salary_loop()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨为原来的1.1倍。直到全公司的平均薪资达到12000结束。并统计循环次数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 DELIMITER // CREATE PROCEDURE update_salary_loop(OUT num INT) BEGIN DECLARE avg_salary DOUBLE; DECLARE loop_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_salary FROM employees; label_loop:LOOP IF avg_salary \u0026gt;= 12000 THEN LEAVE label_loop; END IF; UPDATE employees SET salary = salary * 1.1; SET loop_count = loop_count + 1; SELECT AVG(salary) INTO avg_salary FROM employees; END LOOP label_loop; SET num = loop_count; END // DELIMITER ; 4) 循环结构之WHILE WHILE语句创建一个带条件判断的循环过程。WHILE在执行语句执行时，先对指定的表达式进行判断，如 果为真，就执行循环内的语句，否则退出循环。WHILE语句的基本格式如下：\n1 2 3 [while_label:] WHILE 循环条件 DO 循环体 END WHILE [while_label]; while_label为WHILE语句的标注名称；如果循环条件结果为真，WHILE语句内的语句或语句群被执行，直 至循环条件为假，退出循环。\n举例1：WHILE语句示例，i值小于10时，将重复执行循环过程，代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 DELIMITER // CREATE PROCEDURE test_while() BEGIN DECLARE i INT DEFAULT 0; WHILE i \u0026lt; 10 DO SET i = i + 1; END WHILE; SELECT i; END // DELIMITER ; #调用 CALL test_while(); 举例2：市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程 “update_salary_while()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家降薪，薪资降 为原来的90%。直到全公司的平均薪资达到5000结束。并统计循环次数。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 DELIMITER // CREATE PROCEDURE update_salary_while(OUT num INT) BEGIN DECLARE avg_sal DOUBLE ; DECLARE while_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_sal FROM employees; WHILE avg_sal \u0026gt; 5000 DO UPDATE employees SET salary = salary * 0.9; SET while_count = while_count + 1; SELECT AVG(salary) INTO avg_sal FROM employees; END WHILE; SET num = while_count; END // DELIMITER ; 5) 循环结构之REPEAT REPEAT语句创建一个带条件判断的循环过程。与WHILE循环不同的是，REPEAT 循环首先会执行一次循环，然后在 UNTIL 中进行表达式的判断，如果满足条件就退出，即 END REPEAT；如果条件不满足，则会就继续执行循环，直到满足退出条件为止。\nREPEAT语句的基本格式如下：\n1 2 3 4 [repeat_label:] REPEAT 循环体的语句 UNTIL 结束循环的条件表达式 END REPEAT [repeat_label] repeat_label为REPEAT语句的标注名称，该参数可以省略；REPEAT语句内的语句或语句群被重复，直至 expr_condition为真。\n举例1：\n1 2 3 4 5 6 7 8 9 10 11 DELIMITER // CREATE PROCEDURE test_repeat() BEGIN DECLARE i INT DEFAULT 0; REPEAT SET i = i + 1; UNTIL i \u0026gt;= 10 END REPEAT; SELECT i; END // DELIMITER ; 举例2：当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程 “update_salary_repeat()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨 为原来的1.15倍。直到全公司的平均薪资达到13000结束。并统计循环次数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DELIMITER // CREATE PROCEDURE update_salary_repeat(OUT num INT) BEGIN DECLARE avg_sal DOUBLE ; DECLARE repeat_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_sal FROM employees; REPEAT UPDATE employees SET salary = salary * 1.15; SET repeat_count = repeat_count + 1; SELECT AVG(salary) INTO avg_sal FROM employees; UNTIL avg_sal \u0026gt;= 13000 END REPEAT; SET num = repeat_count; END // DELIMITER ; 对比三种循环结构：\n这三种循环都可以省略名称，但如果循环中添加了循环控制语句（LEAVE或ITERATE）则必须添加名称。\nLOOP：一般用于实现简单的\u0026quot;死\u0026quot;循环 WHILE：先判断后执行\nREPEAT：先执行后判断，无条件至少执行一次\n6) 跳转语句之LEAVE语句 LEAVE语句：可以用在循环语句内，或者以 BEGIN 和 END 包裹起来的程序体内，表示跳出循环或者跳出 程序体的操作。如果你有面向过程的编程语言的使用经验，你可以把 LEAVE 理解为 break。\n基本格式如下：\n1 LEAVE 标记名 其中，label参数表示循环的标志。LEAVE和BEGIN \u0026hellip; END或循环一起被使用。\n举例1：创建存储过程 “leave_begin()”，声明INT类型的IN参数num。给BEGIN\u0026hellip;END加标记名，并在 BEGIN\u0026hellip;END中使用IF语句判断num参数的值。\n如果num\u0026lt;=0，则使用LEAVE语句退出BEGIN\u0026hellip;END； 如果num=1，则查询“employees”表的平均薪资； 如果num=2，则查询“employees”表的最低薪资； 如果num\u0026gt;2，则查询“employees”表的最高薪资。\nIF语句结束后查询“employees”表的总人数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DELIMITER // CREATE PROCEDURE leave_begin(IN num INT) begin_label: BEGIN IF num\u0026lt;=0 THEN LEAVE begin_label; ELSEIF num=1 THEN SELECT AVG(salary) FROM employees; ELSEIF num=2 THEN SELECT MIN(salary) FROM employees; ELSE SELECT MAX(salary) FROM employees; END IF; SELECT COUNT(*) FROM employees; END // DELIMITER ; 举例2： 当市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程“leave_while()”，声明 OUT参数num，输出循环次数，存储过程中使用WHILE循环给大家降低薪资为原来薪资的90%，直到全公司的平均薪资小于等于10000，并统计循环次数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 DELIMITER // CREATE PROCEDURE leave_while(OUT num INT) BEGIN DECLARE avg_sal DOUBLE;#记录平均工资 DECLARE while_count INT DEFAULT 0; #记录循环次数 SELECT AVG(salary) INTO avg_sal FROM employees; #① 初始化条件 while_label:WHILE TRUE DO #② 循环条件 #③ 循环体 IF avg_sal \u0026lt;= 10000 THEN LEAVE while_label; END IF; UPDATE employees SET salary = salary * 0.9; SET while_count = while_count + 1; #④ 迭代条件 SELECT AVG(salary) INTO avg_sal FROM employees; END WHILE; #赋值 SET num = while_count; END // DELIMITER ; 7) 跳转语句之ITERATE语句 ITERATE语句：只能用在循环语句（LOOP、REPEAT和WHILE语句）内，表示重新开始循环，将执行顺序转到语句段开头处。如果你有面向过程的编程语言的使用经验，你可以把 ITERATE 理解为 continue，意思为“再次循环”。\n语句基本格式如下：\n1 ITERATE label label参数表示循环的标志。ITERATE语句必须跟在循环标志前面。\n举例： 定义局部变量num，初始值为0。循环结构中执行num + 1操作。\n如果num \u0026lt; 10，则继续执行循环； 如果num \u0026gt; 15，则退出循环结构； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DELIMITER // CREATE PROCEDURE test_iterate() BEGIN DECLARE num INT DEFAULT 0; my_loop:LOOP SET num = num + 1; IF num \u0026lt; 10 THEN ITERATE my_loop; ELSEIF num \u0026gt; 15 THEN LEAVE my_loop; END IF; SELECT \u0026#39;MySQL\u0026#39;; END LOOP my_loop; END // DELIMITER ; 4. 游标 1) 什么是游标（或光标） 虽然我们也可以通过筛选条件 WHERE 和 HAVING，或者是限定返回记录的关键字 LIMIT 返回一条记录，但是，却无法在结果集中像指针一样，向前定位一条记录、向后定位一条记录，或者是随意定位到某一条记录 ，并对记录的数据进行处理。\n这个时候，就可以用到游标。游标，提供了一种灵活的操作方式，让我们能够对结果集中的每一条记录进行定位，并对指向的记录中的数据进行操作的数据结构。游标让 SQL 这种面向集合的语言有了面向过程开发的能力。\n在 SQL 中，游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标充当了指针的作用 ，我们可以通过操作游标来对数据行进行操作。\nMySQL中游标可以在存储过程和函数中使用。\n2) 使用游标步骤 游标必须在声明处理程序之前被声明，并且变量和条件还必须在声明游标或处理程序之前被声明。\n如果我们想要使用游标，一般需要经历四个步骤。不同的 DBMS 中，使用游标的语法可能略有不同。\n第一步，声明游标\n在MySQL中，使用DECLARE关键字来声明游标，其语法的基本形式如下：\n1 DECLARE cursor_name CURSOR FOR select_statement; 这个语法适用于 MySQL，SQL Server，DB2 和 MariaDB。如果是用 Oracle 或者 PostgreSQL，需要写成：\n1 DECLARE cursor_name CURSOR IS select_statement; 要使用 SELECT 语句来获取数据结果集，而此时还没有开始遍历数据，这里 select_statement 代表的是 SELECT 语句，返回一个用于创建游标的结果集。\n比如：\n1 2 DECLARE cur_emp CURSOR FOR SELECT employee_id,salary FROM employees; 第二步，打开游标\n打开游标的语法如下：\n1 OPEN cursor_name 当我们定义好游标之后，如果想要使用游标，必须先打开游标。打开游标的时候 SELECT 语句的查询结果集就会送到游标工作区，为后面游标的 逐条读取 结果集中的记录做准备。\n1 OPEN cur_emp; 第三步，使用游标（从游标中取得数据）\n语法如下：\n1 FETCH cursor_name INTO var_name [, var_name] ... 这句的作用是使用 cursor_name 这个游标来读取当前行，并且将数据保存到 var_name 这个变量中，游标指针指到下一行。如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。\n注意：var_name必须在声明游标之前就定义好。\n1 FETCH cur_emp INTO emp_id, emp_sal ; 注意：游标的查询结果集中的字段数，必须跟 INTO 后面的变量数一致，否则，在存储过程执行的时 候，MySQL 会提示错误。\n第四步，关闭游标\n1 CLOSE cursor_name 有 OPEN 就会有 CLOSE，也就是打开和关闭游标。当我们使用完游标后需要关闭掉该游标。因为游标会 占用系统资源 ，如果不及时关闭，游标会一直保持到存储过程结束，影响系统运行的效率。而关闭游标 的操作，会释放游标占用的系统资源。\n关闭游标之后，我们就不能再检索查询结果中的数据行，如果需要检索只能再次打开游标。\n1 CLOSE cur_emp; 3) 举例 创建存储过程“get_count_by_limit_total_salary()”，声明IN参数 limit_total_salary，DOUBLE类型；声明 OUT参数total_count，INT类型。函数的功能可以实现累加薪资最高的几个员工的薪资值，直到薪资总和达到limit_total_salary参数的值，返回累加的人数给total_count。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 DELIMITER // CREATE PROCEDURE get_count_by_limit_total_salary(IN limit_total_salary DOUBLE, OUT total_count INT) BEGIN DECLARE sum_salary DOUBLE DEFAULT 0; # 记录累加的总工资 DECLARE cursor_salary DOUBLE DEFAULT 0; # 记录某一个工资值 DECLARE emp_count INT DEFAULT 0; # 记录循环个数 # 定义游标 DECLARE emp_cursor CURSOR FOR SELECT salary FROM employees ORDER BY salary DESC; # 打开游标 OPEN emp_cursor; REPEAT # 使用游标(从游标中获取数据) FETCH emp_cursor INTO cursor_salary; SET sum_salary = sum_salary + cursor_salary; SET emp_count = emp_count + 1; UNTIL sum_salary \u0026gt;= limit_total_salary END REPEAT; set total_count = emp_count; # 关闭游标 CLOSE emp_cursor; END // DELIMITER; 4) 小结 游标是 MySQL 的一个重要的功能，为 逐条读取 结果集中的数据，提供了完美的解决方案。跟在应用层面实现相同的功能相比，游标可以在存储程序中使用，效率高，程序也更加简洁。\n但同时也会带来一些性能问题，比如在使用游标的过程中，会对数据行进行 加锁 ，这样在业务并发量大 的时候，不仅会影响业务之间的效率，还会 消耗系统资源 ，造成内存不足，这是因为游标是在内存中进行的处理。\n建议：养成用完之后就关闭的习惯，这样才能提高系统的整体效率。\n补充：MySQL 8.0的新特性—全局变量的持久化 在MySQL数据库中，全局变量可以通过SET GLOBAL语句来设置。例如，设置服务器语句超时的限制，可以通过设置系统变量max_execution_time来实现：\n1 SET GLOBAL MAX_EXECUTION_TIME=2000; 使用SET GLOBAL语句设置的变量值只会临时生效 。 数据库重启后，服务器又会从MySQL配置文件中读取变量的默认值。 MySQL 8.0版本新增了 SET PERSIST 命令。例如，设置服务器的最大连接数为1000：\n1 SET PERSIST global max_connections = 1000; MySQL会将该命令的配置保存到数据目录下的 mysqld-auto.cnf文件中，下次启动时会读取该文件，用其中的配置来覆盖默认的配置文件。\n第17章_触发器 在实际开发中，我们经常会遇到这样的情况：有 2 个或者多个相互关联的表，如 商品信息 和 库存信息分别存放在 2 个不同的数据表中，我们在添加一条新商品记录的时候，为了保证数据的完整性，必须同时在库存表中添加一条库存记录。\n这样一来，我们就必须把这两个关联的操作步骤写到程序里面，而且要用事务包裹起来，确保这两个操作成为一个原子操作 ，要么全部执行，要么全部不执行。要是遇到特殊情况，可能还需要对数据进行手动维护，这样就很容易忘记其中的一步，导致数据缺失。\n这个时候，咱们可以使用触发器。你可以创建一个触发器，让商品信息数据的插入操作自动触发库存数据的插入操作。这样一来，就不用担心因为忘记添加库存数据而导致的数据缺失了。\n1. 触发器概述 触发器是由 事件来触发某个操作，这些事件包括 INSERT 、 UPDATE 、 DELETE 事件。所谓事件就是指用户的动作或者触发某项行为。如果定义了触发程序，当数据库执行这些语句时候，就相当于事件发生了，就会自动激发触发器执行相应的操作。\n当对数据表中的数据执行插入、更新和删除操作，需要自动执行一些数据库逻辑时，可以使用触发器来实现。\n2. 触发器的创建 1) 语法 1 2 3 4 CREATE TRIGGER 触发器名称 {BEFORE|AFTER} {INSERT|UPDATE|DELETE} ON 表名 FOR EACH ROW 触发器执行的语句块 说明：\n表名 ：表示触发器监控的对象。 BEFORE|AFTER ：表示触发的时间。BEFORE 表示在事件之前触发；AFTER 表示在事件之后触发。 INSERT|UPDATE|DELETE ：表示触发的事件。 INSERT 表示插入记录时触发； UPDATE 表示更新记录时触发； DELETE 表示删除记录时触发。 触发器执行的语句块 ：可以是单条SQL语句，也可以是由BEGIN…END结构组成的复合语句块。 2) 代码举例 举例1：\n创建数据表： 1 2 3 4 5 6 7 8 9 CREATE TABLE test_trigger ( id INT PRIMARY KEY AUTO_INCREMENT, t_note VARCHAR(30) ); CREATE TABLE test_trigger_log ( id INT PRIMARY KEY AUTO_INCREMENT, t_log VARCHAR(30) ); 创建触发器：创建名称为before_insert的触发器，向test_trigger数据表插入数据之前，向 test_trigger_log数据表中插入before_insert的日志信息。 1 2 3 4 5 6 7 8 9 DELIMITER // CREATE TRIGGER before_insert BEFORE INSERT ON test_trigger FOR EACH ROW BEGIN INSERT INTO test_trigger_log (t_log) VALUES(\u0026#39;before_insert\u0026#39;); END // DELIMITER ; 向test_trigger数据表中插入数据 1 INSERT INTO test_trigger (t_note) VALUES (\u0026#39;测试 BEFORE INSERT 触发器\u0026#39;); 查看test_trigger_log数据表中的数据 1 2 3 4 5 6 7 mysql\u0026gt; SELECT * FROM test_trigger_log; +----+---------------+ | id | t_log | +----+---------------+ | 1 | before_insert | +----+---------------+ 1 row in set (0.00 sec) 举例2：\n定义触发器“salary_check_trigger”，基于员工表“employees”的INSERT事件，在INSERT之前检查 将要添加的新员工薪资是否大于他领导的薪资，如果大于领导薪资，则报sqlstate_value为\u0026rsquo;HY000\u0026rsquo;的错误，从而使得添加失败。\n1 2 3 4 5 6 7 8 9 10 11 DELIMITER // CREATE TRIGGER salary_check_trigger BEFORE INSERT ON employees FOR EACH ROW BEGIN DECLARE mgrsalary DOUBLE; SELECT salary INTO mgrsalary FROM employees WHERE employee_id = NEW.manager_id; IF NEW.salary \u0026gt; mgrsalary THEN SIGNAL SQLSTATE \u0026#39;HY000\u0026#39; SET MESSAGE_TEXT = \u0026#39;薪资高于领导薪资错误\u0026#39;; END IF; END // DELIMITER ; 上面触发器声明过程中的NEW关键字代表INSERT添加语句的新记录。\n3. 查看、删除触发器 1) 查看触发器 查看触发器是查看数据库中已经存在的触发器的定义、状态和语法信息等。\n方式1：查看当前数据库的所有触发器的定义\n1 SHOW TRIGGERS\\G 方式2：查看当前数据库中某个触发器的定义\n1 SHOW CREATE TRIGGER 触发器名 方式3：从系统库information_schema的TRIGGERS表中查询“salary_check_trigger”触发器的信息。\n1 SELECT * FROM information_schema.TRIGGERS; 2) 删除触发器 触发器也是数据库对象，删除触发器也用DROP语句，语法格式如下：\n1 DROP TRIGGER IF EXISTS 触发器名称; 4. 触发器的优缺点 1) 优点 1、触发器可以确保数据的完整性。\n假设我们用进货单头表 （demo.importhead）来保存进货单的总体信息，包括进货单编号、供货商编号、仓库编号、总计进货数量、总计进货金额和验收日期。\nlistnumber (进货单编号) supplierid (进货商编号) stockid (参库编号) quantity (总计数量) importvalue (总计金额) confirmationdate （验收日期) 用进货单明细表 （demo.importdetails）来保存进货商品的明细，包括进货单编号、商品编号、进货数 量、进货价格和进货金额。\nlistnumber (进货单编号) itemnumber (商品编号) quantity (进货数量) importprice (进货价格) importvalue （进货金额) 每当我们录入、删除和修改一条进货单明细数据的时候，进货单明细表里的数据就会发生变动。这个时候，在进货单头表中的总计数量和总计金额就必须重新计算，否则，进货单头表中的总计数量和总计金 额就不等于进货单明细表中数量合计和金额合计了，这就是数据不一致。\n为了解决这个问题，我们就可以使用触发器，规定每当进货单明细表有数据插入、修改和删除的操作时，自动触发 2 步操作：\n1）重新计算进货单明细表中的数量合计和金额合计；\n2）用第一步中计算出来的值更新进货单头表中的合计数量与合计金额。\n这样一来，进货单头表中的合计数量与合计金额的值，就始终与进货单明细表中计算出来的合计数量与 合计金额的值相同，数据就是一致的，不会互相矛盾。\n2、触发器可以帮助我们记录操作日志。\n利用触发器，可以具体记录什么时间发生了什么。比如，记录修改会员储值金额的触发器，就是一个很好的例子。这对我们还原操作执行时的具体场景，更好地定位问题原因很有帮助。\n3、触发器还可以用在操作数据前，对数据进行合法性检查。\n比如，超市进货的时候，需要库管录入进货价格。但是，人为操作很容易犯错误，比如说在录入数量的时候，把条形码扫进去了；录入金额的时候，看串了行，录入的价格远超售价，导致账面上的巨亏…… 这些都可以通过触发器，在实际插入或者更新操作之前，对相应的数据进行检查，及时提示错误，防止错误数据进入系统。\n2) 缺点 1、触发器最大的一个问题就是可读性差。\n因为触发器存储在数据库中，并且由事件驱动，这就意味着触发器有可能不受应用层的控制 。这对系统维护是非常有挑战的。\n2、相关数据的变更，可能会导致触发器出错。\n特别是数据表结构的变更，都可能会导致触发器出错，进而影响数据操作的正常运行。这些都会由于触发器本身的隐蔽性，影响到应用中错误原因排查的效率。\n3) 注意点 注意，如果在子表中定义了外键约束，并且外键指定了ON UPDATE/DELETE CASCADE/SET NULL子句，此时修改父表被引用的键值或删除父表被引用的记录行时，也会引起子表的修改和删除操作，此时基于子表的UPDATE和DELETE语句定义的触发器并不会被激活。\n例如：基于子表员工表（t_employee）的DELETE语句定义了触发器t1，而子表的部门编号（did）字段定义了外键约束引用了父表部门表（t_department）的主键列部门编号（did），并且该外键加了“ON DELETE SET NULL”子句，那么如果此时删除父表部门表（t_department）在子表员工表（t_employee） 有匹配记录的部门记录时，会引起子表员工表（t_employee）匹配记录的部门编号（did）修改为NULL， mysql\u0026gt; update demo.membermaster set memberdeposit=20 where memberid = 2; ERROR 1054 (42S22): Unknown column \u0026lsquo;aa\u0026rsquo; in \u0026lsquo;field list\u0026rsquo; 但是此时不会激活触发器t1。只有直接对子表员工表（t_employee）执行DELETE语句时才会激活触发器 t1。\n第18章_MySQL8其他新特性 1. MySQL8新特性概述 MySQL从5.7版本直接跳跃发布了8.0版本 ，可见这是一个令人兴奋的里程碑版本。MySQL 8版本在功能上做了显著的改进与增强，开发者对MySQL的源代码进行了重构，最突出的一点是多MySQL Optimizer优化器进行了改进。不仅在速度上得到了改善，还为用户带来了更好的性能和更棒的体验。\n1) MySQL8.0 新增特性 更简便的NoSQL支持 NoSQL泛指非关系型数据库和数据存储。随着互联网平台的规模飞速发展，传统 的关系型数据库已经越来越不能满足需求。从5.6版本开始，MySQL就开始支持简单的NoSQL存储功能。 MySQL 8对这一功能做了优化，以更灵活的方式实现NoSQL功能，不再依赖模式（schema）。\n更好的索引 在查询中，正确地使用索引可以提高查询的效率。MySQL 8中新增了 隐藏索引 和 降序索引 。隐藏索引可以用来测试去掉索引对查询性能的影响。在查询中混合存在多列索引时，使用降序索引 可以提高查询的性能。\n更完善的JSON支持 MySQL从5.7开始支持原生JSON数据的存储，MySQL 8对这一功能做了优化，增加 了聚合函数 JSON_ARRAYAGG() 和 JSON_OBJECTAGG() ，将参数聚合为JSON数组或对象，新增了行内 操作符 -\u0026raquo;，是列路径运算符 -\u0026gt;的增强，对JSON排序做了提升，并优化了JSON的更新操作。\n安全和账户管理 MySQL 8中新增了 caching_sha2_password 授权插件、角色、密码历史记录和FIPS 模式支持，这些特性提高了数据库的安全性和性能，使数据库管理员能够更灵活地进行账户管理工作。\nInnoDB的变化 InnoDB是MySQL默认的存储引擎 ，是事务型数据库的首选引擎，支持事务安全表 （ACID），支持行锁定和外键。在MySQL 8 版本中，InnoDB在自增、索引、加密、死锁、共享锁等方面 做了大量的改进和优化 ，并且支持原子数据定义语言（DDL），提高了数据安全性，对事务提供更好的支持。\n数据字典 在之前的MySQL版本中，字典数据都存储在元数据文件和非事务表中。从MySQL 8开始新增了事务数据字典，在这个字典里存储着数据库对象信息，这些数据字典存储在内部事务表中。\n原子数据定义语句 MySQL 8开始支持原子数据定义语句（Automic DDL），即 原子DDL 。目前，只有 InnoDB存储引擎支持原子DDL。原子数据定义语句（DDL）将与DDL操作相关的数据字典更新、存储引擎 操作、二进制日志写入结合到一个单独的原子事务中，这使得即使服务器崩溃，事务也会提交或回滚。 使用支持原子操作的存储引擎所创建的表，在执行DROP TABLE、CREATE TABLE、ALTER TABLE、 RENAME TABLE、TRUNCATE TABLE、CREATE TABLESPACE、DROP TABLESPACE等操作时，都支持原子操 作，即事务要么完全操作成功，要么失败后回滚，不再进行部分提交。 对于从MySQL 5.7复制到MySQL 8 版本中的语句，可以添加 IF EXISTS 或 IF NOT EXISTS 语句来避免发生错误。\n资源管理 MySQL 8开始支持创建和管理资源组，允许将服务器内运行的线程分配给特定的分组，以便 线程根据组内可用资源执行。组属性能够控制组内资源，启用或限制组内资源消耗。数据库管理员能够 根据不同的工作负载适当地更改这些属性。 目前，CPU时间是可控资源，由“虚拟CPU”这个概念来表 示，此术语包含CPU的核心数，超线程，硬件线程等等。服务器在启动时确定可用的虚拟CPU数量。拥有 对应权限的数据库管理员可以将这些CPU与资源组关联，并为资源组分配线程。 资源组组件为MySQL中的资源组管理提供了SQL接口。资源组的属性用于定义资源组。MySQL中存在两个默认组，系统组和用户 组，默认的组不能被删除，其属性也不能被更改。对于用户自定义的组，资源组创建时可初始化所有的 属性，除去名字和类型，其他属性都可在创建之后进行更改。 在一些平台下，或进行了某些MySQL的配 置时，资源管理的功能将受到限制，甚至不可用。例如，如果安装了线程池插件，或者使用的是macOS 系统，资源管理将处于不可用状态。在FreeBSD和Solaris系统中，资源线程优先级将失效。在Linux系统 中，只有配置了CAP_SYS_NICE属性，资源管理优先级才能发挥作用。\n字符集支持 MySQL 8中默认的字符集由 latin1 更改为 utf8mb4 ，并首次增加了日语所特定使用的集合，utf8mb4_ja_0900_as_cs。\n优化器增强 MySQL优化器开始支持隐藏索引和降序索引。隐藏索引不会被优化器使用，验证索引的必 要性时不需要删除索引，先将索引隐藏，如果优化器性能无影响就可以真正地删除索引。降序索引允许 优化器对多个列进行排序，并且允许排序顺序不一致。\n公用表表达式 (Common Table Expressions）简称为CTE，MySQL现在支持递归和非递归两种形式的CTE。CTE通过在SELECT语句或其他特定语句前使用WITH语句对临时结果集进行命名。\n基础语法如下：\n1 2 WITH cte_name (col_name1,col_name2 ...) AS (Subquery) SELECT * FROM cte_name; ​\tSubquery代表子查询，子查询前使用WITH语句将结果集命名为cte_name，在后续的查询中即可使用 cte_name进行查询。\n窗口函数 MySQL 8开始支持窗口函数。在之前的版本中已存在的大部分 聚合函数 在MySQL 8中也可以作为窗口函数来使用。 正则表达式支持 MySQL在8.0.4以后的版本中采用支持Unicode的国际化组件库实现正则表达式操作， 这种方式不仅能提供完全的Unicode支持，而且是多字节安全编码。MySQL增加了REGEXP_LIKE()、 EGEXP_INSTR()、REGEXP_REPLACE()和 REGEXP_SUBSTR()等函数来提升性能。另外，regexp_stack_limit和 regexp_time_limit 系统变量能够通过匹配引擎来控制资源消耗。 内部临时表 TempTable存储引擎取代MEMORY存储引擎成为内部临时表的默认存储引擎 。TempTable存储 引擎为VARCHAR和VARBINARY列提供高效存储。internal_tmp_mem_storage_engine会话变量定义了内部 临时表的存储引擎，可选的值有两个，TempTable和MEMORY，其中TempTable为默认的存储引擎。 temptable_max_ram系统配置项定义了TempTable存储引擎可使用的最大内存数量。 日志记录 在MySQL 8中错误日志子系统由一系列MySQL组件构成。这些组件的构成由系统变量 log_error_services来配置，能够实现日志事件的过滤和写入。 WITH cte_name (col_name1,col_name2 \u0026hellip;) AS (Subquery) SELECT * FROM cte_name; 备份锁 新的备份锁允许在线备份期间执行数据操作语句，同时阻止可能造成快照不一致的操作。新 备份锁由 LOCK INSTANCE FOR BACKUP 和 UNLOCK INSTANCE 语法提供支持，执行这些操作需要备份管理 员特权。 增强的MySQL复制 MySQL 8复制支持对 JSON文档 进行部分更新的二进制日志记录 ，该记录 使用紧凑 的二进制格式 ，从而节省记录完整JSON文档的空间。当使用基于语句的日志记录时，这种紧凑的日志记 录会自动完成，并且可以通过将新的binlog_row_value_options系统变量值设置为PARTIAL_JSON来启用。 2) MySQL8.0 移除的旧特性 在MySQL 5.7版本上开发的应用程序如果使用了MySQL8.0 移除的特性，语句可能会失败，或者产生不同 的执行结果。为了避免这些问题，对于使用了移除特性的应用，应当尽力修正避免使用这些特性，并尽 可能使用替代方法。\n查询缓存 查询缓存已被移除 ，删除的项有： （1）语句：FLUSH QUERY CACHE和RESET QUERY CACHE。 （2）系统变量：query_cache_limit、query_cache_min_res_unit、query_cache_size、 query_cache_type、query_cache_wlock_invalidate。 （3）状态变量：Qcache_free_blocks、 Qcache_free_memory、Qcache_hits、Qcache_inserts、Qcache_lowmem_prunes、Qcache_not_cached、 Qcache_queries_in_cache、Qcache_total_blocks。 （4）线程状态：checking privileges on cached query、checking query cache for query、invalidating query cache entries、sending cached result to client、storing result in query cache、waiting for query cache lock。 加密相关 删除的加密相关的内容有：ENCODE()、DECODE()、ENCRYPT()、DES_ENCRYPT()和 DES_DECRYPT()函数，配置项des-key-file，系统变量have_crypt，FLUSH语句的DES_KEY_FILE选项， HAVE_CRYPT CMake选项。 对于移除的ENCRYPT()函数，考虑使用SHA2()替代，对于其他移除的函数，使 用AES_ENCRYPT()和AES_DECRYPT()替代。 空间函数相关 在MySQL 5.7版本中，多个空间函数已被标记为过时。这些过时函数在MySQL 8中都已被 移除，只保留了对应的ST_和MBR函数。 \\N和NULL 在SQL语句中，解析器不再将\\N视为NULL，所以在SQL语句中应使用NULL代替\\N。这项变化 不会影响使用LOAD DATA INFILE或者SELECT\u0026hellip;INTO OUTFILE操作文件的导入和导出。在这类操作中，NULL 仍等同于\\N。 mysql_install_db 在MySQL分布中，已移除了mysql_install_db程序，数据字典初始化需要调用带着\u0026ndash; initialize或者\u0026ndash;initialize-insecure选项的mysqld来代替实现。另外，\u0026ndash;bootstrap和INSTALL_SCRIPTDIR CMake也已被删除。 通用分区处理程序 通用分区处理程序已从MySQL服务中被移除。为了实现给定表分区，表所使用的存 储引擎需要自有的分区处理程序。 提供本地分区支持的MySQL存储引擎有两个，即InnoDB和NDB，而在 MySQL 8中只支持InnoDB。 系统和状态变量信息 在INFORMATION_SCHEMA数据库中，对系统和状态变量信息不再进行维护。 GLOBAL_VARIABLES、SESSION_VARIABLES、GLOBAL_STATUS、SESSION_STATUS表都已被删除。另外，系 统变量show_compatibility_56也已被删除。被删除的状态变量有Slave_heartbeat_period、 Slave_last_heartbeat,Slave_received_heartbeats、Slave_retried_transactions、Slave_running。以上被删除 的内容都可使用性能模式中对应的内容进行替代。 mysql_plugin工具 mysql_plugin工具用来配置MySQL服务器插件，现已被删除，可使用\u0026ndash;plugin-load或- -plugin-load-add选项在服务器启动时加载插件或者在运行时使用INSTALL PLUGIN语句加载插件来替代该 工具。 2. 新特性1：窗口函数 1) 使用窗口函数前后对比 假设我现在有这样一个数据表，它显示了某购物网站在每个城市每个区的销售额：\n1 2 3 4 5 6 7 8 9 10 11 12 CREATE TABLE sales( id INT PRIMARY KEY AUTO_INCREMENT, city VARCHAR(15), county VARCHAR(15), sales_value DECIMAL ); INSERT INTO sales(city,county,sales_value) VALUES (\u0026#39;北京\u0026#39;,\u0026#39;海淀\u0026#39;,10.00), (\u0026#39;北京\u0026#39;,\u0026#39;朝阳\u0026#39;,20.00), (\u0026#39;上海\u0026#39;,\u0026#39;黄埔\u0026#39;,30.00), (\u0026#39;上海\u0026#39;,\u0026#39;长宁\u0026#39;,10.00); 查询：\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; SELECT * FROM sales; +----+------+--------+-------------+ | id | city | county | sales_value | +----+------+--------+-------------+ | 1 | 北京 | 海淀 | 10 | | 2 | 北京 | 朝阳 | 20 | | 3 | 上海 | 黄埔 | 30 | | 4 | 上海 | 长宁 | 10 | +----+------+--------+-------------+ 4 rows in set (0.00 sec) 需求：现在计算这个网站在每个城市的销售总额、在全国的销售总额、每个区的销售额占所在城市销售额中的比率，以及占总销售额中的比率。\n如果用分组和聚合函数，就需要分好几步来计算。\n第一步，计算总销售金额，并存入临时表 a：\n1 2 3 CREATE TEMPORARY TABLE a -- 创建临时表 SELECT SUM(sales_value) AS sales_value -- 计算总计金额 FROM sales; 查看一下临时表 a ：\n1 2 3 4 5 6 7 mysql\u0026gt; SELECT * FROM a; +-------------+ | sales_value | +-------------+ | 70 | +-------------+ 1 row in set (0.00 sec) 第二步，计算每个城市的销售总额并存入临时表 b：\n1 2 3 4 CREATE TEMPORARY TABLE b -- 创建临时表 SELECT city, SUM(sales_value) AS sales_value -- 计算城市销售合计 FROM sales GROUP BY city; 查看临时表 b ：\n1 2 3 4 5 6 7 8 mysql\u0026gt; SELECT * FROM b; +------+-------------+ | city | sales_value | +------+-------------+ | 北京 | 30 | | 上海 | 40 | +------+-------------+ 2 rows in set (0.00 sec) 第三步，计算各区的销售占所在城市的总计金额的比例，和占全部销售总计金额的比例。我们可以通过下面的连接查询获得需要的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 mysql\u0026gt; SELECT s.city AS 城市,s.county AS 区,s.sales_value AS 区销售额, -\u0026gt; b.sales_value AS 市销售额,s.sales_value/b.sales_value AS 市比率, -\u0026gt; a.sales_value AS 总销售额,s.sales_value/a.sales_value AS 总比率 -\u0026gt; FROM sales s -\u0026gt; JOIN b ON (s.city=b.city) -- 连接市统计结果临时表 -\u0026gt; JOIN a -- 连接总计金额临时表 -\u0026gt; ORDER BY s.city,s.county; +------+------+----------+----------+--------+----------+--------+ | 城市 | 区 | 区销售额 | 市销售额 | 市比率 | 总销售额 | 总比率 | +------+------+----------+----------+--------+----------+--------+ | 上海 | 长宁 | 10 | 40 | 0.2500 | 70 | 0.1429 | | 上海 | 黄埔 | 30 | 40 | 0.7500 | 70 | 0.4286 | | 北京 | 朝阳 | 20 | 30 | 0.6667 | 70 | 0.2857 | | 北京 | 海淀 | 10 | 30 | 0.3333 | 70 | 0.1429 | +------+------+----------+----------+--------+----------+--------+ 4 rows in set (0.00 sec) 结果显示：市销售金额、市销售占比、总销售金额、总销售占比都计算出来了。\n同样的查询，如果用窗口函数，就简单多了。我们可以用下面的代码来实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 mysql\u0026gt; SELECT city AS 城市,county AS 区,sales_value AS 区销售额, -\u0026gt; SUM(sales_value) OVER(PARTITION BY city) AS 市销售额, -- 计算市销售额 -\u0026gt; sales_value/SUM(sales_value) OVER(PARTITION BY city) AS 市比率, -\u0026gt; SUM(sales_value) OVER() AS 总销售额, -- 计算总销售额 -\u0026gt; sales_value/SUM(sales_value) OVER() AS 总比率 -\u0026gt; FROM sales -\u0026gt; ORDER BY city,county; +------+------+----------+----------+--------+----------+--------+ | 城市 | 区 | 区销售额 | 市销售额 | 市比率 | 总销售额 | 总比率 | +------+------+----------+----------+--------+----------+--------+ | 上海 | 长宁 | 10 | 40 | 0.2500 | 70 | 0.1429 | | 上海 | 黄埔 | 30 | 40 | 0.7500 | 70 | 0.4286 | | 北京 | 朝阳 | 20 | 30 | 0.6667 | 70 | 0.2857 | | 北京 | 海淀 | 10 | 30 | 0.3333 | 70 | 0.1429 | +------+------+----------+-----------+--------+----------+--------+ 4 rows in set (0.00 sec) 结果显示，我们得到了与上面那种查询同样的结果。\n使用窗口函数，只用了一步就完成了查询。而且，由于没有用到临时表，执行的效率也更高了。很显然，在这种需要用到分组统计的结果对每一条记录进行计算的场景下，使用窗口函数更好。\n2) 窗口函数分类 MySQL从8.0版本开始支持窗口函数。窗口函数的作用类似于在查询中对数据进行分组，不同的是，分组操作会把分组的结果聚合成一条记录，而窗口函数是将结果置于每一条数据记录中。\n窗口函数可以分为 静态窗口函数 和 动态窗口函数 。\n静态窗口函数的窗口大小是固定的，不会因为记录的不同而不同； 动态窗口函数的窗口大小会随着记录的不同而变化。 MySQL官方网站窗口函数的网址为https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptio ns.html#function_row-number。\n窗口函数总体上可以分为序号函数、分布函数、前后函数、首尾函数和其他函数，如下表：\n3) 语法结构 窗口函数的语法结构是：\n1 函数 OVER（[PARTITION BY 字段名 ORDER BY 字段名 ASC|DESC]） 或者是：\n1 函数 OVER 窗口名 … WINDOW 窗口名 AS （[PARTITION BY 字段名 ORDER BY 字段名 ASC|DESC]） OVER 关键字指定函数窗口的范围。 如果省略后面括号中的内容，则窗口会包含满足WHERE条件的所有记录，窗口函数会基于所有满足WHERE条件的记录进行计算。 如果OVER关键字后面的括号不为空，则可以使用如下语法设置窗口。 窗口名：为窗口设置一个别名，用来标识窗口。 PARTITION BY子句：指定窗口函数按照哪些字段进行分组。分组后，窗口函数可以在每个分组中分别执行。 ORDER BY子句：指定窗口函数按照哪些字段进行排序。执行排序操作使窗口函数按照排序后的数据记录的顺序进行编号。 FRAME子句：为分区中的某个子集定义规则，可以用来作为滑动窗口使用。 4) 分类讲解 创建表：\n1 2 3 4 5 6 7 8 9 CREATE TABLE goods( id INT PRIMARY KEY AUTO_INCREMENT, category_id INT, category VARCHAR(15), NAME VARCHAR(30), price DECIMAL(10,2), stock INT, upper_time DATETIME ); 添加数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 INSERT INTO goods(category_id,category,NAME,price,stock,upper_time) VALUES (1, \u0026#39;女装/女士精品\u0026#39;, \u0026#39;T恤\u0026#39;, 39.90, 1000, \u0026#39;2020-11-10 00:00:00\u0026#39;), (1, \u0026#39;女装/女士精品\u0026#39;, \u0026#39;连衣裙\u0026#39;, 79.90, 2500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (1, \u0026#39;女装/女士精品\u0026#39;, \u0026#39;卫衣\u0026#39;, 89.90, 1500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (1, \u0026#39;女装/女士精品\u0026#39;, \u0026#39;牛仔裤\u0026#39;, 89.90, 3500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (1, \u0026#39;女装/女士精品\u0026#39;, \u0026#39;百褶裙\u0026#39;, 29.90, 500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (1, \u0026#39;女装/女士精品\u0026#39;, \u0026#39;呢绒外套\u0026#39;, 399.90, 1200, \u0026#39;2020-11-10 00:00:00\u0026#39;), (2, \u0026#39;户外运动\u0026#39;, \u0026#39;自行车\u0026#39;, 399.90, 1000, \u0026#39;2020-11-10 00:00:00\u0026#39;), (2, \u0026#39;户外运动\u0026#39;, \u0026#39;山地自行车\u0026#39;, 1399.90, 2500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (2, \u0026#39;户外运动\u0026#39;, \u0026#39;登山杖\u0026#39;, 59.90, 1500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (2, \u0026#39;户外运动\u0026#39;, \u0026#39;骑行装备\u0026#39;, 399.90, 3500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (2, \u0026#39;户外运动\u0026#39;, \u0026#39;运动外套\u0026#39;, 799.90, 500, \u0026#39;2020-11-10 00:00:00\u0026#39;), (2, \u0026#39;户外运动\u0026#39;, \u0026#39;滑板\u0026#39;, 499.90, 1200, \u0026#39;2020-11-10 00:00:00\u0026#39;); 下面针对goods表中的数据来验证每个窗口函数的功能。\n1) 序号函数 1. ROW_NUMBER()函数\nROW_NUMBER()函数能够对数据中的序号进行顺序显示。\n举例：查询 goods 数据表中每个商品分类下价格降序排列的各个商品信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; SELECT ROW_NUMBER() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, id, category_id, category, NAME, price, stock FROM goods; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 3 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 4 | 2 | 1 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | | 5 | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | | 6 | 5 | 1 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | | 1 | 8 | 2 | 户外运动 | 山地自行车 | 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | | 4 | 7 | 2 | 户外运动 | 自行车 | 399.90 | 1000 | | 5 | 10 | 2 | 户外运动 | 骑行装备 | 399.90 | 3500 | | 6 | 9 | 2 | 户外运动 | 登山杖 | 59.90 | 1500 | +---------+----+-------------+---------------+------------+---------+-------+ 12 rows in set (0.00 sec) 举例：查询 goods 数据表中每个商品分类下价格最高的3种商品信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 mysql\u0026gt; SELECT * -\u0026gt; FROM ( -\u0026gt; SELECT ROW_NUMBER() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, -\u0026gt; id, category_id, category, NAME, price, stock -\u0026gt; FROM goods) t -\u0026gt; WHERE row_num \u0026lt;= 3; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 3 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 1 | 8 | 2 | 户外运动 | 山地自行车 | 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | +---------+----+-------------+---------------+------------+----------+-------+ 6 rows in set (0.00 sec) 在名称为“女装/女士精品”的商品类别中，有两款商品的价格为89.90元，分别是卫衣和牛仔裤。两款商品 的序号都应该为2，而不是一个为2，另一个为3。此时，可以使用RANK()函数和DENSE_RANK()函数解 决。\n2．RANK()函数\n使用RANK()函数能够对序号进行并列排序，并且会跳过重复的序号，比如序号为1、1、3。\n举例：使用RANK()函数获取 goods 数据表中各类别的价格从高到低排序的各商品信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; SELECT RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, -\u0026gt; id, category_id, category, NAME, price, stock -\u0026gt; FROM goods; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 2 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 4 | 2 | 1 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | | 5 | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | | 6 | 5 | 1 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | | 1 | 8 | 2 | 户外运动 | 山地自行车 | 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | | 4 | 7 | 2 | 户外运动 | 自行车 | 399.90 | 1000 | | 4 | 10 | 2 | 户外运动 | 骑行装备 | 399.90 | 3500 | | 6 | 9 | 2 | 户外运动 | 登山杖 | 59.90 | 1500 | +---------+----+-------------+---------------+------------+---------+-------+ 12 rows in set (0.00 sec) 3．DENSE_RANK()函数\nDENSE_RANK()函数对序号进行并列排序，并且不会跳过重复的序号，比如序号为1、1、2。 举例：使用DENSE_RANK()函数获取 goods 数据表中各类别的价格从高到低排序的各商品信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mysql\u0026gt; SELECT DENSE_RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, -\u0026gt; id, category_id, category, NAME, price, stock -\u0026gt; FROM goods; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 2 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 3 | 2 | 1 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | | 4 | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | | 5 | 5 | 1 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | | 1 | 8 | 2 | 户外运动 | 山地自行车| 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | | 4 | 7 | 2 | 户外运动 | 自行车 | 399.90 | 1000 | | 4 | 10 | 2 | 户外运动 | 骑行装备 | 399.90 | 3500 | | 5 | 9 | 2 | 户外运动 | 登山杖 | 59.90 | 1500 | +---------+----+-------------+---------------+------------+---------+-------+ 12 rows in set (0.00 sec) 2) 分布函数 1．PERCENT_RANK()函数\nPERCENT_RANK()函数是等级值百分比函数。按照如下方式进行计算。\n1 (rank - 1) / (rows - 1) 其中，rank的值为使用RANK()函数产生的序号，rows的值为当前窗口的总记录数。\n举例：计算 goods 数据表中名称为“女装/女士精品”的类别下的商品的PERCENT_RANK值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #写法一： SELECT RANK() OVER (PARTITION BY category_id ORDER BY price DESC) AS r, PERCENT_RANK() OVER (PARTITION BY category_id ORDER BY price DESC) AS pr, id, category_id, category, NAME, price, stock FROM goods WHERE category_id = 1; #写法二： mysql\u0026gt; SELECT RANK() OVER w AS r, -\u0026gt; PERCENT_RANK() OVER w AS pr, -\u0026gt; id, category_id, category, NAME, price, stock -\u0026gt; FROM goods -\u0026gt; WHERE category_id = 1 WINDOW w AS (PARTITION BY category_id ORDER BY price DESC); +---+-----+----+-------------+---------------+----------+--------+-------+ | r | pr | id | category_id | category | NAME | price | stock | +---+-----+----+-------------+---------------+----------+--------+-------+ | 1 | 0 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 0.2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 2 | 0.2 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 4 | 0.6 | 2 | 1 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | | 5 | 0.8 | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | | 6 | 1 | 5 | 1 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | +---+-----+----+-------------+---------------+----------+--------+-------+ 6 rows in set (0.00 sec) 2．CUME_DIST()函数\nCUME_DIST()函数主要用于查询小于或等于某个值的比例。\n举例：查询goods数据表中小于或等于当前价格的比例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; SELECT CUME_DIST() OVER(PARTITION BY category_id ORDER BY price ASC) AS cd, -\u0026gt; id, category, NAME, price -\u0026gt; FROM goods; +---------------------+----+---------------+------------+---------+ | cd | id | category | NAME | price | +---------------------+----+---------------+------------+---------+ | 0.16666666666666666 | 5 | 女装/女士精品 | 百褶裙 | 29.90 | | 0.3333333333333333 | 1 | 女装/女士精品 | T恤 | 39.90 | | 0.5 | 2 | 女装/女士精品 | 连衣裙 | 79.90 | | 0.8333333333333334 | 3 | 女装/女士精品 | 卫衣 | 89.90 | | 0.8333333333333334 | 4 | 女装/女士精品 | 牛仔裤 | 89.90 | | 1 | 6 | 女装/女士精品 | 呢绒外套 | 399.90 | | 0.16666666666666666 | 9 | 户外运动 | 登山杖 | 59.90 | | 0.5 | 7 | 户外运动 | 自行车 | 399.90 | | 0.5 | 10 | 户外运动 | 骑行装备 | 399.90 | | 0.6666666666666666 | 12 | 户外运动 | 滑板 | 499.90 | | 0.8333333333333334 | 11 | 户外运动 | 运动外套 | 799.90 | | 1 | 8 | 户外运动 | 山地自行车 | 1399.90 | +---------------------+----+---------------+------------+---------+ 12 rows in set (0.00 sec) 3) 前后函数 1．LAG(expr,n)函数\nLAG(expr,n)函数返回当前行的前n行的expr的值。\n举例：查询goods数据表中前一个商品价格与当前商品价格的差值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 mysql\u0026gt; SELECT id, category, NAME, price, pre_price, price - pre_price AS diff_price -\u0026gt; FROM ( -\u0026gt; SELECT id, category, NAME, price,LAG(price,1) OVER w AS pre_price -\u0026gt; FROM goods -\u0026gt; WINDOW w AS (PARTITION BY category_id ORDER BY price)) t; +----+---------------+------------+---------+-----------+------------+ | id | category | NAME | price | pre_price | diff_price | +----+---------------+------------+---------+-----------+------------+ | 5 | 女装/女士精品 | 百褶裙 | 29.90 | NULL | NULL | | 1 | 女装/女士精品 | T恤 | 39.90 | 29.90 | 10.00 | | 2 | 女装/女士精品 | 连衣裙 | 79.90 | 39.90 | 40.00 | | 3 | 女装/女士精品 | 卫衣 | 89.90 | 79.90 | 10.00 | | 4 | 女装/女士精品 | 牛仔裤 | 89.90 | 89.90 | 0.00 | | 6 | 女装/女士精品 | 呢绒外套 | 399.90 | 89.90 | 310.00 | | 9 | 户外运动 | 登山杖 | 59.90 | NULL | NULL | | 7 | 户外运动 | 自行车 | 399.90 | 59.90 | 340.00 | | 10 | 户外运动 | 骑行装备 | 399.90 | 399.90 | 0.00 | | 12 | 户外运动 | 滑板 | 499.90 | 399.90 | 100.00 | | 11 | 户外运动 | 运动外套 | 799.90 | 499.90 | 300.00 | | 8 | 户外运动 | 山地自行车 | 1399.90 | 799.90 | 600.00 | +----+---------------+------------+---------+-----------+------------+ 12 rows in set (0.00 sec) 2．LEAD(expr,n)函数\nLEAD(expr,n)函数返回当前行的后n行的expr的值。\n举例：查询goods数据表中后一个商品价格与当前商品价格的差值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 mysql\u0026gt; SELECT id, category, NAME, behind_price, price,behind_price - price AS diff_price -\u0026gt; FROM( -\u0026gt; SELECT id, category, NAME, price,LEAD(price, 1) OVER w AS behind_price -\u0026gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price)) t; +----+---------------+------------+--------------+---------+------------+ | id | category | NAME | behind_price | price | diff_price | +----+---------------+------------+--------------+---------+------------+ | 5 | 女装/女士精品 | 百褶裙 | 39.90 | 29.90 | 10.00 | | 1 | 女装/女士精品 | T恤 | 79.90 | 39.90 | 40.00 | | 2 | 女装/女士精品 | 连衣裙 | 89.90 | 79.90 | 10.00 | | 3 | 女装/女士精品 | 卫衣 | 89.90 | 89.90 | 0.00 | | 4 | 女装/女士精品 | 牛仔裤 | 399.90 | 89.90 | 310.00 | | 6 | 女装/女士精品 | 呢绒外套 | NULL | 399.90 | NULL | | 9 | 户外运动 | 登山杖 | 399.90 | 59.90 | 340.00 | | 7 | 户外运动 | 自行车 | 399.90 | 399.90 | 0.00 | | 10 | 户外运动 | 骑行装备 | 499.90 | 399.90 | 100.00 | | 12 | 户外运动 | 滑板 | 799.90 | 499.90 | 300.00 | | 11 | 户外运动 | 运动外套 | 1399.90 | 799.90 | 600.00 | | 8 | 户外运动 | 山地自行车 | NULL | 1399.90 | NULL | +----+---------------+------------+--------------+---------+------------+ 12 rows in set (0.00 sec) 4) 首尾函数 1．FIRST_VALUE(expr)函数\nFIRST_VALUE(expr)函数返回第一个expr的值。\n举例：按照价格排序，查询第1个商品的价格信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; SELECT id, category, NAME, price, stock,FIRST_VALUE(price) OVER w AS first_price -\u0026gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price); +----+---------------+------------+---------+-------+-------------+ | id | category | NAME | price | stock | first_price | +----+---------------+------------+---------+-------+-------------+ | 5 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | 29.90 | | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | 29.90 | | 2 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | 29.90 | | 3 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | 29.90 | | 4 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | 29.90 | | 6 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | 29.90 | | 9 | 户外运动 | 登山杖 | 59.90 | 1500 | 59.90 | | 7 | 户外运动 | 自行车 | 399.90 | 1000 | 59.90 | | 10 | 户外运动 | 骑行装备 | 399.90 | 3500 | 59.90 | | 12 | 户外运动 | 滑板 | 499.90 | 1200 | 59.90 | | 11 | 户外运动 | 运动外套 | 799.90 | 500 | 59.90 | | 8 | 户外运动 | 山地自行车 | 1399.90 | 2500 | 59.90 | +----+---------------+------------+---------+-------+-------------+ 12 rows in set (0.00 sec) LAST_VALUE(expr)函数\nLAST_VALUE(expr)函数返回最后一个expr的值。\n举例：按照价格排序，查询最后一个商品的价格信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mysql\u0026gt; SELECT id, category, NAME, price, stock,LAST_VALUE(price) OVER w AS last_price -\u0026gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price); +----+---------------+------------+---------+-------+------------+ | id | category | NAME | price | stock | last_price | +----+---------------+------------+---------+-------+------------+ | 5 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | 29.90 | | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | 39.90 | | 2 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | 79.90 | | 3 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | 89.90 | | 4 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | 89.90 | | 6 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | 399.90 | | 9 | 户外运动 | 登山杖 | 59.90 | 1500 | 59.90 | | 7 | 户外运动 | 自行车 | 399.90 | 1000 | 399.90 | | 10 | 户外运动 | 骑行装备 | 399.90 | 3500 | 399.90 | | 12 | 户外运动 | 滑板 | 499.90 | 1200 | 499.90 | | 11 | 户外运动 | 运动外套 | 799.90 | 500 | 799.90 | | 8 | 户外运动 | 山地自行车 | 1399.90 | 2500 | 1399.90 | +----+---------------+------------+---------+-------+------------+ 12 rows in set (0.00 sec) 5) 其他函数 1．NTH_VALUE(expr,n)函数\nNTH_VALUE(expr,n)函数返回第n个expr的值。 举例：查询goods数据表中排名第2和第3的价格信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mysql\u0026gt; SELECT id, category, NAME, price,NTH_VALUE(price,2) OVER w AS second_price, -\u0026gt; NTH_VALUE(price,3) OVER w AS third_price -\u0026gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price); +----+---------------+------------+---------+--------------+-------------+ | id | category | NAME | price | second_price | third_price | +----+---------------+------------+---------+--------------+-------------+ | 5 | 女装/女士精品 | 百褶裙 | 29.90 | NULL | NULL | | 1 | 女装/女士精品 | T恤 | 39.90 | 39.90 | NULL | | 2 | 女装/女士精品 | 连衣裙 | 79.90 | 39.90 | 79.90 | | 3 | 女装/女士精品 | 卫衣 | 89.90 | 39.90 | 79.90 | | 4 | 女装/女士精品 | 牛仔裤 | 89.90 | 39.90 | 79.90 | | 6 | 女装/女士精品 | 呢绒外套 | 399.90 | 39.90 | 79.90 | | 9 | 户外运动 | 登山杖 | 59.90 | NULL | NULL | | 7 | 户外运动 | 自行车 | 399.90 | 399.90 | 399.90 | | 10 | 户外运动 | 骑行装备 | 399.90 | 399.90 | 399.90 | | 12 | 户外运动 | 滑板 | 499.90 | 399.90 | 399.90 | | 11 | 户外运动 | 运动外套 | 799.90 | 399.90 | 399.90 | | 8 | 户外运动 | 山地自行车 | 1399.90 | 399.90 | 399.90 | +----+---------------+------------+---------+--------------+-------------+ 12 rows in set (0.00 sec) 2．NTILE(n)函数\nNTILE(n)函数将分区中的有序数据分为n个桶，记录桶编号。\n举例：将goods表中的商品按照价格分为3组。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mysql\u0026gt; SELECT NTILE(3) OVER w AS nt,id, category, NAME, price -\u0026gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price); +----+----+---------------+------------+---------+ | nt | id | category | NAME | price | +----+----+---------------+------------+---------+ | 1 | 5 | 女装/女士精品 | 百褶裙 | 29.90 | | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | | 2 | 2 | 女装/女士精品 | 连衣裙 | 79.90 | | 2 | 3 | 女装/女士精品 | 卫衣 | 89.90 | | 3 | 4 | 女装/女士精品 | 牛仔裤 | 89.90 | | 3 | 6 | 女装/女士精品 | 呢绒外套 | 399.90 | | 1 | 9 | 户外运动 | 登山杖 | 59.90 | | 1 | 7 | 户外运动 | 自行车 | 399.90 | | 2 | 10 | 户外运动 | 骑行装备 | 399.90 | | 2 | 12 | 户外运动 | 滑板 | 499.90 | | 3 | 11 | 户外运动 | 运动外套 | 799.90 | | 3 | 8 | 户外运动 | 山地自行车 | 1399.90 | +----+----+---------------+------------+---------+ 12 rows in set (0.00 sec) 5) 小结 窗口函数的特点是可以分组，而且可以在分组内排序。另外，窗口函数不会因为分组而减少原表中的行 数，这对我们在原表数据的基础上进行统计和排序非常有用。\n3. 新特性2：公用表表达式 公用表表达式（或通用表表达式）简称为CTE（Common Table Expressions）。CTE是一个命名的临时结果集，作用范围是当前语句。CTE可以理解成一个可以复用的子查询，当然跟子查询还是有点区别的， CTE可以引用其他CTE，但子查询不能引用其他子查询。所以，可以考虑代替子查询。\n依据语法结构和执行方式的不同，公用表表达式分为 普通公用表表达式 和 递归公用表表达式 2 种。\n1) 普通公用表表达式 普通公用表表达式的语法结构是：\n1 2 3 WITH CTE名称 AS （子查询） SELECT|DELETE|UPDATE 语句; 普通公用表表达式类似于子查询，不过，跟子查询不同的是，它可以被多次引用，而且可以被其他的普 通公用表表达式所引用。\n举例：查询员工所在的部门的详细信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mysql\u0026gt; SELECT * FROM departments -\u0026gt; WHERE department_id IN ( -\u0026gt; SELECT DISTINCT department_id -\u0026gt; FROM employees -\u0026gt; ); +---------------+------------------+------------+-------------+ | department_id | department_name | manager_id | location_id | +---------------+------------------+------------+-------------+ | 10 | Administration | 200 | 1700 | | 20 | Marketing | 201 | 1800 | | 30 | Purchasing | 114 | 1700 | | 40 | Human Resources | 203 | 2400 | | 50 | Shipping | 121 | 1500 | | 60 | IT | 103 | 1400 | | 70 | Public Relations | 204 | 2700 | | 80 | Sales | 145 | 2500 | | 90 | Executive | 100 | 1700 | | 100 | Finance | 108 | 1700 | | 110 | Accounting | 205 | 1700 | +---------------+------------------+------------+-------------+ 11 rows in set (0.00 sec) 这个查询也可以用普通公用表表达式的方式完成：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mysql\u0026gt; WITH emp_dept_id -\u0026gt; AS (SELECT DISTINCT department_id FROM employees) -\u0026gt; SELECT * -\u0026gt; FROM departments d JOIN emp_dept_id e -\u0026gt; ON d.department_id = e.department_id; +---------------+------------------+------------+-------------+---------------+ | department_id | department_name | manager_id | location_id | department_id | +---------------+------------------+------------+-------------+---------------+ | 90 | Executive | 100 | 1700 | 90 | | 60 | IT | 103 | 1400 | 60 | | 100 | Finance | 108 | 1700 | 100 | | 30 | Purchasing | 114 | 1700 | 30 | | 50 | Shipping | 121 | 1500 | 50 | | 80 | Sales | 145 | 2500 | 80 | | 10 | Administration | 200 | 1700 | 10 | | 20 | Marketing | 201 | 1800 | 20 | | 40 | Human Resources | 203 | 2400 | 40 | | 70 | Public Relations | 204 | 2700 | 70 | | 110 | Accounting | 205 | 1700 | 110 | +---------------+------------------+------------+-------------+---------------+ 11 rows in set (0.00 sec) 例子说明，公用表表达式可以起到子查询的作用。以后如果遇到需要使用子查询的场景，你可以在查询 之前，先定义公用表表达式，然后在查询中用它来代替子查询。而且，跟子查询相比，公用表表达式有 一个优点，就是定义过公用表表达式之后的查询，可以像一个表一样多次引用公用表表达式，而子查询 则不能。\n2) 递归公用表表达式 递归公用表表达式也是一种公用表表达式，只不过，除了普通公用表表达式的特点以外，它还有自己的特点，就是可以调用自己。它的语法结构是：\n1 2 3 WITH RECURSIVE CTE名称 AS （子查询） SELECT|DELETE|UPDATE 语句; 递归公用表表达式由 2 部分组成，分别是种子查询和递归查询，中间通过关键字 UNION [ALL]进行连接。 这里的种子查询，意思就是获得递归的初始值。这个查询只会运行一次，以创建初始数据集，之后递归 查询会一直执行，直到没有任何新的查询数据产生，递归返回。\n案例：针对于我们常用的employees表，包含employee_id，last_name和manager_id三个字段。如果a是b 的管理者，那么，我们可以把b叫做a的下属，如果同时b又是c的管理者，那么c就是b的下属，是a的下下 属。\n下面我们尝试用查询语句列出所有具有下下属身份的人员信息。\n如果用我们之前学过的知识来解决，会比较复杂，至少要进行 4 次查询才能搞定：\n第一步，先找出初代管理者，就是不以任何别人为管理者的人，把结果存入临时表； 第二步，找出所有以初代管理者为管理者的人，得到一个下属集，把结果存入临时表； 第三步，找出所有以下属为管理者的人，得到一个下下属集，把结果存入临时表。 第四步，找出所有以下下属为管理者的人，得到一个结果集。 如果第四步的结果集为空，则计算结束，第三步的结果集就是我们需要的下下属集了，否则就必须继续 进行第四步，一直到结果集为空为止。比如上面的这个数据表，就需要到第五步，才能得到空结果集。 而且，最后还要进行第六步：把第三步和第四步的结果集合并，这样才能最终获得我们需要的结果集。\n如果用递归公用表表达式，就非常简单了。我介绍下具体的思路。\n用递归公用表表达式中的种子查询，找出初代管理者。字段 n 表示代次，初始值为 1，表示是第一 代管理者。 用递归公用表表达式中的递归查询，查出以这个递归公用表表达式中的人为管理者的人，并且代次 的值加 1。直到没有人以这个递归公用表表达式中的人为管理者了，递归返回。 在最后的查询中，选出所有代次大于等于 3 的人，他们肯定是第三代及以上代次的下属了，也就是 下下属了。这样就得到了我们需要的结果集。 这里看似也是 3 步，实际上是一个查询的 3 个部分，只需要执行一次就可以了。而且也不需要用临时表 保存中间结果，比刚刚的方法简单多了。\n代码实现：\n1 2 3 4 5 6 7 8 9 10 WITH RECURSIVE cte AS ( SELECT employee_id,last_name,manager_id,1 AS n FROM employees WHERE employee_id = 100 -- 种子查询，找到第一代领导 UNION ALL SELECT a.employee_id,a.last_name,a.manager_id,n+1 FROM employees AS a JOIN cte ON (a.manager_id = cte.employee_id) -- 递归查询，找出以递归公用表表达式的人为领导的人 ) SELECT employee_id,last_name FROM cte WHERE n \u0026gt;= 3; 总之，递归公用表表达式对于查询一个有共同的根节点的树形结构数据，非常有用。它可以不受层级的 限制，轻松查出所有节点的数据。如果用其他的查询方式，就比较复杂了。\n3) 小结 公用表表达式的作用是可以替代子查询，而且可以被多次引用。递归公用表表达式对查询有一个共同根节点的树形结构数据非常高效，可以轻松搞定其他查询方式难以处理的查询。\n","permalink":"https://cold-bin.github.io/post/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/","tags":["dml","dql","ddl","子查询","连接查询"],"title":"MySQL基础篇"},{"categories":["golang","分布式系统","微服务"],"contents":"RPC与微服务 一、RPC 1. 什么是RPC RPC(即：Remote Procedure Call) 远程过程调用，简单地理解是一个节点请求另一个节点提供的服务。当然这两个节点可能部署在不同的主机上，也可能是相同的主机上，但是两个节点是进程隔离级别。\n对应RPC的是，本地过程调用。调用本地代码里的某个函数就是最常见的本地过程调用，显然本地过程调用不是进程隔离级别，而是共享在一个进程。\n将本地过程调用编程远程过程调用会面临各种问题\nCall的id映射\n假设现在我们要RPC调用Multiply函数实现功能。那我们怎么告诉远程机器我们要调用Multiply，而不是Add或者FooBar呢？在本地过程调用中，函数体是直接通过函数指针来指定的，我们调用Multiply，编译器就自动帮我们调用它相应的函数指针。但是在远程调用中，函数指针是不行的，因为两个进程的地址空间是完全不一样的。所以，**在RPC中，所有的函数都必须有自己的一个ID，这个ID在所有进程中都是唯一确定的。客户端在做远程过程调用时，必须附上这个ID。**然后我们还需要在客户端和服务端分别维护一个 (函数 \u0026lt;=\u0026gt; Call ID) 的对应表。两者的表不一定需要完全相同，但相同的函数对应的Call ID必须相同。当客户端需要进行远程调用时，它就查一下这个表，找出相应的Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。\n序列化与反序列化\n客户端怎么把参数值传给远程的函数呢？在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。但是在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数，甚至有时候客户端和服务端使用的都不是同一种语言（比如服务端用C++，客户端用Java或者Python）。这时候就需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。\n序列化：将某种格式的数据转化为字节流\n反序列化：将字节流数据转化为某种格式的数据\n例如json序列化与反序列化，json序列化指先将json格式的数据序列化未字节流数据；json反序列化指将字节流数据提取出来并解读成json这种格式\n网络传输\n远程调用往往用在网络上，客户端和服务端是通过网络连接的。所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆就用了HTTP2。Java的Netty也属于这层的东西。\n解决了上面三个机制，就能实现RPC了，具体过程如下：\nclient端RPC过程\n1 2 3 4 5 1. 将这个调用映射为Call ID。这里假设用最简单的字符串当Call ID的方法 2. 将Call ID和传入的参数序列化。可以直接将它们的值以二进制形式打包 3. 把步骤2中得到的数据包发送给ServerAddr，这需要使用网络传输层 4. 等待服务器返回结果 4. 如果服务器调用成功，那么就将结果反序列化 server端RPC过程\n1 2 3 4 5 6 1. 在本地维护一个Call ID到函数指针的映射call_id_map，可以用dict完成 2. 等待请求，包括多线程的并发处理能力，能够并发地RPC 3. 得到一个请求后，将其数据包反序列化，得到Call ID 4. 通过在call_id_map中查找，得到相应的函数指针 5. 将参数反序列化后，在本地调用Call ID对应的函数，得到结果 6. 将结果序列化后通过网络返回给Client 对于上面三个机制地实现可以参考\nCall ID映射可以直接使用函数字符串，也可以使用整数ID，保证所有进程里的函数ID都唯一即可。映射表一般就是一个哈希表，毕竟哈希表的查询时间复杂度为O(1)。 序列化反序列化可以自己写，也可以使用Protobuf（这种方式序列化比http传输数据更加轻量与高效）或者FlatBuffers之类的。 网络传输库可以自己写socket，或者用tcp、udp或者http之类的。 实际上真正的开发过程中，除了上面的基本功能（三个机制）以外还需要更多的细节：\n链路追踪、服务发现、服务熔断、请求限流、服务降级、网络错误、流量控制、超时和重试、动态扩容、DDD领域驱动设计等。\n2. rpc、http以及restful之间的区别 restful只能说是一种设计风格，还没有形成一种协议、一种约束，是一种针对资源划分的更好的设计风格。\nrpc是框架，既不是某种风格。也不是某种协议或规范。rpc调用时，在网络上数据传输的格式，可以是http协议的格式，还可以是json格式，还可以是xml格式，当然还可以是更加轻量、更加高效的protobuf协议格式数据\nhttp其实是一种网络传输协议，基于tcp，规定了数据传输的格式。现在客户端浏览器与服务端通信基本都是采用http协议。也可以用来进行远程服务调用。缺点是消息封装臃肿。现在热门的Restful风格，就可以通过http协议来实现\n3. 基于json的rpc 服务端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main import ( \u0026#34;net\u0026#34; \u0026#34;net/rpc\u0026#34; \u0026#34;net/rpc/jsonrpc\u0026#34; ) type HelloService struct {} func (s *HelloService) Hello(request string, reply *string) error { *reply = \u0026#34;hello \u0026#34;+ request return nil } func main(){ rpc.RegisterName(\u0026#34;HelloService\u0026#34;, new(HelloService)) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:1234\u0026#34;) if err != nil { panic(\u0026#34;启动错误\u0026#34;) } for { conn, err := listener.Accept() if err != nil { panic(\u0026#34;接收\u0026#34;) } go rpc.ServeCodec(jsonrpc.NewServerCodec(conn)) // 服务端使用json编解码 } } 客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/rpc\u0026#34; \u0026#34;net/rpc/jsonrpc\u0026#34; ) func main(){ conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:1234\u0026#34;) if err != nil { panic(\u0026#34;连接错误\u0026#34;) } client := rpc.NewClientWithCodec(jsonrpc.NewClientCodec(conn))// 客户端需要使用json编解码 var reply string err = client.Call(\u0026#34;HelloService.Hello\u0026#34;, \u0026#34;imooc\u0026#34;, \u0026amp;reply) if err != nil { panic(\u0026#34;调用错误\u0026#34;) } fmt.Println(reply) } 4. 基于http的rpc 服务端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { rpc.RegisterName(\u0026#34;HelloService\u0026#34;, new(HelloService)) http.HandleFunc(\u0026#34;/jsonrpc\u0026#34;, func(w http.ResponseWriter, r *http.Request) { var conn io.ReadWriteCloser = struct { io.Writer io.ReadCloser }{ ReadCloser: r.Body, Writer: w, } rpc.ServeRequest(jsonrpc.NewServerCodec(conn)) // 将单次请求转化为rpc }) http.ListenAndServe(\u0026#34;:1234\u0026#34;, nil) } 显然，以上基于json、http的rpc封装性不是很好，缺乏一个更好的框架。\n二、GRPC和protobuf 1. 什么是GRPC、protobuf GRPC是一个高性能、开源和通用的 RPC 框架，面向移动和 HTTP/2 设计。\nGRPC调用如下图：\nprotoBuf是结构数据序列化的方法，可简单类比于XML，其具有以下特点：\n语言无关、平台无关。即 protoBuf 支持 Java、C++、Python 等多种语言，支持多个平台 高效。即比 XML 更小（3 ~ 10倍）、更快（20 ~ 100倍）、更为简单 扩展性、兼容性好。你可以更新数据结构，而不影响和破坏原有的旧程序 2. protobuf语法指南 现阶段比较流行的是proto3。通过编写少许的proto代码，然后通过插件就可以自动生成服务端的GRPC调用框架，只需添加服务的具体实现逻辑即可。\nproto3\n3. GRPC demo 安装grpc和protobuf 用go mod一键sync一下两个库\n1 2 3 4 import( \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/protobuf\u0026#34; ) protocol buffer编译器 这个编译器可以单独下载，但我们也可以使用Goland里面的protocol buffer的编译器插件\n1 file-\u0026gt;settings-\u0026gt;plugins-\u0026gt;搜protocol 安装protoc 到protobuf release，选择适合自己操作系统的压缩包文件\n将解压后得到的protoc二进制文件移动到$GOPATH/bin里\n不会有人不知道$GOPATH/bin，也没有将这个文件夹加到path环境变量里吧\ngo的protoc编辑器插件 具体可以看其他网上教程\n1 2 go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest demo 我们把讲protobuf语法时的示例文件作为例子，将其改名为login.proto并执行以下命令：\n1 2 protoc --go_out=. ./login.proto protoc --go-grpc_out=. ./login.proto 这两条命令会生成两个文件login.pb.go和login_grpc.pb.go(放在项目的proto文件夹里)，服务端和客户端都需要这两个文件。\nlogin.pb.go放置了将login.proto里的结构翻译成go语言结构体和相关东西。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type LoginReq struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields UserName string `protobuf:\u0026#34;bytes,1,opt,name=UserName,proto3\u0026#34; json:\u0026#34;UserName,omitempty\u0026#34;` PassWord string `protobuf:\u0026#34;bytes,2,opt,name=PassWord,proto3\u0026#34; json:\u0026#34;PassWord,omitempty\u0026#34;` } type LoginResp struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields OK bool `protobuf:\u0026#34;varint,1,opt,name=OK,proto3\u0026#34; json:\u0026#34;OK,omitempty\u0026#34;` } login_grpc.pb.go放置了gRPC框架封装好的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func NewBiliClient(cc grpc.ClientConnInterface) BiliClient { return \u0026amp;biliClient{cc} } func (c *biliClient) Login(ctx context.Context, in *LoginReq, opts ...grpc.CallOption) (*LoginResp, error) { out := new(LoginResp) err := c.cc.Invoke(ctx, \u0026#34;/grpcinclass.Bili/Login\u0026#34;, in, out, opts...) if err != nil { return nil, err } return out, nil } type BiliServer interface { Login(context.Context, *LoginReq) (*LoginResp, error) mustEmbedUnimplementedBiliServer() } func RegisterBiliServer(s grpc.ServiceRegistrar, srv BiliServer) { s.RegisterService(\u0026amp;Bili_ServiceDesc, srv) } 接下来使用下面的server代码起一个rpc服务端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 //server（被调用rpc的一方） package main import ( \u0026#34;context\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; ) const ( port = \u0026#34;:50051\u0026#34; ) func main() { // 监听端口 lis, err := net.Listen(\u0026#34;tcp\u0026#34;, port) if err != nil { log.Fatalf(\u0026#34;failed to listen: %v\u0026#34;, err) } s := grpc.NewServer() //获取新服务示例 proto.RegisterBiliServer(s, \u0026amp;server{}) // 开始处理 if err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;failed to serve: %v\u0026#34;, err) } } type server struct { proto.UnimplementedBiliServer // 用于实现proto包里BiliServer接口 } func (s *server) Login(ctx context.Context, req *proto.LoginReq) (*proto.LoginResp, error) { resp := \u0026amp;proto.LoginResp{} log.Println(\u0026#34;recv:\u0026#34;, req.UserName, req.PassWord) if req.PassWord != GetPassWord(req.UserName) { resp.OK = false return resp, nil } resp.OK = true return resp, nil } func GetPassWord(userName string) (password string) { return userName + \u0026#34;123456\u0026#34; } 接下来使用下面的client代码起一个rpc客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 //client（调用rpc的一方） package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/credentials/insecure\u0026#34; \u0026#34;log\u0026#34; ) const ( address = \u0026#34;localhost:50051\u0026#34; ) func main() { //建立链接 conn, err := grpc.Dial(address, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\u0026#34;did not connect: %v\u0026#34;, err) } defer conn.Close() c := proto.NewBiliClient(conn) for { //这段不重要 fmt.Println(\u0026#34;input username\u0026amp;password:\u0026#34;) iptName := \u0026#34;\u0026#34; _, _ = fmt.Scanln(\u0026amp;iptName) iptPassword := \u0026#34;\u0026#34; _, _ = fmt.Scanln(\u0026amp;iptPassword) loginResp, _ := c.Login(context.Background(), \u0026amp;proto.LoginReq{ UserName: iptName, PassWord: iptPassword, }) if loginResp.OK { fmt.Println(\u0026#34;success\u0026#34;) break } fmt.Println(\u0026#34;retry\u0026#34;) } } protobuf 3中还有一种数据类型——steam（流），其用于传输流式数据，感兴趣可以参考这篇文章(这篇文章是我随便找的用于简要了解)\n4. protobuf类型补充 前面的proto官网语法介绍里包含了许多丰富的类型，但是并没有将所有常用的类型包含，包括时间戳、时间段、还有任意类型消息。类型足够丰富，才能更好地描述一次行为产生的数据。\nTimestamp类型 源码包：\ngoogle/protobuf/timestamp.proto里\n1 2 3 4 message Timestamp { int64 seconds = 1; // 时间戳秒数 int32 nanos = 2; // 纳秒数 } 可以先导入这个消息定义的源码包后再引用，这是protobuf协议里官方自己给出的时间戳表示法，需要使用到时间类型时，可以考虑嵌入该类型。\n1 2 3 4 5 message Msg { int64 id=1; string content=2; google.protobuf.Timestamp at_time=3; } 使用插件编译自动生成代码时，时间类型*timestamp.Timestamp只是用来表示时间的结构体，和go表示时间的基本类型显然不一致。当然，google官方提供了转化方法。\n1 2 3 4 5 6 7 type Msg struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields ...//其他字段 AtTime *timestamp.Timestamp `protobuf:\u0026#34;bytes,1,opt,name=birthday,proto3\u0026#34; json:\u0026#34;birthday,omitempty\u0026#34;` } Duration类型 与Timestamp类型类似\n5. GRPC的metadata和RPC自定义认证 metadata介绍 在http请求当中我们可以设置header用来传递数据，grpc底层采用http2协议也是支持传递数据的，采用的是metadata。 Metadata 对于 gRPC 本身来说透明， 它使得 client 和 server 能为对方提供本次调用的信息。就像一次 http 请求的 RequestHeader 和 ResponseHeader，http header 的生命周期是一次 http 请求， Metadata 的生命周期则是一次 RPC调用。\n在http/1.1协议里，header是明确存在请求报文里，那么GRPC的header在哪里呢？\n答案：GRPC的header是metadata，metadata在代码里是放在上下文中存储数据，在网络传输上是放在HEADERS帧字段的几个字节上。\n在 gRPC 中，Metadata 实际上就是一个 map 结构，其原型如下：\n1 type MD map[string][]string 是一个字符串与字符串切片的映射结构。\nmetadata创建 在 google.golang.org/grpc/metadata 中分别提供了两个方法来创建 metadata，第一种是 metadata.New 方法，如下：\n1 metadata.New(map[string]string{\u0026#34;go\u0026#34;: \u0026#34;programming\u0026#34;, \u0026#34;tour\u0026#34;: \u0026#34;book\u0026#34;}) 使用 New 方法所创建的 metadata，将会直接被转换为对应的 MD 结构，参考结果如下：\n1 2 go: []string{\u0026#34;programming\u0026#34;} tour: []string{\u0026#34;book\u0026#34;} 第二种是 metadata.Pairs 方法，如下：\n1 2 3 4 5 metadata.Pairs( \u0026#34;go\u0026#34;, \u0026#34;programming\u0026#34;, \u0026#34;tour\u0026#34;, \u0026#34;book\u0026#34;, \u0026#34;go\u0026#34;, \u0026#34;eddycjy\u0026#34;, ) 使用 Pairs 方法所创建的 metadata，将会以奇数来配对，并且所有的 Key 都会被默认转为小写，若出现同名的 Key，将会追加到对应 Key 的切片（slice）上，参考结果如下：\n1 2 go: []string{\u0026#34;programming\u0026#34;, \u0026#34;eddycjy\u0026#34;} tour: []string{\u0026#34;book\u0026#34;} 设置/获取metadata 1 2 3 4 5 ctx := context.Background() md := metadata.New(map[string]string{\u0026#34;go\u0026#34;: \u0026#34;programming\u0026#34;, \u0026#34;tour\u0026#34;: \u0026#34;book\u0026#34;}) newCtx1 := metadata.NewIncomingContext(ctx, md) newCtx2 := metadata.NewOutgoingContext(ctx, md) 在 gRPC 中对于 metadata 进行了区别，分为了传入和传出用的 metadata，这是为了防止 metadata 从入站 RPC 转发到其出站 RPC 的情况，针对此提供了两种方法来分别进行设置，如下：\nNewIncomingContext：创建一个附加了所传入的 md 新上下文，仅供自身的 gRPC 服务端内部使用。 NewOutgoingContext：创建一个附加了传出 md 的新上下文，可供外部的 gRPC 客户端、服务端使用。 因此相对的在 metadata 的获取上，也区分了两种方法，分别是 FromIncomingContext 和 NewOutgoingContext，与设置的方法所相对应的含义，如下：\n1 2 md1, _ := metadata.FromIncomingContext(ctx) md2, _ := metadata.FromOutgoingContext(ctx) 那么总的来说，这两种方法在实现上有没有什么区别呢，我们可以一起深入看看：\n1 2 3 4 5 6 7 8 9 10 type mdIncomingKey struct{} type mdOutgoingKey struct{} func NewIncomingContext(ctx context.Context, md MD) context.Context { return context.WithValue(ctx, mdIncomingKey{}, md) } func NewOutgoingContext(ctx context.Context, md MD) context.Context { return context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md}) } 实际上主要是在内部进行了 Key 的区分，以所指定的 Key 来读取相对应的 metadata，以防造成脏读，其在实现逻辑上本质上并没有太大的区别。另外大家可以看到，其对 Key 的设置，是用一个结构体去定义的，这是 Go 语言官方一直在推荐的写法，建议大家也这么写。\n实际使用场景 在上面我们已经介绍了关键的 metadata 以及其相对的 IncomingContext、OutgoingContext 类别的相关方法.\n那么我们回过来想，假设我现在有一个 ServiceA 作为服务端，然后有一个 Client 去调用 ServiceA，我想传入我们自定义的 metadata 信息，那我们应该怎么写才合适，流程图如下：\n在常规情况下，我们在 ServiceA 的服务端，应当使用 metadata.FromIncomingContext 方法进行读取，如下：\n1 2 3 4 5 func (t *ServiceA) GetXXX(ctx context.Context, r *pb.GetXXXRequest) (*pb.GetXXXReply, error) { md, _ := metadata.FromIncomingContext(ctx) log.Printf(\u0026#34;md: %+v\u0026#34;, md) ... } 而在 Client，我们应当使用 metadata.AppendToOutgoingContext 方法，如下：\n1 2 3 4 5 6 7 8 9 10 11 func main() { ctx := context.Background() newCtx := metadata.AppendToOutgoingContext(ctx, \u0026#34;cookie\u0026#34;, \u0026#34;asdfghjkll\u0026#34;) clientConn, _ := GetClientConn(newCtx, ...) defer clientConn.Close() xxxClient := pb.NewXXXClient(clientConn) resp, _ := xxxClient.GetXXX(newCtx, \u0026amp;pb.GetXXXRequest{Name: \u0026#34;xxx\u0026#34;}) ... } 这里需要注意一点，在新增 metadata 信息时，务必使用 Append 类别的方法，否则如果直接 New 一个全新的 md，将会导致原有的 metadata 信息丢失（除非你确定你希望得到这样的结果）。\n6. GRPC拦截器 我想在每一个RPC方法的前面或后面做某些操作，我想对RPC方法进行鉴权校验，我想对RPC方法进行上下文的超时控制，我想对每个RPC方法的请求都做日志记录，我想对每个RPC请求做一些头部帧数据监控，避免恶意爬虫\u0026hellip;等等一些针对某个业务模块的RPC方法进行统一的特殊处理。（类似于gin框架中间件的意思）\n这诸如类似的一切需求的答案在拦截器（Interceptor）上，你能够借助它实现许许多多的定制功能且不直接侵入业务代码。\n拦截器类型 客户端\n一元拦截器\n客户端的一元拦截器类型为 UnaryClientInterceptor，方法原型如下：\n1 2 3 4 5 6 7 8 9 type UnaryClientInterceptor func( ctx context.Context, // 上下文 method string, //调用方法 req, //请求参数 reply interface{},//响应结果数据 cc *ClientConn, //连接指针实体 invoker UnaryInvoker, //调用程序的实体 opts ...CallOption,// 调用的配置 ) error 一元拦截器的实现通常可以分为三个部分：预处理，调用RPC方法和后处理。\n流拦截器\n客户端的流拦截器类型为 StreamClientInterceptor，方法原型如下：\n1 2 3 4 5 6 7 8 type StreamClientInterceptor func( ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, streamer Streamer, opts ...CallOption, ) (ClientStream, error) 流拦截器的实现包括预处理和流操作拦截，并不能在事后进行 RPC 方法调用和后处理，而是拦截用户对流的操作.\n服务端\n一元拦截器\n服务端的一元拦截器类型为 UnaryServerInterceptor，方法原型如下：\n1 2 3 4 5 6 type UnaryServerInterceptor func( ctx context.Context, req interface{}, info *UnaryServerInfo, handler UnaryHandler, ) (resp interface{}, err error) 其一共包含四个参数，分别是RPC上下文、RPC方法的请求参数、RPC方法的所有信息、RPC方法本身。\n流拦截器\n服务端的流拦截器类型为 StreamServerInterceptor，方法原型如下：\n1 2 3 4 5 6 type StreamServerInterceptor func( srv interface{}, ss ServerStream, info *StreamServerInfo, handler StreamHandler, ) error 实现一个拦截器 如何实现一个拦截器呢？在启动一个GRPC服务端时，可以通过配置GRPC服务端的GRPC拦截器，拦截器的具体逻辑只需要自己实现即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 opts := []grpc.ServerOption{ grpc.UnaryInterceptor(XXXInterceptor), } s := grpc.NewServer(opts...)// 函数选项模式 // 实现XXXInterceptor逻辑 func XXXInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) { log.Println(\u0026#34;be pending\u0026#34;) //RPC调用前 // 还可以获取rpc请求头的元数据是否合乎预期。例如可以做：认证与鉴权、还可以识别RPC请求来源并做一些限制等等 resp, err := handler(ctx, req) // RPC的处理方法 log.Println(\u0026#34;be processed\u0026#34;) // RPC调用后 return resp, err } 上面代码就是配置了一个XXXInterceptor的GRPC一元拦截器。XXXInterceptor的逻辑需要自己实现。\n对流的拦截器逻辑一样，实现对应的接口方法即可。\n如何使用多个拦截器 理论上是不能直接多次配置单个拦截器来实现多拦截，以下代码会报错\n1 2 3 4 5 6 opts := []grpc.ServerOption{ grpc.UnaryInterceptor(XXX1Interceptor), grpc.UnaryInterceptor(XXX2Interceptor), grpc.UnaryInterceptor(XXX3Interceptor), } s := grpc.NewServer(opts...)// 函数选项模式 这里需要使用第三方库来实现多个拦截器的逻辑，安装：\n1 go get -u github.com/grpc-ecosystem/go-grpc-middleware 使用\n1 2 3 4 5 6 7 8 9 10 11 import grpc_middleware \u0026#34;github.com/grpc-ecosystem/go-grpc-middleware\u0026#34; ... opts := []grpc.ServerOption{ grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer( XXX1Interceptor, XXX2Interceptor, XXX3Interceptor, )), } s := grpc.NewServer(opts...) ... 第三方库实现的原理\nO_o 没看懂\nGRPC社区生态还有很多好用的middlware，诸如：grpc_zap、grpc_auth、grpc_recovery等等\ngithub速通 \u0026ndash;\u0026gt; https://github.com/grpc-ecosystem/go-grpc-middleware\nGRPC自定义认证 关于GRPC token认证鉴权，其实可以参照http/1.1来进行设计：在metadata里放入token信息，每次请求都需要携带这个metadata token信息，服务端可以写一个拦截器专门用来token拦截认证。\n当然这样的逻辑，官方已经封装好了如下接口\n1 2 3 4 type PerRPCCredentials interface { GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error)// 获取当前请求认证所需的元数据（metadata） RequireTransportSecurity() bool// 是否需要基于 TLS 认证进行安全传输（https） } 实现如上两个接口的结构体auth，就可以在客户端拨号连接服务端时，配置一个认证的grpc.WithPerRPCCredentials(\u0026amp;auth)的连接选项。这样，每次拨号连接的时候，会将每个RPC上下文里塞入想要的认证信息。服务端只需要统一拦截请求，检查请求的metadata是否存在需要的认证信息即可。\n客户端代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 type Auth struct { AppKey string AppSecret string } func (a *Auth) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) { return map[string]string{\u0026#34;app_key\u0026#34;: a.AppKey, \u0026#34;app_secret\u0026#34;: a.AppSecret}, nil } func (a *Auth) RequireTransportSecurity() bool { return false } func main() { auth := Auth{ AppKey: \u0026#34;auth\u0026#34;, AppSecret: \u0026#34;XXXXXXXXXX\u0026#34;,// 例如token } ctx := context.Background() opts := []grpc.DialOption{grpc.WithPerRPCCredentials(\u0026amp;auth)} clientConn, err := GetClientConn(ctx, \u0026#34;localhost:8004\u0026#34;, opts) if err != nil { log.Fatalf(\u0026#34;err: %v\u0026#34;, err) } defer clientConn.Close() ... } ... 服务端代码：\n貌似使用拦截器进行认证时，意味着需要对所有RPC请求做出统一拦截和限制。这与gin框架的中间件有点区别，gin的中间件可以针对部分路由实现拦截，而GRPC的拦截器只能对所有的RPC请求做出限制，不能只对某一部分RPC请求作出限制。所以，也可以不用拦截器来实现服务端的同意拦截与认证。可以侵入式地给每一个需要认证的RPC补上认证的逻辑即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // 实现AuthInterceptor逻辑，用于配置服务端token认证拦截器 func AuthInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (res interface{},err error) { //拦截普通方法请求，验证token if err = auth(ctx);err != nil { return } // 继续处理请求 return handler(ctx, req) } func auth(ctx context.Context) error { md, ok := metadata.FromIncomingContext(ctx) if !ok { return fmt.Errorf(\u0026#34;missing credentials\u0026#34;) } var auth string var token string if val, ok := md[\u0026#34;app_key\u0026#34;]; ok { auth = val[0] } if val, ok := md[\u0026#34;app_secret\u0026#34;]; ok { token = val[0] } if auth != \u0026#34;auth\u0026#34; || token != \u0026#34;XXXXXXXXXX\u0026#34; { return status.Errorf(codes.Unauthenticated, \u0026#34;客户端请求的token不合法\u0026#34;) } return nil } 7. GRPC验证器 github速通用法 \u0026ndash;\u0026gt; https://github.com/mwitkow/go-proto-validators\n类似于gin的validator验证器，有了验证器，我们可以在用户发起请求并携带参数时，可以更加优雅地检查入参是否符合预期。\n上手使用 这里使用第三方插件go-proto-validators自动生成验证规则。\n1 go get github.com/mwitkow/go-proto-validators 新建simple.proto文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 syntax = \u0026#34;proto3\u0026#34;; package proto; import \u0026#34;github.com/mwitkow/go-proto-validators/validator.proto\u0026#34;; message InnerMessage { // some_integer can only be in range (1, 100). int32 some_integer = 1 [(validator.field) = {int_gt: 0, int_lt: 100}]; // some_float can only be in range (0;1). double some_float = 2 [(validator.field) = {float_gte: 0, float_lte: 1}]; } message OuterMessage { // important_string must be a lowercase alpha-numeric of 5 to 30 characters (RE2 syntax). string important_string = 1 [(validator.field) = {regex: \u0026#34;^[a-z]{2,5}$\u0026#34;}]; // proto3 doesn\u0026#39;t have `required`, the `msg_exist` enforces presence of InnerMessage. InnerMessage inner = 2 [(validator.field) = {msg_exists : true}]; } service Simple{ rpc Route (InnerMessage) returns (OuterMessage){}; } 代码import \u0026quot;github.com/mwitkow/go-proto-validators/validator.proto\u0026quot;，文件validator.proto需要import \u0026quot;google/protobuf/descriptor.proto\u0026quot;;包，不然会报错。google/protobuf地址：https://github.com/protocolbuffers/protobuf/tree/master/src/google/protobuf/descriptor.proto。把src文件夹中的protobuf目录下载到GOPATH目录下。\n编译simple.proto文件\n1 go get github.com/mwitkow/go-proto-validators/protoc-gen-govalidators 指令编译：protoc --govalidators_out=. --go-grpc_out=./ --go_out=./ ./simple.proto\n编译完成后，自动生成simple.pb.go和simple.validator.pb.go文件，simple.pb.go文件不再介绍，我们看下simple.validator.pb.go文件，里面自动生成了message中属性的验证规则。\n然后把grpc_validator验证拦截器添加到服务端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 grpcServer := grpc.NewServer( grpc.StreamInterceptor(grpc_middleware.ChainStreamServer( grpc_validator.StreamServerInterceptor(), grpc_auth.StreamServerInterceptor(auth.AuthInterceptor), grpc_zap.StreamServerInterceptor(zap.ZapInterceptor()), grpc_recovery.StreamServerInterceptor(recovery.RecoveryInterceptor()), )), grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer( grpc_validator.UnaryServerInterceptor(), grpc_auth.UnaryServerInterceptor(auth.AuthInterceptor), grpc_zap.UnaryServerInterceptor(zap.ZapInterceptor()), grpc_recovery.UnaryServerInterceptor(recovery.RecoveryInterceptor()), )), ) 运行后，当输入数据验证失败后，会有以下错误返回\n1 Call Route err: rpc error: code = InvalidArgument desc = invalid field SomeInteger: value \u0026#39;101\u0026#39; must be less than \u0026#39;100\u0026#39; 为了更友好的针对参数错误的处理，针对所有RPC调用返回的错误，如果是validator的参数不匹配的错误，则表明参数出现不符合服务端预期，所以，需要返回一个对应的提示。这个可以结合拦截器对所有RPC，做一个统一的校验处理.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type Validator interface{ Validate() error } // 实现ValidatorInterceptor逻辑，用于验证器优雅地拦截 func ValidatorInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (res interface{},err error) { //拦截普通方法请求，只针对使用validator规则的rpc进行校验 if r,ok:=req.(Validator);ok{ // 将空接口断言成具有Validate方法的实体类型 if err:=r.Validate();err!=nil{// 参数有问题 return nil,status.Error(codes.InvalidArgument,err.Error())// 错误的兼容处理 } } // 继续处理请求 return handler(ctx, req) } 其他类型验证规则设置 enum验证\n1 2 3 4 5 6 7 8 9 10 11 12 13 Copysyntax = \u0026#34;proto3\u0026#34;; package proto; import \u0026#34;github.com/mwitkow/go-proto-validators/validator.proto\u0026#34;;// 也可以将这个proto文件拉下来 message SomeMsg { Action do = 1 [(validator.field) = {is_in_enum : true}]; } enum Action { ALLOW = 0; DENY = 1; CHILL = 2; } UUID验证\n1 2 3 4 5 6 7 8 syntax = \u0026#34;proto3\u0026#34;; package proto; import \u0026#34;github.com/mwitkow/go-proto-validators/validator.proto\u0026#34;; message UUIDMsg { // user_id must be a valid version 4 UUID. string user_id = 1 [(validator.field) = {uuid_ver: 4, string_not_empty: true}]; } 8. GRPC状态码及错误处理 GRPC状态码 那我们更细致来看，这些 gRPC 的内部状态又分别有哪些呢，目前官方给出的全部状态响应码如下：\nCode Status Notes 0 OK 成功 1 CANCELLED 该操作被调用方取消 2 UNKNOWN 未知错误 3 INVALID_ARGUMENT 无效的参数 4 DEADLINE_EXCEEDED 在操作完成之前超过了约定的最后期限。 5 NOT_FOUND 找不到 6 ALREADY_EXISTS 已经存在 7 PERMISSION_DENIED 权限不足 8 RESOURCE_EXHAUSTED 资源耗尽 9 FAILED_PRECONDITION 该操作被拒绝，因为未处于执行该操作所需的状态 10 ABORTED 该操作被中止 11 OUT_OF_RANGE 超出范围，尝试执行的操作超出了约定的有效范围 12 UNIMPLEMENTED 未实现 13 INTERNAL 内部错误 14 UNAVAILABLE 该服务当前不可用。 15 DATA_LOSS 不可恢复的数据丢失或损坏。 那么对应在我们刚刚的调用结果，状态码是 UNKNOWN，这是为什么呢，我们可以查看底层的处理源码，如下：\n1 2 3 4 5 6 7 8 9 func FromError(err error) (s *Status, ok bool) { ... if se, ok := err.(interface {// 匿名接口 GRPCStatus() *Status }); ok { return se.GRPCStatus(), true } return New(codes.Unknown, err.Error()), false } 我们可以看到，实际上若不是含有GRPCStatus方法的类型，都是默认返回codes.Unknown，也就是未知。也就是说，我们需要做一层兼容处理，让原生的Error类型转化为对应的类型。可以使用status.Error(codes.InvalidArgument,err.Error())来进行兼容处理。同时，也可以自己是实现一个MyError类型实现GRPCStatus、Error等方法，就可以实现无缝衔接（隐式接口的魅力）。\n错误处理 如上，我们可以自定义自己的错误码无缝衔接至GRPC框架中使用。\n9. GRPC的超时机制 超时时间的设置和适当控制，是在微服务架构中非常重要的一个保全项。\n我们假设一个应用场景，你有多个服务，他们分别是 A、B、C、D，他们之间是最简单的关联依赖，也就是 A=\u0026gt;B=\u0026gt;C=\u0026gt;D。在某一天，你有一个需求上线了，修改的代码内容正正好就是与服务 D 相关的，恰好这个需求就对应着一轮业务高峰的使用，但你发现不知道为什么，你的服务 A、B、C、D 全部都出现了响应缓慢，整体来看，开始出现应用系统雪崩….这到底是怎么了？\n从根本上来讲，是服务D出现了问题，所导致的这一系列上下游服务出现连锁反应，因为在服务调用中默认没有设置超时时间，或者所设置的超时时间过长，都会导致多服务下的整个调用链雪崩，导致非常严重的事故，因此任何调用的默认超时时间的设置是非常有必要的，在gRPC中更是强调 TL;DR（Too long, Don’t read）并建议始终设定截止日期. 应当给每一个rpc都设置调用的超时时间.\n站在巨人的肩膀上\u0026ndash;\u0026gt;go语言标准包context.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 客户端超时拦截器，控制整个调用链的调用时间，如果未在指定时间内返回结果，表明RPC过程中存在超时或不可用的服务 func UnaryContextTimeout() grpc.UnaryClientInterceptor { return func(ctx context.Context, method string, req, resp interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error { ctx, cancel := defaultContextTimeout(ctx) if cancel != nil { defer cancel() } return invoker(ctx, method, req, resp, cc, opts...) } } // 默认超时设置 func defaultContextTimeout(ctx context.Context) (context.Context, context.CancelFunc) { var cancel context.CancelFunc if _, ok := ctx.Deadline(); !ok { defaultTimeout := 60 * time.Second ctx, cancel = context.WithTimeout(ctx, defaultTimeout) } return ctx, cancel } 10. 重试 gRPC生态圈中的grpc_retry拦截器.\n11. GRPC gateway grpc-gateway是protoc的一个插件 。它读取GRPC服务定义，并生成反向代理服务器，将RESTful JSON API请求转换为GRPC的方式调用。简单来说，给客户端的调用提供了双协议的支持，即http1.1协议和protobuf协议，可以使用http请求发送json数据，也可以使用RPC请求发送protobuf二进制数据帧来达到调用的目的。（方便测试、方便前端）\ngrpc-gateway 介绍和安装 我们需要安装 grpc-gateway 的 protoc-gen-grpc-gateway 插件，安装命令如下：\n1 $ go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway@v1.14.5 将所编译安装的 Protoc Plugin 的可执行文件从 $GOPATH 中移动到相应的 bin 目录下，例如：\n1 $ mv $GOPATH/bin/protoc-gen-grpc-gateway /usr/local/go/bin/ 这里的命令操作并非是绝对必须的，主要目的是将二进制文件 protoc-gen-grpc-gateway 移动到 bin 目录下，让其可以执行，确保在 $PATH 下，只要达到这个效果就可以了。\nProto 文件的处理 Proto 文件修改和编译 那么针对 grpc-gateway 的使用，我们需要调整项目 proto 命令下的 tag.proto 文件，修改为如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 syntax = \u0026#34;proto3\u0026#34;; package proto; import \u0026#34;proto/common.proto\u0026#34;; import \u0026#34;google/api/annotations.proto\u0026#34;; service TagService { rpc GetTagList (GetTagListRequest) returns (GetTagListReply) { option (google.api.http) = { get: \u0026#34;/api/v1/tags\u0026#34; }; } } // 其他http方法和数据的写法，在proto文件源码可以找到：\u0026#34;google/api/http.proto\u0026#34;里的注释有 ... 我们在 proto 文件中增加了 google/api/annotations.proto 文件的引入，并在对应的 RPC 方法中新增了针对 HTTP 路由的注解。接下来我们重新编译 proto 文件，在项目根目录执行如下命令：\n1 2 3 4 5 $ protoc -I/usr/local/include -I. \\ -I$GOPATH/src \\ -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\ --grpc-gateway_out=logtostderr=true:. \\ ./proto/*.proto 执行完毕后将生成 tag.pb.gw.go 文件，也就是目前 proto 目录下用.pb.go 和.pb.gw.go 两种文件，分别对应两类功能支持。\n我们这里使用到了一个新的 protoc 命令选项 -I 参数，它的格式为：-IPATH, --proto_path=PATH，作用是指定 import 搜索的目录（也就是 Proto 文件中的 import 命令），可指定多个，如果不指定则默认当前工作目录。\n另外在实际使用场景中，还有一个较常用的选项参数，M 参数，例如 protoc 的命令格式为：Mfoo/bar.proto=quux/shme，则在生成、编译 Proto 时将所指定的包名替换为所要求的名字（如：foo/bar.proto 编译后为包名为 quux/shme），更多的选项支持可执行 protoc --help 命令查看帮助文档。\nannotations.proto 是什么 我们刚刚在 grpc-gateway 的 proto 文件生成中用到了 google/api/annotations.proto 文件，实际上它是 googleapis 的产物，在前面的章节我们有介绍过。\n另外你可以结合 grpc-gateway 的 protoc 的生成命令来看，你会发现它在 grpc-gateway 的仓库下的 third_party 目录也放了个 googleapis，因此在引用 annotations.proto 时，用的就是 grpc-gateway 下的，这样子可以保证其兼容性和稳定性（版本可控）。\n那么 annotations.proto 文件到底是什么，又有什么用呢，我们一起看看它的文件内容，如下：\n1 2 3 4 5 6 7 8 9 10 syntax = \u0026#34;proto3\u0026#34;; package google.api; import \u0026#34;google/api/http.proto\u0026#34;; import \u0026#34;google/protobuf/descriptor.proto\u0026#34;; ... extend google.protobuf.MethodOptions { HttpRule http = 72295728; } 查看核心使用的 http.proto 文件中的一部分内容，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 message HttpRule { string selector = 1; oneof pattern { string get = 2; string put = 3; string post = 4; string delete = 5; string patch = 6; CustomHttpPattern custom = 8; } string body = 7; string response_body = 12; repeated HttpRule additional_bindings = 11; } 总的来说，主要是针对的 HTTP 转换提供支持，定义了 Protobuf 所扩展的 HTTP Option，在 Proto 文件中可用于定义 API 服务的 HTTP 的相关配置，并且可以指定每一个 RPC 方法都映射到一个或多个 HTTP REST API 方法上。\n因此如果你没有引入 annotations.proto 文件和在 Proto 文件中填写相关 HTTP Option 的话，执行生成命令，不会报错，但也不会生成任何东西。\n服务逻辑实现 接下来我们开始实现基于 grpc-gateway 的在同端口下同 RPC 方法提供 gRPC（HTTP/2）和 HTTP/1.1 双流量的访问支持，我们打开项目根目录下的启动文件 main.go，修改为如下代码：\n1 2 3 4 5 6 var port string func init() { flag.StringVar(\u0026amp;port, \u0026#34;port\u0026#34;, \u0026#34;8004\u0026#34;, \u0026#34;启动端口号\u0026#34;) flag.Parse() } 不同协议的分流 我们调整了这个案例的服务启动端口号，然后继续在 main.go 中写入如下代码：\n1 2 3 4 5 6 7 8 9 func grpcHandlerFunc(grpcServer *grpc.Server, otherHandler http.Handler) http.Handler { return h2c.NewHandler(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { if r.ProtoMajor == 2 \u0026amp;\u0026amp; strings.Contains(r.Header.Get(\u0026#34;Content-Type\u0026#34;), \u0026#34;application/grpc\u0026#34;) { grpcServer.ServeHTTP(w, r) } else { otherHandler.ServeHTTP(w, r) } }), \u0026amp;http2.Server{}) } 这是一个很核心的方法，重要的分流和设置一共有两个部分，如下：\ngRPC 和 HTTP/1.1 的流量区分： 对 ProtoMajor 进行判断，该字段代表客户端请求的版本号，客户端始终使用 HTTP/1.1 或 HTTP/2。 Header 头 Content-Type 的确定：grpc 的标志位 application/grpc 的确定。 gRPC 服务的非加密模式的设置：关注代码中的\u0026quot;h2c\u0026quot;标识，“h2c” 标识允许通过明文 TCP 运行 HTTP/2 的协议，此标识符用于 HTTP/1.1 升级标头字段以及标识 HTTP/2 over TCP，而官方标准库 golang.org/x/net/http2/h2c 实现了 HTTP/2 的未加密模式，我们直接使用即可。 在整体的方法逻辑上来讲，我们可以看到关键之处在于调用了 h2c.NewHandler 方法进行了特殊处理，h2c.NewHandler 会返回一个 http.handler，其主要是在内部逻辑是拦截了所有 h2c 流量，然后根据不同的请求流量类型将其劫持并重定向到相应的 Hander 中去处理，最终以此达到同个端口上既提供 HTTP/1.1 又提供 HTTP/2 的功能了。\nServer 实现 完成了不同协议的流量分发和处理后，我们需要实现其 Server 的具体逻辑，继续在 main.go 文件中写入如下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import ( \u0026#34;github.com/grpc-ecosystem/grpc-gateway/runtime\u0026#34; ... ) func RunServer(port string) error { httpMux := runHttpServer() grpcS := runGrpcServer() gatewayMux := runGrpcGatewayServer() httpMux.Handle(\u0026#34;/\u0026#34;, gatewayMux) return http.ListenAndServe(\u0026#34;:\u0026#34;+port, grpcHandlerFunc(grpcS, httpMux)) } func runHttpServer() *http.ServeMux { serveMux := http.NewServeMux() serveMux.HandleFunc(\u0026#34;/ping\u0026#34;, func(w http.ResponseWriter, r *http.Request) { _, _ = w.Write([]byte(`pong`)) }) return serveMux } func runGrpcServer() *grpc.Server { s := grpc.NewServer() pb.RegisterTagServiceServer(s, server.NewTagServer()) reflection.Register(s) return s } func runGrpcGatewayServer() *runtime.ServeMux { endpoint := \u0026#34;0.0.0.0:\u0026#34; + port gwmux := runtime.NewServeMux() dopts := []grpc.DialOption{grpc.WithInsecure()} _ = pb.RegisterTagServiceHandlerFromEndpoint(context.Background(), gwmux, endpoint, dopts) return gwmux } 在上述代码中，与先前的案例中主要差异在于 RunServer 方法中的 grpc-gateway 相关联的注册，核心在于调用了 RegisterTagServiceHandlerFromEndpoint 方法去注册 TagServiceHandler 事件，其内部会自动转换并拨号到 gRPC Endpoint，并在上下文结束后关闭连接。\n另外在注册 TagServiceHandler 事件时，我们在 grpc.DialOption 中通过设置 grpc.WithInsecure 指定了 Server 为非加密模式，否则程序在运行时将会出现问题，因为 gRPC Server/Client 在启动和调用时，必须明确其是否加密。\n运行和验证 接下来我们编写 main 启动方法，调用 RunServer 方法，如下：\n1 2 3 4 5 6 func main() { err := RunServer(port) if err != nil { log.Fatalf(\u0026#34;Run Serve err: %v\u0026#34;, err) } } 完成服务的再启动后我们进行 RPC 方法的验证，如下：\n1 2 3 $ curl http://127.0.0.1:8004/ping $ curl http://127.0.0.1:8004/api/v1/tags $ grpcurl -plaintext localhost:8004 proto.TagService.GetTagList 正确的情况下，都会返回响应数据，分别对应心跳检测、RPC 方法的 HTTP/1.1 和 RPC 方法的 gRPC（HTTP/2）的响应。\n12. 生成接口文档 https://golang2.eddycjy.com/posts/ch3/07-api-doc/\n","permalink":"https://cold-bin.github.io/post/rpc%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1/","tags":["grpc","微服务"],"title":"RPC与微服务"},{"categories":["golang","分布式系统"],"contents":"微服务概念 微服务的粒度小，服务之间耦合度低，由于每个微服务都由独立的小团队负责，因此它敏捷性更高，分布式服务最后都会向微服务架构演化，这是一种趋势， 不过服务微服务化后带来的挑战也是显而易见的，例如服务粒度小，数量大，后期运维将会比较困难。所以又有容器+微服务的出现。\n这里就不细说容器与微服务了，着重理清微服务的一些概念、好处。\n快速开发与迭代 微服务，顾名思义，就是微小的一个服务。与传统单机架构不同，微服务的功能比较单一，专做一件事情，而单机架构需要把所有事情全部完成，可想而知，如果单机架构的应用出现了问题，那么整个项目都会下线，然后不断排查并检修，这个过程明显影响了用户体验，不应该只是因为单机架构的某一个小部分出了问题就需要停掉整个应用。因此，为了解决这个问题，微服务应运而生。微服务将原来的单机架构应用进行拆分，拆分成多个粒度更小的微服务，进行分别开发并部署，服务之间显然是RPC调用。这样做，就不会因为某种小原因而把整个应用都停掉，哪个服务出了问题，只需要将哪个服务停掉就行了。另外，进行服务拆分时，不同的服务还可以是不同编程语言进行开发，这样可以更具灵活性。\n自动化部署 自动化部署的目标就是持续交付，对于微服务来说，多个服务的自动化是必不可少的。通过自动化编译，自动化测试，自动化集成和自动化部署，可以大大的减轻开发团队和运维团队的任务，提升开发效率。\n独立部署 原来的单机架构只能一次性部署，而多个微服务基本上都是单独部署在不同的主机上。这样做可以减少粒度，哪个服务更新就只需要部署对应的服务即可，而不必更新所有微服务。\n错误隔离 在传统单机架构下一旦应用发生故障，整个服务的可用性都会受到影响，因为所有的模块都耦合在一个大的单体进程里，这个进程的所有程序都共享进程内的资源。所以，发生故障、错误的位置很难确定，应用也很难从故障种恢复过来。所以，我们需要将单体应用拆分成功能独立、相互隔离的微服务应用，这些微服务通过定义良好的协议进行通信。这样互相隔离的微服务之间，错误、故障也被隔离起来。\n负载均衡 可以将相同的服务部署到多台主机上，实现负载均衡\n出错处理 在传统单机架构的应用里，调用函数时，错误只会是自己内部的错误，包括入参错误、自身设计的一些缺陷等。但是在微服务里，就显得不同了。一个微服务需要使用到另一个微服务时，中间需要RPC调用，这样出错的原因不仅是入参、自身的设计缺陷，还可能是网络延时、另一个微服务挂掉等情况。\n链路追踪 在大型系统的微服务化构建中，一个系统被拆分成了许多模块。这些模块负责不同的功能，组合成系统，最终可以提供丰富的功能。在这种架构中，一次请求往往需要涉及到多个服务。互联网应用构建在不同的软件模块集上，这些软件模块，有可能是由不同的团队开发、可能使用不同的编程语言来实现、有可能布在了几千台服务器，横跨多个不同的数据中心。\n微服务架构是通过业务来划分服务的，使用 REST 调用。对外暴露的一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者网络超时，都会形成导致接口调用失败。随着业务的不断扩张，服务之间互相调用会越来越复杂。\n所以，为了更好地掌握一次请求到底经历了哪些服务，需要进行链路追踪\n服务发现 现代网络程序中主要分为前端和后端两部分，当前端的程序访问后端的时候，必须要知道后端服务的网络地址（即IP地址或者端口号）。以前每一个服务都被固定的部署到某一台机器上，所以说每个服务的端口号和IP地址都是固定的，可以通过配置文件进行修改。但是，在微服务体系中，由于服务的实例有可能发送失败重启、自动扩容、升级等情况，导致这些服务实例对应的网络地址是在动态变化的。如何精准、准确地使前端能够发现后端是一个难以解决的问题。\n在微服务体系结构中，所谓的服务发现就是用户可以通过服务的名字，在注册中心找到可以提供正常服务的实例的网络地址（即IP地址和端口号）。这种根据服务名字发现服务的可用地址的机制就叫做服务发现。\n服务发现的过程：\n服务提供者将实例信息注册到服务注册中心。 服务调用者根据服务标识，从注册中心查询服务，获取包含网络地址的服务实例列表。 与服务实现通信。 动态扩容 某个时间段里，出现一个热点事件，导致某一个服务被频繁调用，那么为了应对这个大流量的需求，显然需要将这个服务动态扩容：在高负载的时候自动扩容，低负载的时候自动缩容。\n熔断、降级 熔断主要针对具体某个接口出现问题时，对这个接口采用的临时方案，它站在接口或者服务的层面来考虑问题。防止因为某个接口出现问题，导致问题蔓延，直至整个系统不可用。例如：某个接口的提供方宕机，导致接口调用方频繁超时，调用方的上游受到影响，进而也会超时，最终整个链路都会变得超时，如果超时时间很长的话，会导致客户端系统资源浪费(一些池化的资源，长时间得不到释放)。此时可以在接口的调用方进行对该接口的熔断，具体做法，可以直接返回一个默认值，防止频繁的调用超时。同时通过不断的对该接口进行可用性探测，当检测到该接口可用时，对熔断逻辑进行撤销\n降级是系统应对突发流量的解决方案，它站在系统层面来考虑问题，目的是为了保证整个系统的可用性。因为受系统自身资源的限制，可以处理的流量是是有限的，不能对突发流量进行完全处理，那么此时就应该对流量进行有选择的处理，让系统只处理优先级高的流量，优先级低的流量不进行处理，直接返回默认值(或者其他处理策略)，也就是对部分低优先级流量进行舍弃，例如，双十一期间，很多电商网站的评论，收藏功能变得不可用，这里就是系统为了保证网站核心功能可用，对评论和收藏功能进行了降级，将系统资源用在核心业务上，正所谓\u0026quot;好钢用在刀刃上\u0026quot;。\n但是，这种方案为什么叫做\u0026quot;降级\u0026quot;呢？好像没有体现出\u0026quot;降级\u0026quot;的语义。其实降级，是站在整个系统维度来说的，举例来说：系统正常情况下，可以对外提供10个接口服务，在降级的情况下，只能对外提供5个接口服务，那么此时的系统，就是有\u0026quot;瑕疵\u0026quot;的系统，能够完整提供服务的系统，我们给系统打5星，现在系统中一部分功能不可用了，那么此时系统只能打3星，系统的\u0026quot;级别\u0026quot;有所下降了。\nDDD领域驱动设计 ","permalink":"https://cold-bin.github.io/post/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/","tags":["微服务"],"title":"微服务一些概念"},{"categories":["golang"],"contents":"一、go面向对象 封装 结构体与方法\n多态 接口来实现多态，不同结构体可以实现相同的接口。这样就能拥有同种行为的不同具体状态 \u0026ndash;\u0026gt; 多态\n继承、覆盖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // 标准包里的实现，大概是直接把文件里的所有内容给提取出来 func (f *File) Read(b []byte) (n int, err error) { if err := f.checkValid(\u0026#34;read\u0026#34;); err != nil { return 0, err } n, e := f.read(b) return n, f.wrapErr(\u0026#34;read\u0026#34;, e) } //main.go // 为了满足某种业务需求，文件里的内容不是所有都需要读出来，或者需要将某些内容屏蔽掉，这个时候，就凸显出接口的魅力了 func main() { file, err := os.Open(\u0026#34;./1.txt\u0026#34;) if err != nil { panic(err) return } myFile := NewMyFile(file) defer myFile.File.Close() bs := make([]byte, 10) // read 10 bytes if _, err = myFile.Read(bs); err != nil { panic(err) return } fmt.Println(\u0026#34;result: \u0026#34;,string(bs)) } type MyFile struct { *os.File } func NewMyFile(f *os.File) *MyFile { return \u0026amp;MyFile{ File: f, } } // 覆盖掉标准包里的io.File的Read方法，实现自己的逻辑 func (f *MyFile) Read(bs []byte) (n int, err error) { // 在原来的读取逻辑修改即可 // 在系统底层读的时候，监控字节流数据，每取到一个有效的unicode字符， // 就监视它的值是否是期望输出的，不是就替换，是的话就通过并写入bs切片 return } //其他未覆盖的方法都被继承 二、接口 什么是接口呢？接口，顾名思义就是一个插座，一个合适某种行为的规范。在Java里，接口是通过类来显式地调用（implements）；而在Go语言里，主要是通过隐式调用：某类型所属方法的名字、入参、出参和某个接口里的方法完全一样（全部），那么Go语言就默认实现了该接口。\n来看看使用接口与不使用接口作为函数参数的例子：\n不使用接口\n不使用接口作为一种约定，而使用某种特定类型的参数作为入参，这样做只能实现当前函数的特定功能，也就是说当前函数只能做到一件事情，不能做做到多件事情。\n使用接口\n使用接口作为函数入参的约定，即要求当前传入的类型，必须具备函数入参接口里约定好的所有行为、方法。这样才能进行调用该函数。\n下面的接口抽象\n接口抽象 对于大多数的动态语言而言，如Java，接口主要是用来抽象解耦代码间的依赖的，即将约定和具体实现进行分离，或者说，就是让调用者无需关心底层的实现，只需要遵照接口方法里的定义调用即可，当然也可以按照业务需求自己再实现一个覆盖掉原方法实现也行（无缝衔接）。\n示例\n假设first的实现方法是第一次实现，但是由于业务增长，急需更换另一种实现方式second，如何快速无缝衔接呢？\n第一次实现的时候先抽象出某一个行为要完成所需的一系列动作（或者叫方法、函数）。然后自己给这些接口提供一个实现，实现之后，所有的入参或接收者都采用接口类型来接收。\n这样做可以达到更好地解耦：即下一位觉得你的实现缺乏一些更好地优化，只需要根据自己的逻辑实现这个接口，就可以无缝衔接到代码里，无需大改代码如果不使用接口抽象显然，每个人都有自己的实现，这样会导致类型不一样，就会看底层代码的实现，哪里不对就修改哪里，显然比较繁琐。通过接口进行抽象之后，无需关注实现，只需要给出自己的实现就可以无缝衔接调用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 type Retriever interface { GetRetriever(url string) (bytes []byte, err error) } //first/retriever.go type First struct{} //某个团队提供的获取url里内容的一种方式 func (First) GetRetriever(url string) (bytes []byte, err error) { res, err := http.Get(url) if err != nil { return nil, err } bytes, err = ioutil.ReadAll(res.Body) return } //second/retriever.go type Second struct{} //测试团队提供的一种方式 func (Second) GetRetriever(url string) (bytes []byte, err error) { return []byte(\u0026#34;test content\u0026#34;), nil } //main.go func main() { var r Retriever = first.First{} //想要更换实现，只需修改此处即可，无需关注后面 bytes, err := r.GetRetriever(\u0026#34;https://www.baidu.com\u0026#34;) if err != nil { return } fmt.Println(string(bytes)) } 接口组合 接口组合类似与多继承，就是可以将一些接口组合在一起，这样就拥有这些接口里的方法，当然也可以定义自己方法，实现了子接口，也就实现了部分方法。\n接口的组合就是将一些接口整合在一起，有点像多继承一样。可以直接看下边的例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 type Retriver interface { Get(url string) string } type Poster interface { Post(url string, form map[string]string) string } func download(r Retriver) string { return r.Get(\u0026#34;\u0026lt;http://www.baidu.com\u0026gt;\u0026#34;) } func post(poster Poster) { poster.Post(\u0026#34;\u0026lt;http://ww.baidu.com\u0026gt;\u0026#34;, map[string]string{ \u0026#34;name\u0026#34;: \u0026#34;shulv\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;twenty\u0026#34;, }) } 上边定义了两个接口，并且分别有这两个接口的调用者，download和post。现在假设有个session函数，它需要一个参数，即是一个Retriver，又是一个Poster。此时就可以用到接口的组合\n1 2 3 4 5 6 7 8 9 10 11 12 type RetrieverPoster interface { Retriver Poster //Connect(host string) //当然还可以添加别的方法 } func session(s RetrieverPoster) string { s.Post(\u0026#34;\u0026lt;http://www.baidu.com\u0026gt;\u0026#34;, map[string]string{ \u0026#34;Contents\u0026#34;: \u0026#34;facked\u0026#34;, }) return s.Get(\u0026#34;\u0026lt;http://www.baidu.com\u0026gt;\u0026#34;) } 上边的代码中定义了一个组合接口RetrieverPoster，它直接包含了Retriver和Poster这两个接口，也就是说他可以直接使用这两个接口中的方法。当然，这个组合接口中还可以增加自己的方法。有没有发现它和结构体的嵌套非常的像（不记得结构体嵌套的，可以点这里）\n下边为了方便理解，我直接在一个文件中定义结构体，并实现结构体的一些方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 type Mock struct { Contains string } func (r Mock) Get(url string) string { return r.Contains } func (r *Mock) Post(url string, form map[string]string) string { r.Contains = form[\u0026#34;Contents\u0026#34;] return \u0026#34;ok\u0026#34; } type Real struct { UserAgent string TimeOut time.Duration } func (r Real) Get(url string) string { resp, err := http.Get(url) if err != nil { panic(err) } result, err := httputil.DumpResponse(resp, true) resp.Body.Close() if err != nil { panic(err) } return string(result) } 上边定义了两个结构体，分别是Mock和Real，并且实现了对应的方法（可以看到，Mock实现了组合接口RetrieverPoster）。下边在main函数中调用session函数\n1 2 3 4 5 6 func main() { retriver := Mock{\u0026#34;just a test\u0026#34;} fmt.Println(session(\u0026amp;retriever)) } //输出：facked（因为在Post方法中对Mock.Content进行了修改，说明session里边调用了Mock的Post方法） 其实在Go语言的库中，有很多接口组合的例子，比如io包里的ReadWriteCloser，它就由多个接口组合而成的一个接口，内容如下:\n1 2 3 4 5 6 // ReadWriteCloser is the interface that groups the basic Read, Write and Close methods. type ReadWriteCloser interface { Reader Writer Closer } 可以看到它是由Reader、Writer和Closer这三个接口组合而成的。当然，你可以进到io这个包里边，里边有很多这样的组合接口\n上边就是接口的组合，下边分享一些Go语言中标准的接口\n常见系统接口 Stringer 在fmt这个包中，有一个接口叫Stringer，它里边只有一个String函数，所以一个类型只要实现了String函数，就实现了Stringer接口\n1 2 3 4 5 package fmt type Stringer interface { String() string } 下边定义一个结构体类型，该结构体中实现了String方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;fmt\u0026#34; type Mock struct { Contents string } func (m Mock) String() string { return fmt.Sprintf(\u0026#34;Mock: {Contents = %s}\u0026#34;, m.Contents) } func main() { m := Mock{\u0026#34;a test, too\u0026#34;} fmt.Println(m.String()) } //输出：Mock: {Contents = a test, too} Reader 在io这个包中有一个Reader接口，它里边只有一个Read接口。实现Read方法的可以是一个文件，可以从文件中读取内容，放入到byte（当然不止有文件，还有网络、slice等等，都可以）\n1 2 3 4 5 package io type Reader interface { Read(p []byte) (n int, err error) } 可以看一下os.Open方法的内部实现，它的返回值是一个File的结构，然后进入到File中可以看到它是一个结构体类型，并且可以发现这个结构体中内嵌了一个file的结构体，而这个file实际上是 *File的真实表示。而 *File又实现了Read方法，所以它实际上就实现了Reader接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 //file.go package os func Open(name string) (*File, error) { return OpenFile(name, O_RDONLY, 0) } //types.go package os type File struct { *file // os specific } .... // file is the real representation of *File. type file struct { pfd poll.FD name string dirinfo *dirInfo // nil unless directory being read nonblock bool // whether we set nonblocking mode stdoutOrErr bool // whether this is stdout or stderr appendMode bool // whether file is opened for appending } 而*File实现了Read接口，内容如下: //file.go package os func (f *File) Read(b []byte) (n int, err error) { if err := f.checkValid(\u0026#34;read\u0026#34;); err != nil { return 0, err } n, e := f.read(b) return n, f.wrapErr(\u0026#34;read\u0026#34;, e) } 在*File中可以发现，它不会说我实现了哪些接口。实现了哪些接口是用的人来说的，它只需要实现某些方法就行了\n所以我们在获取文件中的内容的时候调用的bufio.NewScanner()方法，它需要的参数是一个io.Reader类型（所以Open返回的File类型，是可以直接传递给NewScanner，因为它实现了Reader接口）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 filename := \u0026#34;a.txt\u0026#34; file, err := os.Open(filename) scanner := bufio.NewScanner(file) for scanner.Scan() { fmt.Println(scanner.Text())//逐行读取文件中的内容 } //下边是NewScanner内部实现 func NewScanner(r io.Reader) *Scanner { return \u0026amp;Scanner{ r: r, split: ScanLines, maxTokenSize: MaxScanTokenSize, } } Writer 跟Reader接口一样，它也在io包中，接口中只有一个Write方法。实现Write方法的可以是一个文件，将提供的byte数据写入到文件中\n1 2 3 4 5 package io type Writer interface { Write(p []byte) (n int, err error) } 在fmt包中有一个Fprintf函数（fmt中各种输出函数的使用及作用，可以点这里），Fprintf的作用是返回一个格式化后的字符串。它的第一个参数是一个io.Writer类型，所以任何实现了这个Writer接口的，都能够传给它来用\n1 2 3 4 5 6 7 func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) { p := newPrinter() p.doPrintf(format, a) n, err = w.Write(p.buf) p.free() return } 示例：读取文件内容 以读取文件内容为例，通常的做法如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func printFile(filename string) { file, err := os.Open(filename) if err != nil { panic(err) } scanner := bufio.NewScanner(file) for scanner.Scan() { fmt.Println(scanner.Text())//逐行读取文件中的内容 } } func main() { printFile(\u0026#34;a.txt\u0026#34;) } 这样的做法缺点就是，printFile只能接收一个文件名字符串，只能读取文件中的内容。下边对他进行优化，让他不仅能够打印文件内容，并且还能够通过像打印文件一样打印字符串\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func printFile(filename string) { file, err := os.Open(filename) if err != nil { panic(err) } printFileContents(file) } // 使用接口作为函数入参，达到解耦 func printFileContents(read io.Reader) { scanner := bufio.NewScanner(read) for scanner.Scan() { fmt.Println(scanner.Text())//逐行读取文件中的内容 } } func main() { printFile(\u0026#34;a.txt\u0026#34;) s := `uiieo \u0026#34;dgsjg\u0026#34; s d 3 ` printFileContents(strings.NewReader(s)) } 可以看到将实际的打印内容工作交由printFileContents方法来做，它接收一个Reader类型的参数，所以可以利用它来打印文件内容，也能将字符串类型以文件的方式打印，这样它就变得更加通用\n三、函数式编程 函数式选项模式Option 作为一个类库作者，迟早会面临到接口变更问题。或者是因为外部环境变化，或者是因为功能升级而扩大了外延，或者是因为需要废弃掉过去的不完善的设计，或者是因为个人水平的提升，无论哪一种理由，你都可能会发现必须要修改掉原有的接口，替换之以一个更完美的新接口。\n旧的方式 想象下有一个早期的类库：\n1 2 3 4 5 6 7 8 9 10 11 package tut func New(a int) *Holder { return \u0026amp;Holder{ a: a, } } type Holder struct { a int } 后来，我们发现需要增加一个布尔量 b，于是修改 tut 库为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 package tut func New(a int, b bool) *Holder { return \u0026amp;Holder{ a: a, b: b, } } type Holder struct { a int b bool } 没过几天，现在我们认为有必要增加一个字符串变量，tut 库不得不被修改为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package tut func New(a int, b bool, c string) *Holder { return \u0026amp;Holder{ a: a, b: b, c: c, } } type Holder struct { a int b bool c string } 想象一下，tut 库的使用者在面对三次接口 New() 的升级时，那么势必会导致：由于New方法更改函数的入口，所以，任何使用该方法创建对象的地方都必须修改。显然这样是很麻烦的，有没有一种方式，可以不改变New方法的入参，及时需要更新迭代对象版本时（增添改属性），New方法在调用的时候，即使不变也可以维护以前的所有属性及功能。答案就是，函数选项模式\n对此我们需要 Functional Options 模式来解救之。\n新的方式 这里以colly框架源码为例介绍函数选项模式\n1 go get -u github.com/gocolly/colly colly提供了这样一个Collector实例构建方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 // NewCollector creates a new Collector instance with default configuration func NewCollector(options ...func(*Collector)) *Collector { c := \u0026amp;Collector{} c.Init() for _, f := range options { f(c) } c.parseSettingsFromEnv() return c } 可以看到，本实例初始化函数的入参是一个...func(*Collector)类型。为什么这样做呢？即，为什么要将初始化函数的Colletor指针作为当前函数的入参呢？这个函数又有什么作用呢？继续往下看。\n在colly的源码里，可以翻阅到一些函数的返回值类型为func(*Collector)的函数，这些函数是干嘛的呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // AllowURLRevisit instructs the Collector to allow multiple downloads of the same URL func AllowURLRevisit() func(*Collector) { return func(c *Collector) { c.AllowURLRevisit = true } } // CacheDir specifies the location where GET requests are cached as files. func CacheDir(path string) func(*Collector) { return func(c *Collector) { c.CacheDir = path } } ... 我们可以看到：这些函数主要是将Colletor实例的一些属性进行更新，而且，翻阅源码，可以看到：Colly几乎为每一个Colletor的字段属性都提供了一个这样的函数。而在初始化实例的时候，可以通过提供这些函数（即选项）来配置当前的Collector实例.\n实例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package main type Option func(h *Holder) func NewHolder(opts ...Option) *Holder { h := \u0026amp;Holder{a: -1} for _, opt := range opts { opt(h) } return h } type Holder struct { a int b string c bool // 业务需求，更改参数 d interface{} } func (h Holder) GetA() int { return h.a } func (h Holder) GetB() string { return h.b } func (h Holder) GetC() bool { return h.c } func OptionWithA(a int) Option { return func(h *Holder) { h.a = a } } func OptionWithB(b string) Option { return func(h *Holder) { h.b = b } } // Deprecated:已弃用的标志 func OptionWithC(c bool) Option { return func(h *Holder) { h.c = c } } func OptionWithD(d interface{}) Option { return func(h *Holder) { h.d = d } } func main() { h := NewHolder( OptionWithB(\u0026#34;b\u0026#34;), OptionWithC(false), ) println(h.GetA()) println(h.GetB()) println(h.GetC()) } 好处\n这样做虽然看似，增添了很多无用代码，但是这样做可以使得，colly的NewCollector在创建实例对象时具有更好的灵活性： 这样做，以后在更新版本时，不需要更改原有的New方法，即，这对于调用方来说，这样做的意义在于向前兼容性（增添一些属性不会影响原来的调用方的逻辑代码，一般不会更改或减少原来的元素） 当配置某个实例时，配置项很多，而且有一些字段不是必须的，是可选的，就可以考虑使用函数选项模式 四、并发 先看一个最简单的并发问题\n1 2 3 4 5 6 7 8 9 10 func main(){ for i := 0; i \u0026lt; 9; i++ { go func() { fmt.Print(i) }() } time.Sleep(2 * time.Second) } // 运行结果：296667939 // 可以看到有并发问题出现 解决思路\n匿名函数可以捕获父函数的成员变量。这里的匿名函数里的i指的是多次循环里的同一个地址里的i，开多个协程来操作同一个地址上的变量，显然存在并发问题\n解决方法：不再使得一个地址的变量被多个goroutine操作，在创建协程的作用域上，cv一份变量，这样就可以得到一个goroutine只操作一个地址的变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main (){ for i := 0; i \u0026lt; 9; i++ { go func(i int) { fmt.Println(i) }(i) } } // or func main(){ for i := 0; i \u0026lt; 9; i++ { i := i go func() { fmt.Print(i) }() } time.Sleep(2 * time.Second) } ","permalink":"https://cold-bin.github.io/post/golang%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3%E9%9B%86%E9%94%A6/","tags":["go编程思想"],"title":"Golang编程思想集锦"},{"categories":["golang"],"contents":"Unicode编码和utf-8编码的关系 需要注意的是，Unicode 只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。\n比如，汉字\u0026rsquo;严\u0026rsquo;的 Unicode 是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说，这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。\n这里就有两个严重的问题\n第一个问题，如何才能区别 Unicode 和 ASCII ？ 计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？\n第二个问题，我们已经知道，英文字母只用一个字节表示就够了，如果 Unicode 统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。\n它们造成的结果是：\n1）出现了 Unicode 的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示 Unicode。\n2）Unicode 在很长一段时间内无法推广，直到互联网的出现。\nutf-8的必要性\n互联网的普及，强烈要求出现一种统一的编码方式。UTF-8 就是在互联网上使用最广的一种 Unicode 的实现方式。其他实现方式还包括 UTF-16（字符用两个字节或四个字节表示）和 UTF-32（字符用四个字节表示），不过在互联网上基本不用。这里的关系是，UTF-8 是 Unicode 的实现方式之一。\nUTF-8 最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。\nUTF-8 的编码规则很简单，只有二条：\n1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。\n2）对于n字节的符号（n \u0026gt; 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。\n下表总结了编码规则，字母x表示可用编码的位。\nUnicode符号范围 | UTF-8编码方式\n(十六进制) | （二进制）\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\n0000 0000-0000 007F | 0xxxxxxx\t\u0026ndash;\u0026gt; 8位\n0000 0080-0000 07FF | 110xxxxx 10xxxxxx\t\u0026ndash;\u0026gt; 16位\n0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx\t\u0026ndash;\u0026gt; 24位\n0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx \u0026ndash;\u0026gt; 32位\n跟据上表，解读 UTF-8 编码非常简单。\n如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，就表示当前字符占用多少个字节。\n下面，还是以汉字严为例，演示如何实现 UTF-8 编码。\n\u0026lsquo;严\u0026rsquo;的 Unicode 是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800 - 0000 FFFF），因此\u0026rsquo;严\u0026rsquo;的 UTF-8 编码需要三个字节，即格式是1110xxxx 10xxxxxx 10xxxxxx。然后，从严的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。\n这样就得到了，\u0026lsquo;严\u0026rsquo;的 UTF-8 编码是11100100 10111000 10100101，转换成十六进制就是E4B8A5。\nrune类型是什么 rune 类型是 Go 语言的一种特殊数字类型。在 builtin/builtin.go 文件中，它的定义：type rune = int32；官方对它的解释是：rune 是类型 int32 的别名，在所有方面都等价于它，用来区分字符值跟整数值。使用单引号定义 ，返回采用 UTF-8 编码的 Unicode 码点。Go 语言通过 rune 处理中文，支持国际化多语言。\n众所周知，Go 语言有两种类型声明方式：一种叫类型定义声明，另一种叫类型别名声明。其中，别名的使用在大型项目重构中作用最为明显，它能解决代码升级或迁移过程中可能存在的类型兼容性问题。而rune 跟 byte 是 Go 语言中仅有的两个类型别名，专门用来处理字符。当然，我们也可以通过 type 关键字加等号的方式声明更多的类型别名。\nrune类型怎么用 我们知道，字符串由字符组成，字符的底层由字节组成，而一个字符串在底层的表示是一个字节序列。在 Go 语言中，字符可以被分成两种类型处理：对占 1 个字节的英文类字符，可以使用 byte（或者 unit8 ）；对占 1 ~ 4 个字节的其他字符，可以使用 rune（或者 int32 ），如中文、特殊符号等。\n下面，我们通过示例应用来具体感受一下。\n统计带中文字符串长度 1 2 // 使用内置函数 len() 统计字符串长度 fmt.Println(len(\u0026#34;Go语言编程\u0026#34;)) // 输出：14 前面说到，字符串在底层的表示是一个字节序列。其中，英文字符占用 1 字节，中文字符占用 3 字节，所以得到的长度 14 显然是底层占用字节长度，而不是字符串长度，这时，便需要用到 rune 类型。\n1 2 // 转换成 rune 数组后统计字符串长度 fmt.Println(len([]rune(\u0026#34;Go语言编程\u0026#34;))) // 输出：6 这回对了。很容易，我们解锁了 rune 类型的第一个功能，即统计字符串长度。\n截取带中文字符串 如果想要截取字符串中 ”Go语言“ 这一段，考虑到底层是一个字节序列，或者说是一个数组，通常情况下，我们会这样：\n1 2 3 s := \u0026#34;Go语言编程\u0026#34; // 8=2*1+2*3 fmt.Println(s[0:8]) // 输出：Go语言 结果符合预期。但是，按照字节的方式进行截取，必须预先计算出需要截取字符串的字节数，如果字节数计算错误，就会显示乱码，比如这样：（因为字符串里含有非asccii编码的码点值，所以单个字节的读有时候无法正确读出字符串，会乱码）\n1 2 s := \u0026#34;Go语言编程\u0026#34; fmt.Println(s[0:7]) // 输出：Go语� 此外，如果截取的字符串较长，那通过字节的方式进行截取显然不是一个高效准确的办法。那有没有不用计算字节数，简单又不会出现乱码的方法呢？不妨试试这样：（针对含有非asccii码的码点，而是在utf-8表里的其他码点，需要2~4个字节才能表述一个正确的符号，例如汉字“烈”的utf-8码值为：28872）\n1 2 3 s := \u0026#34;Go语言编程\u0026#34; // 转成 rune 数组，需要几个字符，取几个字符 fmt.Println(string([]rune(s)[:4])) // 输出：Go语言 到这里，我们解锁了 rune 类型的第二个功能，即截取字符串。\n为什么 rune 类型可以 通过上面的示例，我们发现似乎在处理带中文的字符串时，都需要用到 rune 类型，这究竟是为什么呢？除了使用 rune 类型，还有其他方法吗？\n在深入思考之前，我们需要首先弄清楚 string 、byte、rune 三者间的关系。\n字符串在底层的表示是由单个字节组成的一个不可修改的字节序列，字节使用 UTF-8[1] 编码标识 Unicode[2] 文本。Unicode 文本意味着 .go 文件内可以包含世界上的任意语言或字符，该文件在任意系统上打开都不会乱码。UTF-8 是 Unicode 的一种实现方式，是一种针对 Unicode 可变长度的字符编码，它定义了字符串具体以何种方式存储在内存中。UFT-8 使用 1 ~ 4 为每个字符编码。\nGo 语言把字符分 byte 和 rune 两种类型处理。byte 是类型 unit8 的别名，用于存放占 1 字节的 ASCII 字符，如英文字符，返回的是字符原始字节。rune 是类型 int32 的别名，用于存放多字节字符，如占 3 字节的中文字符，返回的是字符 Unicode 码点值。如下图所示：\n1 2 3 4 5 s := \u0026#34;Go语言编程\u0026#34; // byte fmt.Println([]byte(s)) // 输出：[71 111 232 175 173 232 168 128 231 188 150 231 168 139] // rune fmt.Println([]rune(s)) // 输出：[71 111 35821 35328 32534 31243] 它们的对应关系如下图：\n可以看到：字符串转化为Unicode编码时，就会将每个字符的Unicode码值打印出来（毕竟Unicode的所有码值涵盖了所有符号和文字）；再将Unicode编码转化为utf-8编码时，原来的assiic码值不变，但是超过assiic（即一个字节）可以表示的字符，会拆分成多个字节表示字符。\n了解了这些，我们再回过来看看，刚才的问题是不是清楚明白很多？接下来，让我们再来看看源码中是如何处理的，以 utf8.RuneCountInString()[3] 函数为例。\n示例：\n1 2 // 统计字符串长度 fmt.Println(utf8.RuneCountInString(\u0026#34;Go语言编程\u0026#34;)) // 输出：6 源码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // RuneCountInString is like RuneCount but its input is a string. func RuneCountInString(s string) (n int) { // 调用 len() 函数得到字节数 ns := len(s) for i := 0; i \u0026lt; ns; n++ { c := s[i] // 如码点值小于 128，则为占 1 字节的 ASCII 字符（或者说英文字符），长度 + 1 if c \u0026lt; RuneSelf { // RuneSelf = 128 // ASCII fast path i++ continue } // 查询首字节信息表，得到中文占 3 字节，所以这里的 x = 3 x := first[c] // 判断 x = 3,xx = 241（0xF1） if x == xx { i++ // invalid. continue } // 提取有效的 UTF-8 字节长度编码信息，size = 3 size := int(x \u0026amp; 7) if i+size \u0026gt; ns { i++ // Short or invalid. continue } // 提取有效字节范围 accept := acceptRanges[x\u0026gt;\u0026gt;4] // accept.lo，accept.hi，表示 UTF-8 中第二字节的有效范围 // locb = 0b10000000，表示 UTF-8 编码非首字节的数值下限 // hicb = 0b10111111，表示 UTF-8 编码非首字节的数值上限 if c := s[i+1]; c \u0026lt; accept.lo || accept.hi \u0026lt; c { size = 1 } else if size == 2 { } else if c := s[i+2]; c \u0026lt; locb || hicb \u0026lt; c { size = 1 } else if size == 3 { } else if c := s[i+3]; c \u0026lt; locb || hicb \u0026lt; c { size = 1 } i += size } return n } 调用该函数时，传入一个原始的字符串，代码会根据每个字符的码点大小判断是否为 ASCII 字符，如果是，则算做 1 位；如果不是，则查询首字节表，明确字符占用的字节数，验证有效性后再进行计数。\n文章出处1\ngo中rune类型的详解\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://cold-bin.github.io/post/%E8%AF%A6%E8%A7%A3go%E4%B8%AD%E7%9A%84rune%E7%B1%BB%E5%9E%8B/","tags":["go中的rune类型","unicode与utf8"],"title":"详解Go中的rune类型"},{"categories":["golang"],"contents":"一. 什么是内存对齐, 为啥要内存对齐？ 在解释什么是内存对齐之前，我们需要先了解一下CPU和内存数据交互的过程。CPU和内存是通过总线进行数据交互的。其中地址总线用来传递CPU需要的数据地址，内存将数据通过数据总线传递给CPU， 或者CPU将数据通过数据总线回传给内存。\n首先我们需要知道以下概念：\n(1) 机器字长 在计算机领域，对于某种特定的计算机设计而言，字（word）是用于表示其自然的数据单位的术语，是用来表示一次性处理事务的固定长度。一个字的位数，即字长。\n(2) 地址总线 专门用来传送地址的，由于地址只能从CPU传向外部存储器或I／O端口，所以地址总线总是单向的。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微型机的地址总线为16位，则其最大可寻址空间为2^16＝64KB，16位微型机的地址总线为20位，其可寻址空间为2^20＝1MB。\n(3) 数据总线 是CPU与内存或其他器件之间的数据传送的通道。每条传输线一次只能传输1位二进制数据, 数据总线每次可以传输的字节总数就称为机器字长或者数据总线的宽度。 它决定了CPU和外界的数据传送速度。我们现在日常使用的基本上是32位(每次可以传输4字节)或者64位(每次可以传输8字节)机器字长的机器。\n由于数据是通过总线进行传输，若数据未经一定规则的对齐，CPU的访址操作与总线的传输操作将会异常的复杂，所以编译器在程序编译期间会对各种类型的数据按照一定的规则进行对齐, 对齐过程会按一定规则对内存的数据段进行的字节填充， 这就是字节对齐。\n例如: 现在要存储变量A（int32）和B（int64）那么不做任何字节对齐优化的情况下，内存布局是这样的\n字节对齐优化后是这样子的：\n一看感觉字节对齐后浪费了内存， 但是当我们去读取内存中的数据给CPU时，64位的机器（一次可以原子读取8字节）在内存对齐和不对齐的情况下A变量都只需要原子读取一次就行， 但是对齐后B变量的读取只需一次， 而不对齐的情况下，B需要读取2次，且需要额外的处理牺牲性能来保证2次读取的原子性。所以本质上，内存填充是一种以空间换时间， 通过额外的内存填充来提高内存读取的效率的手段。\n总的来说，内存对齐主要解决以下两个问题：\n【1】跨平台问题：如果数据不对齐，那么在64位字长机器存储的数据可能在32位字长的机器可能就无法正常的读取。\n【2】性能问题：如果不对齐，那么每个数据要通过多少次总线传输是未知的，如果每次都要处理这些复杂的情况，那么数据的读/写性能将会收到很大的影响。之所以有些CPU支持访问任意地址，是因为处理器在后面多做了很多额外处理。\n二. 内存对齐的规则是什么? 内存对齐主要是为了保证数据的原子读取， 因此内存对齐的最大边界只可能为当前机器的字长。当然如果每种类型都使用最大的对齐边界，那么对内存将是一种浪费，实际上我们只要保证同一个数据不要分开在多次总线事务中便可。\nGo在其官方文档 Size and alignment guarantees - Golang spec 就描述了其在内存对齐方面的细节。\nGo也提供了unsafe.Alignof(x)来返回一个类型的对齐值，并且作出了如下约定：\n对于任意类型的变量 x ，unsafe.Alignof(x) 至少为 1。 对于 struct 结构体类型的变量 x，计算 x 每一个字段 f 的 unsafe.Alignof(x.f)，unsafe.Alignof(x) 等于其中的最大值。 对于 array 数组类型的变量 x，unsafe.Alignof(x) 等于构成数组的元素类型的对齐倍数。 没有任何字段的空 struct{} 和没有任何元素的 array 占据的内存空间大小为 0，不同的大小为 0 的变量可能指向同一块地址。 总结来说，分为基本类型对齐和结构体类型对齐\n(1) 基本类型对齐 go语言的基本类型的内存对齐是按照基本类型的大小和机器字长中最小值进行对齐\n数据类型 类型大小（32/64位） 最大对齐边界（32位） 最大对齐边界（64位） int8/uint8/byte 1字节 1 1 int16/uint16 2字节 2 2 int32/uint32/rune/float32/complex32 4字节 4 4 int64/uint64/float64/complex64 8字节 4 8 string 8字节/16字节 4 8 slice 12字节/24字节 4 8 我们可以在自己的机器上编码测试了一下（我的机器是64位的 Mac OS X）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package service import ( \u0026#34;testing\u0026#34; \u0026#34;unsafe\u0026#34; ) func TestAlign(t *testing.T) { var byteTest byte = \u0026#39;a\u0026#39; var int8Test int8 = 0 var int16Test int16 = 0 var int32Test int32 = 0 var int64Test int64 = 0 var uint8Test uint8 = 0 var uint16Test uint16 = 0 var uint32Test uint32 = 0 var uint64Test uint64 = 0 var float32Test float32 = 0.0 var float64Test float64 = 0.0 println(\u0026#34;byte max align size =\u0026gt;\u0026#34;, unsafe.Alignof(byteTest)) println(\u0026#34;int8/uint8 max align size =\u0026gt;\u0026#34;, unsafe.Alignof(int8Test), \u0026#34;/\u0026#34; , unsafe.Alignof(uint8Test)) println(\u0026#34;int16/uint16 max align size =\u0026gt;\u0026#34;, unsafe.Alignof(int16Test), \u0026#34;/\u0026#34; , unsafe.Alignof(uint16Test)) println(\u0026#34;int32/uint32/float32 max align size =\u0026gt;\u0026#34;, unsafe.Alignof(int32Test), \u0026#34;/\u0026#34; , unsafe.Alignof(uint32Test), \u0026#34;/\u0026#34;, unsafe.Alignof(float32Test)) println(\u0026#34;int64/uint64/float64 max align size =\u0026gt;\u0026#34;, unsafe.Alignof(int64Test), \u0026#34;/\u0026#34; , unsafe.Alignof(uint64Test), \u0026#34;/\u0026#34;, unsafe.Alignof(float64Test)) var s string = \u0026#34;343240000000000\u0026#34; println(\u0026#34;string max align size =\u0026gt;\u0026#34;, unsafe.Alignof(s)) var sliceTest []string println(\u0026#34;slice\u0026#39;s size/max align size =\u0026gt;\u0026#34;, unsafe.Alignof(sliceTest), \u0026#34;/\u0026#34; , unsafe.Sizeof(sliceTest)) var structTest struct{} println(\u0026#34;struct{}\u0026#39;s size / max align size =\u0026gt;\u0026#34;, unsafe.Alignof(structTest), \u0026#34;/\u0026#34; , unsafe.Sizeof(structTest)) } 复制\n运行结果：\n1 2 3 4 5 6 7 8 byte max align size =\u0026gt; 1 int8/uint8 max align size =\u0026gt; 1 / 1 int16/uint16 max align size =\u0026gt; 2 / 2 int32/uint32/float32 max align size =\u0026gt; 4 / 4 / 4 int64/uint64/float64 max align size =\u0026gt; 8 / 8 / 8 string max align size =\u0026gt; 8 slice\u0026#39;s size/max align size =\u0026gt; 8 / 24 struct{}\u0026#39;s size / max align size =\u0026gt; 1 / 0 复制\n(2) 结构体类型对齐 go语言的结构体的对齐是先对结构体的每个字段进行对齐，然后对总体的大小按照最大对齐边界的整数倍进行对齐。有一个特殊的情况就是，如果空结构体嵌套到一个结构体尾部，那么这个结构体也是要额外对齐的，因为如果有指针指向该字段, 返回的地址将在结构体之外，如果此指针一直存活不释放对应的内存，就会有内存泄露的问题。\n下面通过一些列的例子来说明一下结构体对齐的规则，需要额外说明的是结构体内的字段位置其实都是通过计算到结构体首地址的偏移量来确定的，对所有的字段来说，首地址就是结构体内索引值为0的地址。\n案例一\n1 2 3 4 5 type TestStruct1 struct { a int8 // 1 字节====\u0026gt; max align 1 字节 b int32 // 4 字节====\u0026gt; max align 4 字节 c []string // 24 字节====\u0026gt; max align 8 字节 } 复制\nTestStruct1在编译期就会进行字节对齐的优化。优化后各个变量的相对位置如下图(以64位字长下环境为例)：\nimage.png\nTestStruct1 内存占用大小分析：最大对齐边界为8，总体字节数 = 1 + （align 3） + 4 + 24 = 32, 由于32刚好是8的倍数，所以末尾无需额外填充，最后这个结构体的大小为32字节。\n案例二\n1 2 3 4 5 type TestStruct2 struct { a []string // 24 字节====\u0026gt; max align 8 字节 b int64 // 8 字节====\u0026gt; max align 8 字节 c int32 // 4 字节====\u0026gt; max align 4 字节 } 复制\nimage.png\nTestStruct2 内存占用大小分析：最大对齐边界为8字节，总体字节数 = 24（a） + 8（b） + 4（c） + 4（填充） = 40, 由于40刚好是8的倍数，所以c字段填充完后无需额外填充了。\n案例三\n1 2 3 4 5 type TestStruct3 struct { a int8 b int64 c struct{} } 复制\nimage.png\nTestStruct3 内存占用大小分析：最大对齐边界为8字节，总体字节数 = 1（a）+ 7(填充) + 8（b） + 8（c填充）=24, 空结构体理论上不占字节，但是如果在另一个结构体尾部则需要进行额外字节对齐 。\n案例四\n1 2 3 4 5 type TestStruct4 struct { a struct{} b int8 c int32 } 复制\nimage.png\nTestStruct4 内存占用大小分析：最大对齐边界为4字节，总体字节数 = 0(a)+ 1（b）+ 7(填充) + 4（c) = 8。\n(3) 测试验证 执行以下代码（环境是64位机器字长的）就可以看到我们之前案例中分析的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 func TestAlignStruct(t *testing.T) { var testStruct1 TestStruct1 println(\u0026#34;size of testStruct1:\u0026#34;, unsafe.Sizeof(testStruct1)) var testStruct2 TestStruct2 println(\u0026#34;size of testStruct2:\u0026#34;, unsafe.Sizeof(testStruct2)) var testStruct3 TestStruct3 println(\u0026#34;size of testStruct4 / testStruct4\u0026#39;s a size:\u0026#34;, unsafe.Sizeof(testStruct3), \u0026#34;/\u0026#34; , unsafe.Sizeof(testStruct3.c)) var testStruct4 TestStruct4 println(\u0026#34;size of testStruct4 / testStruct4\u0026#39;s a size:\u0026#34;, unsafe.Sizeof(testStruct4), \u0026#34;/\u0026#34; , unsafe.Sizeof(testStruct4.a)) } 输出为：\n1 2 3 4 5 6 7 === RUN TestAlignStruct size of testStruct1: 32 size of testStruct2: 40 size of testStruct4 / testStruct4\u0026#39;s a size: 24 / 0 size of testStruct4 / testStruct4\u0026#39;s a size: 8 / 0 --- PASS: TestAlignStruct (0.00s) PASS ","permalink":"https://cold-bin.github.io/post/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/","tags":["go内存对齐"],"title":"Go的结构体内存对齐"},{"categories":["golang"],"contents":"阅读本文前，需具备flag基础\n通常我们有种需求：可以在不修改程序源码的情况下，控制一些程序内部的变化。比如配置文件，我们可以手动更改配置文件，也可以在程序启动时，通过命令行参数在程序运行时，给程序提供一些配置选项，这样可以方便地不需要更改程序源码就可以达到在外部控制程序内部一些变化。\n以下是通过命令行操作给项目地配置文件进行动态指定，这样就可以不用手动修改配置文件了（比较在非图形化的 Linux 上部署服务，直接通过命令行实现配置文件内容变化算是比直接修改要方便一点点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 // @author cold bin // @date 2022/7/17 package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; ) /* 命令行操作结合配置文件进行操作,命令行提供默认操作 */ func main() { var port, mode, StartTime, loglevel string //使用命令前需要先解析 flag.Parse() args := flag.Args() fmt.Println(args) if len(args) \u0026lt;= 0 { log.Println(\u0026#34;请输入命令参数\u0026#34;) return } //子命令 switch args[0] { case \u0026#34;conf\u0026#34;: cmdSet := flag.NewFlagSet(\u0026#34;conf\u0026#34;, flag.ExitOnError) cmdSet.StringVar( \u0026amp;port, \u0026#34;port\u0026#34;, \u0026#34;8080\u0026#34;, \u0026#34;指定项目运行监听端口\u0026#34;, ) cmdSet.StringVar( \u0026amp;port, \u0026#34;p\u0026#34;, \u0026#34;8080\u0026#34;, \u0026#34;指定项目运行监听端口\u0026#34;, ) cmdSet.StringVar( \u0026amp;mode, \u0026#34;mode\u0026#34;, \u0026#34;debug\u0026#34;, \u0026#34;指定项目启动模式，默认debug为开发模式，release为发布模式，test为测试模式\u0026#34;, ) cmdSet.StringVar( \u0026amp;mode, \u0026#34;m\u0026#34;, \u0026#34;debug\u0026#34;, \u0026#34;指定项目启动模式，默认debug为开发模式，release为发布模式，test为测试模式\u0026#34;, ) cmdSet.StringVar( \u0026amp;StartTime, \u0026#34;time\u0026#34;, time.Now().Format(\u0026#34;2006-01-02\u0026#34;), \u0026#34;项目的启动时间，未指定时，默认当前系统时间\u0026#34;, ) cmdSet.StringVar( \u0026amp;StartTime, \u0026#34;t\u0026#34;, time.Now().Format(\u0026#34;2006-01-02\u0026#34;), \u0026#34;项目的启动时间，未指定时，默认当前系统时间\u0026#34;, ) cmdSet.StringVar( \u0026amp;loglevel, \u0026#34;logLevel\u0026#34;, \u0026#34;debug\u0026#34;, \u0026#34;默认日志是warning级别，还有debug、error级别\u0026#34;, ) cmdSet.StringVar( \u0026amp;loglevel, \u0026#34;l\u0026#34;, \u0026#34;debug\u0026#34;, \u0026#34;默认日志是warning级别，还有debug、error级别\u0026#34;, ) //使用命令前需要解析 _ = cmdSet.Parse(args[1:]) fmt.Println(port, mode, StartTime, loglevel) case \u0026#34;java\u0026#34;: log.Println(\u0026#34;java命令\u0026#34;) default: log.Fatal(\u0026#34;命令错误\u0026#34;) } } 当然也可以将出现地常量或字符串进行更好的迁移，使用有意义地变量名代替阅读观感更佳\n","permalink":"https://cold-bin.github.io/post/flag%E5%8C%85%E7%9A%84%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/","tags":["go语言flag包实现命令行功能"],"title":"Flag包的在go项目的实践"},{"categories":["golang"],"contents":"Make介绍 make命令是GNU的工程化编译工具，用以实现工程化的管理，提高开发效率。\nMake解释Makefile 中的指令（应该说是规则）。在Makefile文件中描述了整个工程所有文件的编译顺序、编译规则。Makefile 有自己的书写格式、关键字、函数。像C 语言有自己的格式、关键字和函数一样。而且在Makefile 中可以使用系统shell所提供的任何命令来完成想要的工作。\nMakefile文件 构建规则都写在Makefile文件里面，要学会如何Make命令，就必须学会如何编写Makefile文件。\n文件格式 Makefile文件由一系列规则（rules）构成。\n每条规则要说明构建的依赖条件，及怎么样去构建。那么格式如下，\n1 2 3 # rule \u0026lt;target\u0026gt; : \u0026lt;prerequisites\u0026gt; [tab] \u0026lt;commands\u0026gt; target: 目标\nprerequisites： 先决条件，或者说依赖条件\ntab: 使用tab来缩进\ncommand: 要执行的命令（可以说是小型的shell代码块）\ntarget目标 一个目标，可以是文件名，也可以是某个操作的名字（伪目标），这个名字由自己定义，用来指明要构建的对象。\n1 2 create: touch newfile 比如上面这条规则，伪目标为create，命令作用为创建一个文件。要想构建这个操作，调用make create(指定目标进行构建编译)即可。\n但是如果目录下，存在一个文件名为create，那么构建命令就不会去执行。为了解决这个问题，当使用伪目标时，可以明确声明create是“伪目标“，告诉make跳过文件检查。\n1 2 3 .PHONY: clean create: touch newfile 如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。\nprerequisites先决条件 先决条件，通常是文件名，多个名字用空格分隔。\n它定义了一个是否进行重新构建的判断标准： 如果有任何一个先决文件发生改变（时间戳更新），就要重新构建。\n1 2 result.txt: source.txt cp source.txt result.txt 上面代码中，构建 result.txt 的前置条件是 source.txt 。如果当前目录中，source.txt 已经存在，那么make result.txt可以正常运行，否则必须再写一条规则，来生成 source.txt 。\n1 2 source.txt: echo \u0026#34;this is the source\u0026#34; \u0026gt; source.txt 上面代码中，source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成。\n1 2 make result.txt make result.txt 上面命令连续执行两次make result.txt。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt 也不会重新生成。\n如果需要生成多个文件，往往采用下面的写法。\n1 source: file1 file2 file3 上面代码中，source 是一个伪目标，只有三个前置文件，没有任何对应的命令。\n1 make source 执行make source命令后，就会一次性生成 file1，file2，file3 三个文件。这比下面的写法要方便很多。\n1 2 3 make file1 make file2 make file3 如果先决条件也是伪目标，即不是一个实实在在的、真实存在的文件，而是仅仅是一个目标的标识符。那么在当前目标执行前，就会先去先决条件对应的伪目标去递归执行。当前目标被指定make时，会先调用先决条件的目标，如果先决条件也具备这样的依赖时，也会如此递归调用下去\ntarget: target1 target2\n​\techo \u0026gt; \u0026ldquo;this tartget\u0026rdquo;\ntarget1:\n​\techo \u0026gt; \u0026ldquo;this is target1\u0026rdquo;\ntarget2:\n​\techo \u0026gt; \u0026ldquo;this is target2\u0026rdquo;\ncommands命令 命令是构建目标时具体执行的指令，由一行或多行shell组成。每行命令之前必须有一个tab键缩进。如果想用其他键缩进，可以用内置变量.RECIPEPREFIX声明。\n1 2 3 .RECIPEPREFIX = \u0026gt; hello: \u0026gt; echo Hello, world 需要注意的是，每行shell在一个单独的bash进程中执行，多进程间没有继承关系。\n1 2 3 var: export name=wangpeng echo \u0026#34;myname is $name\u0026#34; 运行上面的构建 ，发现变量name是取不到的，因为两行shell在两个独立的bash中运行。\n最直接的方法就是将两行shell写到一行中，\n1 2 var: export name=wangpeng; echo \u0026#34;myname is $name\u0026#34; 第二种办法，在换行前加反斜杠\\转义，\n1 2 3 var: export name=wangpeng \\ echo \u0026#34;myname is $name\u0026#34; 还有第三种办法是使用。ONESHELL内置命令。\n1 2 3 4 .ONESHELL: var: export name=wangpeng echo \u0026#34;myname is $name\u0026#34; 文件语法 注释 行首井号（#）表示注释。\n回显 回显是指，在执行到每行命令前，将命令本身打印出来。\n1 2 test: # 这是测试 执行上面构建会输出\n1 2 make test # 这是测试 在命令的前面加上@，就可以关闭回声。\n1 2 test: @# 这是测试 这下构建时就不会有任何输出。\n通配符 Makefile 的通配符与 Bash 一致，主要有星号（）、问号（？）.比如 .text 表示所有后缀名为text的文件。\n模式匹配 Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。\n1 %.o: %.c 等同于下面的写法。\n1 2 f1.o: f1.c f2.o: f2.c 使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。\n变量和赋值符 Makefile 允许自定义变量。\n1 2 3 txt = Hello World test: @echo $(txt) 调用shell中的变量，需要使用两个美元符号$$。\nMakefile一共提供了四个赋值运算符 （=、:=、?=、+=），它们的区别请看StackOverflow。\n1 2 3 4 5 6 7 8 9 10 11 VARIABLE = value # 在执行时扩展，允许递归扩展。 VARIABLE := value # 在定义时扩展。 VARIABLE ?= value # 只有在该变量为空时才设置值。 VARIABLE += value # 将值追加到变量的尾端。 内置变量 Make命令提供一系列内置变量，比如，$(CC)指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。\n1 2 output: $(CC) -o output input.c 判断和循环 Makefile使用 Bash 语法，完成判断和循环。\n1 2 3 4 5 ifeq ($(CC),gcc) libs=$(libs_for_gcc) else libs=$(normal_libs) endif 上面代码判断当前编译器是否 gcc ，然后指定不同的库文件。\n1 2 3 4 5 6 7 8 9 10 11 12 LIST = one two three all: for i in $(LIST); do \\ echo $$i; \\ done # 等同于 all: for i in one two three; do \\ echo $i; \\ done 上面代码的运行结果。\n1 2 3 one two three Makefile例子演示 1. 执行多个目标 1 2 3 4 5 6 7 8 9 10 .PHONY: cleanall cleanobj cleandiff cleanall: cleanobj cleandiff rm all cleanobj: rm *.o cleandiff: rm *.diff 上面代码可以调用不同目标，删除不同后缀名的文件，也可以调用一个目标（cleanall），删除所有指定类型的文件。\n2. 构建golang项目 以下Makefile仅供参考，项目仓库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # 伪目标，如果未指定终极目标，将默认使用Makefile里的第一个目标， # 即Makefile里最靠前的规则（本Makefile是all这个目标） # 因此，当执行make时，会默认第一个目标为终极目标，所以all后面的依赖都会执行下去 # 因此，可以将all后面放置一些必须编译的默认选项，在只使用make时；当然也可以使用make指定终极目标进行构建 .PHONY: openssl format build build_linux build_win clean swag \\ docker-build help format test run all: openssl format build # 声明编译项目的文件名 BUILD_NAME=web_app # swagger接口文档初始化 swag: @swag init # 在./config目录，签发自建的tls证书 # 或者使用go标准库：go run $GOROOT/src/crypto/tls/generate_cert.go --host localhost openssl: @openssl genrsa -out ./config/key.pem 2048;openssl req -new -x509 -key ./config/key.pem -out ./config/cert.pem -days 3650 # 使用Dockerfile对项目打包编译出镜像 docker-build: swag @docker build -t ${BUILD_NAME}:1.0 # 格式化项目 format: @go fmt ./ @go vet ./ # 测试代码 test: swag @go test -v #回归测试 # 直接运行项目根目录下已经编译好的二进制文件 run: ./${BUILD_NAME} # 默认编译 build: test @go build -o ${BUILD_NAME} ${SOURCE} # 交叉编译--适应linux系统 build_linux: test @CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ${BUILD_NAME} . # 交叉编译--适应windows系统 build_win: test @CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o ${BUILD_NAME} . # 清除编译文件 clean: @go clean # 帮助命令 help: @echo \u0026#34;make - 格式化 Go 代码、更新swagger文档、生成tls证书、测试代码、编译生成二进制文件\u0026#34; @echo \u0026#34;make docker-build - 构建本项目的Docker镜像\u0026#34; @echo \u0026#34;make build - 编译 Go 代码, 生成当前环境默认的二进制文件\u0026#34; @echo \u0026#34;make build_linux - 编译 Go 代码, 生成linux环境二进制文件\u0026#34; @echo \u0026#34;make build_win - 编译 Go 代码, 生成windows环境二进制文件\u0026#34; @echo \u0026#34;make run - 直接运行 Go 代码\u0026#34; @echo \u0026#34;make clean - 移除二进制文件和 vim swap files\u0026#34; @echo \u0026#34;make format - 运行 Go 工具 \u0026#39;fmt\u0026#39; and \u0026#39;vet\u0026#39;\u0026#34; 另一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 .PHONY: all build clean run check cover lint docker help BIN_FILE=hello all: check build build: @go build -o \u0026#34;${BIN_FILE}\u0026#34; clean: @go clean rm --force \u0026#34;xx.out\u0026#34; test: @go test check: @go fmt ./ @go vet ./ cover: @go test -coverprofile xx.out @go tool cover -html=xx.out run: ./\u0026#34;${BIN_FILE}\u0026#34; lint: golangci-lint run --enable-all docker: @docker build -t leo/hello:latest . help: @echo \u0026#34;make 格式化go代码 并编译生成二进制文件\u0026#34; @echo \u0026#34;make build 编译go代码生成二进制文件\u0026#34; @echo \u0026#34;make clean 清理中间目标文件\u0026#34; @echo \u0026#34;make test 执行测试case\u0026#34; @echo \u0026#34;make check 格式化go代码\u0026#34; @echo \u0026#34;make cover 检查测试覆盖率\u0026#34; @echo \u0026#34;make run 直接运行程序\u0026#34; @echo \u0026#34;make lint 执行代码检查\u0026#34; @echo \u0026#34;make docker 构建docker镜像\u0026#34; 这样就很方便地通过一个make命令完成对项目的构建。\n","permalink":"https://cold-bin.github.io/post/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/","tags":["makefile"],"title":"Makefile在go项目的实践"},{"categories":["golang"],"contents":"Go语言内置的flag包实现了命令行参数的解析，flag包使得开发命令行工具更为简单。\nos.Args 如果你只是简单的想要获取命令行参数，可以像下面的代码示例一样使用os.Args来获取命令行参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) //os.Args demo func main() { //os.Args是一个[]string if len(os.Args) \u0026gt; 0 { for index, arg := range os.Args { fmt.Printf(\u0026#34;args[%d]=%v\\n\u0026#34;, index, arg) } } } 将上面的代码执行go build -o \u0026quot;args_demo\u0026quot;编译之后，执行：\n1 2 3 4 5 6 $ ./args_demo a b c d args[0]=./args_demo args[1]=a args[2]=b args[3]=c args[4]=d os.Args是一个存储命令行参数的字符串切片，它的第一个元素是执行文件的名称。\nflag包基本使用 本文介绍了flag包的常用函数和基本用法，更详细的内容请查看官方文档。\n导入flag包 1 import flag flag参数类型 flag包支持的命令行参数类型有bool、int、int64、uint、uint64、float float64、string、duration。\nflag参数 有效值 字符串flag 合法字符串 整数flag 1234、0664、0x1234等类型，也可以是负数。 浮点数flag 合法浮点数 bool类型flag 1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False。 时间段flag 任何合法的时间段字符串。如”300ms”、”-1.5h”、”2h45m”。 合法的单位有”ns”、”us” 、“µs”、”ms”、”s”、”m”、”h”。 定义命令行flag参数 有以下两种常用的定义命令行flag参数的方法。\nflag.Type() 基本格式如下：\nflag.Type(flag名, 默认值, 帮助信息)*Type 例如我们要定义姓名、年龄、婚否三个命令行参数，我们可以按如下方式定义：\n1 2 3 4 name := flag.String(\u0026#34;name\u0026#34;, \u0026#34;张三\u0026#34;, \u0026#34;姓名\u0026#34;) age := flag.Int(\u0026#34;age\u0026#34;, 18, \u0026#34;年龄\u0026#34;) married := flag.Bool(\u0026#34;married\u0026#34;, false, \u0026#34;婚否\u0026#34;) delay := flag.Duration(\u0026#34;d\u0026#34;, 0, \u0026#34;时间间隔\u0026#34;) 需要注意的是，此时name、age、married、delay均为对应类型的指针。\nflag.TypeVar() 基本格式如下： flag.TypeVar(Type指针, flag名, 默认值, 帮助信息) 例如我们要定义姓名、年龄、婚否三个命令行参数，我们可以按如下方式定义：\n1 2 3 4 5 6 7 8 var name string var age int var married bool var delay time.Duration flag.StringVar(\u0026amp;name, \u0026#34;name\u0026#34;, \u0026#34;张三\u0026#34;, \u0026#34;姓名\u0026#34;) flag.IntVar(\u0026amp;age, \u0026#34;age\u0026#34;, 18, \u0026#34;年龄\u0026#34;) flag.BoolVar(\u0026amp;married, \u0026#34;married\u0026#34;, false, \u0026#34;婚否\u0026#34;) flag.DurationVar(\u0026amp;delay, \u0026#34;d\u0026#34;, 0, \u0026#34;时间间隔\u0026#34;) flag.Parse() 通过以上两种方法定义好命令行flag参数后，需要通过调用flag.Parse()来对命令行参数进行解析。\n支持的命令行参数格式有以下几种：\n-flag xxx （使用空格，一个-符号） --flag xxx （使用空格，两个-符号） -flag=xxx （使用等号，一个-符号） --flag=xxx （使用等号，两个-符号） 其中，布尔类型的参数必须使用等号的方式指定。\nFlag解析在第一个非flag参数（单个”-“不是flag参数）之前停止，或者在终止符”–“之后停止。\nflag其他函数 1 2 3 flag.Args() ////返回命令行参数后的其他参数，以[]string类型 flag.NArg() //返回命令行参数后的其他参数个数 flag.NFlag() //返回使用的命令行参数个数 完整示例 定义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { //定义命令行参数方式1 var name string var age int var married bool var delay time.Duration flag.StringVar(\u0026amp;name, \u0026#34;name\u0026#34;, \u0026#34;张三\u0026#34;, \u0026#34;姓名\u0026#34;) flag.IntVar(\u0026amp;age, \u0026#34;age\u0026#34;, 18, \u0026#34;年龄\u0026#34;) flag.BoolVar(\u0026amp;married, \u0026#34;married\u0026#34;, false, \u0026#34;婚否\u0026#34;) flag.DurationVar(\u0026amp;delay, \u0026#34;d\u0026#34;, 0, \u0026#34;延迟的时间间隔\u0026#34;) //解析命令行参数 flag.Parse() fmt.Println(name, age, married, delay) //返回命令行参数后的其他参数 fmt.Println(flag.Args()) //返回命令行参数后的其他参数个数 fmt.Println(flag.NArg()) //返回使用的命令行参数个数 fmt.Println(flag.NFlag()) } 使用 命令行参数使用提示：\n1 2 3 4 5 6 7 8 9 10 $ ./flag_demo -help Usage of ./flag_demo: -age int 年龄 (default 18) -d duration 时间间隔 -married 婚否 -name string 姓名 (default \u0026#34;张三\u0026#34;) 正常使用命令行flag参数：\n1 2 3 4 5 $ ./flag_demo -name 沙河娜扎 --age 28 -married=false -d=1h30m 沙河娜扎 28 false 1h30m0s [] 0 4 使用非flag命令行参数：\n1 2 3 4 5 $ ./flag_demo a b c 张三 18 false 0s [a b c] 3 0 ","permalink":"https://cold-bin.github.io/post/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[],"title":"Go语言标准库flag基本使用"},{"categories":["golang"],"contents":"Golang创建最简单的HTTP和HTTPS服务 HTTP服务 HTTP是基于传输层TCP协议的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request){ fmt.Fprint(w, \u0026#34;Hello world\u0026#34;) }) log.Fatal(http.ListenAndServe(\u0026#34;:5001\u0026#34;, nil)) } HTTPS服务 HTTPS服务不同于HTTP服务，HTTPS是HTTP over SSL或HTTP over TLS。\nSSL是“Secure Sockets Layer”的缩写，中文叫做“安全套接层”。它是在上世纪90年代中期，由NetScape公司设计的。为啥要发明 SSL 这个协议捏？因为原先互联网上使用的 HTTP 协议是明文的，存在很多缺点——比如传输内容会被偷窥（嗅探）和篡改。发明 SSL 协议，就是为了解决这些问题。 到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为TLS是“Transport Layer Security”的缩写，中文叫做“传输层安全协议”。 很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。参考\n要启用HTTPS首先需要创建私钥和证书。\n有两种方式生成私钥和证书：\nOpenSSL方式，生成私钥key.pem和证书cert.pem，3650代表有效期为10年\n其他OpenSSL使用方式\n1 2 openssl genrsa -out key.pem 2048 openssl req -new -x509 -key key.pem -out cert.pem -days 3650 Golang标准库crypto/tls里有generate_cert.go，可以生成私钥key.pem和证书cert.pem，host参数是必须的，需要注意的是默认有效期是1年 1 go run $GOROOT/src/crypto/tls/generate_cert.go --host localhost 将生成的key.pem、cert.pem和以下代码放在同一目录下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func handler(w http.ResponseWriter, req *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain\u0026#34;) w.Write([]byte(\u0026#34;This is an example server.\\n\u0026#34;)) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Printf(\u0026#34;About to listen on 10443. Go to https://127.0.0.1:10443/\u0026#34;) // One can use generate_cert.go in crypto/tls to generate cert.pem and key.pem. // ListenAndServeTLS always returns a non-nil error. err := http.ListenAndServeTLS(\u0026#34;:10443\u0026#34;, \u0026#34;cert.pem\u0026#34;, \u0026#34;key.pem\u0026#34;, nil) log.Fatal(err) } 当然鉴于以上go的https方式，是自建证书，并不是CA机构签发的证书，虽然也使用了https加密，但是不是权威机构，浏览器访问时，会提示不安全，容易被中间人劫持。\nhttps原理 HTTPS 随着 HTTPS 建站的成本下降，现在大部分的网站都已经开始用上 HTTPS 协议。大家都知道 HTTPS 比 HTTP 安全，也听说过与 HTTPS 协议相关的概念有 SSL 、非对称加密、 CA证书等，但对于以下灵魂三拷问可能就答不上了：\n1.为什么用了 HTTPS 就是安全的？\n2.HTTPS 的底层原理如何实现？\n3.用了 HTTPS 就一定安全吗？\n本文将层层深入，从原理上把 HTTPS 的安全性讲透。\nHTTPS 的实现原理 大家可能都听说过 HTTPS 协议之所以是安全的是因为 HTTPS 协议会对传输的数据进行加密，而加密过程是使用了非对称加密实现。但其实，HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。\nHTTPS的整体过程分为证书验证和数据传输阶段，具体的交互过程如下：\n① 证书验证阶段\n浏览器发起 HTTPS 请求 服务端返回 HTTPS 证书 客户端验证证书是否合法，如果不合法则提示告警 ② 数据传输阶段\n当证书验证合法后，在本地生成随机数 通过公钥加密随机数，并把加密后的随机数传输到服务端 服务端通过私钥对随机数进行解密 服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输 为什么数据传输是用对称加密？ 首先，非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的；\n另外，在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以 HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密。\n为什么需要 CA 认证机构颁发证书？ HTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题。\n首先我们假设不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的“中间人攻击”问题。\n“中间人攻击”的具体过程如下：\n过程原理：\n1.本地请求被劫持（如DNS劫持等），所有请求均发送到中间人的服务器\n2.中间人服务器返回中间人自己的证书\n3.客户端创建随机数，通过中间人证书的公钥对随机数加密后传送给中间人，然后凭随机数构造对称加密对传输内容进行加密传输\n4.中间人因为拥有客户端的随机数，可以通过对称加密算法进行内容解密\n5.中间人以客户端的请求内容再向正规网站发起请求\n6.因为中间人与服务器的通信过程是合法的，正规网站通过建立的安全通道返回加密后的数据\n7.中间人凭借与正规网站建立的对称加密算法对内容进行解密\n8.中间人通过与客户端建立的对称加密算法对正规内容返回的数据进行加密传输\n9.客户端通过与中间人建立的对称加密算法对返回结果数据进行解密\n由于缺少对证书的验证，所以客户端虽然发起的是 HTTPS 请求，但客户端完全不知道自己的网络已被拦截，传输内容被中间人全部窃取。\n推荐阅读：用户密码到底要怎么加密存储？\n浏览器是如何确保 CA 证书的合法性？ 1. 证书包含什么信息？ 颁发机构信息 公钥 公司信息 域名 有效期 指纹 \u0026hellip;\u0026hellip; 2. 证书的合法性依据是什么？ 首先，权威机构是要有认证的，不是随便一个机构都有资格颁发证书，不然也不叫做权威机构。\n另外，证书的可信性基于信任制，权威机构需要对其颁发的证书进行信用背书，只要是权威机构生成的证书，我们就认为是合法的。\n所以权威机构会对申请者的信息进行审核，不同等级的权威机构对审核的要求也不一样，于是证书也分为免费的、便宜的和贵的。\n3. 浏览器如何验证证书的合法性？ 浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证：\n验证域名、有效期等信息是否正确。证书上都有包含这些信息，比较容易完成验证； 判断证书来源是否合法。每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证； 判断证书是否被篡改。需要与 CA 服务器进行校验； 判断证书是否已吊销。通过CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率 以上任意一步都满足的情况下浏览器才认为证书是合法的。\n这里插一个我想了很久的但其实答案很简单的问题：\n既然证书是公开的，如果要发起中间人攻击，我在官网上下载一份证书作为我的服务器证书，那客户端肯定会认同这个证书是合法的，如何避免这种证书冒用的情况？\n其实这就是非加密对称中公私钥的用处，虽然中间人可以得到证书，但私钥是无法获取的，一份公钥是不可能推算出其对应的私钥，中间人即使拿到证书也无法伪装成合法服务端，因为无法对客户端传入的加密数据进行解密。\n4. 只有认证机构可以生成证书吗？ 如果需要浏览器不提示安全风险，那只能使用认证机构签发的证书。但浏览器通常只是提示安全风险，并不限制网站不能访问，所以从技术上谁都可以生成证书，只要有证书就可以完成网站的 HTTPS 传输。例如早期的 12306 采用的便是手动安装私有证书的形式实现 HTTPS 访问。\n推荐阅读：12306 的架构也太 \u0026ldquo;牛X\u0026rdquo; 了吧！\n本地随机数被窃取怎么办？ 证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？\n其实 HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等。\n用了 HTTPS 会被抓包吗？ HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。关注微信公众号：Java技术栈，在后台回复：工具，可以获取我整理的 N 篇最新开发工具教程，都是干货。\n但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求。因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理。\n通常 HTTPS 抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环。\n既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？\nA: 客户端发起 HTTPS 请求，服务端返回证书，客户端对证书进行验证，验证通过后本地生成用于改造对称加密算法的随机数，通过证书中的公钥对随机数进行加密传输到服务端，服务端接收后通过私钥解密得到随机数，之后的数据交互通过对称加密算法进行加解密。\nQ: 为什么需要证书？\nA: 防止”中间人“攻击，同时可以为网站提供身份证明。\nQ: 使用 HTTPS 会被抓包吗？\nA: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。\n出自腾讯云《终于有人把 HTTPS 原理讲清楚了！》1\nhttps://cloud.tencent.com/developer/article/1601995 ↩\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://cold-bin.github.io/post/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/","tags":["https","tls"],"title":"Https在golang的实践"},{"categories":["golang"],"contents":"Golang杂烩 一、结构体 和 C/C++ 的结构体类似，Go 语言的结构体 struct 是一种聚合的数据类型，可以包含任意类型的值。\n方法 值接收者（go语言特有）\n所谓值接收者，指的是，当调用值接收者的方法时，会临时copy一份当前对象的副本，然后在这个副本的基础上再操作。显然，这个副本已经与原来的对象没有指向关系了，修改副本属性，无法更改原对象的值。\n应用场景：当对象主要运用于读数据，而非写数据时；当前对象的内存开销比较小\n指针接收者\n应用场景：当对象需要修改对象属性时，需要使用指针接收者；当对象内存开销比较大。\n注：不能一味使用指针接收者，有时候，指针接收者带来的开销可能比值接收者还要大\n两者实现某接口时的区别\n指针接收者实现某接口时，在类型转换上，只能使用指针类型的接收者；而值接收者实现某接口时，在类型转换上，无论是值接收者还是指针接收者都可以\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package main import ( \u0026#34;fmt\u0026#34; ) type AnimalInterface interface { bake(string) error } //Dog ... type Dog struct { name string } func (dog Dog) bake(w string) error { fmt.Printf(\u0026#34;%s bake %s \\n\u0026#34;, dog.name, w) return nil } type Cat struct { name string } func (cat *Cat) bake(w string) error { fmt.Printf(\u0026#34;%s bake %s \\n\u0026#34;, cat.name, w) return nil } func main() { var dogBig AnimalInterface = Dog{ name: \u0026#34;大黄\u0026#34;, } dogBig.bake(\u0026#34;吴奇隆\u0026#34;) var dogLittle AnimalInterface = \u0026amp;Dog{ name: \u0026#34;小黄\u0026#34;, } dogLittle.bake(\u0026#34;李易峰\u0026#34;) // cannot use Cat literal (type Cat) as type AnimalInterface in assignment:Cat does not implement AnimalInterface (bake method has pointer receiver) var catHua AnimalInterface = Cat{ name: \u0026#34;小花\u0026#34;, } catHua.bake(\u0026#34;刘亦菲\u0026#34;) var catBlue AnimalInterface = \u0026amp;Cat{ name: \u0026#34;小蓝\u0026#34;, } catBlue.bake(\u0026#34;张园园\u0026#34;) } 自动转换 先定义一个 struct\n1 2 3 4 type Person struct { Name string Age int } 初始化\n1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { me := \u0026amp;Person{ Name: \u0026#34;HF\u0026#34;, Age: 1, } fmt.Printf(\u0026#34;type of me: %T\\n\u0026#34;, me) fmt.Printf(\u0026#34;my name: %s, type of my name: %T\\n\u0026#34;, (*me).Name, (*me).Name) fmt.Printf(\u0026#34;my age: %d, type of my age: %T\\n\u0026#34;, (*me).Age, (*me).Age) fmt.Printf(\u0026#34;my name: %s, type of my name: %T\\n\u0026#34;, me.Name, me.Name) fmt.Printf(\u0026#34;my age: %d, type of my Age: %T\\n\u0026#34;, me.Age, me.Age) } 执行命令源码文件后输出：\n1 2 3 4 5 type of me: *main.Person my name: HF, type of my name: string my age: 1, type of my age: int my name: HF, type of my name: string my age: 1, type of my Age: int me 是一个指针型变量，访问 (*me).XXX 和 me.XXX 的效果是一样的，说明编译器会将 me.XXX 自动转换成 (*me).XXX，前者稍显笨拙。\n内存分配 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { me := \u0026amp;Person{ Name: \u0026#34;HF\u0026#34;, Age: 1, } fmt.Printf(\u0026#34;pointer of me: %p, size of me: %d\\n\u0026#34;, me, unsafe.Sizeof(me)) fmt.Printf(\u0026#34;pointer of my name: %p, size of my name: %d\\n\u0026#34;, \u0026amp;me.Name, unsafe.Sizeof(me.Name)) fmt.Printf(\u0026#34;pointer of my age: %p, size of my age: %d\\n\u0026#34;, \u0026amp;me.Age, unsafe.Sizeof(me.Age)) } 执行命令源码文件后输出：\n1 2 3 pointer of me: 0xc00000a060, size of me: 8 pointer of my name: 0xc00000a060, size of my name: 16 pointer of my age: 0xc00000a070, size of my age: 8 指针变量 me 指向从 0xc00000a060 开始的内存区域，也是 Person 结构体 Name 字段的内存起始地址。Name 字段是一个 string 类型的变量，占用16字节内存，所以 Age 字段的内存起始地址是 0xc00000a060 + 0x10 也就是 0xc00000a070。而 me 变量只是存放了结构体变量的内存地址，所以占用8个字节的内存而不是24字节。\n1 2 3 4 5 6 7 8 9 10 11 12 13 import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { me := \u0026amp;Person{ Name: \u0026#34;HF\u0026#34;, Age: 1, } fmt.Printf(\u0026#34;pointer of me: %p, size of real struct: %d\\n\u0026#34;, me, unsafe.Sizeof(*me)) } 执行命令源码文件后输出：\n1 pointer of me: 0xc00000a060, size of real struct: 24 这里真正的结构体变量确实占用了16 + 8也就是24字节的内存。\n结构体比较 如果结构体的全部成员都是可以比较的，那么结构体也是可以比较的。\n1 2 3 4 5 6 7 func main() { me := Person{Name: \u0026#34;HF\u0026#34;} he := Person{} fmt.Println(me == he) // false fmt.Println(me.Name == he.Name \u0026amp;\u0026amp; me.Age == he.Age) // false } 上下两种写法是等价的。\n继承（嵌入）\u0026ndash;匿名结构体 一个结构体可以继承多个结构体。 继承的结构体不能有同名字段。\n1 2 3 4 5 6 7 8 import ( \u0026#34;net/http\u0026#34; ) type Request struct{} type T struct { http.Request // field name is \u0026#34;Request\u0026#34; Request // field name is \u0026#34;Request\u0026#34; } 编译时会报错 duplicate field Request\nGo语言有一个特性让我们只声明一个成员对应的数据类型而不指名成员的名字，叫匿名成员。匿名成员的数据类型必须是命名的类型或指向一个命名的类型的指针。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Name struct { FirstName string LastName string } type Person struct { Name Age int } func main() { me := new(Person) me.Name.FirstName = \u0026#34;Fluoride\u0026#34; me.Name.LastName = \u0026#34;Hydrogen\u0026#34; fmt.Printf(\u0026#34;me: %#v\u0026#34;, *me) // me: \u0026amp;main.Person{Name:main.Name{FirstName:\u0026#34;Fluoride\u0026#34;, LastName:\u0026#34;Hydrogen\u0026#34;}, Age:0} } 但是这样类似文件目录递进的访问形式略显繁琐，得益于匿名成员的特性，直接访问子字段而不必给出完整的路径：\n1 2 3 4 5 6 func main() { me := new(Person) me.FirstName = \u0026#34;Fluoride\u0026#34; me.LastName = \u0026#34;Hydrogen\u0026#34; fmt.Printf(\u0026#34;me: %#v\u0026#34;, *me) } 所有的内置类型和自定义类型都是可以作为匿名字段的，但是结构体面值并不支持省略写法。\n嵌套带有方法的匿名结构体 示例\n嵌入的匿名结构的方法将被父结构体拥有；而且，切入的匿名结构的方法，还可以被父级结构体再实现一遍，但是实现过后，会被覆盖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 type Name struct { FirstName string LastName string } func (n *Name) string() string{ return n.FirstName+\u0026#34; \u0026#34;+n.LastName } type Person struct { Name Age int } // 覆盖匿名结构体方法 func (p *Person) string() string{ return \u0026#34;已被覆盖\u0026#34; } 好处\n上面的例子还没体现出匿名结构体的用途。\n试想这样一个场景：已经开发好了某一个工具类A，但是由于业务增长、扩容，原有的部分实现遇到性能瓶颈，那么怎么更好的修改这块代码呢？\n一种方案是：直接在原有的实现基础上，将有性能瓶颈的代码块进行更改，但是对代码侵入性比较大； 另一种方案是：依然保留原方案的实现，在原方案的基础上，提供一个新的方案，将源对象作为匿名结构体嵌入到新方案的结构体，然后“重写”那些有性能瓶颈的方法，以实现更新换代。 反射 Go 语言通过 reflect 包实现运行时的反射，允许程序操作任意类型的对象。 典型用法是用静态类型 interface{} 保存一个值，通过调用 TypeOf() 方法获取其动态类型信息，返回一个 Type 类型值。调用 ValueOf 方法返回一个 Value 类型值，该值代表运行时的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; ) type Person struct { Name string Age int } func getFieldString(person *Person, field string) string { r := reflect.ValueOf(person) f := reflect.Indirect(r).FieldByName(field) return f.String() } func getFieldInteger(person *Person, field string) int { r := reflect.ValueOf(person) f := reflect.Indirect(r).FieldByName(field) return int(f.Int()) } func main() { me := \u0026amp;Person{Name: \u0026#34;HF\u0026#34;, Age: 1} fmt.Printf(\u0026#34;my name: %s\\n\u0026#34;, getFieldString(me, \u0026#34;Name\u0026#34;)) // my name: HF fmt.Printf(\u0026#34;my age: %d\\n\u0026#34;, getFieldInteger(me, \u0026#34;Age\u0026#34;)) // my age: 1 } 标签 在 Go 语言中首字母大小写有特殊的语法含义，小写包外无法引用。由于需要和其它的系统进行数据交互，例如转换成 JSON。这时如果用属性名来作为键值可能不一定会符合项目要求。tag 在转换成其它数据格式的时候，强制使用自定义的字段作为键值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } func main() { me := \u0026amp;Person{Name: \u0026#34;HF\u0026#34;, Age: 1} json, err := json.Marshal(me) if err != nil { panic(err) } fmt.Printf(\u0026#34;JSON of me: %s\\n\u0026#34;, string(json)) // JSON of me: {\u0026#34;name\u0026#34;:\u0026#34;HF\u0026#34;,\u0026#34;age\u0026#34;:1} } 如果不带上自定义 tag，编组后的 JSON 是 {\u0026quot;Name\u0026quot;:\u0026quot;HF\u0026quot;,\u0026quot;Age\u0026quot;:1}，直接使用 Person 结构体的字段名作为 JSON 字段名。\ntag 甚至支持更丰富的自定义项，比如 omitempty：\n1 2 3 4 5 6 7 8 9 10 11 12 13 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age,omitempty\u0026#34;` } func main() { me := \u0026amp;Person{Name: \u0026#34;HF\u0026#34;} json, err := json.Marshal(me) if err != nil { panic(err) } fmt.Printf(\u0026#34;JSON of me: %s\\n\u0026#34;, string(json)) // JSON of me: {\u0026#34;name\u0026#34;:\u0026#34;HF\u0026#34;} } 如果结构体的某个字段为零值或者空值，JSON 编组时将跳过这个字段。\n关于tag的映射，gin框架集成了validator库v10版的反射校验，可以通过设置结构体tag，从而达到参数校验的目的，省去诸多业务代码。\n二、java与go的并发比对 很多有经验的工程师在使用基于 JVM 的语言时，都会看到这样的错误：\n1 2 3 4 5 6 [error] (run-main-0) java.lang.OutOfMemoryError: unable to create native thread: [error] java.lang.OutOfMemoryError: unable to create native thread: [error] at java.base/java.lang.Thread.start0(Native Method) [error] at java.base/java.lang.Thread.start(Thread.java:813) ... [error] at java.base/java.lang.Thread.run(Thread.java:844) 呃，这是由线程所造成的OutOfMemory。在我的笔记本电脑上运行 Linux 操作系统时，仅仅创建 11500 个线程之后，就会出现这个错误。\n如果你在 Go 语言上做相同的事情，启动永远处于休眠状态的 Goroutines，那么你会看到非常不同的结果。在我的笔记本电脑上，在我觉得实在乏味无聊之前，我能够创建七千万个 Goroutines。那么，为什么 Goroutines 的数量能够远远超过线程呢？要揭示问题的答案，我们需要一直向下沿着操作系统进行一次往返旅行。这不仅仅是一个学术问题，它对你如何设计软件有现实的影响。在生产环境中，我曾经多次遇到 JVM 线程的限制，有些是因为糟糕的代码泄露线程，有的则是因为工程师没有意识到 JVM 的线程限制。\n那到底什么是线程？ 术语“线程”可以用来描述很多不同的事情。在本文中，我会使用它来代指一个逻辑线程。也就是：按照线性顺序的一系列操作；一个执行的逻辑路径。CPU 的每个核心只能真正并发同时执行一个逻辑线程。这就带来一个固有的问题：如果线程的数量多于内核的数量，那么有的线程必须要暂停以便于其他的线程来运行工作，当再次轮到自己的执行的时候，会将任务恢复。为了支持暂停和恢复，线程至少需要如下两件事情：\n某种类型的指令指针。也就是，当我暂停的时候，我正在执行哪行代码？ 一个栈。也就是，我当前的状态是什么？栈中包含了本地变量以及指向变量所分配的堆的指针。同一个进程中的所有线程共享相同的堆。 鉴于以上两点，系统在将线程调度到 CPU 上时就有了足够的信息，能够暂停某个线程、允许其他的线程运行，随后再次恢复原来的线程。这种操作通常对线程来说是完全透明的。从线程的角度来说，它是连续运行的。线程能够感知到重新调度的唯一方式是测量连续操作之间的计时。\n回到我们最原始的问题：我们为什么能有这么多的 Goroutines 呢？\nJVM 使用操作系统线程 尽管并非规范所要求，但是据我所知所有的现代、通用 JVM 都将线程委托给了平台的操作系统线程来处理。在接下来的内容中，我将会使用“用户空间线程（user space thread）”来代指由语言进行调度的线程，而不是内核 /OS 所调度的线程。操作系统实现的线程有两个属性，这两个属性极大地限制了它们可以存在的数量；任何将语言线程和操作系统线程进行 1:1 映射的解决方案都无法支持大规模的并发。\n在 JVM 中，固定大小的栈 使用操作系统线程将会导致每个线程都有固定的、较大的内存成本\n采用操作系统线程的另一个主要问题是每个 OS 线程都有大小固定的栈。尽管这个大小是可以配置的，但是在 64 位的环境中，JVM 会为每个线程分配 1M 的栈。你可以将默认的栈空间设置地更小一些，但是你需要权衡内存的使用，因为这会增加栈溢出的风险。代码中的递归越多，就越有可能出现栈溢出。如果你保持默认值的话，那么 1000 个线程就将使用 1GB 的 RAM。虽然现在 RAM 便宜了很多，但是几乎没有人会为了运行上百万个线程而准备 TB 级别的 RAM。\nGo 的行为有何不同：动态大小的栈 Golang 采取了一种很聪明的技巧，防止系统因为运行大量的（大多数是未使用的）栈而耗尽内存：Go 的栈是动态分配大小的，随着存储数据的数量而增长和收缩。这并不是一件简单的事情，它的设计经历了多轮的迭代。我并不打算讲解内部的细节（关于这方面的知识，有很多的博客文章和其他材料进行了详细的阐述），但结论就是每个新建的 Goroutine 只有大约 4KB 的栈。每个栈只有 4KB，那么在一个 1GB 的 RAM 上，我们就可以有 250 万个 Goroutine 了，相对于 Java 中每个线程的 1MB，这是巨大的提升。\n在 JVM 中：上下文切换的延迟 从上下文切换的角度来说，使用操作系统线程只能有数万个线程\n因为 JVM 使用了操作系统线程，所以依赖操作系统内核来调度它们。操作系统有一个所有正在运行的进程和线程的列表，并试图为它们分配“公平”的 CPU 运行时间。当内核从一个线程切换至另一个线程时，有很多的工作要做。新运行的线程和进程必须要将其他线程也在同一个 CPU 上运行的事实抽象出去。我不会在这里讨论细节问题，但是如果你对此感兴趣的话，可以阅读更多的材料。这里比较重要的就是，切换上下文要消耗 1 到 100 微秒。这看上去时间并不多，相对现实的情况是每次切换 10 微秒，如果你想要每秒钟内至少调度每个线程一次的话，那么每个核心上只能运行大约 10 万个线程。这实际上还没有给线程时间来执行有用的工作。\nGo 的行为有何不同：在一个操作系统线程上运行多个 Goroutines Golang 实现了自己的调度器，允许众多的 Goroutines 运行在相同的 OS 线程上。就算 Go 会运行与内核相同的上下文切换，但是它能够避免切换至ring-0以运行内核，然后再切换回来，这样就会节省大量的时间。但是，这只是纸面上的分析。为了支持上百万的 Goroutines，Go 需要完成更复杂的事情。\n即便 JVM 将线程放到用户空间，它也无法支持上百万的线程。假设在按照这样新设计系统中，新线程之间的切换只需要 100 纳秒。即便你所做的只是上下文切换，如果你想要每秒钟调度每个线程十次的话，你也只能运行大约 100 万个线程。更重要的是，为了完成这一点，我们需要最大限度地利用 CPU。要支持真正的大并发需要另外一项优化：当你知道线程能够做有用的工作时，才去调度它。如果你运行大量线程的话，其实只有少量的线程会执行有用的工作。Go 通过集成通道（channel）和调度器（scheduler）来实现这一点。如果某个 Goroutine 在一个空的通道上等待，那么调度器会看到这一点并且不会运行该 Goroutine。Go 更近一步，将大多数空闲的线程都放到它的操作系统线程上。通过这种方式，活跃的 Goroutine（预期数量会少得多）会在同一个线程上调度执行，而数以百万计的大多数休眠的 Goroutine 会单独处理。这样有助于降低延迟。\n除非 Java 增加语言特性，允许调度器进行观察，否则的话，是不可能支持智能调度的。但是，你可以在“用户空间”中构建运行时调度器，它能够感知线程何时能够执行工作。这构成了像 Akka 这种类型的框架的基础，它能够支持上百万的 Actor。\n结论 操作系统线程模型与轻量级、用户空间的线程模型之间的转换在不断发生，未来可能还会继续。对于高度并发的用户场景来说，这是唯一的选择。然而，它具有相当的复杂性。如果 Go 选择采用 OS 线程而不是采用自己的调度器和递增的栈模式的话，那么他们能够在运行时中减少数千行的代码。对于很多用户场景来说，这确实是更好的模型。复杂性可以被语言和库的编写者抽象出去，这样软件工程师就能编写大量并发的程序了。\n三、接口 Go 语言内置了以下这些基础类型：\n布尔型：bool 整型：int、byte、uint 等 浮点型：float32、float64 字符串：string 字符：rune 错误：error 这些都是具体的类型，Go 语言中还存在一种抽象类型，也就是接口类型，它不会暴露它所代表的对象的内部值的结构，而是展示出它们自己方法。简单来说，接口是一组方法的集合。\nGo 语言的接口引入了“非侵入式”概念，不需要使用“显式”的首发去实现。而 Java 和 C++ 通常会这样来“显示”地创建一个类去实现接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Java interface Animal { public void eat(); public void sleep(); } public class Mammal implements Animal { @Override public void eat() { System.out.println(\u0026#34;Mammal eats.\u0026#34;); } @Override public void sleep() { System.out.println(\u0026#34;Mammal sleeps.\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // C++ class Animal { public: virtual void eat() = 0; virtual void sleep() = 0; } class Mammal: public Animal { public: void eat() { cout \u0026lt;\u0026lt; \u0026#34;Mammal eats.\u0026#34; \u0026lt;\u0026lt; endl; } void sleep() { cout \u0026lt;\u0026lt; \u0026#34;Mammal sleeps.\u0026#34; \u0026lt;\u0026lt; endl; } } 在实现接口前必须要先定义接口，并且将类型和接口紧密绑定（明确声明实现类继承了某个接口），这种就是“侵入式”接口。修改接口会影响到所有实现了该接口的类型。\n而在 Go 语言中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 type Animal interface { eat() sleep() } type Mammal struct {} func (m *Mammal) eat() { fmt.Println(\u0026#34;Mammal eats.\u0026#34;) } func (m *Mammal) sleep() { fmt.Println(\u0026#34;Mammal sleeps.\u0026#34;) } 以上第二段代码本身也是完整可用的，和第一段没有直接关系。但是有了上面的接口类型定义后，Mammal 结构确实也拥有了 Animal 接口定义的所有方法，那么 Mammal 类型也就自动实现了 Animal 接口。\n所以接口指定的规则非常简单：表达一个类型属于某个接口只要这个类型实现这个接口。\n1 2 3 4 5 func main() { var animal Animal = new(Mammal) animal.eat() animal.sleep() } 接口和类型可以直接转换，这种松散的对应关系系可以大幅降低因为接 口调整而导致的大量代码调整工作。\n接口值 接口赋值在 Go 语言中分为两种:\n将对象实例赋值给接口 将一个接口赋值给另一个接口 将某种类型的对象实例赋值给接口，要求该对象实例实现了接口要求的所有方法。上面的代码对应的是第一种情况，Mammal 结构体类型实现了 eat() 和 sleep() 两个 Animal 接口要求的方法。\n定义两个接口，一个叫 Animal，一个叫 Mammal，两个接口都定义了 eat() 和 sleep() 方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 type Animal interface { eat() sleep() } type Mammal interface { sleep() eat() } type Human struct {} func (h *Human) eat() { fmt.Println(\u0026#34;Human eats.\u0026#34;) } func (h *Human) sleep() { fmt.Println(\u0026#34;Human sleeps.\u0026#34;) } func main() { var human1 *Human = new(human) var human2 *Mammal = human1 var human3 *Animal = human2 } 两个接口的拥有的方法完全相同（次序不同也没关系），那两者就是相同的，可以相互赋值。\n空接口 interface 空接口不包含任何方法，对实现不做任何要求，类似 Java/C# 中所有类的基类：\n1 type Any interface{} 所以空接口可以赋值任何类型的实例\n最常用到的 fmt 标准库中的 Print() 系列方法，可以接受任意类型的参数：\n1 2 3 4 5 6 7 8 9 10 11 func Printf(format string, a ...interface{}) (n int, err error) { return Fprintf(os.Stdout, format, a...) } func Fprintln(w io.Writer, a ...interface{}) (n int, err error) { p := newPrinter() p.doPrintln(a) n, err = w.Write(p.buf) p.free() return } 类型断言 类型断言检查它操作对象的动态类型是否和断言的类型匹配\n1 t := i.(T) 检查 i 的类型是否为 T，如果是的话返回 i 的动态值，如果不是，类型断言失败产生运行时错误。\n确保被断言的接口值不是 nil。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type Animal interface { eat() sleep() } type Mammal struct {} func (m *Mammal) eat() { fmt.Println(\u0026#34;Mammal eats.\u0026#34;) } func (m *Mammal) sleep() { fmt.Println(\u0026#34;Mammal sleeps.\u0026#34;) } func main() { var animal Animal = new(Mammal) mammal, ok := animal.(*Mammal) if ok { mammal.eat() mammal.sleep() } } error 接口 error 类型实质上就是一个接口：\n1 2 3 type error interface { Error() string } 对照 error 接口实现一个自定义的错误类型：\n1 2 3 4 5 6 7 8 type CustomError struct { Op string Err error } func (customErr *CustomError) Error() string { return fmt.Sprintf(\u0026#34;op: %s, err: %s\\n\u0026#34;, customErr.Op, customErr.Err.Error()) } 错误处理：\n1 2 3 4 5 6 err := op() // 某个操作 if err != nil { if customErr, ok := err.(*CustomError); ok \u0026amp;\u0026amp; e.Err != nil { // 错误处理 } } 类型开关 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func newPrint(x interface{}) string { switch x := x.(type) { case nil: return \u0026#34;NULL\u0026#34; case int: return fmt.Sprintf(\u0026#34;%d is int\u0026#34;, x) case string: return fmt.Sprintf(\u0026#34;%s is string\u0026#34;, x) case bool: return fmt.Sprintf(\u0026#34;%v is bool\u0026#34;, x) case uint: return fmt.Sprintf(\u0026#34;%d is uint\u0026#34;, x) default: return fmt.Sprintf(\u0026#34;unknown type!\u0026#34;) } } func main() { fmt.Println(newPrint(1)) fmt.Println(newPrint(\u0026#34;hello\u0026#34;)) fmt.Println(newPrint(true)) fmt.Println(newPrint(1.0)) } 一个类型开关使用 switch 选择语句判断 x.(type)，每个 case 有至少一种类型。每一个 case 会被顺序的进行考虑，如果匹配就会执行 case 中的语句。\n建议 因为在 Go 语言中只有当两个或更多的类型实现一个接口时才使用接口，它们必定会从任意特 定的实现细节中抽象出来。结果就是有更少和更简单方法的更小的接口。当新的类型出现时，小的接口更容易满足。对于接口设计的一 个好的标准就是 ask only for what you need（只考虑你需要的东西）。\n四、接口与反射的关系 作为 Go 语言中为抽象而生的基本工具之一，接口在分配值时存储类型信息，而反射则是一种在运行时检查类型和值信息的方法。\nreflect 包提供了检查接口在运行时，检查类型、修改值的方法，Go 使用它实现了反射。\n向接口分配值 接口打包了三样东西：\n值 方法集 存储的值的类型 如下图： 图中可以清楚地看到接口的三个部分：_type 是类型信息，*data 是指向真实值的指针，itab 包含方法集。\n当一个函数接受了一个接口作为参数时，传递给函数的接口打包了值、方法集和类型。\n通过反射包在运行时检查接口数据 一旦值存储在接口中，可以使用 reflect 包来检查它的各个部分。我们不能直接检查接口结构，而反射包维护一份有权限访问的接口结构的副本。\n甚至通过反射对象访问接口，也是和底层接口直接相关的。\nreflect.Type 和 reflect.Value 提供访问接口成分的方法。reflect.Type 暴露接口的 _type 部分（有关类型的信息），而 reflect.Value 允许程序员检查和操作值。\nreflect.Type （检查类型） reflect.TypeOf() 函数用来提取值的类型。既然它唯一的参数是空接口，传递给它的参数将被分配成一个接口，因此就有了类型、方法集和值。\nreflect.TypeOf() 函数返回 reflect.Type，它有些方法可以让你查出值的类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;reflect\u0026#34; ) type Gift struct { Sender string Recipient string Number uint Contents string } func main() { g := Gift{ Sender: \u0026#34;Hank\u0026#34;, Recipient: \u0026#34;Sue\u0026#34;, Number: 1, Contents: \u0026#34;Scarf\u0026#34;, } t := reflect.TypeOf(g) if kind := t.Kind(); kind != reflect.Struct { log.Fatalf(\u0026#34;This program expects to work on a struct; we got a %v instead.\u0026#34;, kind) } for i := 0; i \u0026lt; t.NumField(); i++ { f := t.Field(i) fmt.Printf(\u0026#34;Field %03d: %-10.10s %v\u0026#34;, i, f.Name, f.Type.Kind()) } } 这段代码的目的是打印 Gift 结构的字段。当 g 作为参数传递给 reflect.TypeOf() 函数，g 就被分配成一个接口，编译器自动填充类型、方法集和值三样东西。这就允许我们遍历接口结构中类型部分的 []fields：\n1 2 3 4 Field 000: Sender string Field 001: Recipient string Field 002: Number uint Field 003: Contents string reflect.Method（检查方法集） reflect.Type 类型也允许访问 itab 部分来提取接口的方法信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 package main import ( \u0026#34;log\u0026#34; \u0026#34;reflect\u0026#34; ) type Reindeer string func (r Reindeer) TakeOff() { log.Printf(\u0026#34;%q lifts off.\u0026#34;, r) } func (r Reindeer) Land() { log.Printf(\u0026#34;%q gently lands.\u0026#34;, r) } func (r Reindeer) ToggleNose() { if r != \u0026#34;rudolph\u0026#34; { panic(\u0026#34;invalid reindeer operation\u0026#34;) } log.Printf(\u0026#34;%q nose changes state.\u0026#34;, r) } func main() { r := Reindeer(\u0026#34;rudolph\u0026#34;) t := reflect.TypeOf(r) for i := 0; i \u0026lt; t.NumMethod(); i++ { m := t.Method(i) log.Printf(\u0026#34;%s\u0026#34;, m.Name) } } 这段代码迭代 itab 中存储的数据并展示了每个方法的名称：\n1 2 3 Land TakeOff ToggleNose reflect.Value（检查值） 到此为止我们已经讨论了如何查看类型和方法，最后 reflect.Value 提供了接口中存储的真实值的信息。\n与 reflect.Value 相关的方法必然将类型信息与真实值联合。举个例子，要提取结构体的字段，反射包必须结合接口的布局信息——尤其是字段的信息和 _type 中存储的字段偏移量还有 *data 指向的真实值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;reflect\u0026#34; ) type Child struct { Name string Grade int Nice bool } type Adult struct { Name string Occupation string Nice bool } // Search a slice of structs for Name field that is \u0026#34;Hank\u0026#34; and set its Nice field to true. func nice(i interface{}) { // Retrieve the underlying value of i. We know that i is an interface. v := reflect.ValueOf(i) // we\u0026#39;re only interested in slices to let\u0026#39;s check what kind of value v is. If it isn\u0026#39;t a slice, return immediately. if v.Kind() != reflect.Slice { return } // v is a slice. Now let\u0026#39;s ensure that it is a slice of structs. If not, return immediately. if e := v.Type().Elem(); e.Kind() != reflect.Struct { return } // Determine if our struct has a Name field of type string and a Nice field of type bool st := v.Type().Elem() if nameField, found := st.FieldByName(\u0026#34;Name\u0026#34;); found == false || nameField.Type.Kind() != reflect.String { return } if niceField, found := st.FieldByName(\u0026#34;Nice\u0026#34;); found == false || niceField.Type.Kind() != reflect.Bool { return } // Set any Nice fields to true where the Name is \u0026#34;Hank\u0026#34; for i := 0; i \u0026lt; v.Len(); i++ { e := v.Index(i) name := e.FieldByName(\u0026#34;Name\u0026#34;) nice := e.FieldByName(\u0026#34;Nice\u0026#34;) if name.String() == \u0026#34;Hank\u0026#34; { nice.SetBool(true) } } } func main() { children := []Child{ {Name: \u0026#34;Sue\u0026#34;, Grade: 1, Nice: true}, {Name: \u0026#34;Ava\u0026#34;, Grade: 3, Nice: true}, {Name: \u0026#34;Hank\u0026#34;, Grade: 6, Nice: false}, {Name: \u0026#34;Nancy\u0026#34;, Grade: 5, Nice: true}, } adults := []Adult{ {Name: \u0026#34;Bob\u0026#34;, Occupation: \u0026#34;Carpenter\u0026#34;, Nice: true}, {Name: \u0026#34;Steve\u0026#34;, Occupation: \u0026#34;Clerk\u0026#34;, Nice: true}, {Name: \u0026#34;Nikki\u0026#34;, Occupation: \u0026#34;Rad Tech\u0026#34;, Nice: false}, {Name: \u0026#34;Hank\u0026#34;, Occupation: \u0026#34;Go Programmer\u0026#34;, Nice: false}, } fmt.Printf(\u0026#34;adults before nice: %v\u0026#34;, adults) nice(adults) fmt.Printf(\u0026#34;adults after nice: %v\u0026#34;, adults) fmt.Printf(\u0026#34;children before nice: %v\u0026#34;, children) nice(children) fmt.Printf(\u0026#34;children after nice: %v\u0026#34;, children) } 1 2 3 4 adults before nice: [{Bob Carpenter true} {Steve Clerk true} {Nikki Rad Tech false} {Hank Go Programmer false}] adults after nice: [{Bob Carpenter true} {Steve Clerk true} {Nikki Rad Tech false} {Hank Go Programmer true}] children before nice: [{Sue 1 true} {Ava 3 true} {Hank 6 false} {Nancy 5 true}] children after nice: [{Sue 1 true} {Ava 3 true} {Hank 6 true} {Nancy 5 true}] 注意，nice() 可以更改你传递给它的任意切片的值，不管它接收到了什么类型的参数。\n结论 Go 中的反射使用接口和 reflect 反射包实现，没啥黑科技——使用反射时就是直接访问接口里面的内容。\n接口更像是一面镜子，允许程序来检查自身。\n尽管 Go 是一门静态语言，但是反射和接口相结合非常强大，这个通常动态语言才有。\n要想更多关于反射的信息，可以看看包的文档。\n五、Go 避免在堆中的高开销垃圾回收 ","permalink":"https://cold-bin.github.io/post/go%E6%9D%82%E7%83%A9/","tags":[],"title":"Go杂烩"},{"categories":["golang","grpc","分布式系统"],"contents":"[toc]\n集中式与分布式 集中式系统架构 集中式是指由一台或多台主计算机组成的中心节点，数据集中存储于这个中心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上，系统的所有功能均由其集中处理。在集中式系统中，每个终端或客户端机器仅仅负责数据的录入和输出，而数据的存储与控制处理完全交由主机来完成。\n集中式系统架构在服务器的部署上有两种方式，一种是单机式，一种是集群式。\n单体架构 跟着学习的大家现在都有单机应用开发的经验了，有些同学在寒假作业中采用了MVC(Model, View, Controller)模式开发，这种属于单体应用开发模式，其特点如下：\n所有的代码统一打包成一个部署文件\n代码调用的api都是本地库或者是代码中的，基本上不存在远程服务调用\n当需要发布新版本时，必须停下所有业务进行升级\n在业务发展的初期，大家使用的初始架构都是单体应用，应用的服务体量小，也不需要考虑高并发、扩展性的问题，仅仅只需要解决业务需求。但是这种单体应用架构随着公司体量的增大和使用人群的扩展，会出现许多问题：\n随着业务的增加，代码量也随之增加，代码之间相互调用，对于新人的学习成本增加\n出现bug时，问题可能出现在代码的每一个角落，排查问题成本增加\n单体应用的可维护性差，扩展性差，当有新业务接入时，基本上需要修改所有的业务逻辑，牵一发而动全身\n单机式系统 一个系统业务量很小的时候，把所有的代码都放在一个项目中（也就是说，这个项目采用的是单体架构），而且这个项目只部署在一台服务器上，整个项目所有的服务都由这台服务器提供，并且整个系统中只配置一台计算机和相应的外部设备，这就是单机系统。\n单机系统的缺点是显而易见的：单机的处理能力是有限的。遇到处理上限问题，我们的第一反应是加钱，上最好的。但就算把硬件资源更换成当前情况下的最好的，当你的业务增长到一定程度的时候，单机的硬件资源也无法满足你的业务需求。\nLinux内核发送TCP的极限包频 ≈ 100万/秒。假设我们有一个300万在线人数的直播间，每个人每秒发一条信息，高负载下服务端发包频率将跟不上用户群产出消息频率。\n所以这种系统只适合对性能，并发等指标要求不高的系统。\n集群 加钱的想法是对的，关键在与钱加在哪里。既然一台机子不够，那我们就多加几台机子。\n单机处理到达瓶颈的时候，我们把单机复制几份，这样就构成了一个\u0026quot;集群\u0026quot;。集群中每台服务器就叫做这个集群的一个\u0026quot;\u0026ldquo;节点\u0026rdquo;，所有节点构成了一个集群。每个节点都提供相同的服务，那么这样系统的处理能力就相当于提升了好几倍。集群系统相当于单机的多实例。\n那么新的问题来了，用户的请求究竟由哪个节点来处理呢？最好能够让此时此刻负载较小的节点来处理，这样使得每个节点的压力比较平均。要实现这个功能，就需要在所有节点之前增加一个\u0026quot;调度者\u0026quot;的角色，用户的所有请求都先交给它，然后它根据当前所有节点的负载情况，决定将这个请求交给哪个节点处理。这个\u0026quot;调度者\u0026quot;最后被叫做负载均衡服务器。\n集群系统的好处就是系统扩展非常容易。如果随着你们系统业务的发展，当前的系统又支撑不住了，那么给这个集群再增加节点就行了。我们无需改动任何的项目代码，只需要新增服务器部署相同的应用并配置好负载均衡，就可以很好的减轻随着业务增量带来的系统压力，并且可以直接在单机架构上直接进行调整。\n但是，当你的业务发展到一定程度的时候，你会发现一个问题——无论怎么增加节点，貌似整个集群性能的提升效果并不明显了。这时候，你就需要使用微服务结构了。\n总结 集中式系统架构最大的特点是部署结构简单，由于集中式系统往往基于底层性能卓越的大型主机，因此，无需考虑如何对服务进行多个节点的部署，也就不用考虑负载均衡问题。但缺点也明显，比如集中式架构在设计上是一个单点，且单服务器的造价昂贵，所以系统横向扩展性差。如果发生单点故障（单机不可用即全部不可用）会导致系统停机，且维护时要暂停全部业务，影响严重。\n分布式系统架构 为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务，分布式系统应运而生。分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。其目的是利用更多的机器，处理更多的数据。\n分布式结构就是将一个完整的系统，按照业务功能，拆分成一个个独立的子系统，在分布式结构中，每个子系统就被称为“服务”。这些子系统能够独立运行在web容器中，它们之间通过RPC方式通信。从进程角度看，两个程序分别运行在两个台主机的进程上，它们相互协作最终完成同一个服务（或者功能），那么理论上这两个程序所组成的系统，也可以称作是\u0026quot;分布式系统\u0026quot;。\n假设开发一个在线商城。按照微服务的思想，我们需要按照功能模块拆分成多个独立的服务，如：用户服务、产品服务、订单服务、后台管理服务、数据分析服务等等。这一个个服务都是一个个独立的项目，可以独立运行。如果服务之间有依赖关系，那么通过RPC方式调用。\n这样做有很多好处：\n**系统之间的耦合度大大降低：**可以独立开发、独立部署、独立测试，系统与系统之间的边界非常明确，排错也变得相当容易，开发效率大大提升。 **系统更易于扩展：**我们可以针对性地扩展某些服务。假设这个商城要搞一次大促，下单量可能会大大提升，因此我们可以针对性地提升订单系统、产品系统的节点数量，而对于后台管理系统、数据分析系统而言，节点数量维持原有水平即可。 **服务的复用性更高：**比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发。 分布式与集群 咋一看，是不是觉得分布式和集群很相似：分布式使用多个计算机作为节点，集群也是。\n分布式和集群都是用来提高系统效率的，只是方式不同。\n分布式：一个业务拆成多个子业务，部署在不同的服务器上，以缩短单个任务的执行时间来提升效率\n集群：同一个业务，部署在多个服务器上，通过提高单位时间内执行的任务数来提升效率\n例如，如果一个任务由10个子任务组成，每个子任务单独执行需1小时，则在一台服务器上执行改任务需10小时。\n采用分布式方案，提供10台服务器，每台服务器只负责处理一个子任务，不考虑子任务间的依赖关系，执行完这个任务只需一个小时。采用集群方案，同样提供10台服务器，每台服务器都能独立处理这个任务。假设有10个任务同时到达，10个服务器将同时工作，10小时后，10个任务同时完成，这样，整体来看，还是1小时内完成一个任务。\n集群是个物理形态，分布式是个工作方式。只要是一堆机器，就可以叫集群，他们是不是一起协作着干活，这个谁也不知道；一个程序或系统，只要运行在不同的机器上，就可以叫分布式。集群一般是物理集中、统一管理的，而分布式系统则不强调这一点。所以，集群可能运行着一个或多个分布式系统，也可能根本没有运行分布式系统；分布式系统可能运行在一个集群上，也可能运行在不属于一个集群的多台机器上。这两者是不冲突的。\n分布式和集群通常结合起来使用，分布式提供去中心化的能力，可以把系统的不同业务拆分出来，不同的服务器提供不同的业务服务，解决了之前单一入口压力过大问题，但当某个服务器出现问题，此服务器中的业务就失效了，集群提供了高可用性能力，就可以对每个业务构建集群，这样就保证了业务稳定性，集群同时还有很好的扩展性，当某个业务压力过大时，可以对此业务所在集群动态添加服务器，增强此业务的性能。\n微服务 微服务架构是采用一组服务的方式来构建一个应用，服务独立部署在不同的服务器或者相同服务器的不同进程中。服务之间使用数据进行通信，比如RPC或者HTTP等。不同的服务之间相互不影响，甚至可以使用不同的编程语言进行开发。\n官方给微服务的定义为：\n一些独立的服务共同组成系统 每个服务单独部署，跑在自己的进程中 各个服务为独立的业务开发 分布式管理 强隔离性。 微服务相比分布式服务来说，它的粒度更小，服务之间耦合度更低，由于每个微服务都由独立的小团队负责，因此它敏捷性更高，分布式服务最后都会向微服务架构演化，这是一种趋势， 不过服务微服务化后带来的挑战也是显而易见的，例如服务粒度小，数量大，后期运维将会很难。\n一般而可以按两种方式拆分微服务：\n**按照不同的业务域进行拆分：**通过对业务进行梳理，根据业务的特性把应用拆开，不同的业务模块独立部署。例如订单、营销、风控、积分资源等。形成独立的业务领域微服务集群\n**按照一个业务功能里的不同模块或者组件进行拆分：**例如把公共组件拆分成独立的原子服务，下沉到底层，形成相对独立的原子服务层\nRPC 我们之前讲了将服务拆分，但是通常来讲服务直接一般会有依赖，这代表两个服务之间需要通信以进行数据交换，这个过程称为远程调用。\n在分布式计算中，远程过程调用（英语：Remote Procedure Call，RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。\nRPC是一种进程间通信的模式，程序分布在不同的地址空间里。如果在同一主机里，RPC可以通过不同的虚拟地址空间（即便使用相同的物理地址）进行通讯，而在不同的主机间，则通过不同的物理地址进行交互。许多技术（通常是不兼容）都是基于这种概念而实现的。\n简单的来说是一个节点请求另一个节点提供的服务，并且不需要知道底层的网络技术\n本地过程调用\nRPC就是要像调用本地的函数一样去调远程函数。在研究RPC前，我们先看看本地调用是怎么调的。假设我们要调用函数Multiply来计算lvalue * rvalue的结果:\n1 2 3 4 5 6 7 8 9 10 11 int Multiply(int l, int r) { int y = l * r return y } func main(){ int lvalue = 10 int rvalue = 20 int l_times_r = Multiply(lvalue, rvalue) fmt.Println(l_times_r) } 那么在第8行时，我们实际上执行了以下操作：\n将 lvalue 和rvalue的值压栈 进入Multiply函数，取出栈中的值10 和 20，将其赋予 l 和 r 执行第2行代码，计算l * r，并将结果存在 y 将 y 的值压栈，然后从Multiply返回 第8行，从栈中取出返回值 200 ，并赋值给 l_times_r 以上5步就是执行本地调用的过程。（注：以上步骤只是为了说明原理。事实上编译器经常会做优化，对于参数和返回值少的情况会直接将其存放在寄存器，而不需要压栈弹栈的过程，甚至都不需要调用call，而直接做inline操作。仅就原理来说，这5步是没有问题的。）\n远程过程调用带来的新问题\n在远程调用时，我们需要执行的函数体是在远程的机器上的，也就是说，Multiply是在另一个进程中执行的。这就带来了几个新问题：\nCall ID映射。我们怎么告诉远程机器我们要调用Multiply，而不是Add或者FooBar呢？在本地调用中，函数体是直接通过函数指针来指定的，我们调用Multiply，编译器就自动帮我们调用它相应的函数指针。但是在远程调用中，函数指针是不行的，因为两个进程的地址空间是完全不一样的。所以，在RPC中，所有的函数都必须有自己的一个ID。这个ID在所有进程中都是唯一确定的。客户端在做远程过程调用时，必须附上这个ID。然后我们还需要在客户端和服务端分别维护一个 (函数 \u0026lt;=\u0026gt; Call ID) 的对应表。两者的表不一定需要完全相同，但相同的函数对应的Call ID必须相同。当客户端需要进行远程调用时，它就查一下这个表，找出相应的Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。 序列化和反序列化。客户端怎么把参数值传给远程的函数呢？在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。但是在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数。甚至有时候客户端和服务端使用的都不是同一种语言（比如服务端用C++，客户端用Java或者Python）。这时候就需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。 网络传输。远程调用往往用在网络上，客户端和服务端是通过网络连接的。所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆就用了HTTP2。Java的Netty也属于这层的东西。 有了这三个机制，就能实现RPC了，具体过程如下：\nClient端\n将这个调用映射为Call ID。这里假设用最简单的字符串当Call ID的方法\n将Call ID，lvalue和rvalue序列化\n把上一步中得到的数据包发送给Server端对应服务地址+端口\n等待服务器返回结果\n如果服务器调用成功，那么就将结果反序列化，并赋给l_times_r\nServer端\n在本地维护一个Call ID到函数指针的映射call_id_map\n等待请求\n得到一个请求后，将其数据包反序列化，获取调用参数\n根据Call ID在call_id_map中查找，得到相应的函数指针\n将value和rvalue反序列化后，在本地调用Multiply函数，得到结果\n将结果序列化后通过网络返回给Client\n所以要实现一个RPC框架，其实只需要按以上流程实现就基本完成了。\nRPC与HTTP HTTP 和RPC 是两个维度的东西。HTTP 指的是通信协议， 而RPC 则是远程调用。 RPC 的通信可以用HTTP协议，也可以自定义协议，是不做约束的，其从传输层横跨到应用层。\n构建于HTTP 之上的远程调用解决方案会有更好的通用性，如WebServices 或REST 架构，使用HTTP + JSON 可以说是一个无脑的标准解决方案。选择构建在 HTTP 之上，有两个最大的优势：\nHTTP 的语义和可扩展性能很好的满足 RPC 调用需求\nHTTP 协议几乎被网络上的所有设备所支持，具有很好的协议穿透性\n但也存在比较明显的问题：\n有用信息占比少\n效率低\n使用HTTP协议调用远程方法比较复杂，要封装各种参数名和参数值\n通用定义的http1.1协议的tcp报文包含太多废信息，一个POST协议的格式大致如下：\n1 2 3 4 5 6 7 8 9 10 HTTP/1.0 200 OK Content-Type: text/plain Content-Length: 137582 Expires: Thu, 05 Dec 1997 16:00:00 GMT Last-Modified: Wed, 5 August 1996 15:55:28 GMT Server: Apache 0.84 \u0026lt;html\u0026gt; \u0026lt;body\u0026gt;Hello World\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 如上图所使用的报文中有效字节数仅仅占约 30%，也就是70%的时间用于传输元数据废编码。当然实际情况下报文内容可能会比这个长，但是报头所占的比例也是非常可观的。而假如我们使用自定义的tcp协议的报文，除去上面无用的字段便可以极大地精简传输内容，这也就是为什么后端进程间通常会采用自定义tcp协议的rpc来进行通信的原因。\n公司内部的服务之间调用，一般采用 rpc 的方式\nhttp 的特点是比较通用，像对外的 openAPI，第三方的接口，一般都是http的格式的\nProtobuf 由于在系统底层，数据的传输形式是简单的字节序列形式传递，即在底层，系统不认识对象，只认识字节序列，而为了达到进程通讯的目的，需要先将数据序列化，而序列化就是将对象转化字节序列的过程。相反地，当字节序列被运到相应的进程的时候，进程为了识别这些数据，就要将其反序列化，即把字节序列转化为对象\nProtobuf 是 Protocol Buffers 的简称, 它是Google 开发的一种跨语言、跨平台、可扩展的用于序列化数据协议， Protobuf 可以用于结构化数据序列化，它序列化出来的数据量少，再加上以K-V 的方式来存储数据，非常适用于在网络通讯中的数据载体。它很适合做数据存储或RPC数据交换格式。也是目前最流行的rpc通信协议。\n相比于 json 和 XML，ProtoBuf 的优势比较明显。例如 json 虽然表达方便，语法清晰，但是，有一个硬伤就是没有 schema，对于 Client-Server 的应用/服务来说，这就意味着双方需要使用其他方式进行沟通 schema，否则将无法正确的交流；XML 确实提供了强大的 Schema 支持，但是，可能因为年纪更大的缘故，XML 自身的语法啰嗦，更别说定义它的 Schema 了，一句话概括，那就是非常得不现代。\n语法 Protobuf协议规定：使用该协议进行数据序列化和反序列化操作时，首先定义传输数据的格式，并命名为以.proto为扩展名的消息定义文件，下面是一个proto文件示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 syntax = \u0026#34;proto3\u0026#34;; // 版本声明 package grpcinclass; // 包名，包名可以避免对message类型之间的名字冲突，同名的message可以通过package进行区分 option go_package = \u0026#34;/proto\u0026#34;; // 生成的go文件的package名（基于一定规则生成） message LoginResp{ bool OK = 1; } message LoginReq{ string UserName = 1; string PassWord = 2; } service Bili{ rpc Login(LoginReq) returns (LoginResp){} } 一般禁止将proto文件分开放。若分开放就必须为其编写Makefile，定义输出路径\nmessage proto文件就是围绕着消息体进行通信的，与Go语言中的struct类似，不同的协议规则对应不同的描述，我们只介绍\u0026quot;proto3\u0026quot;的规则协议。\n消息体内容组成：\n1 [字段修饰] 数据类型|消息体|引用外部消息体 名称 = 标识号 [字段可选项]; message与message之间可以嵌套定义，相当于go里结构体嵌套匿名结构体\n你也可以使用其它message类型作为字段的类型值，如果要使用的类型在其它proto文件中定义，你需要使用import引入对应的文件\n标识号：\n在消息的定义中，每个字段等号后面都有唯一的标识号，用于在反序列化过程中识别各个字段的，一旦开始使用就不能改变。标识号可以是乱序的，但是我们规定项目中的标识号一定是从1开始且连续，标识号的范围为[1,2^29 – 1]。\n其中[19000-19999]为Protobuf协议预留字段，开发者不建议使用该范围的标识号；一旦使用，在编译时Protoc编译器会报出警告\n常见数据类型 常见的数据类型与protoc协议中的数据类型映射见官网\n字段修饰\nprotoc3只有一种字段修饰词repeated，它表示允许字段重复，对于Go语言来说，它会编译成切片类型。其中类型可以是以下几种类型：\n数字类型： double、float、int32、int64、uint32、uint64、sint32、sint64\n存储固定大小的数字类型：fixed32、fixed64、sfixed32、sfixed64\n布尔类型: bool\n字符串: string\n字节数组: bytes\n消息类型: message\n枚举类型:enum\noneof\n枚举类型 proto协议支持使用枚举类型，和正常的编程语言一样，枚举类型可以使用enum关键字定义在.proto文件中：\n1 2 3 4 enum Sex { male = 0; female = 1; } Oneof类型 如果你有一组字段，同时最多允许这一组中的一个字段出现，就可以使用Oneof定义这一组字段，这有像C语言的Union。\n1 2 3 4 5 6 message OneofMessage { oneof test_oneof { string name = 4; int64 value = 9; } } map类型 map类型需要设置键和值的类型，格式是如下：\n1 map \u0026lt;键类型,值类型\u0026gt; 字段名 = 标识号; gRPC ProtoBuf 除了经常被用于数据保存交换之外，还被用于定义 gRPC 服务，gRPC 也是 Google 公开的高性能 RPC 调用框架，号称高效，支持广。\n安装grpc和protobuf 用go mod一键sync一下两个库\n1 2 google.golang.org/grpc google.golang.org/protobuf protocol buffer编译器 这个编译器可以单独下载，但我们也可以使用Goland里面的protocol buffer的编译器插件\n1 file-\u0026gt;settings-\u0026gt;plugins-\u0026gt;搜protocol 安装protoc 到protobuf release，选择适合自己操作系统的压缩包文件\n将解压后得到的protoc二进制文件移动到$GOPATH/bin里\n不会有人不知道$GOPATH/bin，也没有将这个文件夹加到path环境变量里吧\ngo的protoc编辑器插件 具体可以看其他网上教程\n1 2 go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 简单的demo 我们把讲protobuf语法时的示例文件作为例子，将其改名为login.proto并执行以下命令：\n1 2 protoc --go_out=. ./login.proto protoc --go-grpc_out=. ./login.proto 这两条命令会生成两个文件login.pb.go和login_grpc.pb.go(放在项目的proto文件夹里)，服务端和客户端都需要这两个文件。\n如果已经在服务端部署好了某项服务，在远程或者另外一台服务器上需要调用这个服务器上部署的这个微服务，则需要：首先，将服务端部署好的最新自动生成的那几个文件拉到现在这个服务器，这样服务端和客户端都持有同一份‘’服务协议‘’,根据这个协议就可以使用远程的微服务了\n值得一提的是，protoc支持很多种语言代码生成，也就是说，客户端和服务端可以是不同语言开发的。服务端go语言开发，但是可以通过protoc生成对应java、c++等代码，这样客户端是哪种语言写的只需要拉取相应的protoc代码生成的文件，如果没有，可以拉取proto语法源码，自己本地生成自己需要的代码\nlogin.pb.go放置了将login.proto里的结构翻译成go语言结构体和相关东西。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type LoginReq struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields UserName string `protobuf:\u0026#34;bytes,1,opt,name=UserName,proto3\u0026#34; json:\u0026#34;UserName,omitempty\u0026#34;` PassWord string `protobuf:\u0026#34;bytes,2,opt,name=PassWord,proto3\u0026#34; json:\u0026#34;PassWord,omitempty\u0026#34;` } type LoginResp struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields OK bool `protobuf:\u0026#34;varint,1,opt,name=OK,proto3\u0026#34; json:\u0026#34;OK,omitempty\u0026#34;` } login_grpc.pb.go放置了gRPC框架封装好的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func NewBiliClient(cc grpc.ClientConnInterface) BiliClient { return \u0026amp;biliClient{cc} } func (c *biliClient) Login(ctx context.Context, in *LoginReq, opts ...grpc.CallOption) (*LoginResp, error) { out := new(LoginResp) err := c.cc.Invoke(ctx, \u0026#34;/grpcinclass.Bili/Login\u0026#34;, in, out, opts...) if err != nil { return nil, err } return out, nil } type BiliServer interface { Login(context.Context, *LoginReq) (*LoginResp, error) mustEmbedUnimplementedBiliServer() } func RegisterBiliServer(s grpc.ServiceRegistrar, srv BiliServer) { s.RegisterService(\u0026amp;Bili_ServiceDesc, srv) } 接下来使用下面的server代码起一个rpc服务端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 //server（被调用rpc的一方） package main import ( \u0026#34;context\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; ) const ( port = \u0026#34;:50051\u0026#34; ) func main() { // 监听端口 lis, err := net.Listen(\u0026#34;tcp\u0026#34;, port) if err != nil { log.Fatalf(\u0026#34;failed to listen: %v\u0026#34;, err) } s := grpc.NewServer() //获取新服务示例 proto.RegisterBiliServer(s, \u0026amp;server{}) // 开始处理 if err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;failed to serve: %v\u0026#34;, err) } } type server struct { proto.UnimplementedBiliServer // 用于实现proto包里BiliServer接口 } func (s *server) Login(ctx context.Context, req *proto.LoginReq) (*proto.LoginResp, error) { resp := \u0026amp;proto.LoginResp{} log.Println(\u0026#34;recv:\u0026#34;, req.UserName, req.PassWord) if req.PassWord != GetPassWord(req.UserName) { resp.OK = false return resp, nil } resp.OK = true return resp, nil } func GetPassWord(userName string) (password string) { return userName + \u0026#34;123456\u0026#34; } 接下来使用下面的client代码起一个rpc客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 //client（调用rpc的一方） package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/credentials/insecure\u0026#34; \u0026#34;log\u0026#34; ) const ( address = \u0026#34;localhost:50051\u0026#34; ) func main() { //建立链接 conn, err := grpc.Dial(address, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\u0026#34;did not connect: %v\u0026#34;, err) } defer conn.Close() c := proto.NewBiliClient(conn) for { //这段不重要 fmt.Println(\u0026#34;input username\u0026amp;password:\u0026#34;) iptName := \u0026#34;\u0026#34; _, _ = fmt.Scanln(\u0026amp;iptName) iptPassword := \u0026#34;\u0026#34; _, _ = fmt.Scanln(\u0026amp;iptPassword) loginResp, _ := c.Login(context.Background(), \u0026amp;proto.LoginReq{ UserName: iptName, PassWord: iptPassword, }) if loginResp.OK { fmt.Println(\u0026#34;success\u0026#34;) break } fmt.Println(\u0026#34;retry\u0026#34;) } } protobuf 3中还有一种数据类型——steam（流），其用于传输流式数据，感兴趣可以参考这篇文章(这篇文章是我随便找的用于简要了解)\nBegonia 可以看到，上面讲到的rpc通信协议——Protobuf和rpc框架——gRPC都避免不了编写proto文件+代码生成的操作，而且一旦更改rpc接口，客户端和服务端都要更新相应的文件。\n为了更加方便、快速的起rpc服务，红岩巨佬仓仓子开发了一个轻量级、API友好的RPC框架Begonia，详细文档可见Begonia README。\n依旧以上面讲的login为例子，如果采用begonia，你只需要以下步骤（前提是你已经安装了Begonia）：\n启动服务中心 1 bgacenter start 编写service代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \u0026#34;github.com/MashiroC/begonia\u0026#34; \u0026#34;github.com/MashiroC/begonia/app/option\u0026#34; \u0026#34;time\u0026#34; ) func main() { // 一般情况下，addr是服务中心的地址。 s := begonia.NewServer(option.Addr(\u0026#34;:12306\u0026#34;)) // 会通过反射的方式把service结构体下面所有公开的方法注册到LoginServer服务上。 s.Register(\u0026#34;LoginServer\u0026#34;, \u0026amp;service{}) // 让服务器持续睡眠，不然service会因为主进程退出而直接结束。 for { time.Sleep(1 * time.Hour) } } type service struct{} // Login 函数的参数和返回值会被反射解析，注册为一个远程函数。 // 注册的函数没有特定的格式和写法。 func (*service) Login(name string, password string) bool { return password == GetPassWord(name) } func GetPassWord(userName string) (password string) { return userName + \u0026#34;123456\u0026#34; } 编写client代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/MashiroC/begonia\u0026#34; \u0026#34;github.com/MashiroC/begonia/app/client\u0026#34; \u0026#34;github.com/MashiroC/begonia/app/option\u0026#34; \u0026#34;log\u0026#34; ) var ( loginFunc client.RemoteFunSync // 远程调用函数（这个是同步的），初始化之后就可以重复调用 ) func Init() { c := begonia.NewClient(option.Addr(\u0026#34;:12306\u0026#34;)) // 获取LoginServer服务 s, err := c.Service(\u0026#34;LoginServer\u0026#34;) if err != nil { panic(err) } // 获取一个远程函数Login的同步调用 loginFunc, err = s.FuncSync(\u0026#34;Login\u0026#34;) if err != nil { panic(err) } } func main() { // 初始化 Init() for { //这段不重要 fmt.Println(\u0026#34;input username\u0026amp;password:\u0026#34;) iptName := \u0026#34;\u0026#34; _, _ = fmt.Scanln(\u0026amp;iptName) iptPassword := \u0026#34;\u0026#34; _, _ = fmt.Scanln(\u0026amp;iptPassword) // 调用 i, err := loginFunc(iptName, iptPassword) if err != nil { log.Println(err) break } if i.(bool) { fmt.Println(\u0026#34;success\u0026#34;) break } fmt.Println(\u0026#34;retry\u0026#34;) } } Reference（partial） Protobuf 终极教程\n[什么是分布式系统，如何学习分布式系统](https://www.cnblogs.com/xybaby/p/7787034.html)\n分布式与微服务\n","permalink":"https://cold-bin.github.io/post/%E9%9B%86%E4%B8%AD%E5%BC%8F%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/","tags":[],"title":"集中式与分布式"},{"categories":["杂"],"contents":"API向前兼容性的解决方案 应该有不少程序员受过这样的困扰，特别是做后端的。项目一直在持续不断的迭代，但是呢老版本的代码还不能直接删掉，因为不一定所有的调用端都能在同一时间点完成升级。\n你是怎么处理这种情况的呢？\n方法上增加参数？方法实现里增加if else？\n论难度的话，这个问题是不难，肯定每个人都有办法解决。但是解决的是否优雅？是否会给后续带来更大的成本？有这考虑的人可能就不多了。\n我看到过一些项目里做兼容的方法是真的粗暴，直接怼if else。\n比如，原本创建一笔交易的时候，我们需要的数据是用户的姓名、收货地址、邮编。现在迭代的一个新版本需要增加手机号信息，并且不需要邮编了。\n粗暴的写法怎么写呢？\n1 2 3 4 5 6 7 if(postData != null ){ if(postData.mobile != null){ //新版本的处理方式 }else{ //旧版本的处理方式 } } 如果以后再新加了一个参数呢？if else再多嵌套一层……\n你见过类似上面那样的代码吗？\n这可能是最容易想到的改法，但针对向前兼容不仅仅只有这一种方式。容我一个个来说。\n通过接口上增加版本号 最常用的增加版本号的方式是直接加到接口上。比如，\n1 http://api.xxx.com/1/user/login 如果后续的改动完全是新业务的追加，那么可以继续沿用这个1版本的接口。如果有做删减，那么可以新增一个版本2的接口，调整后的业务逻辑在这个版本的接口里实现。\n1 http://api.xxx.com/2/user/login 这样一来，你的项目中就同时存在了两个版本的接口。新的客户端调用2版本的，老的客户端调用1版本的。\n在这个期间，让客户端尽快都升级到2版本，然后后续就把1版本的接口删掉。\n有的人有代码洁癖，觉得一个项目里同时存在两个相同业务逻辑的接口不太舒服，那么可以试试下面这个方法。\n一个前端版本对应一个后端版本 与上面的区别就是，在项目的代码里，你不用新增加一个接口出来，直接把原先的接口修改成新的业务逻辑即可。\n但是呢，新版本上线的时候，老版本不能被全部替换掉，需要保留一段时间给旧版本的客户端使用。这件事的逻辑与前面一样，给客户端一段时间来升级。\n其实所谓的「服务发现」或者说「服务治理」就是这类思想的体现，只是玩的更加体系化。\n在服务发现的服务端，针对不同的服务本身就有版本的概念，比如：\n1 2 3 4 5 6 7 8 9 OrderService/v1 : 192.168.0.1:8000 , 192.168.0.1:8001 OrderService/v2 : 192.168.0.1:8100 , 192.168.0.1:8101 UserService/v1 : 192.168.0.1:9000 , 192.168.0.1:9001 UserService/v2 : 192.168.0.1:9100 , 192.168.0.1:9101 …… 客户端调用的时候，根据其传入的版本号，自动路由到对应版本的服务目标地址。\n当然这个方案有一个问题是，由于产生环境同时运行着多个版本的程序，所以数据库的字段只能增加，不能删除，并且增加的字段需要给上默认值。\n删除字段只能随着后续旧版本的去除进行。\n除了以上两种方案外，我们还可以做一些其他的工作以保障向前兼容更好地进行。\n数据库设计预留扩展字段 我们在设计数据表的时候，如果对后续业务预判会扩展。那么可以在做数据表设计的时候预留几个扩展字段（ext1,ext2,ext3,……）。\n这样的话配合上面提到的方案2能更好地进行，因为后续新增的字段可以暂时存在这些扩展字段里，不需要对表结构作出变更。\nAPP提供强制更新功能 如果是针对APP的向前兼容，那么务必在APP里留出一个强制更新的口子。以免有些用户长期不升级，导致你的旧版本迟迟无法去除。\n针对你可以承受的多版本情况，可以给予提示性的更新提醒，由用户自行选择是否更新。如果针对少数隔了多个版本还未升级的用户，可以给出强制性的更新，让其无法继续在旧版本上操作。\n强制更新也分局部更新和全量更新。现在针对局部更新的「热更新」受到了一些应用市场的限制，所以在用「热更新」之前谨慎了解对应平台的规则。\n另外，向前兼容不可能是无止境，不丢掉一些包袱，不断的新增包袱只会让后续的维护成本不堪重负。\n所以需要做好对各个版本使用情况的监控，一旦某个版本的使用量低于某个值，就应该给予强制更新的提醒。\n其实类似的向前兼容问题，除了api之外，在对外提供的三方库SDK、公司内给其他项目调用的二方库SDK中，也有一样的情况。\n他们的解决思路也是类似的。\n最后我建议你多看一些知名的开源项目，从中你可以学到很多优雅的编程技巧和代码设计理念。\n好了，总结一下。\n这篇呢Z哥和你分享了软件向前兼容的一些方法。除了粗暴的if else之外，你可以尝试一下以下两种方法：\n通过接口上增加版本号\n一个前端版本对应一个后端版本\n除此之外，以下这两件事也对做好向前兼容有很大帮助。\n数据库设计预留扩展字段\nAPP提供强制更新功能\n","permalink":"https://cold-bin.github.io/post/api%E5%90%91%E5%89%8D%E5%85%BC%E5%AE%B9%E6%80%A7%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","tags":["api向前兼容"],"title":"API向前兼容性的解决方案"},{"categories":["golang"],"contents":"go测试与性能优化 一、单元测试 不写测试的开发不是好程序员。我个人非常崇尚TDD（Test Driven Development）的，然而可惜的是国内的程序员都不太关注测试这一部分。 这篇文章主要介绍下在Go语言中如何做单元测试和基准测试。\ngo test工具 Go语言中的测试依赖go test命令。编写测试代码和编写普通的Go代码过程是类似的，并不需要学习新的语法、规则或工具。\ngo test命令是一个按照一定约定和组织的测试代码的驱动程序。在包目录内，所有以_test.go为后缀名的源代码文件都是go test测试的一部分，不会被go build编译到最终的可执行文件中。\n在*_test.go文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。\n类型 格式 作用 测试函数 函数名前缀为Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为Benchmark 测试函数的性能 示例函数 函数名前缀为Example 为文档提供示例文档 go test命令会遍历所有的*_test.go文件中符合上述命名规则的函数，然后生成一个临时的main包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。\n测试函数 格式 每个测试函数必须导入testing包，测试函数的基本格式（签名）如下：\n1 2 3 func TestName(t *testing.T){ // ... } 测试函数的名字必须以Test开头，可选的后缀名必须以大写字母开头，举几个例子：\n1 2 3 func TestAdd(t *testing.T){ ... } func TestSum(t *testing.T){ ... } func TestLog(t *testing.T){ ... } 其中参数t用于报告测试失败和附加的日志信息。 testing.T的拥有的方法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (c *T) Cleanup(func()) func (c *T) Error(args ...interface{}) func (c *T) Errorf(format string, args ...interface{}) func (c *T) Fail() func (c *T) FailNow() func (c *T) Failed() bool func (c *T) Fatal(args ...interface{}) func (c *T) Fatalf(format string, args ...interface{}) func (c *T) Helper() func (c *T) Log(args ...interface{}) func (c *T) Logf(format string, args ...interface{}) func (c *T) Name() string func (c *T) Skip(args ...interface{}) func (c *T) SkipNow() func (c *T) Skipf(format string, args ...interface{}) func (c *T) Skipped() bool func (c *T) TempDir() string 单元测试示例 就像细胞是构成我们身体的基本单位，一个软件程序也是由很多单元组件构成的。单元组件可以是函数、结构体、方法和最终用户可能依赖的任意东西。总之我们需要确保这些组件是能够正常运行的。单元测试是一些利用各种方法测试单元组件的程序，它会将结果与预期输出进行比较。\n接下来，我们在base_demo包中定义了一个Split函数，具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // base_demo/split.go package base_demo import \u0026#34;strings\u0026#34; // Split 把字符串s按照给定的分隔符sep进行分割返回字符串切片 func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u0026gt; -1 { result = append(result, s[:i]) s = s[i+1:] i = strings.Index(s, sep) } result = append(result, s) return } 在当前目录下，我们创建一个split_test.go的测试文件，并定义一个测试函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // split/split_test.go package split import ( \u0026#34;reflect\u0026#34; \u0026#34;testing\u0026#34; ) func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(\u0026#34;a🅱️c\u0026#34;, \u0026#34;:\u0026#34;) // 程序输出的结果 want := []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(\u0026#34;expected:%v, got:%v\u0026#34;, want, got) // 测试失败输出错误提示 } } 在当前路径下执行go test命令，可以看到输出结果如下：\n1 2 3 ❯ go test PASS ok golang-unit-test-demo/base_demo 0.005s go test -v 现在我们有多个测试用例了，为了能更好的在输出结果中看到每个测试用例的执行情况，我们可以为go test命令添加-v参数，让它输出完整的测试结果（测试所有测试用例）。\n1 2 3 4 5 6 7 8 9 ❯ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestSplitWithComplexSep split_test.go:20: expected:[a d], got:[a cd] --- FAIL: TestSplitWithComplexSep (0.00s) FAIL exit status 1 FAIL golang-unit-test-demo/base_demo 0.009s 从上面的输出结果我们能清楚的看到是TestSplitWithComplexSep这个测试用例没有测试通过。\ngo test -run 在执行go test命令的时候可以添加-run参数，它对应一个正则表达式，只有函数名匹配上的测试函数才会被go test命令执行。\n例如通过给go test添加-run=Sep参数来告诉它本次测试只运行TestSplitWithComplexSep这个测试用例：\n1 2 3 4 5 ❯ go test -run=Sep -v === RUN TestSplitWithComplexSep --- PASS: TestSplitWithComplexSep (0.00s) PASS ok golang-unit-test-demo/base_demo 0.010s 回归测试 我们修改了代码之后仅仅执行那些失败的测试用例或新引入的测试用例是错误且危险的，正确的做法应该是完整运行所有的测试用例，保证不会因为修改代码而引入新的问题。\n1 2 3 4 5 6 7 ❯ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestSplitWithComplexSep --- PASS: TestSplitWithComplexSep (0.00s) PASS ok golang-unit-test-demo/base_demo 0.011s 跳过某些测试用例 为了节省时间支持在单元测试时跳过某些耗时的测试用例。\n1 2 3 4 5 6 func TestTimeConsuming(t *testing.T) { if testing.Short() { t.Skip(\u0026#34;short模式下会跳过该测试用例\u0026#34;) } ... } 当执行go test -short时就不会执行上面的TestTimeConsuming测试用例。\n子测试 在上面的示例中我们为每一个测试数据编写了一个测试函数，而通常单元测试中需要多组测试数据保证测试的效果。Go1.7+中新增了子测试，支持在测试函数中使用t.Run执行一组测试用例，这样就不需要为不同的测试数据定义多个测试函数了。\n1 2 3 4 5 func TestXXX(t *testing.T){ t.Run(\u0026#34;case1\u0026#34;, func(t *testing.T){...}) t.Run(\u0026#34;case2\u0026#34;, func(t *testing.T){...}) t.Run(\u0026#34;case3\u0026#34;, func(t *testing.T){...}) } 表格驱动测试 介绍 表格驱动测试不是工具、包或其他任何东西，它只是编写更清晰测试的一种方式和视角。\n编写好的测试并非易事，但在许多情况下，表格驱动测试可以涵盖很多方面：表格里的每一个条目都是一个完整的测试用例，包含输入和预期结果，有时还包含测试名称等附加信息，以使测试输出易于阅读。\n使用表格驱动测试能够很方便的维护多个测试用例，避免在编写单元测试时频繁的复制粘贴。\n表格驱动测试的步骤通常是定义一个测试用例表格，然后遍历表格，并使用t.Run对每个条目执行必要的测试。\n示例 官方标准库中有很多表格驱动测试的示例，例如fmt包中便有如下测试代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 var flagtests = []struct { in string out string }{ {\u0026#34;%a\u0026#34;, \u0026#34;[%a]\u0026#34;}, {\u0026#34;%-a\u0026#34;, \u0026#34;[%-a]\u0026#34;}, {\u0026#34;%+a\u0026#34;, \u0026#34;[%+a]\u0026#34;}, {\u0026#34;%#a\u0026#34;, \u0026#34;[%#a]\u0026#34;}, {\u0026#34;% a\u0026#34;, \u0026#34;[% a]\u0026#34;}, {\u0026#34;%0a\u0026#34;, \u0026#34;[%0a]\u0026#34;}, {\u0026#34;%1.2a\u0026#34;, \u0026#34;[%1.2a]\u0026#34;}, {\u0026#34;%-1.2a\u0026#34;, \u0026#34;[%-1.2a]\u0026#34;}, {\u0026#34;%+1.2a\u0026#34;, \u0026#34;[%+1.2a]\u0026#34;}, {\u0026#34;%-+1.2a\u0026#34;, \u0026#34;[%+-1.2a]\u0026#34;}, {\u0026#34;%-+1.2abc\u0026#34;, \u0026#34;[%+-1.2a]bc\u0026#34;}, {\u0026#34;%-1.2abc\u0026#34;, \u0026#34;[%-1.2a]bc\u0026#34;}, } func TestFlagParser(t *testing.T) { var flagprinter flagPrinter for _, tt := range flagtests { t.Run(tt.in, func(t *testing.T) { s := Sprintf(tt.in, \u0026amp;flagprinter) if s != tt.out { t.Errorf(\u0026#34;got %q, want %q\u0026#34;, s, tt.out) } }) } } 通常表格是匿名结构体切片，可以定义结构体或使用已经存在的结构进行结构体数组声明。name属性用来描述特定的测试用例。\n接下来让我们试着自己编写表格驱动测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func TestSplitAll(t *testing.T) { // 定义测试表格 // 这里使用匿名结构体定义了若干个测试用例 // 并且为每个测试用例设置了一个名称 tests := []struct { name string input string sep string want []string }{ {\u0026#34;base case\u0026#34;, \u0026#34;a🅱️c\u0026#34;, \u0026#34;:\u0026#34;, []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}}, {\u0026#34;wrong sep\u0026#34;, \u0026#34;a🅱️c\u0026#34;, \u0026#34;,\u0026#34;, []string{\u0026#34;a🅱️c\u0026#34;}}, {\u0026#34;more sep\u0026#34;, \u0026#34;abcd\u0026#34;, \u0026#34;bc\u0026#34;, []string{\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;}}, {\u0026#34;leading sep\u0026#34;, \u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;, []string{\u0026#34;\u0026#34;, \u0026#34;河有\u0026#34;, \u0026#34;又有河\u0026#34;}}, } // 遍历测试用例 for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tt.input, tt.sep) if !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;expected:%#v, got:%#v\u0026#34;, tt.want, got) } }) } } 在终端执行go test -v，会得到如下测试输出结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ❯ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestSplitWithComplexSep --- PASS: TestSplitWithComplexSep (0.00s) === RUN TestSplitAll === RUN TestSplitAll/base_case === RUN TestSplitAll/wrong_sep === RUN TestSplitAll/more_sep === RUN TestSplitAll/leading_sep --- PASS: TestSplitAll (0.00s) --- PASS: TestSplitAll/base_case (0.00s) --- PASS: TestSplitAll/wrong_sep (0.00s) --- PASS: TestSplitAll/more_sep (0.00s) --- PASS: TestSplitAll/leading_sep (0.00s) PASS ok golang-unit-test-demo/base_demo 0.010s 并行测试 表格驱动测试中通常会定义比较多的测试用例，而Go语言又天生支持并发，所以很容易发挥自身并发优势将表格驱动测试并行化。 想要在单元测试过程中使用并行测试，可以像下面的代码示例中那样通过添加t.Parallel()来实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func TestSplitAll(t *testing.T) { t.Parallel() // 将 TLog 标记为能够与其他测试并行运行 // 定义测试表格 // 这里使用匿名结构体定义了若干个测试用例 // 并且为每个测试用例设置了一个名称 tests := []struct { name string input string sep string want []string }{ {\u0026#34;base case\u0026#34;, \u0026#34;a🅱️c\u0026#34;, \u0026#34;:\u0026#34;, []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}}, {\u0026#34;wrong sep\u0026#34;, \u0026#34;a🅱️c\u0026#34;, \u0026#34;,\u0026#34;, []string{\u0026#34;a🅱️c\u0026#34;}}, {\u0026#34;more sep\u0026#34;, \u0026#34;abcd\u0026#34;, \u0026#34;bc\u0026#34;, []string{\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;}}, {\u0026#34;leading sep\u0026#34;, \u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;, []string{\u0026#34;\u0026#34;, \u0026#34;河有\u0026#34;, \u0026#34;又有河\u0026#34;}}, } // 遍历测试用例 for _, tt := range tests { tt := tt // 注意这里重新声明tt变量（避免多个goroutine中使用了相同的变量） t.Run(tt.name, func(t *testing.T) { // 使用t.Run()执行子测试 t.Parallel() // 将每个测试用例标记为能够彼此并行运行 got := Split(tt.input, tt.sep) if !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;expected:%#v, got:%#v\u0026#34;, tt.want, got) } }) } } 这样我们执行go test -v的时候就会看到每个测试用例并不是按照我们定义的顺序执行，而是互相并行了。\n使用工具生成测试代码 社区里有很多自动生成表格驱动测试函数的工具，比如gotests等，很多编辑器如Goland也支持快速生成测试文件。这里简单演示一下gotests的使用。\n安装\n1 go get -u github.com/cweill/gotests/... 执行\n1 gotests -all -w split.go 上面的命令表示，为split.go文件的所有函数生成测试代码至split_test.go文件（目录下如果事先存在这个文件就不再生成）。\n生成的测试代码大致如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package base_demo import ( \u0026#34;reflect\u0026#34; \u0026#34;testing\u0026#34; ) func TestSplit(t *testing.T) { type args struct { s string sep string } tests := []struct { name string args args wantResult []string }{ // TODO: Add test cases. } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if gotResult := Split(tt.args.s, tt.args.sep); !reflect.DeepEqual(gotResult, tt.wantResult) { t.Errorf(\u0026#34;Split() = %v, want %v\u0026#34;, gotResult, tt.wantResult) } }) } } 代码格式与我们上面的类似，只需要在TODO位置添加我们的测试逻辑就可以了。\n测试覆盖率 测试覆盖率是指代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。在公司内部一般会要求测试覆盖率达到80%左右。\nGo提供内置功能来检查你的代码覆盖率，即使用go test -cover来查看测试覆盖率。\n1 2 3 4 ❯ go test -cover PASS coverage: 100.0% of statements ok golang-unit-test-demo/base_demo 0.009s 从上面的结果可以看到我们的测试用例覆盖了100%的代码。\nGo还提供了一个额外的-coverprofile参数，用来将覆盖率相关的记录信息输出到一个文件。例如：\n1 2 3 4 ❯ go test -cover -coverprofile=c.out PASS coverage: 100.0% of statements ok golang-unit-test-demo/base_demo 0.009s 上面的命令会将覆盖率相关的信息输出到当前文件夹下面的c.out文件中。\n1 2 3 4 5 ❯ tree . . ├── c.out ├── split.go └── split_test.go 然后我们执行go tool cover -html=c.out，使用cover工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告。上图中每个用绿色标记的语句块表示被覆盖了，而红色的表示没有被覆盖。\ntestify/assert testify是一个社区非常流行的Go单元测试工具包，其中使用最多的功能就是它提供的断言工具——testify/assert或testify/require。\n安装 1 go get github.com/stretchr/testify 使用示例 我们在写单元测试的时候，通常需要使用断言来校验测试结果，但是由于Go语言官方没有提供断言，所以我们会写出很多的if...else...语句。而testify/assert为我们提供了很多常用的断言函数，并且能够输出友好、易于阅读的错误描述信息。\n比如我们之前在TestSplit测试函数中就使用了reflect.DeepEqual来判断期望结果与实际结果是否一致。\n1 2 3 4 5 6 t.Run(tt.name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tt.input, tt.sep) if !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;expected:%#v, got:%#v\u0026#34;, tt.want, got) } }) 使用testify/assert之后就能将上述判断过程简化如下：\n1 2 3 4 t.Run(tt.name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tt.input, tt.sep) assert.Equal(t, got, tt.want) // 使用assert提供的断言函数 }) 当我们有多个断言语句时，还可以使用assert := assert.New(t)创建一个assert对象，它拥有前面所有的断言方法，只是不需要再传入Testing.T参数了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func TestSomething(t *testing.T) { assert := assert.New(t) // assert equality assert.Equal(123, 123, \u0026#34;they should be equal\u0026#34;) // assert inequality assert.NotEqual(123, 456, \u0026#34;they should not be equal\u0026#34;) // assert for nil (good for errors) assert.Nil(object) // assert for not nil (good when you expect something) if assert.NotNil(object) { // now we know that object isn\u0026#39;t nil, we are safe to make // further assertions without causing any errors assert.Equal(\u0026#34;Something\u0026#34;, object.Value) } } testify/assert提供了非常多的断言函数，这里没办法一一列举出来，大家可以查看官方文档了解。\ntestify/require拥有testify/assert所有断言函数，它们的唯一区别就是——testify/require遇到失败的用例会立即终止本次测试。\n此外，testify包还提供了mock、http等其他测试工具。\n基准测试 基准测试函数格式 基准测试就是在一定的工作负载之下检测程序性能的一种方法。基准测试的基本格式如下：\n1 2 3 func BenchmarkName(b *testing.B){ // ... } 基准测试以Benchmark为前缀，需要一个*testing.B类型的参数b，基准测试必须要执行b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性。 testing.B拥有的方法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (c *B) Error(args ...interface{}) func (c *B) Errorf(format string, args ...interface{}) func (c *B) Fail() func (c *B) FailNow() func (c *B) Failed() bool func (c *B) Fatal(args ...interface{}) func (c *B) Fatalf(format string, args ...interface{}) func (c *B) Log(args ...interface{}) func (c *B) Logf(format string, args ...interface{}) func (c *B) Name() string func (b *B) ReportAllocs() func (b *B) ResetTimer() func (b *B) Run(name string, f func(b *B)) bool func (b *B) RunParallel(body func(*PB)) func (b *B) SetBytes(n int64) func (b *B) SetParallelism(p int) func (c *B) Skip(args ...interface{}) func (c *B) SkipNow() func (c *B) Skipf(format string, args ...interface{}) func (c *B) Skipped() bool func (b *B) StartTimer() func (b *B) StopTimer() 基准测试示例 我们为split包中的Split函数编写基准测试如下：\n1 2 3 4 5 func BenchmarkSplit(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;) } } 基准测试并不会默认执行，需要增加-bench参数，所以我们通过执行go test -bench=Split命令执行基准测试，输出结果如下：\n1 2 3 4 5 6 7 split $ go test -bench=Split goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 203 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 2.255s 其中BenchmarkSplit-8表示对Split函数进行基准测试，数字8表示GOMAXPROCS的值，这个对于并发基准测试很重要。10000000和203ns/op表示每次调用Split函数耗时203ns，这个结果是10000000次调用的平均值。\n我们还可以为基准测试添加-benchmem参数，来获得内存分配的统计数据。\n1 2 3 4 5 6 7 split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 215 ns/op 112 B/op 3 allocs/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 2.394s 其中，112 B/op表示每次操作内存分配了112字节，3 allocs/op则表示每次操作进行了3次内存分配。 我们将我们的Split函数优化如下：\n1 2 3 4 5 6 7 8 9 10 11 func Split(s, sep string) (result []string) { result = make([]string, 0, strings.Count(s, sep)+1) i := strings.Index(s, sep) for i \u0026gt; -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们提前使用make函数将result初始化为一个容量足够大的切片，而不再像之前一样通过调用append函数来追加。我们来看一下这个改进会带来多大的性能提升：\n1 2 3 4 5 6 7 split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 127 ns/op 48 B/op 1 allocs/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 1.423s 这个使用make函数提前分配内存的改动，减少了2/3的内存分配次数，并且减少了一半的内存分配。\n性能比较函数 上面的基准测试只能得到给定操作的绝对耗时，但是在很多性能问题是发生在两个不同操作之间的相对耗时，比如同一个函数处理1000个元素的耗时与处理1万甚至100万个元素的耗时的差别是多少？再或者对于同一个任务究竟使用哪种算法性能最佳？我们通常需要对两个不同算法的实现使用相同的输入来进行基准比较测试。\n性能比较函数通常是一个带有参数的函数，被多个不同的Benchmark函数传入不同的值来调用。举个例子如下：\n1 2 3 4 func benchmark(b *testing.B, size int){/* ... */} func Benchmark10(b *testing.B){ benchmark(b, 10) } func Benchmark100(b *testing.B){ benchmark(b, 100) } func Benchmark1000(b *testing.B){ benchmark(b, 1000) } 例如我们编写了一个计算斐波那契数列的函数如下：\n1 2 3 4 5 6 7 8 9 // fib.go // Fib 是一个计算第n个斐波那契数的函数 func Fib(n int) int { if n \u0026lt; 2 { return n } return Fib(n-1) + Fib(n-2) } 我们编写的性能比较函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // fib_test.go func benchmarkFib(b *testing.B, n int) { for i := 0; i \u0026lt; b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } func BenchmarkFib10(b *testing.B) { benchmarkFib(b, 10) } func BenchmarkFib20(b *testing.B) { benchmarkFib(b, 20) } func BenchmarkFib40(b *testing.B) { benchmarkFib(b, 40) } 运行基准测试：\n1 2 3 4 5 6 7 8 9 10 11 12 split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/fib BenchmarkFib1-8 1000000000 2.03 ns/op BenchmarkFib2-8 300000000 5.39 ns/op BenchmarkFib3-8 200000000 9.71 ns/op BenchmarkFib10-8 5000000 325 ns/op BenchmarkFib20-8 30000 42460 ns/op BenchmarkFib40-8 2 638524980 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/fib 12.944s 这里需要注意的是，默认情况下，每个基准测试至少运行1秒。如果在Benchmark函数返回时没有到1秒，则b.N的值会按1,2,5,10,20,50，…增加，并且函数再次运行。\n最终的BenchmarkFib40只运行了两次，每次运行的平均值只有不到一秒。像这种情况下我们应该可以使用-benchtime标志增加最小基准时间，以产生更准确的结果。（因为默认基准执行1s,时间没到时，会自动调整，重新执行直到1s，如果某个函数的执行用时超过默认执行用时1s，可能无法得到返回）例如：\n1 2 3 4 5 6 7 split $ go test -bench=Fib40 -benchtime=20s goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/fib BenchmarkFib40-8 50 663205114 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/fib 33.849s 这一次BenchmarkFib40函数运行了50次，结果就会更准确一些了。\n使用性能比较函数做测试的时候一个容易犯的错误就是把b.N作为输入的大小，例如以下两个例子都是错误的示范：\n1 2 3 4 5 6 7 8 9 10 11 // 错误示范1 func BenchmarkFibWrong(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { Fib(n) } } // 错误示范2 func BenchmarkFibWrong2(b *testing.B) { Fib(b.N) } 重置时间 b.ResetTimer之前的处理不会放到执行时间里，也不会输出到报告中，所以可以在之前做一些不计划作为测试报告的操作。例如：\n1 2 3 4 5 6 7 func BenchmarkSplit(b *testing.B) { time.Sleep(5 * time.Second) // 假设需要做一些耗时的无关操作 b.ResetTimer() // 重置计时器 for i := 0; i \u0026lt; b.N; i++ { Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;) } } 并行测试 func (b *B) RunParallel(body func(*PB))会以并行的方式执行给定的基准测试。\nRunParallel会创建出多个goroutine，并将b.N分配给这些goroutine执行， 其中goroutine数量的默认值为GOMAXPROCS。用户如果想要增加非CPU受限（non-CPU-bound）基准测试的并行性， 那么可以在RunParallel之前调用SetParallelism 。RunParallel通常会与-cpu标志一同使用。\n1 2 3 4 5 6 7 8 func BenchmarkSplitParallel(b *testing.B) { // b.SetParallelism(1) // 设置使用的CPU数 b.RunParallel(func(pb *testing.PB) { for pb.Next() { Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;) } }) } 执行一下基准测试：\n1 2 3 4 5 6 7 8 split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 131 ns/op BenchmarkSplitParallel-8 50000000 36.1 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 3.308s 还可以通过在测试命令后添加-cpu参数如go test -bench=. -cpu 1来指定使用的CPU数量。\nSetup与TearDown 测试程序有时需要在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）。\nTestMain 通过在*_test.go文件中定义TestMain函数来可以在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）操作。\n如果测试文件包含函数:func TestMain(m *testing.M)那么生成的测试会先调用 TestMain(m)，然后再运行具体测试。TestMain运行在主goroutine中, 可以在调用 m.Run前后做任何设置（setup）和拆卸（teardown）。退出测试的时候应该使用m.Run的返回值作为参数调用os.Exit。\n一个使用TestMain来设置Setup和TearDown的示例如下：\n1 2 3 4 5 6 7 func TestMain(m *testing.M) { fmt.Println(\u0026#34;write setup code here...\u0026#34;) // 测试之前的做一些设置 // 如果 TestMain 使用了 flags，这里应该加上flag.Parse() retCode := m.Run() // 执行测试 fmt.Println(\u0026#34;write teardown code here...\u0026#34;) // 测试之后做一些拆卸工作 os.Exit(retCode) // 退出测试 } 需要注意的是：在调用TestMain时, flag.Parse并没有被调用。所以如果TestMain 依赖于command-line标志 (包括 testing 包的标记), 则应该显示的调用flag.Parse。\n子测试的Setup与Teardown 有时候我们可能需要为每个测试集设置Setup与Teardown，也有可能需要为每个子测试设置Setup与Teardown。下面我们定义两个函数工具函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 测试集的Setup与Teardown func setupTestCase(t *testing.T) func(t *testing.T) { t.Log(\u0026#34;如有需要在此执行:测试之前的setup\u0026#34;) return func(t *testing.T) { t.Log(\u0026#34;如有需要在此执行:测试之后的teardown\u0026#34;) } } // 子测试的Setup与Teardown func setupSubTest(t *testing.T) func(t *testing.T) { t.Log(\u0026#34;如有需要在此执行:子测试之前的setup\u0026#34;) return func(t *testing.T) { t.Log(\u0026#34;如有需要在此执行:子测试之后的teardown\u0026#34;) } } 使用方式如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \u0026#34;simple\u0026#34;: {input: \u0026#34;a🅱️c\u0026#34;, sep: \u0026#34;:\u0026#34;, want: []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}}, \u0026#34;wrong sep\u0026#34;: {input: \u0026#34;a🅱️c\u0026#34;, sep: \u0026#34;,\u0026#34;, want: []string{\u0026#34;a🅱️c\u0026#34;}}, \u0026#34;more sep\u0026#34;: {input: \u0026#34;abcd\u0026#34;, sep: \u0026#34;bc\u0026#34;, want: []string{\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;}}, \u0026#34;leading sep\u0026#34;: {input: \u0026#34;沙河有沙又有河\u0026#34;, sep: \u0026#34;沙\u0026#34;, want: []string{\u0026#34;\u0026#34;, \u0026#34;河有\u0026#34;, \u0026#34;又有河\u0026#34;}}, } teardownTestCase := setupTestCase(t) // 测试之前执行setup操作 defer teardownTestCase(t) // 测试之后执行testdoen操作 for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 teardownSubTest := setupSubTest(t) // 子测试之前执行setup操作 defer teardownSubTest(t) // 测试之后执行testdoen操作 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026#34;expected:%#v, got:%#v\u0026#34;, tc.want, got) } }) } } 测试结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 split $ go test -v === RUN TestSplit === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep === RUN TestSplit/leading_sep --- PASS: TestSplit (0.00s) split_test.go:71: 如有需要在此执行:测试之前的setup --- PASS: TestSplit/simple (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/wrong_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/more_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/leading_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown split_test.go:73: 如有需要在此执行:测试之后的teardown === RUN ExampleSplit --- PASS: ExampleSplit (0.00s) PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 示例函数 示例函数的格式 被go test特殊对待的第三种函数就是示例函数，它们的函数名以Example为前缀。它们既没有参数也没有返回值。标准格式如下：\n1 2 3 func ExampleName() { // ... } 示例函数示例 下面的代码是我们为Split函数编写的一个示例函数：\n1 2 3 4 5 6 7 func ExampleSplit() { fmt.Println(split.Split(\u0026#34;a🅱️c\u0026#34;, \u0026#34;:\u0026#34;)) fmt.Println(split.Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;)) // Output: // [a b c] // [ 河有 又有河] } 为你的代码编写示例代码有如下三个用处：\n示例函数能够作为文档直接使用，例如基于web的godoc中能把示例函数与对应的函数或包相关联。\n示例函数只要包含了// Output:也是可以通过go test运行的可执行测试。\n1 2 3 split $ go test -run Example PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 示例函数提供了可以直接运行的示例代码，可以直接在golang.org的godoc文档服务器上使用Go Playground运行示例代码。下图为strings.ToUpper函数在Playground的示例函数效果。\n二、网络测试 httptest 在Web开发场景下的单元测试，如果涉及到HTTP请求推荐大家使用Go标准库 net/http/httptest 进行测试，能够显著提高测试效率。\n在这一小节，我们以常见的gin框架为例，演示如何为http server编写单元测试。\n假设我们的业务逻辑是搭建一个http server端，对外提供HTTP服务。我们编写了一个helloHandler函数，用来处理用户请求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // gin.go package httptest_demo import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) // Param 请求参数 type Param struct { Name string `json:\u0026#34;name\u0026#34;` } // helloHandler /hello请求处理函数 func helloHandler(c *gin.Context) { var p Param if err := c.ShouldBindJSON(\u0026amp;p); err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;we need a name\u0026#34;, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: fmt.Sprintf(\u0026#34;hello %s\u0026#34;, p.Name), }) } // SetupRouter 路由 func SetupRouter() *gin.Engine { router := gin.Default() router.POST(\u0026#34;/hello\u0026#34;, helloHandler) return router } 现在我们需要为helloHandler函数编写单元测试，这种情况下我们就可以使用httptest这个工具mock一个HTTP请求和响应记录器，让我们的server端接收并处理我们mock的HTTP请求，同时使用响应记录器来记录server端返回的响应内容。\n单元测试的示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // gin_test.go package httptest_demo import ( \u0026#34;encoding/json\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httptest\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; ) func Test_helloHandler(t *testing.T) { // 定义两个测试用例 tests := []struct { name string param string expect string }{ {\u0026#34;base case\u0026#34;, `{\u0026#34;name\u0026#34;: \u0026#34;liwenzhou\u0026#34;}`, \u0026#34;hello liwenzhou\u0026#34;}, {\u0026#34;bad case\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;we need a name\u0026#34;}, } r := SetupRouter() for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // mock一个HTTP请求 req := httptest.NewRequest( \u0026#34;POST\u0026#34;, // 请求方法 \u0026#34;/hello\u0026#34;, // 请求URL strings.NewReader(tt.param), // 请求参数 ) // mock一个响应记录器 w := httptest.NewRecorder() // 让server端处理mock请求并记录返回的响应内容 r.ServeHTTP(w, req) // 校验状态码是否符合预期 assert.Equal(t, http.StatusOK, w.Code) // 解析并检验响应内容是否复合预期 var resp map[string]string err := json.Unmarshal([]byte(w.Body.String()), \u0026amp;resp) assert.Nil(t, err) assert.Equal(t, tt.expect, resp[\u0026#34;msg\u0026#34;]) }) } } 执行单元测试，查看测试结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ❯ go test -v === RUN Test_helloHandler [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in \u0026#34;debug\u0026#34; mode. Switch to \u0026#34;release\u0026#34; mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] POST /hello --\u0026gt; golang-unit-test-demo/httptest_demo.helloHandler (3 handlers) === RUN Test_helloHandler/base_case [GIN] 2021/09/14 - 22:00:04 | 200 | 164.839µs | 192.0.2.1 | POST \u0026#34;/hello\u0026#34; === RUN Test_helloHandler/bad_case [GIN] 2021/09/14 - 22:00:04 | 200 | 23.723µs | 192.0.2.1 | POST \u0026#34;/hello\u0026#34; --- PASS: Test_helloHandler (0.00s) --- PASS: Test_helloHandler/base_case (0.00s) --- PASS: Test_helloHandler/bad_case (0.00s) PASS ok golang-unit-test-demo/httptest_demo 0.055s 通过这个示例我们就掌握了如何使用httptest在HTTP Server服务中为请求处理函数编写单元测试了。\ngock 上面的示例介绍了如何在HTTP Server服务类场景下为请求处理函数编写单元测试，那么如果我们是在代码中请求外部API的场景（比如通过API调用其他服务获取返回值）又该怎么编写单元测试呢？\n例如，我们有以下业务逻辑代码，依赖外部API：http://your-api.com/post提供的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // api.go // ReqParam API请求参数 type ReqParam struct { X int `json:\u0026#34;x\u0026#34;` } // Result API返回结果 type Result struct { Value int `json:\u0026#34;value\u0026#34;` } func GetResultByAPI(x, y int) int { p := \u0026amp;ReqParam{X: x} b, _ := json.Marshal(p) // 调用其他服务的API resp, err := http.Post( \u0026#34;http://your-api.com/post\u0026#34;, \u0026#34;application/json\u0026#34;, bytes.NewBuffer(b), ) if err != nil { return -1 } body, _ := ioutil.ReadAll(resp.Body) var ret Result if err := json.Unmarshal(body, \u0026amp;ret); err != nil { return -1 } // 这里是对API返回的数据做一些逻辑处理 return ret.Value + y } 在对类似上述这类业务代码编写单元测试的时候，如果不想在测试过程中真正去发送请求或者依赖的外部接口还没有开发完成时，我们可以在单元测试中对依赖的API进行mock。\n这里推荐使用gock这个库。\n安装 1 go get -u gopkg.in/h2non/gock.v1 使用示例 使用gock对外部API进行mock，即mock指定参数返回约定好的响应内容。 下面的代码中mock了两组数据，组成了两个测试用例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // api_test.go package gock_demo import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;gopkg.in/h2non/gock.v1\u0026#34; ) func TestGetResultByAPI(t *testing.T) { defer gock.Off() // 测试执行后刷新挂起的mock // mock 请求外部api时传参x=1返回100 gock.New(\u0026#34;http://your-api.com\u0026#34;). Post(\u0026#34;/post\u0026#34;). MatchType(\u0026#34;json\u0026#34;). JSON(map[string]int{\u0026#34;x\u0026#34;: 1}). Reply(200). JSON(map[string]int{\u0026#34;value\u0026#34;: 100}) // 调用我们的业务函数 res := GetResultByAPI(1, 1) // 校验返回结果是否符合预期 assert.Equal(t, res, 101) // mock 请求外部api时传参x=2返回200 gock.New(\u0026#34;http://your-api.com\u0026#34;). Post(\u0026#34;/post\u0026#34;). MatchType(\u0026#34;json\u0026#34;). JSON(map[string]int{\u0026#34;x\u0026#34;: 2}). Reply(200). JSON(map[string]int{\u0026#34;value\u0026#34;: 200}) // 调用我们的业务函数 res = GetResultByAPI(2, 2) // 校验返回结果是否符合预期 assert.Equal(t, res, 202) assert.True(t, gock.IsDone()) // 断言mock被触发 } 执行上面写好的单元测试，看一下测试结果。\n1 2 3 4 5 ❯ go test -v === RUN TestGetResultByAPI --- PASS: TestGetResultByAPI (0.00s) PASS ok golang-unit-test-demo/gock_demo 0.054s 测试结果和预期的完全一致。\n在这个示例中，为了让大家能够清晰的了解gock的使用，我特意没有使用表格驱动测试。给大家留一个小作业：自己动手把这个单元测试改写成表格驱动测试的风格，就当做是对最近两篇教程的复习和测验。\n总结 在日常工作开发中为代码编写单元测试时如何处理外部依赖是最常见的问题，本文介绍了如何使用httptest和gock工具mock相关依赖。 在下一篇中，我们将更进一步，详细介绍针对依赖MySQL和Redis的场景如何编写单元测试。\n","permalink":"https://cold-bin.github.io/post/go%E6%B5%8B%E8%AF%95%E4%B8%8E%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","tags":["单元测试","性能优化","网络测试"],"title":"Go测试与性能优化"},{"categories":["数据结构与算法","java"],"contents":"[TOC]\n引子 数据结构包括：线性结构和非线性结构\n线性结构 特点：数据元素一对一的线性关系 线性结构有两种不同的存储结构：一种是顺序存储结构，元素处于相邻地址空间，顺序存储的线性表又称为顺序表；另一种是链式存储结构，元素节点保存数据元素和相邻元素地址信息，相邻元素不一定在地址空间上连续。 常见的线性结构：数组、队列、链表和栈 非线性结构 常见的有：二维数组、多维数组、广义表、树结构、图结构 时间复杂度 时间频度：一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费时间就多。**一个算法中的语句执行次数成为语句频度或时间频度。**记为T(n) (注意：是语句重复执行的次数，而不是语句条数)\n时间复杂度：一般情况下，算法的基本操作语句的重复执行次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋于无穷大时，T(n)/f(n)的极限值为不等于零的常熟，则成f(n)是T(n)的同数量级函数，记作**T(n)=O{f(n)}，称O{f(n)}为算法的渐进时间复杂度，简称时间复杂度**。\n时间频度不同，但时间复杂度可能相同。\n举例说明\nT(n)=n²+7n+6 与 T(n)=3n²+2n+2 它们的T(n) 不同，但时间复杂度相同，都为O(n²)\n计算时间复杂度的方法\n用常数1代替运行时间中的所有加法常数 T(n) = n²+7n+6 =\u0026gt; T(n) = n²+7n+1 修改后的运行次数函数中，只保留最高阶项 T(n) = n²+7n+1 =\u0026gt; T(n) = n² 去除最高阶项的系数 T(n) = n² =\u0026gt; T(n) = n² =\u0026gt; O(n²) 常见的时间复杂度\n常数阶O(1)\n对数阶O(log2n)：以2为底，n为对数\n线性阶O(n)\n线性对数阶O(nlog2n)\n平方阶O(n^2)\n立方阶O(n^3)\n参考上面的O(n²) 去理解就好了，O(n³)相当于三层n循环，其它的类似\nk次方阶O(n^k)\n参考上面的O(n²) 去理解就好了，O(n³)相当于三层n循环，其它的类似\n指数阶O(2^n) 我们应该尽可能避免使用指数阶的算法\n常见的算法时间复杂度由小到大依次为：Ο(1)＜Ο(log2n)＜Ο(n)＜Ο(nlog2n)＜Ο(n2)＜Ο(n3)＜ Ο(nk) ＜Ο(2n) ，随着问题规模n的不断增大，上述时间复杂度不断增大，算法的执行效率越低\n平均时间复杂度和最坏时间复杂度\n平均时间复杂度是指所有可能的输入实例均以等概率出现的情况下，该算法的运行时间。\n最坏情况下的时间复杂度称最坏时间复杂度。一般讨论的时间复杂度均是最坏情况下的时间复杂度。 这样做的原因是：最坏情况下的时间复杂度是算法在任何输入实例上运行时间的界限，这就保证了算法的运行时间不会比最坏情况更长。\n平均时间复杂度和最坏时间复杂度是否一致，和算法有关。\n空间复杂度 类似于时间复杂度的讨论，一个算法的空间复杂度(Space Complexity)定义为该算法所耗费的存储空间，它也是问题规模n的函数。 空间复杂度(Space Complexity)是对一个算法在运行过程中临时占用存储空间大小的量度。有的算法需要占用的临时工作单元数与解决问题的规模n有关，它随着n的增大而增大，当n较大时，将占用较多的存储单元，例如快速排序和归并排序算法就属于这种情况 在做算法分析时，主要讨论的是时间复杂度。从用户使用体验上看，更看重的程序执行的速度。一些缓存产品(redis, memcache)和算法(基数排序)本质就是用空间换时间. 一、稀疏数组 原理\n原始数组之中有很多重复或无用的数据，可以通过记录大量重复数据的位置和值来代替原来的重复数据，从而达到压缩的目的。\n处理方法\n记录数组一共几行几列，有多少个不同的值 把具有不同数值的元素行列记录至一个新的数组中（这个数组肯定会小于原先的数组），从而达到压缩的目的。 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import java.util.Arrays; public class first { public static void main(String[] args) { //初始化二维数组 int[][] oldArray =new int[10][10]; oldArray[1][3]=1; oldArray[2][7]=3; oldArray[5][2]=10; System.out.println(Arrays.deepToString(oldArray)); int validNum =0; //获取不同值个数 for (int i=0;i\u0026lt;oldArray.length;i++){ for (int j=0;j\u0026lt;oldArray[i].length;j++){ //将非重复元素存到压缩数组 if (oldArray[i][j]!=0){ validNum++; } } } //初始化稀疏数组，确定深度 int[][] parseArray =new int[validNum+1][3]; int index=0; //初始化稀疏数组第一个索引，用来表示原数组的个数及值重值 parseArray[0][0]=oldArray.length; parseArray[0][1]=oldArray[0].length; parseArray[0][2]=0; for (int i=0;i\u0026lt;oldArray.length;i++){ for (int j=0;j\u0026lt;oldArray[i].length;j++){ //将非重复元素存到压缩数组 if (oldArray[i][j]!=0){ index++; //记录非零值 parseArray[index][0]=i; parseArray[index][1]=j; parseArray[index][2]=oldArray[i][j]; } } } //稀疏数组结果 System.out.println(Arrays.deepToString(parseArray)); //还原稀疏数组 //初始化稀疏数组大小 int[][] newArray =new int[parseArray[0][0]][parseArray[0][1]]; //读取稀疏数组数值 for (int i=1;i\u0026lt;parseArray.length;i++){ newArray[parseArray[i][0]][parseArray[i][1]]=parseArray[i][2]; } System.out.println(Arrays.deepToString(newArray)); } } 二、队列 1. 非循环队列(一般不使用) 原理\n队列是一个有序列表，可以使用数组或者链表实现。数组的特点是相邻地址索引快，读操作效率更高；链表是链式存储，写操作效率更高\n遵循先入先出的原则\n示意图\n处理方法（数组）\n队列初始化时，默认front==rear==-1，头指针始终指向队列第一个元素的前一个索引位置（不一定存在），；尾指针始终指向队列队后一个元素的索引位置\n队列元素增加：尾指针后移，rear+1，当rear=maxSize-1表示队列已满\n队列元素取出：头指针后移，front-1，当front==rear表示队列已空\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 public class queue { //队列深度 private int maxSize; //队头指针 private int front; //队尾指针 private int rear; //队列元素使用数组作为队列结构 private int[] arr; //添加队列元素 public void addQueue(int e){ if (isFull()){ System.out.println(\u0026#34;the queue is full...\u0026#34;); return; } this.rear++; this.arr[this.rear]=e; } //取出队列元素 public int getQueue(){ //判断队列是否为空 if (isEmpty()){ throw new RuntimeException(\u0026#34;the queue is empty...\u0026#34;); } this.front++; return this.arr[this.front]; } //显示队列数据 public void list() { for (int v : arr) { System.out.print(v+\u0026#34; \u0026#34;); } } //判断队列是否已满 public boolean isFull(){ return this.maxSize-1==this.rear; } //判断队列是否为空 public boolean isEmpty(){ return this.rear==this.front; } //构造队列 public queue(int maxSize) { this.maxSize = maxSize; this.front = -1; this.rear = -1; this.arr = new int[maxSize]; } } 2. 循环队列 非循环队列的缺陷\n非循环队列的空间不能反复利用，队列空间一旦填满不能再对队列进行写操作。其问题在于，非循环队列元素添加的实现只是简单的将队列进行后移，每添加一次元素，rear后移一个位置，直到rear==maxSize-1时，不能再往后移（队列已满)。也就是说，目前数组使用一次就不能使用了，比较浪费\n但是可以通过对队列的指针取余（也叫取模），可以解决这个缺陷。内存上并没有环形的结构，因此环形队列实际上是数组的线性空间来实现的。当数据到了尾部应该怎么处理呢？它将转回到原来位置进行处理，通过取模操作来实现\n所谓取模，只是将不断增大的rear通过取模（取余）的方式，将rear大小不断缩小在一个“环”里，rear沿着“环”走\n原理图\n处理方法\nfront头指针指向队列第一个元素；rear尾指针指向队列最后一个元素的后一个位置。（与非循环队列相反） 队满时，(rear+1)%maxSize==front，意即rear后一个位置的下一个元素如果是front，表示队列已满 队空时，rear==front 队列初始化时，rear==front==0 队列之中的有效数据个数：(rear+maxSize-front)%maxSize或者Math.abs(rear-front)，两者结果一样，因为rear与front的差值不可能查过maxSize 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 public class circleQueue { private int maxSize; private int front; private int rear; private int[] arr; //判断队列是否为空 public boolean isEmpty(){ return this.front==this.rear; } //判断队列是否满了 public boolean isFull(){ return (this.rear+1)%maxSize==this.front; } //添加队列元素 public void addQueue(int e){ if (isFull()){ throw new RuntimeException(\u0026#34;queue is full\\n\u0026#34;); } this.arr[this.rear]=e; //注意取模 this.rear=(this.rear+1)%this.maxSize; } //取出队列元素 public int getQueue(){ if (isEmpty()){ throw new RuntimeException(\u0026#34;queue is empty\\n\u0026#34;); } int v = arr[front]; //取模 front=(front+1)%maxSize; return v; } //获取有效个数 public void list(){ for (int i = 0; i \u0026lt; (rear + maxSize - front) % maxSize; i++) { System.out.print(arr[front+i]+\u0026#34; \u0026#34;); } } public circleQueue(int maxSize) { this.maxSize = maxSize; this.front=0; this.rear=0; this.arr=new int[maxSize]; } } 三、链表 1. 单链表 原理\n链表是以节点的方式存储的，每个节点包含data域（存放数据）、next域（指向下一个节点）\n链表的各个节点地址不一定是连续\n链表分类：带头节点的链表和没有头节点的链表。\n原理图\nhead头节点：不存放数据\n处理方法\n创建一个head头节点。作用是表示整个链表的地址 添加一个节点就将链表的最后一个节点的next域指向这个节点 插入一个节点就先遍历链表找出需要插入的位置，然后让指针插入对应即可 删除一个节点就先遍历找到这个节点，然后将这个节点的前后节点指针相连，这个节点就没有指针指向他，在java中会被垃圾回收机制回收 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 public class singleLinkedList { public static void main(String[] args) { heroNode h1 = new heroNode(1, \u0026#34;刘海斌\u0026#34;); heroNode h2 = new heroNode(2, \u0026#34;包文杰\u0026#34;); heroNode h3 = new heroNode(3, \u0026#34;王铁霖\u0026#34;); heroNode h4 = new heroNode(4, \u0026#34;牟国柱\u0026#34;); managerSingleList manager = new managerSingleList(); manager.addNode(h1); manager.addNode(h2); manager.addNode(h3); manager.addNode(h4); manager.showList(); manager.updateList(\u0026#34;cold bin\u0026#34;, 1); manager.insertNode(new heroNode(0, \u0026#34;无\u0026#34;), 1); manager.showList(); manager.delNode(3); manager.showList(); } } class managerSingleList { //头节点 private heroNode head; public managerSingleList() { this.head = new heroNode(0, \u0026#34;\u0026#34;); } //添加 public void addNode(heroNode node) { //遍历链表至最后一个节点 heroNode temp = head; while (temp.next != null) { temp = temp.next; } //插入尾节点 temp.next = node; node.next = null; System.out.println(\u0026#34;添加成功\u0026#34;); } //删除 public void delNode(int no) { heroNode head = this.head; if (head.next == null) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode temp = head; while (temp.next != null) { //记录要删除节点的前一个结点 heroNode beforeNode = temp; temp = temp.next; if (temp.no == no) { //将删除节点的前一个节点next指向删除节点的下一个节点 beforeNode.next = temp.next; System.out.println(\u0026#34;删除成功\u0026#34;); return; } } System.out.println(\u0026#34;没有找到编号\u0026#34;); } //插入：在编号之前 public void insertNode(heroNode node, int no) { heroNode head = this.head; if (head.next == null) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode temp = head; while (temp.next != null) { heroNode beforeNode = temp; temp = temp.next; if (temp.no == no) { //记录对应编号节点的前一个节点位置，将新节点插入这个位置，意即beforeNode之后，temp之前； beforeNode.next = node; node.next = temp; System.out.println(\u0026#34;插入成功\u0026#34;); return; } } System.out.println(\u0026#34;没有找到编号\u0026#34;); } //修改 public void updateList(String newName, int no) { heroNode head = this.head; if (head.next == null) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode temp = head; while (temp.next != null) { temp = temp.next; if (temp.no == no) { temp.name = newName; System.out.println(\u0026#34;更新成功\u0026#34;); return; } } System.out.println(\u0026#34;没有找到编号\u0026#34;); } //查询 public void showList() { heroNode head = this.head; if (head.next == null) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode temp = head; while (temp.next != null) { //头节点无数据，因此跳过 temp = temp.next; System.out.println(temp); } } } class heroNode { int no; String name; heroNode next; public heroNode(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \u0026#34;heroNode{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#34;, name=\u0026#34; + name + \u0026#39;}\u0026#39;; } } 删除的原理：被删除的节点将不会有其他引用指向，会被垃圾回收机制回收\n应用场景：\n单链表的应用的话一般能用在 2 种场景下：\n第 1 种基操。 在你应用的场景中，插入和删除的操作特别多，你不想因为这俩操作浪费你太多的时间，此时用单链表，可以改善插入和删除操作浪费的时间。\n第 2 种骚操。在你应用的场景种，不知道有多少个元素，那这个时候你用单链表，每来一个新的元素你就链在表里，这种情况是用链表处理的绝佳方式。也是很容易被大家忽略的。\n单链表常见面试题\n单链表中节点的个数：遍历单链表有效节点个数（不算头节点） 单链表反转：先定义一个反转之后的头节点；然后遍历原始链表，每遍历一个链表元素就将这个链表元素插进紧跟反转链表的头节点之后的位置。然后再将原来头节点换成现在的头节点，意即，反转链表。 逆序打印单链表： 打印链表反转之后的链表，这样做的问题是会破坏单链表的结构；（不建议） 栈，将各个节点压入栈中，打印时，从栈中取出（不同的语言对栈的使用不一样，也可以考虑自己利用队列实现一个栈） 2. 双链表 单链表存在的问题\n单向链表查找的方向只能是一个方向（即头节点往后），而双向链表可以向前或者向后查找。 单向链表不能自我删除，需要借助辅助节点，而双向链表可以自我删除 示意图\n处理方法\n遍历和单链表一样，只是多了一个pre指针，可以双向遍历（向前向后查找）\n添加节点，将节点的两个指针连在双向链表最后一个节点即可\n修改节点，先遍历找到节点，然后改变其值\n删除节点，先便利找到这个节点temp，将前节点的next指针指向后节点，后节点的pre指针指向前节点（此处可以temp.pre.next=temp.next\u0026amp;\u0026amp;temp.next.pre=temp.pre）\n按顺序插入节点，先确定插入位置，然后插入，原理如下图（分四步，顺序不能乱）\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 package linkedList; public class doublyLinkedList { public static void main(String[] args) { heroNode1 h1 = new heroNode1(1, \u0026#34;刘海斌\u0026#34;); heroNode1 h2 = new heroNode1(2, \u0026#34;包文杰\u0026#34;); heroNode1 h3 = new heroNode1(3, \u0026#34;王铁霖\u0026#34;); heroNode1 h4 = new heroNode1(4, \u0026#34;牟国柱\u0026#34;); managerHeroNode1 m =new managerHeroNode1(); m.addNode(h1); m.addNode(h2); m.addNode(h3); m.addNode(h4); m.listNode(); m.updateNode(new heroNode1(2,\u0026#34;小包\u0026#34;)); m.insertNode(new heroNode1(3,\u0026#34;邓涔浩\u0026#34;)); m.deleteNode(4); m.listNode(); } } class managerHeroNode1 { heroNode1 head; public managerHeroNode1() { this.head = new heroNode1(-1, \u0026#34;\u0026#34;); } public boolean isEmpty() { if (head.next != null) return false; else return true; } //末尾添加node public void addNode(heroNode1 node) { heroNode1 temp = head; //遍历到末尾 while (temp.next != null) { temp = temp.next; } temp.next = node; node.pre = temp; System.out.println(\u0026#34;成功添加\u0026#34;); } //修改节点 public void updateNode(heroNode1 node) { if (isEmpty()) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode1 temp = head; while (temp.no != node.no) { temp = temp.next; if (temp == null) { System.out.println(\u0026#34;没找到该节点\u0026#34;); return; } } temp.name = node.name; System.out.println(\u0026#34;修改成功:\u0026#34; + temp); } //删除节点 public void deleteNode(int no) { if (isEmpty()) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode1 temp = head; while (temp.no != no) { temp = temp.next; } //temp的下一个节点的pre指针指向temp的上一个节点 // temp.next.pre=temp.pre;//注意如果temp是最后一个节点，这里有错： if (temp.next != null) temp.next.pre = temp.pre; //temp的上一个节点的next指针指向temp的下一个节点 temp.pre.next = temp.next; System.out.println(\u0026#34;删除成功:\u0026#34; + temp); } //按序插入节点 public void insertNode(heroNode1 node) { if (isEmpty()) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode1 temp = head; while (temp.no \u0026lt;= node.no) { temp = temp.next; //如果插入节点在最后，直接添加 if (temp==null){ addNode(node); return; } } //此操作注意先后顺序，否则会因为链条一部分断裂而导致后面的节点变成null //首先 node.pre=temp; node.next=temp.next; temp.next=node; temp.next.pre=node; System.out.println(\u0026#34;插入成功:\u0026#34; + node); } //遍历链表 public void listNode() { if (isEmpty()) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } heroNode1 temp = head; while (temp.next != null) { temp = temp.next; System.out.println(temp); } } } class heroNode1 { //节点data属性 int no; String name; //指针域 heroNode1 pre; heroNode1 next; public heroNode1(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \u0026#34;heroNode1{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } 3. 单向环形链表 原理\n约瑟夫问题\n在罗马人占领乔塔帕特后，39个犹太人和Josephus及他的朋友躲进一个洞里，39个犹太人决定宁愿死也不要被敌人抓到，于是决定了一个自杀方式，41个人排成一个圆圈，由第一个人开始报数，报数到3的人就自杀，再由下一个人重新报1，报数到3的人就自杀，这样依次下去，知道剩下最后一个人时，那个人可以自由选择自己的命运。这就是著名的约瑟夫问题。现在请用单向链表描述该结构并呈现整个自杀过程。\n单向环形链表解决\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 package linkedList; public class singleRingLinkedList { public static void main(String[] args) { managerList m = new managerList(); m.addNode(41); m.list(); System.out.println(); m.pops(1, 3, 5); } } class managerList { boyNode head; public managerList() { //头节点，标记首位置 head = new boyNode(-1); head.next = head;//注意闭环 } //添加 public void addNode(int num) { if (num \u0026lt; 1) { System.out.println(\u0026#34;参数错误\u0026#34;); return; } boyNode curNode = head; for (int i = 1; i \u0026lt;= num; i++) { boyNode node = new boyNode(i); if (i == 1) { head.no = node.no; continue; } curNode.next = node; node.next = head; //记录当前节点 curNode = node; } System.out.println(\u0026#34;添加成功\u0026#34;); } //遍历链表 public void list() { if (head == null) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } boyNode curNode = head; while (curNode.next != head) { System.out.println(curNode); curNode = curNode.next; } System.out.println(curNode); } //取出节点 public void pops(int startNo, int countNum, int nums) { //先对数据进行检验 if (head==null||countNum\u0026lt;1||countNum\u0026gt;nums){ System.out.println(\u0026#34;参数输入有误\u0026#34;); return; } //helper指针指向链表尾节点 boyNode helper = head; while (helper.next != head) { helper = helper.next; } //移动到报数节点 for (int i = 0; i \u0026lt; startNo-1; i++) { helper = helper.next; head = head.next; } while (head != helper) { //报数移动countNum-1次 for (int i = 1; i \u0026lt; countNum; i++) { helper = helper.next; head = head.next; } int number = head.no; head = head.next; helper.next = head; System.out.println(\u0026#34;移除节点：\u0026#34; + number); } System.out.println(\u0026#34;最后的节点：\u0026#34; + head.no); } } class boyNode { int no;//编号 boyNode next;//指针域 public boyNode(int no) { this.no = no; } @Override public String toString() { return \u0026#34;boyNode{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#39;}\u0026#39;; } } 四、栈 1. 原理 栈是一个先入后出的有序列表\n栈是限制线性表中元素的插入和删除只能在线性表的同一端进行的一种特殊线性表。允许插入和删除的一端，为变化的一端，称为栈顶；另一端为固定的一端，成为栈底\n最先放入栈的元素在栈底，最后放入的元素在栈顶；而删除元素刚好相反，最后放入的元素最先删除，最先放入的元素最后删除\n原理图\n2. 代码实现（数组模拟栈） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package stack; public class stackDemo { public static void main(String[] args) { managerStack m = new managerStack(5); m.push(new node(1)); m.push(new node(2)); m.push(new node(3)); m.list(); System.out.println(m.pop()); System.out.println(m.pop()); m.list(); System.out.println(m.pop()); m.push(new node(4)); m.list(); System.out.println(m.pop()); } //入栈 } class managerStack { int top; int maxSize; node[] nodes; public managerStack(int maxSize) { //初始化栈顶 top = -1; //栈的容量 this.maxSize = maxSize; //初始化栈的大小 nodes = new node[maxSize]; } //入栈 public void push(node node) { if (top \u0026lt; maxSize - 1) { nodes[++top] = node; return; } System.out.println(\u0026#34;栈溢出\u0026#34;); } //出栈 public node pop() { if (top == -1) { throw new RuntimeException(\u0026#34;栈空\u0026#34;); } return nodes[top--]; } //遍历 public void list() { if (top == -1) { throw new RuntimeException(\u0026#34;栈空\u0026#34;); } System.out.println(\u0026#34;栈元素：\u0026#34;); for (int i = top; i \u0026gt;= 0; i--) { System.out.printf(\u0026#34;nodes[%d]=%d\\n\u0026#34;, i, nodes[i].no); } } } class node { int no; public node(int no) { this.no = no; } @Override public String toString() { return \u0026#34;node{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#39;}\u0026#39;; } } 3. 波兰计算器 ① 无括号版波兰计算器 思路\n针对多位数运算时的逻辑：如果扫描到数字就继续扫描下一位直到不是数字为止，将扫描的数字字符拼接成串然后转化为数字\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 package stack; import java.util.Scanner; public class calculatorByStackDemo { public static void main(String[] args) { Scanner sc = new Scanner(System.in); System.out.print(\u0026#34;请输入合法表达式：\u0026#34;); String express = sc.next(); char[] chars = express.toCharArray(); //数字栈 stack stackNumber = new stack(10); //符号栈 stack stackOperation = new stack(10); int num1 = 0; int num2 = 0; int val = 0; int operation = 0; String number = \u0026#34;\u0026#34;; for (int i = 0; i \u0026lt; chars.length; ) { //符号 if (stackOperation.isOperation(chars[i])) { //符号栈为空，将符号压入符号栈 if (stackOperation.isEmpty()) stackOperation.push(chars[i]); else if (!stackOperation.isFull()) { //如果符号栈有操作符，进行比较，如果当前操作符的优先级低于或等于栈中的操作符， //就需要从数栈中pop两个数，再从符号栈里pop出一个符号进行运算，将得到的结果压 //入数栈，然后再将当前的操作符压入符号栈中 if (stackOperation.priority(chars[i]) \u0026lt;= stackOperation.priority(stackOperation.showTop())) { num1 = stackNumber.pop(); num2 = stackNumber.pop(); operation = stackOperation.pop(); val = stackNumber.calculate(num2, num1, operation); stackNumber.push(val); stackOperation.push(chars[i]); } else { //当前操作符优先级更高，再压入符号栈中 stackOperation.push(chars[i]); } } i++; } //如果是数字，先别慌入栈，再判断下一位是否是数字,是数字就拼接在一起 // if (stackNumber.isDigital(chars[i])){ // stackNumber.push(Character.getNumericValue(number)); // } while (stackNumber.isDigital(chars[i])) { number = number + chars[i]; i++; if (i == chars.length) break; } System.out.println(number); stackNumber.push(Integer.parseInt(number)); number = \u0026#34;\u0026#34;;//清空上次数字残余 } //扫描完毕后，顺序从数栈和符号栈pop出两个数字和一个符号，并运算，最后数栈只有一个数字就是结果 //最后的栈中运算符都是同等优先级的运算符 while (!stackOperation.isEmpty()) { num1 = stackNumber.pop(); num2 = stackNumber.pop(); operation = stackOperation.pop(); val = stackOperation.calculate(num2, num1, operation); stackNumber.push(val); } System.out.println(\u0026#34;计算的结果是：\u0026#34; + stackNumber.showButton()); } } class stack { int top; int maxSize; int[] nodes; public stack(int maxSize) { top = -1; this.maxSize = maxSize; nodes = new int[maxSize]; } public boolean isEmpty() { return top == -1; } public boolean isFull() { return top == maxSize - 1; } //入栈 public void push(int node) { if (isFull()) throw new RuntimeException(\u0026#34;栈溢出\u0026#34;); nodes[++top] = node; } //出栈 public int pop() { if (isEmpty()) throw new RuntimeException(\u0026#34;栈空\u0026#34;); return nodes[top--]; } //显示栈顶元素，但不取出 public int showTop() { return nodes[top]; } public int showButton() { return nodes[0]; } //优先级 public int priority(int operation) { if (operation == \u0026#39;*\u0026#39; || operation == \u0026#39;/\u0026#39;) return 1; else if (operation == \u0026#39;+\u0026#39; || operation == \u0026#39;-\u0026#39;) return 0; return -1; } //判断是不是操作符 public boolean isOperation(int operation) { return operation == \u0026#39;+\u0026#39; || operation == \u0026#39;-\u0026#39; || operation == \u0026#39;*\u0026#39; || operation == \u0026#39;/\u0026#39;; } //判断是不是数字 public boolean isDigital(int number) { return Character.isDigit(number); } //计算方法 public int calculate(int num1, int num2, int operation) { switch (operation) { case \u0026#39;+\u0026#39;: return num1 + num2; case \u0026#39;-\u0026#39;: return num1 - num2; case \u0026#39;*\u0026#39;: return num1 * num2; case \u0026#39;/\u0026#39;: return num1 / num2; default: return 0; } } } ② 前缀表达式（波兰表达式） 前缀表达式的运算符位于操作数之前。如(3+4)*5-6这样的中缀表达式对应的前缀表达式-*+3456 前缀表达式的计算机求值：首先从右至左扫描表达式，遇到数字则将数字压入堆栈，遇到运算符则弹出栈顶的两个数字，用运算符对这两数字做运算，并将结果再次入栈，重复上述过程直至表达式最左端，最后数栈的里的最后一个元素就是该表达式的值。 ③ 中缀表达式 中缀表达式是常见的运算表达式，如(3+4)*5-6 中缀表达式的求值是对人类友好的，但是计算机不好理解，在计算机求解过程中往往会将中缀表达式转化为其他表达式来操作（一般往往是后缀表达式，因为中缀表达式的操作符顺序不好确定） ④ 后缀表达式（逆波兰表达式） 与前缀表达式相反，后缀表达式的运算符位于操作数之后，如(3+4)*5-6这样的中缀表达式对应的后缀表达式34+5*6- 后缀表达式的计算机求值，与前缀表达式相反：首先从左至右扫描表达式，遇到数字则将数字压入堆栈，遇到运算符则弹出栈顶的两个数字，用运算符对这两数字做运算，并将结果再次入栈，重复上述过程直至表达式最左端，最后数栈的里的最后一个元素就是该表达式的值。 4. 逆波兰计算器 ① 后缀表达式求解 求解思路如上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 package stack; import java.util.Arrays; /* * 逆波兰计算器 * 假设输入的字符串没有括号，且都是整型数字 * */ public class poLandNotation { public static void main(String[] args) { //先定义一个逆波兰表达式：（30+4）*5-6 -》30 4 + 5 * 6 - String suffixExpression = \u0026#34;30 4 + 5 * 6 -\u0026#34;; char[] chs = suffixExpression.toCharArray(); stackNumber stackNumber = new stackNumber(10); String number = \u0026#34;\u0026#34;;//数字拼接 int num1 = 0; int num2 = 0; int val = 0; for (int i = 0; i \u0026lt; chs.length; i++) { //扫描数字 while (!isSpace(chs[i])) { if (isDigital(chs[i])) { //数字追加 number += chs[i]; i++; } else if (isOpr(chs[i])) { //扫描到操作符，就退出循环，避免死循环 break; } } if (number != \u0026#34;\u0026#34;) { stackNumber.push(Integer.parseInt(number)); } //扫描字符串 if (!isSpace(chs[i])) { if (isOpr(chs[i])) { num1 = stackNumber.pop(); num2 = stackNumber.pop(); val = stackNumber.calculate(num2, num1, chs[i]); stackNumber.push(val); } } number = \u0026#34;\u0026#34;;//清理本次number拼接字符 } // stackNumber.showStack(); System.out.print(\u0026#34;计算结果：\u0026#34;); stackNumber.showBottom(); } //是否是操作符 public static boolean isOpr(char ch) { return ch == \u0026#39;+\u0026#39; || ch == \u0026#39;-\u0026#39; || ch == \u0026#39;*\u0026#39; || ch == \u0026#39;/\u0026#39;; } //是否是数字 public static boolean isDigital(char ch) { return Character.isDigit(ch); } //数字结束符 ‘ ’空格 public static boolean isSpace(char ch) { return ch == \u0026#39; \u0026#39;; } } class stackNumber { int maxSize; int top; int[] arr; public stackNumber(int maxSize) { this.maxSize = maxSize; top = -1; arr = new int[maxSize]; } //是否位空 public boolean isEmpty() { return top == -1; } //是否已满 public boolean isFull() { return top == maxSize - 1; } //入栈 public void push(int num) { if (isFull()) throw new RuntimeException(\u0026#34;栈溢出\u0026#34;); arr[++top] = num; } //出栈 public int pop() { if (isEmpty()) throw new RuntimeException(\u0026#34;栈空\u0026#34;); return arr[top--]; } //显示队列元素（不取出队列元素） public void showStack() { if (isEmpty()) throw new RuntimeException(\u0026#34;栈空\u0026#34;); System.out.println(\u0026#34;栈元素：\u0026#34; + Arrays.toString(arr)); } //显示栈底元素 public void showBottom() { System.out.println(arr[0]); } //计算方法 public int calculate(int num1, int num2, char opr) { switch (opr) { case \u0026#39;+\u0026#39;: return num1 + num2; case \u0026#39;-\u0026#39;: return num1 - num2; case \u0026#39;*\u0026#39;: return num1 * num2; case \u0026#39;/\u0026#39;: return num1 / num2; default: System.out.println(\u0026#34;无效的操作符\u0026#34;); return -1; } } } 该实例演示了对后缀表达式进行计算求解。但是用户往往是输入中缀表达式。以下代码，实现利用中缀表达式转化为后缀表达式，然后再通过以上方法再求解后缀表达式。\n② 中缀表达式转化为后缀表达式 思路（注意各个步骤顺序）：\n初始化两个栈：运算符栈s1和储存中间结果栈s2 从左至右扫描中缀表达式 遇到操作数时，将其压入栈s2 遇到括号时：如果是左括号“（”，则直接入栈；如果是右括号“）”，则依次弹出s1栈顶的运算符，并压入到s2中，直到遇到左括号为止，此时将这一对括号丢弃 遇到运算符时，比较其与s1栈顶运算符的优先级：如果s1为空或栈顶运算符为左括号“（”，再或者优先级比栈顶的运算符的高，就将运算符压入s1栈中；否则（不满足前面三个条件之一），即优先级比栈顶运算符的低或相等，将s1栈顶运算符弹出并压入到s2中，再次转到第四步开始重新比较当前运算符 重复步骤2-5，直至表达式地最右边 将s1剩余地运算符依次弹出并压入s2 依次弹s2中的元素并输出结果，结果的逆序极为中缀表达式对应的后缀表达式。 样例分析\n中缀表达式：1+((2+3)*4)-5\n后缀表达式结果：1 2 3 + 4* + 5 -\n代码展示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 package stack; public class midfixToSuffixDemo { public static void main(String[] args) { //中缀表达式转化为后缀表达式：`10 + ( ( 2 + 3 ) * 4 ) - 5+1` =》`10 2 3 + 4 * + 5 -` //10, 2, 3, 43, 4, 42, 5, 45, 40, 40, 43, stack2 s1 = new stack2(30);//符号栈(运算符+小括号) stack2 s2 = new stack2(30);//中间结果 String expression = \u0026#34;10+((2+3)*4)-5\u0026#34;; String number = \u0026#34;\u0026#34;; char[] chs = expression.toCharArray(); for (int i = 0; i \u0026lt; chs.length; i++) { while (isDigital(chs[i])) { System.out.println(\u0026#34;chs[\u0026#34; + i + \u0026#34;]:\u0026#34; + chs[i]); number += chs[i++];//叠加相邻数字字符 if (i == chs.length) break; } System.out.println(\u0026#34;number:\u0026#34; + number); if (i \u0026lt; chs.length) System.out.println(\u0026#34;chs[\u0026#34; + i + \u0026#34;]:\u0026#34; + chs[i]); if (number != \u0026#34;\u0026#34;) { //拆解数字,放进栈 for (char v : number.toCharArray()) { s2.push(v); } //放入数字和符号隔离符 \u0026#39;|\u0026#39; s2.push(\u0026#39;|\u0026#39;); } number = \u0026#34;\u0026#34;; if (i \u0026lt; chs.length) { if (isLeft(chs[i])) { s1.push(chs[i]); } else if (isRight(chs[i])) { while (!isLeft(s1.getTop())) { s2.push(s1.pop()); s2.push(\u0026#39;|\u0026#39;); } if (isLeft(s1.getTop())) s1.pop();//pop掉左括号 } } if (i \u0026lt; chs.length) { if (isOpr(chs[i])) { if (s1.isEmpty() || isLeft(s1.getTop()) || s1.priority(chs[i]) \u0026gt; s1.priority(s1.getTop())) { System.out.println(\u0026#34;ss \u0026#34; + chs[i]); s1.push(chs[i]); } else if (s1.priority(chs[i]) \u0026lt;= s1.priority(s1.getTop())) { s2.push(s1.pop()); //放入数字和符号隔离符 \u0026#39;|\u0026#39; s2.push(\u0026#39;|\u0026#39;); i--;//重新比较 } } } // s1.showStack(); s2.showStack(); System.out.println(); } while (!s1.isEmpty()) { s2.push(s1.pop()); s2.push(\u0026#39;|\u0026#39;); } System.out.println(); s2.showStack(); } //是否是左括号,支持中英文 public static boolean isLeft(char ch) { return ch == \u0026#39;(\u0026#39; || ch == \u0026#39;（\u0026#39;; } //是否是右括号 public static boolean isRight(char ch) { return ch == \u0026#39;)\u0026#39; || ch == \u0026#39;）\u0026#39;; } //是否是操作符 public static boolean isOpr(char ch) { return ch == \u0026#39;+\u0026#39; || ch == \u0026#39;-\u0026#39; || ch == \u0026#39;*\u0026#39; || ch == \u0026#39;/\u0026#39;; } //是否是数字 public static boolean isDigital(char ch) { return Character.isDigit(ch); } } class stack2 { int maxSize; int top; char[] arr; public stack2(int maxSize) { this.maxSize = maxSize; top = -1; arr = new char[maxSize]; } //是否位空 public boolean isEmpty() { return top == -1; } //是否已满 public boolean isFull() { return top == maxSize - 1; } //入栈 public void push(char num) { if (isFull()) throw new RuntimeException(\u0026#34;栈溢出\u0026#34;); arr[++top] = num; } //出栈 public char pop() { if (isEmpty()) throw new RuntimeException(\u0026#34;栈空\u0026#34;); return arr[top--]; } //返回栈顶元素，但不取出 public char getTop() { return arr[top]; } //显示队列元素（不取出队列元素） public void showStack() { if (isEmpty()) throw new RuntimeException(\u0026#34;栈空\u0026#34;); System.out.print(\u0026#34;stack: \u0026#34;); for (int i = 0; i \u0026lt; top; i++) { System.out.print(arr[i]); } System.out.println(); } //运算符优先级 public int priority(char opr) { if (opr == \u0026#39;+\u0026#39; || opr == \u0026#39;-\u0026#39;) return 0; else if (opr == \u0026#39;*\u0026#39; || opr == \u0026#39;/\u0026#39;) return 1; else return -1;//标识运算符错误 } } 5. 递归 ① 原理 递归，就是在运行的过程中调用自己。\n构成递归需具备的条件\n构成递归需具备的条件 不能无限制地调用本身，须有个出口，化简为非递归状况处理 ② 递归机制 进栈：每递归调用一次函数或方法，就需要进栈一次，最多的进栈元素个数称为递归深度，递归次数越多，递归深度越大，开辟的栈空间越大（应当避免过多的递归使得栈溢出） 出栈：每当遇到递归出口或**完成本次执行（完成执行的意思：执行函数或方法到达底部）**时，需出栈一次并恢复参量值，当全部执行完毕时，栈应为空。 为了完成一次典型的递归调用，系统需要分配一些空间来保存三部分重要的信息 函数的返回地址：一旦函数调用完成，程序应该知道返回到哪里，即函数调用之前的位置 函数传递的参数 函数的局部变量 每次进行递归调用时，都会在栈中有自己的形参和局部变量的拷贝，这个由系统自动完成，即将所有的栈变量压入系统的堆栈。（如果是浅拷贝形参则每个方法或函数压入栈的变量都是相互独立的；但是如果是深拷贝，则会互相影响） 由于有了a)的机制，在递归返回的时候才能将前面压入栈的临时变量又恢复到现场 ④ 递归示例图 ⑤ 递归应用 各种数学问题：八皇后、汉诺塔、阶乘、迷宫等 各种算法也会使用递归，比如快排、归并排序、二分查找、分治算法等 利用栈解决的问题-》递归代码比较简洁 迷宫回溯 如下图，红色方块代表墙，红色圆形代表小球，小球在指定起点位置需要寻找一条到达指定目的地的最短路径\n思路\n初始化地图，假设有墙的地方值为1；没有墙可以走的地方值为0；走过点设置为2；已经探测过没有后路的点设置为3 定制策略，有上下左右四个方向，因此策略有24种策略。只需要找到哪种策略的2的点数最少到达目的地即可 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package recursion; /* * 迷宫回溯 * */ public class miGong { public static void main(String[] args) { //初始化地图，假设有墙的点值为1；没有墙，可以走的点的值为0；走过的点的值设置为2；已经探测过的点设置为3; //走迷宫的策略方法：下-》右-》上-》左；如果该点走不通则回溯; //当然策略也可以更换其他 int[][] map = new int[8][7]; //上下两行赋值为墙 for (int j = 0; j \u0026lt; map[0].length; j++) { map[0][j] = 1; map[7][j] = 1; } //中间 for (int i = 1; i \u0026lt; map.length - 1; i++) { map[i][0] = 1; map[i][6] = 1; } //内部墙 map[3][1] = 1; map[3][2] = 1; //地图 for (int[] arr : map) { for (int e : arr) { System.out.print(e + \u0026#34; \u0026#34;); } System.out.println(); } setWay(map, 1, 1); System.out.println(); //地图 for (int[] arr : map) { for (int e : arr) { System.out.print(e + \u0026#34; \u0026#34;); } System.out.println(); } } //i,j表示从地图哪个位置出发，如果能到map[6][5],则说明通路找到 public static boolean setWay(int[][] map, int i, int j) { if (map[6][5] == 2) {//通路找到 return true; } else { if (map[i][j] == 0) {//当前这个点还没有走过 //假定可以走通，如果走不通会更改其值 map[i][j] = 2; //按照策略走下一步:下（1）-》右（2）-》上（3）-》左（4） if (setWay(map, i + 1, j)) {//先走下 return true; } else if (setWay(map, i, j + 1)) {//下路走不通再走右路 return true; } else if (setWay(map, i - 1, j)) {//右路走不通再走上路 return true; } else if (setWay(map, i, j - 1)) {//上路走不通再走左路 return true; } else {//都走不通，回溯 map[i][j] = 3; return false; } } else {//如果该点不为0，为1 2 3时，表明该点可能为墙，或者已经走过，或者此点已经被探测过以后的路走不通 return false; } } } } 八皇后（回溯算法） 在8*8格的国际象棋上拜访八个皇后，使其不能相互攻击，即：任意两个皇后都不能处于同一行、同一列或同一斜线上，问有多少种摆法？（答案：92种）\n思路（暴力法）\n第一个皇后先放第一列 第二个皇后放在第二行第一列，然后判断是否ok，如果不ok，继续放在第二列、第三列，依次把所有的列放完，找到一个合适的位置 继续第三个皇后，还是第一列、第二列\u0026hellip;直到第八个皇后也能放在一个不冲突的位置，于是就找到了一个正确解 当得到一个正确解时，在栈回退到上一个栈时，就会开始回溯，即将第一个皇后放到第一列的所有正确解全部得到 然后回头继续第一个皇后放到第二列，后面依次循环执行1、2、3、4的步骤 说明：理论上应该创建一个二维数组来表示棋盘地图，但是实际上可以通过算法用一个一维数组解决问题，如int[] arr = new int[]{0,4,7,5,2,6,1,3} arr数组下标表示第几行，即第几个皇后，值表示列的位置：arr[i]表示第i+1个皇后，放在第i+1行的第arr[i]+1列。当然也可以使用二维数组实现，但是\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 package recursion; /* * 八皇后 * */ public class eightQueens { //皇后数 static int max = 8; //保存皇后放置位置结果 static int[] arr = new int[max]; //结果数 static int num = 0; public static void main(String[] args) { placeQueen(0); System.out.println(\u0026#34;总共：\u0026#34; + num + \u0026#34;种结果\u0026#34;); } //放置第n个皇后 public static void placeQueen(int n) { if (n == max) { num++; print(); return; } //放置皇后，并判断是否冲突 for (int i = 0; i \u0026lt; max; i++) { //第n个皇后放到第i列 arr[n] = i; //判断此时放置的位置与前面放好的八皇后是否冲突 if (judge(n)) { //如果不冲突就放置下一个皇后 placeQueen(n + 1); } //如果冲突，把皇后放到下一列 } } //查看当我们放置第n个皇后是否与前面的皇后冲突 public static boolean judge(int n) { for (int i = 0; i \u0026lt; n; i++) { //arr[i]==arr[n] -》表示同列 //Math.abs(n-i)==Math.abs(arr[i]-arr[n]) -》表示同斜线 //是否在同一行，没有必要没有必要 if (arr[i] == arr[n] || Math.abs(n - i) == Math.abs(arr[i] - arr[n])) { return false; } } return true; } //打印八皇后位置 public static void print() { System.out.print(\u0026#34;八皇后位置:\u0026#34;); for (int e : arr) { System.out.print(e + \u0026#34; \u0026#34;); } System.out.println(); } } 五、排序算法 排序也称排序算法，排序是将一组数据依指定顺序进行排列的过程。\n排序的分类：\n内部排序\n指将需要处理的所有数据都加载到内部存储器中进行排序。\n外部排序\n数据量过大，无法全部加载到内存中，需要借助外部存储进行排序\n常见的排序算法分类\n相关术语：\n稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面； 不稳定：如果a原本在b的前面，而a=b，排序之后a可能会出现在b的后面； 内排序：所有排序操作都在内存中完成； 外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 时间复杂度： 一个算法执行所耗费的时间。 空间复杂度：运行完一个程序所需内存的大小。 n: 数据规模 k: “桶”的个数，不做特殊的处理的话，一般为10 In-place: 不占用额外内存 Out-place: 占用额外内存 1. 冒泡排序 原理\n通过对待排序序列从前往后（从下标较小的元素开始），依次比较相邻元素的值，若发现逆序则交换，使值较大。\n算法复杂度：O()\n算法描述\n比较相邻的元素。如果第一个比第二个大，就交换它们两个 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数，同时这个元素也是排好序的元素 针对所有的元素重复以上的步骤，除了最后一个 重复步骤1~3，直到排序完成 优化：因为排序的过程中，各元素不断接近自己的位置，如果一趟比较下 来没有进行过交换，就说明序列已经有序，无需再比较下去 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package sort; import java.util.Arrays; /* * 冒泡排序 * */ public class bubbleSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); bubbleSort(arr); System.out.println(Arrays.toString(arr)); } public static void bubbleSort(int[] arr) { int temp = 0; boolean flag = false; for (int i = 0; i \u0026lt; arr.length - 1; i++) {//需要多少轮两两比较，每一轮都会选出一个排好序的数字 flag = false; for (int j = 0; j \u0026lt; arr.length - 1 - i; j++) { // 从第一个元素到第i个元素 if (arr[j] \u0026gt; arr[j + 1]) { //交换值 temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; flag = true; } } if (!flag) { break; } } } } 2. 选择排序 原理\n选择排序是一种简单直观的排序算法，它也是一种交换排序算法，和冒泡排序有一定的相似度，可以认为选择排序是冒泡排序的一种改进\n算法描述\n在未排序序列中找到最小（大）元素，存放到排序序列的起始位置 从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾 重复第二步，直到所有元素均排序完毕 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package sort; import java.util.Arrays; /* * 选择排序 * */ public class selectionSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); selectionSort(arr); System.out.println(Arrays.toString(arr)); } public static void selectionSort(int[] arr) { int temp, min, index = 0; for (int i = 0; i \u0026lt; arr.length - 1; i++) { min = arr[i];//最小值 for (int j = i + 1; j \u0026lt; arr.length; j++) {//从i的下一个元素开始到最后一个元素进行比较 if (min \u0026gt; arr[j]) { min = arr[j]; index = j;//记录最小值角标 } } if (min != arr[i]) {//min值改变，说明找到更小的值，需要交换 temp = arr[i]; arr[i] = arr[index]; arr[index] = temp; } } } } 3. 插入排序 原理\n插入排序是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入\n算法描述\n把待排序的数组分成已排序和未排序两部分，初始的时候把第一个元素认为是已排好序的\n从第二个元素开始，在已排好序的子数组中寻找到该元素合适的位置并插入该位置\n重复上述过程直到最后一个元素被插入有序子数组中\n问题\n1 2 3 4 5 6 7 8 9 我们看简单的插入排序可能存在的问题. 数组 arr = {2,3,4,5,6,1} 这时需要插入的数 1(最小), 这样的过程是： {2,3,4,5,6,6} {2,3,4,5,5,6} {2,3,4,4,5,6} {2,3,3,4,5,6} {2,2,3,4,5,6} {1,2,3,4,5,6} 结论: 当需要插入的数是较小的数时，后移的次数明显增多，对效率有影响 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package sort; import java.util.Arrays; /* *\t插入排序 */ public class insertionSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); insertionSort(arr); System.out.println(Arrays.toString(arr)); } public static void insertionSort(int[] arr) { // for (int i = 1; i \u0026lt; arr.length; ++i) { // int value = arr[i];//记录当前需要比较的数据 // int position = i;//记录当前数据角标 // while (position \u0026gt; 0 \u0026amp;\u0026amp; arr[position - 1] \u0026gt; value) {//大于value的素数组元素往后挪一位，直到不大于 // arr[position] = arr[position - 1]; // position--; // } // arr[position] = value; // } for (int i = 1; i \u0026lt; arr.length; i++) { int val = arr[i]; int index = i; for (; index \u0026gt; 0; index--) { if (arr[index - 1] \u0026gt; val) { arr[index] = arr[index - 1]; } else { break; } } arr[index] = val; } } } 4. 希尔排序 原理\n移位法思路：\n希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序。希尔排序是记录下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。增量序列的选取：希尔增量、Hibbard增量、Sedgewick增量\n算法描述\n先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述：\n选择一个增量序列t1，t2，…，tk，其中ti\u0026gt;tj，tk=1 按增量序列个数k，对序列进行 k 趟排序 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度 代码实现（选用希尔增量）\n交换法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package sort; import java.util.Arrays; /* *\t希尔排序（交换法） */ public class shellSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); shellSort(arr); System.out.println(Arrays.toString(arr)); } public static void shellSort(int[] arr) { int temp; for (int gap = arr.length / 2; gap \u0026gt; 0; gap /= 2) {//增量序列,gap为步长,分组 for (int i = 0; i \u0026lt; arr.length - gap; i++) {//遍历有多少组：(arr.length-gap)组 for (int j = 0; j \u0026lt; arr.length - gap; j += gap) {//遍历交换组内元素 if (arr[j] \u0026gt; arr[j + gap]) { temp = arr[j]; arr[j] = arr[j + gap]; arr[j + gap] = temp; } } } } } } 注意：交换法对数据进行排序，在数据量大的时候，变得很慢。采用移动法进行优化\n移位法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package sort; import java.util.Arrays; public class shellSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(\u0026#34;排序前：\u0026#34; + Arrays.toString(arr)); shellSortByMove(arr); System.out.println(\u0026#34;移位法排序后：\u0026#34; + Arrays.toString(arr)); } public static void shellSortByMove(int[] arr) { for (int gap = arr.length / 2; gap \u0026gt; 0; gap /= 2) {//分组 //对每组所有元素进行直接插入排序 for (int i = gap; i \u0026lt; arr.length; i++) { int position = i; int value = arr[i]; if (value \u0026lt; arr[position - gap]) { while (position - gap \u0026gt;= 0 \u0026amp;\u0026amp; value \u0026lt; arr[position - gap]) { arr[position] = arr[position - gap]; position -= gap; } arr[position] = value; } } } } } 5. 快速排序 原理\n快速排序（Quicksort）是对冒泡排序的一种改进。基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列\n算法描述\n从数列中挑出一个元素，称为\u0026quot;基准\u0026quot;（pivot） 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任何一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package sort; import java.util.Arrays; public class quickSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; // int[] arr = new int[]{-9,78,0,23,-567,70}; System.out.println(\u0026#34;排序前：\u0026#34; + Arrays.toString(arr)); quickSort(arr); System.out.println(\u0026#34;排序后：\u0026#34; + Arrays.toString(arr)); } public static void quickSort(int[] arr) { qsort(arr, 0, arr.length - 1); } private static void qsort(int[] arr, int low, int high) { if (low \u0026gt;= high) return; int pivotIndex = partition(arr, low, high); //将数组分为两部分 qsort(arr, low, pivotIndex - 1); //递归排序左子数组 qsort(arr, pivotIndex + 1, high); //递归排序右子数组 } private static int partition(int[] arr, int low, int high) { int pivot = arr[low]; //记录基准 while (low \u0026lt; high) { while (low \u0026lt; high \u0026amp;\u0026amp; arr[high] \u0026gt;= pivot) --high; arr[low] = arr[high]; //交换比基准小的记录到左端low角标处 while (low \u0026lt; high \u0026amp;\u0026amp; arr[low] \u0026lt;= pivot) ++low; arr[high] = arr[low]; //交换比基准大的记录到右端high角标处 } //扫描完成，基准移到新的基准 arr[low] = pivot; //返回的是新基准的位置 return low; } } 6. 归并排序 原理\n归并排序是利用归并的思想实现的排序方法，该算法采用经典的分治策略（分治法将问题分成一些小的问题然后递归求解，而治的阶段则将分的阶段得到的各答案“修补”在一起，即分而治之）\n说明：可以看到这种结构很像一棵完全二叉树，本文的归并排序我们采用递归去实现（也可采用迭代的方式去实现）。分阶段可以理解为就是递归拆分子序列的过程。\n再来看看治阶段，我们需要将两个已经有序的子序列合并成一个有序序列，比如上图中的最后一次合并，要将[4,5,7,8]和[1,2,3,6]两个已经有序的子序列，合并为最终序列[1,2,3,4,5,6,7,8]，来看下实现步骤(双指针法)\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package sort; import java.util.Arrays; public class mergeSort { public static void main(String[] args) { int[] arr = {8, 4, 5, 7, 1, 3, 6, 2}; // int[] arr = {4, 5, 7, 8, 1, 2, 3, 6}; int[] temp = new int[arr.length]; System.out.println(\u0026#34;排序前：\u0026#34; + Arrays.toString(arr)); mergeSort(arr, 0, arr.length - 1, temp); System.out.println(\u0026#34;排序后：\u0026#34; + Arrays.toString(arr)); } //分+和 public static void mergeSort(int[] arr, int left, int right, int[] temp) { if (left \u0026lt; right) { int mid = (left + right) / 2;//中间索引 //向左递归分解 mergeSort(arr, left, mid, temp); //向右递归分解 mergeSort(arr, mid + 1, right, temp); //合并 merge(arr, left, mid, right, temp); } } //mid指左子序列最后一个元素 public static void merge(int[] arr, int left, int mid, int right, int[] temp) { int i = left;//指左子序列起点处 int j = mid + 1;//指右子序列起点处 int t = 0;//temp数组索引 //先将左右两边有序数据填充到temp数组，直到又一边填充完毕 while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= right) { if (arr[i] \u0026lt;= arr[j]) { temp[t] = arr[i]; t++; i++; } else if (arr[i] \u0026gt; arr[j]) { temp[t] = arr[j]; t++; j++; } } //再将一边剩下的有序数据移到temp后面 while (i \u0026lt;= mid) { temp[t++] = arr[i++]; } while (j \u0026lt;= right) { temp[t++] = arr[j++]; } //拷贝指定数量至arr数组,注意左右拷贝边界 t = 0; int tLeft = left; while (tLeft \u0026lt;= right) { arr[tLeft++] = temp[t++]; } } } 7. 基数排序 原理\n基数排序（radix sort）属于“分配式排序”，又称“桶子法”，顾名思义，它是通过键值的各个位的值，将要排序的元素分配至某些“桶”中，达到排序的作用。基数排序法是属于稳定性的排序，基数排序法的是效率高的稳定性排序法。基数排序是桶排序的扩展。基数排序是这样实现的：将整数按位数切割成不同的数字，然后按每个位数分别比较。\n注:假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，即在原序列中，r[i]=r[j]，且r[i]在r[j]之前，而在排序后的序列中，r[i]仍在r[j]之前，则称这种排序算法是稳定的；否则称为不稳定的\n**有负数的数组，我们不用基数排序来进行排序, 如果要支持负数，参考: https://code.i-harness.com/zh-CN/q/e98fa9 **\n算法实现\n将所有待比较数值统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序，相同的数放到同一个“桶”（数组），然后再依次从“桶”中取出元素，形成的序列再比较各个数字下一位，这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 package sort; import java.util.Arrays; /* * 基数排序：这个版本的实现只适用于非负数的数组排序 * */ public class radixSort { public static void main(String[] args) { int[] arr = new int[]{1, 210, 10, 5, 41, 6, 14, 1, 21, 32, 56, 689, 9923}; System.out.println(\u0026#34;排序前：\u0026#34; + Arrays.toString(arr)); radixSort(arr); System.out.println(\u0026#34;排序后：\u0026#34; + Arrays.toString(arr)); } public static void radixSort(int[] arr) { //每个“桶”就是一个一维数组，定义10个“桶”，角标表示”桶“能装下的对应某位数字。空间换时间 int[][] buckets = new int[10][arr.length]; //bucketElementCount[0]记录的是buckets[0]中的数据个数，其他依次类推 int[] bucketElementCount = new int[10]; //最大数字 int maxDigits = 0; //最大数字的位数 int numberOfMaxDigits = 0; //取出arr中最大元素 for (int v : arr) { if (v \u0026gt; maxDigits) maxDigits = v; } //计算其位数 while (maxDigits != 0) { numberOfMaxDigits++; maxDigits /= 10; } //进行numberOfMaxDigits轮基数排序 for (int i = 0; i \u0026lt; numberOfMaxDigits; i++) { //放数据入桶 for (int j = 0; j \u0026lt; arr.length; j++) { //依次取出某位数 int digitOfElement = arr[j] / (int) Math.pow(10, i) % 10; //将这位数放到指定的桶 buckets[digitOfElement][bucketElementCount[digitOfElement]] = arr[j]; //记录桶数据个数，这里采用一维数组记录桶的有效数据个数， //也可以采用队列数组的方式来做桶排序，这样就不需要提供这样额外的数组了 bucketElementCount[digitOfElement]++; } int index = 0; //从桶中把数据依次取出，并放入原数组arr中，供下次排序使用 for (int j = 0; j \u0026lt; bucketElementCount.length; j++) { if (bucketElementCount[j] != 0) { //取出对应桶中的数据 for (int l = 0; l \u0026lt; bucketElementCount[j]; l++) { arr[index++] = buckets[j][l]; buckets[j][l] = 0;//清理桶 } bucketElementCount[j] = 0;//清理桶 } } } } } 8. 堆排序 堆排序如下（本处文字用于本文锚点跳转，请勿删除）\n原理\n堆是一种特殊的完全二叉树（complete binary tree）。完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。\n如下图，是一个堆和数组的相互关系：\n数组与完全二叉树的对应关系\n二叉堆一般分为两种：最大堆和最小堆。\n最大堆：\n最大堆中的最大元素值出现在根结点（堆顶），堆中每个父节点的元素值都大于等于其孩子结点（如果存在）\n最小堆：\n最小堆中的最小元素值出现在根结点（堆顶），堆中每个父节点的元素值都小于等于其孩子结点（如果存在）\n算法实现\n以数组升序排列为例\n将待排序序列构造成一个大顶堆。常规方法是从最后一个非叶子结点从左至右、从上至下的筛选，直到根元素筛选完毕。这个方法叫“筛选法”\n此时，整个序列的最大值就是堆顶的根节点\n将其与末尾元素进行交换，此时末尾就为最大值\n然后将剩余n-1个元素重新构造成一个堆，这样就会得到n个元素的次小值。如此反复执行，便得到一个有序序列了\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 package sort; import java.util.Arrays; /* * 堆排序：升序排列 * */ public class heapSort { public static void main(String[] args) { int[] arr = {4, 13, 11, 0, 1, 2, 3, 4, 3, 2, 1, 6, 8, 5, 9, 14, 12, 9, 0}; System.out.println(Arrays.toString(sortArray(arr))); } //堆排序 public static int[] sortArray(int[] nums) { //建立初始大根堆 buildMaxHeap(nums); //调整大根堆 for (int i = nums.length - 1; i \u0026gt; 0; i--) { //将大根堆顶与最后一个元素交换，通过不断交换，最后得到一个升序数组（每次交换从堆中移出已排好序的元素，堆逐渐变小） swap(nums, 0, i); //调整剩余数组，使其满足大顶堆 maxHeapify(nums, 0, i); } return nums; } //建立初始大根堆 public static void buildMaxHeap(int[] nums) { //最后一个非叶子节点开始，这样不断地从左至右，从下至上的调整，每次调整只是调整最多三个节点的小堆，方便处理 for (int i = nums.length / 2 - 1; i \u0026gt;= 0; i--) { //调整每一个子树为大根堆 maxHeapify(nums, i, nums.length); } } //调整大根堆，第二个参数为堆顶，第三个参数为，堆的大小 public static void maxHeapify(int[] nums, int i, int heapSize) { //左子树 int l = 2 * i + 1; //右子树 int r = l + 1; //记录根结点、左子树结点、右子树结点三者中的最大值下标 int largest = i; //与左子树结点比较 if (l \u0026lt; heapSize \u0026amp;\u0026amp; nums[l] \u0026gt; nums[largest]) { largest = l; } //与右子树结点比较 if (r \u0026lt; heapSize \u0026amp;\u0026amp; nums[r] \u0026gt; nums[largest]) { largest = r; } //如果当前节点的左子树或右子树比它大，则赋值 if (largest != i) { //将最大值交换为根结点 swap(nums, i, largest); //再次调整交换数字后的大顶堆 //当树的深度够大时，如果没有下面这条语句， //也就是说，每次交换只会停留在每一个小堆（最多三个节点构成的小堆） //还需要递归看看其左右子树是否存在比当前节点大的数，有则交换上来，无则退出递归 maxHeapify(nums, largest, heapSize); } } //交换 public static void swap(int[] nums, int i, int j) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; } } 六、查找算法 查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。本文简单概括性的介绍了常见的七种查找算法，说是七种，其实二分查找、插值查找以及斐波那契查找都可以归为一类——插值查找。插值查找和斐波那契查找是在二分查找的基础上的优化查找算法。\n查找算法分类：\n静态查找和动态查找；\n注：静态或者动态都是针对查找表而言的。动态表指查找表中有删除和插入操作的表。\n无序查找和有序查找。\n无序查找：被查找数列有序无序均可，如顺序查找\n有序查找：被查找数列必须为有序数列，如：二分查找，插值查找，斐波那契查找\n二分查找，插值查找，斐波那契查找的原理类似，只是分割点的原理不同，二分查找是无脑二分，插值查找是自适应地趋向目标值（采用拉格朗日插值法），斐波那契查找分割是玄学黄金分割。\n平均查找长度（Average Search Length，ASL）：需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度\n​\t对于含有n个数据元素的查找表，查找成功的平均查找长度为：ASL = Pi*Ci的和 Pi：查找表中第i个数据元素的概率 Ci：找到第i个数据元素时已经比较过的次数\n1. 顺序查找 原理\n说明：顺序查找适合于存储结构为顺序存储或链接存储的线性表。\n基本思想：顺序查找也称为线性查找，属于无序查找算法。从数据结构线性表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。\n复杂度分析：\n查找成功时的平均查找长度为：（假设每个数据元素的概率相等） ASL = 1/n(1+2+3+…+n) = (n+1)/2 ;\n当查找不成功时，需要n+1次比较，时间复杂度为O(n);所以，顺序查找的时间复杂度为O(n)。\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package search; public class seqSearch { public static void main(String[] args) { int[] arr = {1, 4, -1, 2, -4, 8, 90}; int index = seqSearch(arr, 90); System.out.println(\u0026#34;查找的元素在索引为\u0026#34; + index + \u0026#34;的位置\u0026#34;); } public static int seqSearch(int[] arr, int value) { for (int i = 0; i \u0026lt; arr.length; i++) { if (arr[i] == value) { return i; } } System.out.println(\u0026#34;没有该数据\u0026#34;); return 0; } } 2. 二分查找 二分查找如下（本处文字用于本文锚点跳转，请勿删除）\n原理\n说明：元素必须是有序的，如果是无序的则要先进行排序操作。\n**基本思想：**也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。\n复杂度分析：最坏情况下，关键词比较次数为log2(n+1)，且期望时间复杂度为O(log2n)；\n注：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 package search; /* * 二分查找有序序列 * */ public class binarySearch { public static void main(String[] args) { int[] arr = {1, 2, 3, 5, 6, 10, 11}; int index = binarySearch1(arr,5); System.out.println(\u0026#34;查找的元素在索引为\u0026#34; + index + \u0026#34;的位置\u0026#34;); int index1 = binarySearch2(arr, 4, 0, arr.length - 1); System.out.println(\u0026#34;查找的元素在索引为\u0026#34; + index1 + \u0026#34;的位置\u0026#34;); } //折半查找:数组已经有序，从小到大的顺序 public static int binarySearch1(int[] arr, int value) { int left = 0, right = arr.length - 1; int mid = 0; while (left \u0026lt;= right) { mid = (left + right) / 2;//边界条件的移动也促使mid的移动 if (arr[mid] \u0026gt; value) {//value在arr[mid]之前 right = mid - 1;//将边界条件缩小至mid-1 } else if (arr[mid] \u0026lt; value) {//value在arr[mid]之后 left = mid + 1;//将边界条件扩大至mid+1 } else { return mid; } } System.out.println(\u0026#34;数据不存在\u0026#34;); return -1; } //递归查找 public static int binarySearch2(int[] arr, int value, int left, int right) { int mid = left + (right - left) / 2;//避免数据过大 if (left \u0026gt; right) { System.out.println(\u0026#34;数据不存在\u0026#34;); return -1; } if (arr[mid] \u0026lt; value) {//向右子序列递归 return binarySearch2(arr, value, mid + 1, right); } else if (arr[mid] \u0026gt; value) {//向左子序列递归 return binarySearch2(arr, value, left, mid - 1); } else {//递归结束条件 return mid; } } } 3. 插值查找 原理\n在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。同样的，比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。\n经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下：\nmid=(low+high)/2, 即mid=low+1/2*(high-low)　通过类比，我们可以将查找的点改进为如下：\nmid=low+(key-a[low])/(a[high]-a[low])*(high-low)\n也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。\n**基本思想：**基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。\n注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。\n复杂度分析：查找成功或者失败的时间复杂度均为O(log2(log2n))\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package search; /* * 插值查找，改进版二分查找，使用自适应的mid来靠近value，以避免盲目的分区，节省比较次数 * */ public class insertionSearch { public static void main(String[] args) { int[] arr = {1, 2, 3, 5, 6, 10, 11}; int index1 = insertionSearch(arr, 3, 0, arr.length - 1); System.out.println(\u0026#34;查找的元素在索引为\u0026#34; + index1 + \u0026#34;的位置\u0026#34;); } public static int insertionSearch(int[] arr, int value, int left, int right) { // int mid = left + (right - left) / 2;//避免数据过大 int mid = left + (value - arr[left]) / (arr[right] - arr[left]) * (right - left);//自适应mid if (left \u0026gt; right) { System.out.println(\u0026#34;数据不存在\u0026#34;); return -1; } if (arr[mid] \u0026lt; value) {//向右子序列递归 return insertionSearch(arr, value, mid + 1, right); } else if (arr[mid] \u0026gt; value) {//向左子序列递归 return insertionSearch(arr, value, left, mid - 1); } else {//递归结束条件 return mid; } } } 4. 斐波那契查找 原理\n**基本思想：**斐波那契查找原理与前两种相似，仅仅改变了中间结点（mid）的位置，mid不再是中间或插值得到，而是位于黄金分割点附近，即mid=low+F(k-1)-1（F代表斐波那契数列）\n对F(k-1)-1的理解： 由斐波那契数列 F[k]=F[k-1]+F[k-2] 的性质，可以得到 （F[k]-1）=（F[k-1]-1）+（F[k-2]-1）+1 。该式说明：只要顺序表的长度为F[k]-1，则可以将该表分成长度为F[k-1]-1和F[k-2]-1的两段，即如上图所示。从而中间位置为mid=low+F(k-1)-1。类似的，每一子段也可以用相同的方式分割。但顺序表长度n不一定刚好等于F[k]-1，所以需要将原来的顺序表长度n增加至F[k]-1。这里的k值只要能使得F[k]-1恰好大于或等于n即可，由以下代码得到,顺序表长度增加后，新增的位置（从n+1到F[k]-1位置），都赋为n位置的值即可。\n1 while(n\u0026gt;fib(k)-1) k++; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1),比较结果也分为三种\n相等，mid位置的元素即为所求。注意扩容后的数组长度带来的影响\n大于，low=mid+1,k-=2\n说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为 n-(F(k-1))= F(k)-1-F(k-1)=F(k)-F(k-1)-1=F(k-2)-1个，所以可以递归地应用斐波那契查找\n小于，high=mid-1,k-=1\n说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为 F(k-1)-1个，所以可以递归地应用斐波那契查找。\n复杂度分析：最坏情况下，时间复杂度为O(log2n)，且其期望复杂度也为O(log2n)。\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 package search; import java.util.Arrays; /* * 斐波那契查找 * */ public class fibonacciSearch { public static int maxSize = 20; public static void main(String[] args) { int[] arr = {1, 8, 10, 89, 1000, 1234}; int index = fibonacciSearch(arr, 1000); System.out.println(index); } //斐波那契查找算法 public static int fibonacciSearch(int[] arr, int val) { int low = 0; int high = arr.length - 1;//代表原数组最高的下标4 int k = 0;//斐波那契分割数值的下标 int mid; int[] f = getFib(); //获取到斐波那契分割数值的下标 1 1 2 3 5 8 while (arr.length \u0026gt; f[k] - 1) {//6 2 k++;//4 } //因为f[k]值可能大于a的长度，因此我们需要使用Arrays类，构造一个新的数组，并指向a[] int[] temp = Arrays.copyOf(arr, f[k]); //填充新的数组，因为新数组长度如果比原数组大，多的部分会被填充0，因此需要使用最后面的最大值来填充，防止数组顺序被破坏 for (int i = high + 1; i \u0026lt; temp.length; i++) { temp[i] = arr[high]; } //找到中间值mid后开始查找 while (low \u0026lt;= high) {//只要这个条件满足，就可以继续查找 mid = low + f[k - 1] - 1;// if (val \u0026lt; temp[mid]) {//向左查找 high = mid - 1; //为什么是k-- //说明 //1.全部元素 = 前面的元素 + 后边元素 //2.f[k] = f[k-1]+f[k-2] //因为前面有f[k-1]个元素，所以可以继续拆分f[k-1] = f[k-2] + f[k-3] //即在f[k-1]的前面继续查找k-- //即下次循环mid = f[k-1-1]-1 k--; } else if (val \u0026gt; temp[mid]) { low = mid + 1; //为什么是k-=2 //说明 //继续拆分f[k-2] = f[k-3] + f[k-4] //即在f[k-2]的前面继续查找k-=2 //即下次循环mid = f[k-1-1-1]-1 k -= 2; } else { if (mid \u0026lt;= high) { return mid; } else { //数组扩容过，此时返回mid就会超出原数组的长度 return high; } } } return -1; } //生成一个斐波那契数列 public static int[] getFib() { int[] f = new int[maxSize]; f[0] = 0; f[1] = 1; for (int i = 2; i \u0026lt; maxSize; i++) { f[i] = f[i - 1] + f[i - 2]; } return f; } } 5. 哈希表数据结构 原理\n散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。\n可以使用数组+链表实现，或者使用数组+二叉数/二叉排序树实现（性能更高）。在业务里的应用可以是：使用哈希表构建自己项目的缓存中间层放在数据库和服务器之间（造个轮子）\n哈希表业务应用\n题目：\n有一个公司,当有新的员工来报道时,要求将该员工的信息加入\u000b(id,性别,年龄,名字,住址..),当输入该员工的id时,要求查找到该员工的 所有信息. 要求: 不使用数据库,,速度越快越好=\u0026gt;哈希表(散列)、添加时，保证按照id从低到高插入\n代码实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 package hashTable; /* * 哈希表 * */ public class hashTable { public static void main(String[] args) { hashTableManager hashTableManager = new hashTableManager(10); hashTableManager.add(new node(114, \u0026#34;cold bin\u0026#34;, \u0026#34;重庆\u0026#34;)); hashTableManager.add(new node(224, \u0026#34;wtl\u0026#34;, \u0026#34;重庆\u0026#34;)); hashTableManager.add(new node(334, \u0026#34;sss\u0026#34;, \u0026#34;ss\u0026#34;)); hashTableManager.add(new node(3, \u0026#34;bwj\u0026#34;, \u0026#34;重庆\u0026#34;)); hashTableManager.show(); hashTableManager.delete(224); hashTableManager.show(); hashTableManager.update(new node(334, \u0026#34;aaa\u0026#34;, \u0026#34;\u0026#34;)); hashTableManager.show(); node node = hashTableManager.findNode(224); if (node != null) { System.out.println(\u0026#34;找到节点\u0026#34; + node); } } } class hashTableManager { int size;//哈希表的数组长度 nodeManager[] hashTable;//哈希表的内存结构 public hashTableManager(int size) { this.size = size; hashTable = new nodeManager[size]; //实例化数组里的对象 for (int i = 0; i \u0026lt; size; i++) { nodeManager nodeManager = new nodeManager(20); hashTable[i] = nodeManager; } } //散列函数 public int hashFunc(int id) { return id % this.size; } //添加 public void add(node node) { int nodeManagerNo = hashFunc(node.id); this.hashTable[nodeManagerNo].add(node); } public void delete(int id) { int nodeManagerNo = hashFunc(id); this.hashTable[nodeManagerNo].delete(id); } public void update(node node) { int nodeManagerNo = hashFunc(node.id); this.hashTable[nodeManagerNo].update(node); } //查询id对应的员工信息 public node findNode(int id) { int nodeManagerNo = hashFunc(id); return this.hashTable[nodeManagerNo].findNode(id); } //遍历 public void show() { for (int i = 0; i \u0026lt; this.size; i++) { if (!this.hashTable[i].isEmpty()) { System.out.print(\u0026#34;哈希表第\u0026#34; + (i + 1) + \u0026#34;行:\u0026#34;); this.hashTable[i].show(); } else { System.out.println(\u0026#34;哈希表第\u0026#34; + (i + 1) + \u0026#34;行为空\u0026#34;); } } } } class nodeManager { int maxSize;//节点最大容量 node head;//带数据的头节点 //初始化空的带头节点链表 public nodeManager(int maxSize) { this.head = new node(-1); this.maxSize = maxSize; } public boolean isEmpty() { return head.next == null; } public boolean isFull() { int num = 0; node temp = head; while (temp.next != null) { num++; temp = temp.next; } return num == maxSize - 1; } //增：按照id递增插入 public void add(node node) { if (isFull()) { System.out.println(\u0026#34;链表已满\u0026#34;); return; } node temp = head; while (temp != null) { node beforeNode = temp; if (temp.next == null) { temp.next = node; return; } temp = temp.next; if (temp.id \u0026gt; node.id) { //记录对应编号节点的前一个节点位置，将新节点插入这个位置，意即beforeNode之后，temp之前； beforeNode.next = node; node.next = temp; System.out.println(\u0026#34;插入成功\u0026#34;); return; } } System.out.println(\u0026#34;没有找到编号\u0026#34;); } //删 public void delete(int id) { if (isEmpty()) { System.out.println(\u0026#34;链表已空\u0026#34;); return; } node temp = head.next; while (temp != null) { node preNode = temp; temp = temp.next; if (temp.id == id) { preNode.next = temp.next; System.out.println(\u0026#34;删除成功\u0026#34;); return; } } System.out.println(\u0026#34;删除失败，没有该id\u0026#34;); } //改 public void update(node node) { if (isEmpty()) { System.out.println(\u0026#34;链表已空\u0026#34;); return; } node temp = head.next; while (temp != null) { temp = temp.next; if (temp.id == node.id) { temp.name = node.name; temp.address = node.address; System.out.println(\u0026#34;更新成功\u0026#34;); return; } } System.out.println(\u0026#34;更新失败，没有该id\u0026#34;); } //查找节点 public node findNode(int id) { if (isEmpty()) { System.out.println(\u0026#34;链表为空\u0026#34;); return null; } node temp = head; while (temp != null \u0026amp;\u0026amp; temp.id != id) { temp = temp.next; } if (temp != null) { return temp; } else { System.out.println(\u0026#34;没有该id\u0026#34;); return null; } } //查 public void show() { if (isEmpty()) { System.out.println(\u0026#34;链表为空\u0026#34;); return; } node temp = head.next; System.out.print(\u0026#34;链表：\u0026#34;); while (temp != null) { System.out.print(temp); temp = temp.next; } System.out.println(); } } class node { int id; String name; String address; node next; public node(int id, String name, String address) { this.id = id; this.name = name; this.address = address; } public node(int id) { this.id = id; } @Override public String toString() { return \u0026#34;node{\u0026#34; + \u0026#34;id=\u0026#34; + id + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, address=\u0026#39;\u0026#34; + address + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } 七、树 为什么需要树这种数据结构\n数组存储方式的分析\n优点：通过下标方式访问元素，速度快。对于有序数组，还可使用二分查找提高检索速度 缺点：如果要检索具体某个值，或者插入值(按一定顺序)会整体移动，效率较低 链式存储方式的分析\n优点：在一定程度上对数组存储方式有优化(比如：插入一个数值节点，只需要将插入节点，链接到链表中即可， 删除效率也很好)。 缺点：在进行检索时，效率仍然较低，比如(检索某个值，需要从头节点开始遍历) 树存储方式的分析\n优点：能提高数据存储，读取的效率, 比如利用 二叉排序树(Binary Sort Tree)，既可以保证数据的检索速度，同时也可以保证数据的插入，删除，修改的速度。案例: [7, 3, 10, 1, 5, 9, 12]\n1. 二叉树 ① 二叉树概念 下图有些错误：H、E、F、G才是叶子节点\n树的常用术语 节点 根节点 父节点 子节点 叶子节点 (没有子节点的节点) 节点的权(节点值) 路径(从root节点找到该节点的路线) 层 子树 树的高度(最大层数) 森林 :多颗子树构成森林 概念 树有很多种，每个节点最多只能有两个子节点的一种形式称为二叉树。二叉树的子节点分为左节点和右节点。\n如果该二叉树的所有叶子节点都在最后一层，并且结点总数= 2^n -1（等比数列前n项和） ,n 为层数，则我们称为满二叉树。\n完全二叉树是由满二叉树而引出来的，若设二叉树的深度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数(即1~h-1层为一个满二叉树)，第 h 层所有的结点都连续集中在最左边，这就是完全二叉树。\n注意：完全二叉树 ， 如果把 (61)节点删除，就不是完全二叉树了，因为叶子节点不连续了\n② 二叉树遍历 tips: 看输出父节点的顺序，就确定是前序，中序还是后序\n示例一 节点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class treeNode { int no; String name; treeNode leftChildNode; treeNode rightChildNode; public treeNode(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \u0026#34;treeNode{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } //前序遍历public void preShow() //中序遍历public void infixShow() //后序遍历public void postShow() } 前序遍历: 先输出父节点，再假递归遍历左子树和右子树\n前序就是 父节点-\u0026gt;左子树-\u0026gt;右子树\nABDHIEJCFG\n1 2 3 4 5 6 7 8 9 public void preShow(){ System.out.println(this); if (this.leftChildNode!=null){ this.leftChildNode.preShow();//此处还不叫递归，只是调用另一个对象的这个方法 } if (this.rightChildNode!=null){ this.rightChildNode.preShow(); } } 中序遍历: 先假递归遍历左子树，再输出父节点，再假递归遍历右子树\n中序就是左子树-\u0026gt;父节点-\u0026gt;右子树\n则图所示二叉树的前序遍历输出为： HDIBJEAFCG\n1 2 3 4 5 6 7 8 9 public void infixShow(){ if (this.leftChildNode!=null){ this.leftChildNode.infixShow(); } System.out.println(this); if (this.rightChildNode!=null){ this.rightChildNode.infixShow(); } } 后序遍历: 先递归遍历左子树，再递归遍历右子树，最后输出父节点\n后序是 左子树 -\u0026gt; 右子树 -\u0026gt;父节点\nHIDJEBFGCA\n1 2 3 4 5 6 7 8 9 public void postShow(){ if (this.leftChildNode!=null){ this.leftChildNode.postShow(); } if (this.rightChildNode!=null){ this.rightChildNode.postShow(); } System.out.println(this); } 示例二 前上图的 3号节点 \u0026ldquo;卢俊\u0026rdquo; , 增加一个左子节点 [5, 关胜]\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 package binaryTree; public class binaryTree { public static void main(String[] args) { treeManager treeManager = new treeManager(new treeNode(1, \u0026#34;宋江\u0026#34;)); treeNode n2 = new treeNode(2, \u0026#34;吴用\u0026#34;); treeNode n3 = new treeNode(3, \u0026#34;卢俊义\u0026#34;); treeNode n4 = new treeNode(4, \u0026#34;林冲\u0026#34;); treeManager.root.leftChildNode = n2; treeManager.root.rightChildNode = n3; n3.rightChildNode = n4; System.out.println(\u0026#34;前序遍历：\u0026#34;); treeManager.preShow(); System.out.println(); System.out.println(\u0026#34;后序遍历：\u0026#34;); treeManager.postShow(); System.out.println(); System.out.println(\u0026#34;中序遍历：\u0026#34;); treeManager.infixShow(); System.out.println(\u0026#34;插入节点后\u0026#34;); treeNode n5 = new treeNode(5, \u0026#34;关胜\u0026#34;); treeManager.add(n5); System.out.println(\u0026#34;前序遍历：\u0026#34;); treeManager.preShow(); System.out.println(); System.out.println(\u0026#34;后序遍历：\u0026#34;); treeManager.postShow(); System.out.println(); System.out.println(\u0026#34;中序遍历：\u0026#34;); treeManager.infixShow(); } } class treeManager { treeNode root; public treeManager(treeNode root) { this.root = root; } //添加节点 public void add(treeNode node) { root.add(node); } public boolean isEmpty() { return this.root == null; } public void preShow() { if (isEmpty()) { System.out.println(\u0026#34;树为空\u0026#34;); return; } root.preShow(); } public void infixShow() { if (isEmpty()) { System.out.println(\u0026#34;树为空\u0026#34;); return; } root.infixShow(); } public void postShow() { if (isEmpty()) { System.out.println(\u0026#34;树为空\u0026#34;); return; } root.postShow(); } } class treeNode { int no; String name; treeNode leftChildNode; treeNode rightChildNode; public treeNode(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \u0026#34;treeNode{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } //前序遍历 public void add(treeNode node) { if (this.no == 3) { this.leftChildNode = node; } if (this.leftChildNode != null) { this.leftChildNode.add(node); } if (this.rightChildNode != null) { this.rightChildNode.add(node); } } public void preShow() { System.out.println(this); if (this.leftChildNode != null) { this.leftChildNode.preShow(); } if (this.rightChildNode != null) { this.rightChildNode.preShow(); } } //中序遍历 public void infixShow() { if (this.leftChildNode != null) { this.leftChildNode.infixShow(); } System.out.println(this); if (this.rightChildNode != null) { this.rightChildNode.infixShow(); } } //后序遍历 public void postShow() { if (this.leftChildNode != null) { this.leftChildNode.postShow(); } if (this.rightChildNode != null) { this.rightChildNode.postShow(); } System.out.println(this); } } 使用前序，中序，后序遍历，请写出各自输出的顺序是什么\n1 2 3 前序遍历：父节点-\u0026gt;左子树-\u0026gt;右子树 1 2 3 5 4 中序遍历：左子树-\u0026gt;父节点-\u0026gt;右子树 2 1 5 3 4 后序遍历：左子树 -\u0026gt; 右子树 -\u0026gt;父节点 2 5 4 3 1 ③ 二叉树查找 前序查找\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public treeNode preSearch(int no) { System.out.println(\u0026#34;进入前序查找\u0026#34;); //如果当前节点满足，则返回当前节点 if (this.no == no) { return this; } treeNode node = null; //再向左子树递归 if (this.leftChildNode != null) { node = this.leftChildNode.preSearch(no); } //先校验左子树递归结果，不然会被右子树的结果给覆盖掉 if (node != null) { return node; } //再向右子树递归 if (this.rightChildNode != null) { node = this.rightChildNode.preSearch(no); } return node; } 中序查找\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public treeNode infixSearch(int no) { treeNode node = null; if (this.leftChildNode != null) { node = this.leftChildNode.infixSearch(no); } //如果已找到提前返回 if (node != null) { return node; } System.out.println(\u0026#34;进入中序查找\u0026#34;); if (this.no == no) { return this; } if (this.rightChildNode != null) { node = this.rightChildNode.infixSearch(no); } return node; } 后序查找\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public treeNode postSearch(int no) { treeNode node = null; if (this.leftChildNode != null) { node = this.leftChildNode.postSearch(no); } //左子节点找到要返回，否则会被右子节点覆盖掉 if (node != null) { return node; } if (this.rightChildNode != null) { node = this.rightChildNode.postSearch(no); } //右子节点不为空，需要提前返回 if (node != null) { return node; } System.out.println(\u0026#34;进入后续查找\u0026#34;); if (this.no == no) { return this; } return null; } ④ 二叉树删除 删除节点及其子树\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public boolean delete(int no) { boolean res = false; //左子节点不为空，且编号相同，删除该节点或其子树 if (this.leftChildNode != null \u0026amp;\u0026amp; this.leftChildNode.no == no) { this.leftChildNode = null; return true; } //右子节点不为空，且编号相同，删除该节点或其子树 if (this.rightChildNode != null \u0026amp;\u0026amp; this.rightChildNode.no == no) { this.rightChildNode = null; return true; } //如果没有找到，继续递归 if (this.leftChildNode != null) { res = this.leftChildNode.delete(no); } if (this.rightChildNode != null) { res = this.rightChildNode.delete(no); } return res; } 只删除节点\n规则：\n如果要删除的节点是非叶子节点，现在我们不希望将该非叶子节点为根节点的子树删除，需要指定规则, 假如规定如下:\n如果该非叶子节点A只有一个子节点B，则子节点B替代节点A\n如果该非叶子节点A有左子节点B和右子节点C，则让左子节点B替代节点A\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 public boolean delete2(int no) { boolean res =false; if (this.leftChildNode!=null\u0026amp;\u0026amp;this.leftChildNode.no==no){ //判断删除节点是否含有子树 if (this.leftChildNode.leftChildNode==null\u0026amp;\u0026amp;this.leftChildNode.rightChildNode==null){ //无子树，删除当前节点 this.leftChildNode=null; } else if (this.leftChildNode.leftChildNode!=null\u0026amp;\u0026amp;this.leftChildNode.rightChildNode==null) { //有左子树，无右子树，左子树代替删除节点 this.leftChildNode=this.leftChildNode.leftChildNode; } else if (this.leftChildNode.leftChildNode == null \u0026amp;\u0026amp; this.leftChildNode.rightChildNode != null) { //无左子树，有右子树，右子树代替删除节点 this.leftChildNode=this.leftChildNode.rightChildNode; }else { //左右子树都有，左子树节点代替删除节点 this.leftChildNode=this.leftChildNode.leftChildNode; } return true; } if (this.rightChildNode!=null\u0026amp;\u0026amp;this.rightChildNode.no==no){ //判断删除节点是否含有子树 if (this.rightChildNode.leftChildNode==null\u0026amp;\u0026amp;this.rightChildNode.rightChildNode==null){ //无子树，删除当前节点 this.rightChildNode=null; } else if (this.rightChildNode.leftChildNode!=null\u0026amp;\u0026amp;this.rightChildNode.rightChildNode==null) { //有左子树，无右子树，左子树代替删除节点 this.rightChildNode=this.rightChildNode.leftChildNode; } else if (this.rightChildNode.leftChildNode == null \u0026amp;\u0026amp; this.rightChildNode.rightChildNode != null) { //无左子树，有右子树，右子树代替删除节点 this.rightChildNode=this.rightChildNode.rightChildNode; }else { //左右子树都有，左子树节点代替删除节点 this.rightChildNode=this.rightChildNode.leftChildNode; } return true; } if (this.leftChildNode!=null){ res=this.leftChildNode.delete(no); } //提前返回结果，避免左子树覆盖 if (res){ return true; } if (this.rightChildNode!=null){ res=this.rightChildNode.delete(no); } return res; } ⑤ 顺序存储二叉树 （跳转的锚点）\n从数据存储来看，数组存储方式和树的存储方式可以相互转换，即数组可以转换成树，树也可以转换成数组，看右面的示意图。也即，将数组通过某种关系可以看成一颗完全二叉树进行遍历，相应地一颗二叉树也可以通过某种关系看成数组进行遍历\n概念：\n顺序二叉树通常只考虑完全二叉树 第n个元素的左子节点为 2 * n + 1 第n个元素的右子节点为 2 * n + 2 第n个元素的父节点为 (n-1) / 2 n : 表示二叉树中的第几个元素(按0开始编号如图所示) 顺序存储二叉树的遍历：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package binaryTree; public class ArrayBinaryTree { public static void main(String[] args) { int[] arr = {1, 2, 3, 4, 5, 6, 7};//将数组看作是二叉树进行遍历，角标与对应二叉树有一个数学关系 managerTree managerTree = new managerTree(arr); System.out.print(\u0026#34;数组转化为二叉树\\n前序遍历：\u0026#34;); managerTree.preShow(0); System.out.println(); System.out.print(\u0026#34;中序遍历：\u0026#34;); managerTree.infixShow(0); System.out.println(); System.out.print(\u0026#34;后序遍历：\u0026#34;); managerTree.postShow(0); } } class managerTree { int[] arr; public managerTree(int[] arr) { this.arr = arr; } //将数组当成二叉树进行前序遍历 public void preShow(int index) { if (arr == null || arr.length == 0) { System.out.println(\u0026#34;二叉树为空\u0026#34;); return; } System.out.print(arr[index] + \u0026#34; \u0026#34;); if (index * 2 + 1 \u0026lt; arr.length) { preShow(index * 2 + 1); } if (index * 2 + 2 \u0026lt; arr.length) { preShow(index * 2 + 2); } } //将数组当成二叉树进行中序遍历 public void infixShow(int index) { if (arr == null || arr.length == 0) { System.out.println(\u0026#34;二叉树为空\u0026#34;); return; } if (index * 2 + 1 \u0026lt; arr.length) { infixShow(index * 2 + 1); } System.out.print(arr[index] + \u0026#34; \u0026#34;); if (index * 2 + 2 \u0026lt; arr.length) { infixShow(index * 2 + 2); } } //将数组当成二叉树进行后序遍历 public void postShow(int index) { if (arr == null || arr.length == 0) { System.out.println(\u0026#34;二叉树为空\u0026#34;); return; } if (index * 2 + 1 \u0026lt; arr.length) { postShow(index * 2 + 1); } if (index * 2 + 2 \u0026lt; arr.length) { postShow(index * 2 + 2); } System.out.print(arr[index] + \u0026#34; \u0026#34;); } } ⑥ 线索化二叉树 线索化概述：\n对于一个二叉树来说，其二叉链表表示形式中正好有两个指针域，一个左子树指针域，一个右子树指针域。并且对于一个有n个节点的二叉链表， 每个节点有指向左右孩子的两个指针域，所以共是 2n 个指针域。而 n 个节点的二叉树一共有n-1条分支数，也就是说，其实是存在2n-(n-1) = n+l个空指针域。将这些空指针利用起来，其中一个指针用作前驱指针，另一个指针指向后继指针，那么这样的话，这颗二叉树的遍历就不需要使用递归来进行，因为线索化的二叉树其实是一个双向链表（有序链表），也利用了那些空闲的空指针资源。\n前驱指针（节点）：按照二叉树的某种序列遍历方式（前序、中序、后序等）得到的有序结果，某个节点的左子树为空就按这个遍历顺序指向当前节点的前一个结点，即前驱节点\n后驱指针（节点）：按照二叉树的某种序列遍历方式（前序、中序、后序等）得到的有序结果，某个节点的右子树为空就按这个遍历顺序指向当前节点的后一个结点，即后继节点\n以上两个属性通过节点的leftTag和rightTag区分\n前序线索化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void preThreadOrder(treeNode1 node) { if (node == null) return; if (node.leftChildNode == null) { node.leftChildNode = this.preNode; node.leftTag = 1; } //右子树为空,指向后继节点:将前一个节点记录下来,当前节点node是preNode的后继节点,因此将后继节点指向preNode的空右子节点 if (this.preNode != null \u0026amp;\u0026amp; this.preNode.rightChildNode == null) { this.preNode.rightChildNode = node; this.preNode.rightTag = 1; } this.preNode = node;//记录下一个节点 if (node.leftTag == 0) preThreadOrder(node.leftChildNode);//防止陷入无限循环 if (node.rightTag == 0) preThreadOrder(node.rightChildNode); } 遍历前序线索化二叉树\n由于二叉树已经线索化，原来的二叉树已经发生改变，不能再使用简单的前序遍历来遍历，采用新的方式（类似于遍历双向链表）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public void preThreadList(treeNode1 root) { treeNode1 node = root; if (node == null) return; while (node != null) { //一直沿左子树遍历，直到遇到线索 while (node.leftTag == 0) { System.out.println(node); node = node.leftChildNode; } System.out.println(node); //此时已遍历一颗子树的根节点和左子树 //再遍历其后继节点 while (node.rightTag == 1) { node = node.rightChildNode; System.out.println(node); } //遇到当前节点无线索时，表示此时的节点有左右子树，左边已经遍历，该遍历右子树 node = node.rightChildNode; } } 中序线索化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public void infixThreadOrder(treeNode1 node) { //递归结束 if (node == null) return; //递归左子树 infixThreadOrder(node.leftChildNode); //将当前节点进行线索化 //当左子树为空,指向前驱节点 if (node.leftChildNode == null) { node.leftChildNode = this.preNode; node.leftTag = 1; } //右子树为空,指向后继节点:将前一个节点记录下来,当前节点node是preNode的后继节点,因此将后继节点指向preNode的空右子节点 if (this.preNode != null \u0026amp;\u0026amp; this.preNode.rightChildNode == null) { this.preNode.rightChildNode = node; this.preNode.rightTag = 1; } this.preNode = node; //递归右子树 infixThreadOrder(node.rightChildNode); } 遍历中序线索化二叉树\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public void infixThreadList(treeNode1 root) { treeNode1 node = root; while (node != null) { //找到遍历开始节点,中序遍历是最后一个左子节点 while (node.leftTag == 0) node = node.leftChildNode; //输出当前节点 System.out.println(node); //不断找寻后继节点 while (node.rightTag == 1) { node = node.rightChildNode;//获取后继节点 System.out.println(node); } //替换这个遍历的节点为前驱 node = node.rightChildNode; } } 后序线索化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public void postThreadOrder(treeNode1 node) { if (node == null) return; if (node.leftTag == 0) postThreadOrder(node.leftChildNode); if (node.rightTag == 0) postThreadOrder(node.rightChildNode); if (node.leftChildNode == null) { node.leftChildNode = this.preNode; node.leftTag = 1; } if (this.preNode != null \u0026amp;\u0026amp; this.preNode.rightChildNode == null) { this.preNode.rightChildNode = node; this.preNode.rightTag = 1; } this.preNode = node; } 遍历后序线索化二叉树\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public void postThreadList(treeNode1 root) { treeNode1 node = root; if (node == null) return; //遍历找到最左子节点，从此处开始遍历 while (node != null \u0026amp;\u0026amp; node.leftTag == 0) node = node.leftChildNode; //暂存上一个节点 treeNode1 preNode = null; while (node != null) { //找到后序遍历输出的头节点时，不断输出后继节点 while (node.rightTag == 1) { System.out.println(node); preNode = node;//记录上一个遍历的节点 node = node.rightChildNode; } //如果上一个处理的节点当前节点的右子节点，则说明左右子树处理完毕，继续处理父节点 while (node.rightChildNode == preNode) { System.out.println(node); if (node.parentNode == null) return; preNode = node; node = node.parentNode;//需要记录父节点 } //如果上一个处理的节点是当前节点的左子节点 if (node.leftChildNode == preNode) { //找到右子树的最左子节点 node = node.rightChildNode; while (node != null \u0026amp;\u0026amp; node.leftTag == 0) node = node.leftChildNode; } } } 前序、中序、后序线索化比较\n前序线索化二叉树遍历相对最容易理解，实现起来也比较简单。由于前序遍历的顺序是：根左右，所以从根节点开始，沿着左子树进行处理，当子节点的left指针类型是线索时，说明到了最左子节点，然后处理子节点的right指针指向的节点，可能是右子树，也可能是后继节点，无论是哪种类型继续按照上面的方式（先沿着左子树处理，找到子树的最左子节点，然后处理right指针指向），以此类推，直到节点的right指针为空，说明是最后一个，遍历完成。 中序线索化二叉树的网上相关介绍最多。中序遍历的顺序是：左根右，因此第一个节点一定是最左子节点，先找到最左子节点，依次沿着right指针指向进行处理（无论是指向子节点还是指向后继节点），直到节点的right指针为空，说明是最后一个，遍历完成。 后序遍历线索化二叉树最为复杂，通用的二叉树数节点存储结构不能够满足后序线索化，因此我们扩展了节点的数据结构，增加了父节点的指针。后序的遍历顺序是：左右根，先找到最左子节点，沿着right后继指针处理，当right不是后继指针时，并且上一个处理节点是当前节点的右节点，则处理当前节点的右子树，遍历终止条件是：当前节点是root节点，并且上一个处理的节点是root的right节点。 2. 树结构的实际应用 ① 堆排序 如右：锚点连接\n② 赫夫曼树 给定n个权值作为n个叶子结点，构造一棵二叉树，若该树的带权路径长度(wpl)达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree), 还有的书翻译为霍夫曼树。赫夫曼树是带权路径长度最短的树，权值较大的结点离根较近。\n路径和路径的长度：在一棵树中，从一个结点往下可以达到的孩子或孙子结点之间的通路，称为路径。通路中分支节点的数目称为路径长度。若规定根结点的层数为1，则从根结点到第L层结点的路径长度为L-1\n结点的权及带权路径长度：若将树中结点赋给一个有着某种含义的数值，则这个数值称为该结点的权。结点的带权路径长度为：从根结点到该结点之间的路径长度与该结点的权的乘积\n树的带权路径长度：树的带权路径长度规定为所有叶子结点的带权路径长度之和，记为WPL(weighted path length) ,权值越大的结点离根结点越近的二叉树才是最优二叉树。WPL最小的就是赫夫曼树\n构成赫夫曼树的步骤：\n从小到大进行排序, 将每一个数据，每个数据都是一个节点，每个节点可以看成是一颗最简单的二叉树\n取出根节点权值最小的两颗二叉树\n组成一颗新的二叉树, 该新的二叉树的根节点的权值是前面两颗二叉树根节点权值的和\n再将这颗新的二叉树，以根节点的权值大小 再次排序,不断重复1-2-3-4的步骤，直到数列中，所有的数据都被处理，就得到一颗赫夫曼树\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package binaryTree; import java.util.ArrayList; import java.util.Collections; /* * 赫夫曼树创建 * */ public class huffmanTree { public static void main(String[] args) { int[] arr = {13, 7, 8, 3, 29, 6, 1}; node tree = createHuffmanTree(arr); tree.preShow(tree); } public static node createHuffmanTree(int[] arr) { //创建节点,并塞入集合之中 ArrayList\u0026lt;node\u0026gt; nodes = new ArrayList\u0026lt;\u0026gt;(); for (int val : arr) { nodes.add(new node(val)); } while (nodes.size() \u0026gt; 1) { //排序 Collections.sort(nodes); //去除权值最小的两个node组成新的二叉树 node left = nodes.get(0); node right = nodes.get(1); node parent = new node(left.val + right.val); parent.left = left; parent.right = right; nodes.remove(0); nodes.remove(0);//这里的索引应该还是0，前面已经移除了一个元素 //将新的节点加进去 nodes.add(parent); } return nodes.get(0); } } class node implements Comparable\u0026lt;node\u0026gt; { int val;//哈夫曼树的权值 node left; node right; public node(int val) { this.val = val; } @Override public String toString() { return \u0026#34;node{\u0026#34; + \u0026#34;val=\u0026#34; + val + \u0026#39;}\u0026#39;; } @Override public int compareTo(node o) { return this.val - o.val; } public void preShow(node root) { node n = root; if (n == null) return; System.out.println(n); //左右子节点分别递归 if (n.left != null) preShow(n.left); if (n.right != null) preShow(n.right); } } ③ 赫夫曼编码 赫夫曼编码也翻译为哈夫曼编码(Huffman Coding)，又称霍夫曼编码，是一种编码方式, 属于一种程序算法。赫夫曼编码是赫哈夫曼树在电讯通信中的经典的应用之一。赫夫曼编码广泛地用于数据文件压缩。其压缩率通常在20%～90%之间赫夫曼码是可变字长编码(VLC)的一种。Huffman于1952年提出一种编码方法，称之为最佳编码\n变长编码（❌）\n1 2 3 4 i like like like java do you like a java// 共40个字符(包括空格) d:1 y:1 u:1 j:2 v:2 o:2 l:4 k:4 e:4 i:5 a:5 :9 // 各个字符对应的个数，9对应的是空格 0= , 1=a, 10=i, 11=e, 100=k, 101=l, 110=o, 111=v, 1000=j, 1001=u, 1010=y, 1011=d 说明：按照各个字符出现的次数进行编码，原则是出现次数越多的，则编码越小，比如 空格出现了9次，编码为0，其它依次类推。按照上面给各个字符规定的编码，则我们在传输\u0026quot;i like like like java do you like a java\u0026quot;数据时，编码就是10010110100...\n字符的编码都不能是其他字符编码的前缀，符合此要求的编码叫做前缀编码， 即不能匹配到重复的编码。意思是：当计算机在扫描编码数据时，首先得到1，匹配a字符，但是后面还有个0,可以匹配空格，也可以10匹配i字符，这样的编码方式就导致某个字符对应的编码是另一个字符对应编码的前缀（即不符合前缀编码），这样就存在不好解码的问题\n赫夫曼编码\n赫夫曼编码原理\n1 2 3 4 i like like like java do you like a java // 共40个字符(包括空格) d:1 y:1 u:1 j:2 v:2 o:2 l:4 k:4 e:4 i:5 a:5 :9 各个字符对应的个数 按照上面字符出现的次数构建一颗赫夫曼树, 次数作为权值(如图)：\n根据赫夫曼树，给各个字符规定编码，向左的路径为0，向右的路径为1。（一定是前缀编码）编码如下：\n1 o: 1000 u: 10010 d: 100110 y: 100111 i: 101a : 110 k: 1110 e: 1111 j: 0000 v: 0001l: 001 : 01//空格 按照上面的赫夫曼编码，我们的i like like like java do you like a java字符串对应的编码为 (注意这里我们使用的无损压缩)：\n1 1010100110111101111010011011110111101001101111011110100001100001110011001111000011001111000100100100110111101111011100100001100001110 长度为：133\n说明:原来长度是359，压缩了(359-133)/359 = 62.9%。此编码满足前缀编码, 即字符的编码都不能是其他字符编码的前缀，不会造成匹配的多义性\n注意：这个赫夫曼树根据排序方法不同，也可能不太一样，这样对应的赫夫曼编码也不完全一样，但是wpl是一样的，都是最小的, 比如：如果我们让每次生成的新的二叉树总是排在权值相同的二叉树的最后一个，则生成的二叉树为：\n最佳实践\u0026ndash;数据压缩与解压\n将给出的一段文本，比如 \u0026ldquo;i like like like java do you like a java\u0026rdquo; ， 根据前面的讲的赫夫曼编码原理，对其进行数据压缩处理 ，形式如下\n1 1010100110111101111010011011110111101001101111011110100001100001110011001111000011001111000100100100110111101111011100100001100001110 步骤1：根据赫夫曼编码压缩数据的原理，需要创建 \u0026ldquo;i like like like java do you like a java\u0026rdquo; 对应的赫夫曼树\n步骤2：生成赫夫曼树对应的赫夫曼编码表，如下表\n1 :=01 a=100 d=11000 u=11001 e=1110 v=11011 i=101 y=11010 j=0010 k=1111 l=000 o=0011 步骤3：使用赫夫曼编码来生成赫夫曼编码数据 ,即按照上面的赫夫曼编码表，将\u0026quot;i like like like java do you like a java\u0026quot;字符串生成对应的编码数据, 形式如下（目前只是编码好了，还未压缩）\n1 1010100010111111110010001011111111001000101111111100100101001101110001110000011011101000111100101000101111111100110001001010011011100 步骤4：将编码好的数据进行压缩，每8位二进制字符串为组成一个十进制或16进制数据，这样可以减少二进制字符串的长度，从而达到压缩的目的，值得注意的是最后8位的特殊处理，因为可能赫夫曼编码过后不满足8位，此时就需要记录这个有多少位。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 package binaryTree; import java.util.*; public class huffmanCode { static Map\u0026lt;Byte, String\u0026gt; CodeTable = new HashMap\u0026lt;\u0026gt;();//哈夫曼编码表 static int LastLength = -1;//记录最后一个数压缩的长度，方便解压正确 public static void main(String[] args) { String text = \u0026#34;i like like like java do you like a java -asasda 1 0 -1\u0026#34;; System.out.println(\u0026#34;字符串长度：\u0026#34; + text.length()); //转化为字符数组 byte[] chs = text.getBytes(); System.out.println(Arrays.toString(huffmanZip(chs))); System.out.println(new String(decode(CodeTable, huffmanZip(chs)))); } public static byte[] decode(Map\u0026lt;Byte, String\u0026gt; huffmanCodeTable, byte[] huffmanBytes) { //将压缩数据转化为二进制字符串，即解压 StringBuilder builder = new StringBuilder(); boolean flag = false; for (int i = 0; i \u0026lt; huffmanBytes.length; i++) { if (i == huffmanBytes.length - 1) flag = true; String bitString = bytesToBitString(flag, huffmanBytes[i]); builder.append(bitString); System.out.println(\u0026#34;结果：\u0026#34; + huffmanBytes[i] + \u0026#34; \u0026#34; + bitString); } System.out.println(builder); //根据哈夫曼编码表，将二进制字符串还原成源字符串，即编码 //调换键值顺序 Map\u0026lt;String, Byte\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (Map.Entry\u0026lt;Byte, String\u0026gt; entry : huffmanCodeTable.entrySet()) { map.put(entry.getValue(), entry.getKey()); } //集合存储byte List\u0026lt;Byte\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; builder.length(); ) { int count = 1;//计数指针 boolean f = true; Byte b = null; while (f) { String key = builder.substring(i, i + count); b = map.get(key); if (b == null) { count++; } else { f = false; } } list.add(b); i += count; } byte[] bytes = new byte[list.size()]; for (int i = 0; i \u0026lt; bytes.length; i++) { bytes[i] = list.get(i); } return bytes; } //将一个byte转化为有个二进制字符串，flag标志最后一位 public static String bytesToBitString(boolean flag, byte b) { StringBuilder prefix = new StringBuilder();//用于正数前缀补零 String str = Integer.toBinaryString(b); int len = str.length(); if (flag) {//如果是最后一位，可能会少于8位，前面记录在LastLength全局变量里 //如果是正数，需要补高位，补足LastLength位 if (b \u0026gt;= 0) { while (len \u0026lt; LastLength) { prefix.append(\u0026#39;0\u0026#39;); len++; } return prefix + str; } else {//如果是负数，就需要只保留后8位 return str.substring(str.length() - 8); } } else {//不是最后一位，一定是满8位 //如果是正数，需要补高位补足8位 if (b \u0026gt;= 0) { while (len \u0026lt; 8) { prefix.append(\u0026#39;0\u0026#39;); len++; } return prefix + str; } else {//如果是负数，就需要只保留后8位 return str.substring(str.length() - 8); } } } //将原始字符串的bytes数据编码并压缩，返回压缩结果 public static byte[] huffmanZip(byte[] bytes) { //根据原始数据生成哈夫曼编码树 List\u0026lt;node1\u0026gt; nodes = getNodes(bytes); node1 root = createHuffmanTree(nodes); //根据生成的哈夫曼编码树生成编码表 StringBuilder result = new StringBuilder(); getHuffmanCode(root, \u0026#34;\u0026#34;, result); //根据哈夫曼编码表和原始bytes数组数据，进行压缩 return zip(bytes, CodeTable); } //根据赫夫曼编码表，将字符串的byte数组，返回一个赫夫曼编码，压缩后的byte数组 public static byte[] zip(byte[] oldBytes, Map\u0026lt;Byte, String\u0026gt; codeTable) { StringBuilder builder = new StringBuilder(); //根据哈夫曼编码表，将byte数组转化为编码数据（变长了） //这个编码数据还不是最后的压缩结果，根据显示，显然这个数据的长度还变长了，并没有压缩 for (byte b : oldBytes) { builder.append(codeTable.get(b)); } System.out.println(builder); //将编码的字符串数据进行压缩，每8位一个字节数据,不足8位的往前补一位 int len; int index = 0; if (builder.length() % 8 == 0) len = builder.length() / 8; else len = builder.length() / 8 + 1; //创建压缩后的byte数组 byte[] compressedBytes = new byte[len]; for (int i = 0; i \u0026lt; builder.length(); i += 8) { String strByte; if (i + 8 \u0026gt; builder.length()) {//将最后剩余的字节加进去（可能少于8个） strByte = builder.substring(i); //todo 为方便解码最后一位数字，需要记录最后一位byte数字的二进制长度 LastLength = strByte.length(); } else { strByte = builder.substring(i, i + 8); } //每8个二进制位压缩成一个数字： compressedBytes[index++] = (byte) Integer.parseInt(strByte, 2); } return compressedBytes; } //得到所有叶子节点的哈夫曼编码表 public static void getHuffmanCode(node1 node, String code, StringBuilder result) { StringBuilder code1 = new StringBuilder(result); code1.append(code); if (node != null) { if (node.data == 0) {//非叶子节点 getHuffmanCode(node.left, \u0026#34;0\u0026#34;, code1);//左子节点递归 getHuffmanCode(node.right, \u0026#34;1\u0026#34;, code1);//右子节点递归 } else {//叶子节点 CodeTable.put(node.data, code1.toString()); } } } //创建哈夫曼树，返回父节点 public static node1 createHuffmanTree(List\u0026lt;node1\u0026gt; nodes) { while (nodes.size() \u0026gt; 1) { //排序 Collections.sort(nodes); //取出首两个节点 node1 left = nodes.get(0); node1 right = nodes.get(1); node1 parent = new node1(left.weight + right.weight); //组成二叉树 parent.left = left; parent.right = right; //移除首两个节点 nodes.remove(0); nodes.remove(0); nodes.add(parent); } return nodes.get(0); } //统计每个字符出现的次数（权重），然后记录在链表里 public static List\u0026lt;node1\u0026gt; getNodes(byte[] chs) { ArrayList\u0026lt;node1\u0026gt; nodes = new ArrayList\u0026lt;\u0026gt;(); HashMap\u0026lt;Byte, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (byte ch : chs) { Integer weight = map.get(ch); if (weight == null) { map.put(ch, 1); } else { map.put(ch, weight + 1); } } //将map里的值转化为node1并化为链表 //遍历map for (Map.Entry\u0026lt;Byte, Integer\u0026gt; entry : map.entrySet()) { nodes.add(new node1(entry.getKey(), entry.getValue())); } return nodes; } } class node1 implements Comparable\u0026lt;node1\u0026gt; { byte data;//存放的数据 int weight;//权值 node1 left; node1 right; //前序遍历 public void preShow(node1 root) { node1 temp = root; if (temp == null) return; System.out.println(temp); if (temp.left != null) preShow(temp.left); if (temp.right != null) preShow(temp.right); } @Override public String toString() { //为空的权值不输出 if (data == 0) return \u0026#34;node1{weight=\u0026#34; + weight + \u0026#39;}\u0026#39;; return \u0026#34;node1{\u0026#34; + \u0026#34;data=\u0026#34; + data + \u0026#34;, weight=\u0026#34; + weight + \u0026#39;}\u0026#39;; } public node1(int weight) { this.weight = weight; } public node1(byte data, int weight) { this.data = data; this.weight = weight; } @Override public int compareTo(node1 o) { return this.weight - o.weight; } } ④ 二叉排序树或搜索树（BST） 与数组、链表比较分析\n数组：未排序数组，可以直接在数组尾添加，速度快，但是，查找速度慢；已排序数组，可以使用二分查找、插值查找、斐波那契查找等查找算法，查找速度快，但是为了保证数组有序，在添加新数据时，找到插入位置后，后面的数据需要整体移动，速度慢\n链表：不管链表是否有序，查找熟读都慢，添加数据的速度比素组快，不需要数据整体移动\n二叉排序树：二叉排序树集中前两者优点，不仅查找速度快，插入数据也快\n原理\n二叉排序树：BST，(Binary Sort(Search) Tree)，也叫二叉搜索树, 对于二叉排序树的任何一个非叶子节点，要求左子节点的值比当前节点的值小，右子节点的值比当前节点的值大。特别说明：如果有相同的值，可以将该节点放在左子节点或右子节点。比如针对前面的数据 (7, 3, 10, 12, 5, 1, 9) ，对应的二叉排序树为：\n二叉排序树的性质：\n就是若它的左子树不空，则左子树上所有节点的值均小于它的根节点的值\n若它的右子树不空，则右子树上所有节点的值均大于其根节点的值\n换句话说就是：任何节点的键值一定大于其左子树中的每一个节点的键值，并小于其右子树中的每一个节点的键值\n创建与遍历\n二叉排序树添加节点时，需要满足以上性质：任何节点的键值一定大于其左子树中的每一个节点的键值，并小于其右子树中的每一个节点的键值。这样，不仅中序遍历得到的结果是升序，而且查找和插入时也很方便。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //添加二叉排序树的节点 public void add(node2 node) { if (node == null) return; if (this.val \u0026gt; node.val) {//添加节点比当前节点小，向左子树处理 if (this.left == null) this.left = node; //向左子树递归 else this.left.add(node); } else {//添加节点比当前节点大，向右子树处理 if (this.right == null) this.right = node; //向右子树递归 else this.right.add(node); } } //中序遍历 public void infixShow() { if (this.left != null) { this.left.infixShow(); } System.out.println(this); if (this.right != null) { this.right.infixShow(); } } 删除\n删除情况比较复杂：\n删除叶子节点 (比如：2, 5, 9, 12)：直接删除叶子节点即可 删除只有一颗子树的节点 (比如：1)： 删除有两颗子树的节点. (比如：7, 3，10)：左右子树均不为空，按中序遍历顺序，将删除节点的前驱节点移至删除节点并代替或者后继节点移至删除节点代替，也就是说：将前驱节点\u0026ndash;》删除节点的左子树里最大的节点（左子树里最右子树节点）；后继节点\u0026ndash;》删除节点右子树里最小的节点（右子树里最左子树节点） 综上代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 //class node2: public node2 search(int val) { if (this.val == val) { return this; } else if (this.val \u0026gt; val) { if (this.left == null) return null; return this.left.search(val); } else { if (this.right == null) return null; return this.right.search(val); } } //查找父节点 public node2 searchParent(int val) { if ((this.left != null \u0026amp;\u0026amp; this.left.val == val) || (this.right != null \u0026amp;\u0026amp; this.right.val == val)) { return this; } else { if (this.val \u0026gt; val \u0026amp;\u0026amp; this.left != null) {//当前节点的值小于查找节点，往左子树递归 return this.left.searchParent(val); } else if (this.val \u0026lt;= val \u0026amp;\u0026amp; this.right != null) {//当前节点的值大于查找节点，往右子树递归(注意相同情况) return this.right.searchParent(val); } else return null; } } //class BinarySortTreeManager: //删除节点 public void delNode(int value) { if (root == null) { return; } //找到删除节点 node2 targetNode = root.search(value); if (targetNode == null) { System.out.println(\u0026#34;没有找到该节点\u0026#34;); return; } //找到删除节点分父节点 node2 parent = root.searchParent(value); if (targetNode.left == null \u0026amp;\u0026amp; targetNode.right == null) {// 如果要删除的节点是叶子节点 if (parent==null){//判断父节点是否为空 root=null; return; } if (parent.left != null \u0026amp;\u0026amp; parent.left.val == value) { parent.left = null; } else if (parent.right != null \u0026amp;\u0026amp; parent.right.val == value) { parent.right = null; } } else if (targetNode.left != null \u0026amp;\u0026amp; targetNode.right != null) { //左右子树均不为空，按中序遍历顺序，将删除节点的前驱节点移至删除节点并代替或者后继节点移至删除节点代替， //也就是说：将前驱节点--》删除节点的左子树里最大的节点；后继节点--》删除节点右子树里最小的节点（右子树里最左子树节点） node2 temp = targetNode.right; node2 tempParent = targetNode; while (temp.left != null) {//找到最左子节点，也就是右子树最小的节点，即后继节点 tempParent = temp; temp = temp.left; } //将后继节点的值覆盖到删除节点 targetNode.val = temp.val; //再将原来的后继节点删除（注意：需要防止最小节点是右子树根节点的情况） if (tempParent == targetNode) { tempParent.right = null; } else { tempParent.left = null; } } else {//如果删除的节点有一个子节点 if (targetNode.left != null) { if (parent==null){ root=targetNode.left; return; } if (parent.left == targetNode) parent.left = targetNode.left; else if (parent.right == targetNode) parent.right = targetNode.left; } else { if (parent==null){ root=targetNode.right; return; } if (parent.left == targetNode) parent.left = targetNode.right; else if (parent.right == targetNode) parent.right = targetNode.right; } } } ⑤ 平衡二叉树（AVL） 给你一个数列{1,2,3,4,5,6}，要求创建一颗二叉排序树(BST), 并分析问题所在：\n上图BST存在的问题分析：左子树全部为空，从形式上看，更像一个单链表。插入速度没有影响查询速度明显降低(因为需要依次比较), 不能发挥BST的优势，因为每次还需要比较左子树，其查询速度比单链表还慢\n解决方案：平衡二叉树(AVL)，对二叉排序树的优化\n原理\n平衡二叉树，也叫平衡二叉搜索树（Self-balancing binary search tree），又被称为AVL树， 可以保证查询效率较高。\n具有以下特点：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。\n平衡二叉树的常用实现方法有红黑树、AVL、替罪羊树、Treap、伸展树等。\n处理方法\n注：失衡根节点，指的是左右子树高度差大于1的节点\n情形一：节点添加在失衡根节点的右子节点的右子节点(RR)\u0026ndash;左旋转\n情形二：节点添加在失衡根节点的左子节点的左子节点(LL)\u0026ndash;右旋转\n情形三：节点添加在失衡根节点的左子节点的右子节点(LR)\u0026ndash;先以左子节点为根节点进行左旋转（局部调整平衡），再以当前节点进行右旋转\n情形四：节点添加在失衡根节点的右子节点的左子节点(RL)\u0026ndash;先以右子节点为根节点进行右旋转（局部调整平衡），再以当前节点进行左旋转\n类似上图的效果\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 //class node3: //左旋转 public void leftRotate() { //创建当前根节点的副本 node3 node = new node3(this.val); //将副本左子节点指向当前节点的左子节点 node.left = this.left; //副本右子节点指向当前节点的右子节点的左子节点 node.right = this.right.left; //将当前节点的值换成右子节点的值 this.val = this.right.val; //将新的当前节点的左子节点指向副本节点 this.left = node; //将新的当前节点的右子树的右子树连接 this.right = this.right.right; } //右旋转 public void rightRotate() { node3 node = new node3(this.val); node.left = this.left.right; node.right = this.right; this.val = left.val; this.right = node; this.left = this.left.left; } //当前节点左子树高度 public int leftHeight() { if (this.left == null) return 0; return this.left.height(); } //当前节点右子树高度 public int rightHeight() { if (this.right == null) return 0; return this.right.height(); } //返回当前节点为跟节点的树的高度 public int height() {//递归查找，每递归一次树的深度+1 return Math.max((this.left == null ? 0 : this.left.height()), this.right == null ? 0 : this.right.height()) + 1; } //添加二叉排序树的节点 public void add(node3 node) { if (node == null) return; if (this.val \u0026gt; node.val) {//添加节点比当前节点小，向左子树处理 if (this.left == null) this.left = node; //向左子树递归 else this.left.add(node); } else {//添加节点比当前节点大，向右子树处理 if (this.right == null) this.right = node; //向右子树递归 else this.right.add(node); } //节点添加完毕，需要判断平衡因子 if (this.rightHeight() - this.leftHeight() \u0026gt; 1) {//右子树比左子树高，触发左旋转 //如果右子节点的左子树高度大于右子节点的右子树高度，先局部右旋转：当前节点的右子节点 if (this.right != null \u0026amp;\u0026amp; this.right.leftHeight() \u0026gt; this.right.rightHeight()) { this.right.rightRotate(); } this.leftRotate(); } else if (this.leftHeight() - this.rightHeight() \u0026gt; 1) {//左子树比右子树高，触发右旋转 //如果左子节点的右子树高度大于左子节点的左子树高度，先局部左旋转：当前节点的左子节点 if (this.left != null \u0026amp;\u0026amp; this.left.rightHeight() \u0026gt; this.left.leftHeight()) { this.left.leftRotate(); } this.rightRotate(); } } 3. B树 B-tree树即B树，B即Balanced，平衡的意思。有人把B-tree翻译成B-树，容易让人产生误解。会以为B-树是一种树，而B树又是另一种树。实际上，B-tree就是指的B树。\n① 二叉树的问题分析 二叉树的操作效率较高，但是也存在问题, 请看下面的二叉树\n二叉树需要加载到内存的，如果二叉树的节点少，没有什么问题，但是如果二叉树的节点很多(比如1亿)，就存在如下问题:\n问题1：在构建二叉树时，需要多次进行i/o操作(海量数据存在数据库或文件中)，节点海量，构建二叉树时，速度有影响\n问题2：节点海量，也会造成二叉树的高度很大，会降低操作速度\n② 多叉树 在二叉树中，每个节点有数据项，最多有两个子节点。如果允许每个节点可以有更多的数据项和更多的子节点，就是多叉树（multiway tree）\n后面我们讲解的2-3树，2-3-4树就是多叉树，多叉树通过重新组织节点，减少树的高度，能对二叉树进行优化。\n举例说明(下面2-3树就是一颗多叉树)\n③ B树 B树通过重新组织节点，降低树的高度，并且减少i/o读写次数来提升效率。\n如图B树通过重新组织节点，降低了树的高度。\n文件系统及数据库系统的设计者利用了磁盘预读原理，将一个节点的大小设为等于一个页(页得大小通常为4k)，这样每个节点只需要一次I/O就可以完全载入\n将树的度M设置为1024，在600亿个元素中最多只需要4次I/O操作就可以读取到想要的元素, B树(B+)广泛应用于文件存储系统以及数据库系统中\nB树的说明：\nB树的阶：节点的最多子节点个数。比如2-3树的阶是3，2-3-4树的阶是4\nB树的搜索：从根结点开始，对结点内的关键字（有序）序列进行二分查找，如果命中则结束，否则进入查询关键字所属范围的儿子结点；重复，直到所对应的儿子指针为空，或已经是叶子结点\n关键字集合分布在整颗树中, 即叶子节点和非叶子节点都存放数据\n搜索有可能在非叶子结点结束\n其搜索性能等价于在关键字全集内做一次二分查找\n2-3树基本介绍\n2-3树是最简单的B树结构, 具有如下特点：\n2-3树的所有叶子节点都在同一层(只要是B树都满足这个条件) 有两个子节点的节点叫二节点，二节点要么没有子节点，要么有两个子节点 有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点 2-3树是由二节点和三节点构成的树。 2-3树应用案例\n将数列{16, 24, 12, 32, 14, 26, 34, 10, 8, 28, 38, 20}构建成2-3树，并保证数据插入的大小顺序。\n2-3树插入规则:\n2-3树的所有叶子节点都在同一层(只要是B树都满足这个条件) 有两个子节点的节点叫二节点，二节点要么没有子节点，要么有两个子节点 有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点 当按照规则插入一个数到某个节点时，不能满足上面三个要求，就需要拆，先向上拆，如果上层满，则拆本层，拆后仍然需要满足上面3个条件。 对于三节点的子树的值大小仍然遵守(BST二叉排序树)的规则 除了23树，还有234树等，概念和23树类似，也是一种B树。\n⑤ B+树 前面已经介绍了2-3树和2-3-4树，他们就是B树(英语：B-tree 也写成B-树)，这里我们再做一个说明，我们在学习Mysql时，经常听到说某种类型的索引是基于B树或者B+树的，如图:\nB+树的说明:\nB+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找 所有关键字都出现在叶子结点的链表中（即数据只能在叶子节点【也叫稠密索引】），且链表中的关键字(数据)恰好是有序的。 不可能在非叶子结点命中 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层 更适合文件索引系统 B树和B+树各有自己的应用场景，不能说B+树完全比B树好，反之亦然. ⑥ B*树 B*树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针。\nB树的说明:\nB树定义了非叶子结点关键字个数至少为(2/3)*M，即块的最低使用率为2/3，而B+树的块的最低使用率为B+树的1/2。\n从第1个特点我们可以看出，B*树分配新结点的概率比B+树要低，空间使用率更高\n八、图 1. 图的基本介绍 为什么要有图\n线性表局限于一个直接前驱和一个直接后继的关系\n树也只能有一个直接前驱也就是父节点\n当我们需要表示多对多的关系时， 这里我们就用到了图\n图的举例说明\n图是一种数据结构，其中结点可以具有零个或多个相邻元素。两个结点之间的连接称为边。 结点也可以称为顶点。如图：\n图的常用概念\n图的表示方式\n图的表示方式有两种：二维数组表示（邻接矩阵）；链表表示（邻接表）。\n邻接矩阵\n邻接矩阵是表示图形中顶点之间相邻关系的矩阵，对于n个顶点的图而言，矩阵是的row和col表示的是1~n个点。1表示直接连接，0表示不能直接连接。\n邻接表\n邻接矩阵需要为每个顶点都分配n个边的空间，其实有很多边都是不存在,会造成空间的一定损失。邻接表的实现只关心存在的边，不关心不存在的边。因此没有空间浪费，邻接表由数组+链表组成。\n创建图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 package graph; import java.util.ArrayList; import java.util.Arrays; /* * 图 * */ public class graph { private ArrayList\u0026lt;String\u0026gt; vertexList;//存储顶点集合 private int[][] edges;//表示图对应的邻接矩阵 private int numOfEdge;//表示边的数目 public static void main(String[] args) { int n = 5; String[] vertexes = {\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;}; graph g = new graph(n); //添加顶点 for (String v : vertexes) { g.insertVertex(v); } //添加边 g.insertEdge(0, 1, 1); g.insertEdge(0, 2, 1); g.insertEdge(1, 2, 1); g.insertEdge(1, 3, 1); g.insertEdge(1, 4, 1); g.show(); } public graph(int n) { this.edges = new int[n][n]; this.numOfEdge = 0; this.vertexList = new ArrayList\u0026lt;\u0026gt;(n); } //返回节点的个数 public int getNumOfVertex() { return this.vertexList.size(); } //返回边的个数 public int getNumOfEdge() { return this.numOfEdge; } //返回节点对应的数据 public String getValueByIndex(int index) { return this.vertexList.get(index); } //返回权值 public int getWeight(int index1, int index2) { return this.edges[index1][index2]; } //插入节点 public void insertVertex(String vertex) { this.vertexList.add(vertex); } //添加边 public void insertEdge(int index1, int index2, int weight) { this.edges[index1][index2] = weight; this.edges[index2][index1] = weight; } //显示图对应的矩阵 public void show() { for (int[] v1 : this.edges) { System.out.println(Arrays.toString(v1)); } } } 2. 图的深度优先算法（DFS） 深度优先搜索类似于树的先序遍历。如其名称中所暗含的意思一样，这种搜索算法所遵循的搜索策略是尽可能“深”地搜索一个图。它的基本思想如下:首先访问图中某一起始顶点v，然后由v出发，访问与v邻接且未被访问的任一顶点w，再访问与w邻接且未被访问的任一顶点\u0026hellip;重复上述过程。当不能再继续向下访问时，依次退回（回溯）到最近被访问的顶点，若它还有邻接顶点未被访问过，则从该点开始继续上述搜索过程，直至图中所有顶点均被访问过为止。\n算法过程\n访问初始结点v，并标记结点v为已访问 查找结点v的第一个邻接结点w 若w存在，则继续执行第4步；如果w不存在，则回到第1步，将从v的下一个结点继续 若w未被访问，对w进行深度优先遍历递归（即把w当做另一个v，然后进行步骤123）；若w已被访问，查找结点v的w邻接结点的下一个未访问的邻接结点，转到第3步 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 bool visited[MAX_VERTEX_NUM];\t//访问标记数组 /*从顶点出发，深度优先遍历图G*/ void DFS(Graph G, int v){ int w; visit(v);\t//访问顶点 visited[v] = TRUE;\t//设已访问标记 //FirstNeighbor(G,v):求图G中顶点v的第一个邻接点，若有则返回顶点号，否则返回-1。 //NextNeighbor(G,v,w):假设图G中顶点w是顶点v的一个邻接点，返回除w外顶点v for(w = FirstNeighbor(G, v); w\u0026gt;=0; w=NextNeighor(G, v, w)){ if(!visited[w]){\t//w为u的尚未访问的邻接顶点 DFS(G, w); } } } /*对图进行深度优先遍历*/ void DFSTraverse(MGraph G){ int v; for(v=0; v\u0026lt;G.vexnum; ++v){ visited[v] = FALSE;\t//初始化已访问标记数据 } for(v=0; v\u0026lt;G.vexnum; ++v){\t//从v=0开始遍历 if(!visited[v]){ DFS(G, v); } } } 3. 图的广度优先算法（BFS） 如果说图的深度优先遍历类似树的前序遍历，那么图的广度优先遍历就类似于树的层序遍历了。 广度优先搜索是一种分层的查找过程，每向前走一步可能访问一批顶点，不像深度优先搜索那样有往回退（回溯）的情况，因此它不是一个递归的算法。为了实现逐层的访问，算法必须借助一个辅助队列，以记忆正在访问的顶点（只有知道了上层顶点，才可以访问邻接的下层顶点，所以需要记录正在访问的顶点。使用一个队列以保持访问过的结点的顺序，以便按这个顺序来访问这些结点的邻接结点）。\n算法过程\n访问初始结点v并标记结点v为已访问 结点v入队列 当队列非空时，继续执行，否则算法结束 出队列，取得队头结点u 查找结点u的第一个邻接结点w 若结点u的邻接结点w不存在，则转到步骤3；否则循环执行以下三个步骤： 若结点w尚未被访问，则访问结点w并标记为已访问 结点w入队列 查找结点u的继w邻接结点后的下一个邻接结点w，转到步骤6 代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /*邻接矩阵的广度遍历算法*/ void BFSTraverse(MGraph G){ int i, j; Queue Q; for(i = 0; i\u0026lt;G,numVertexes; i++){ visited[i] = FALSE; } InitQueue(\u0026amp;Q);\t//初始化一辅助用的队列 for(i=0; i\u0026lt;G.numVertexes; i++){ //若是未访问过就处理 if(!visited[i]){ vivited[i] = TRUE;\t//设置当前访问过 visit(i);\t//访问顶点 EnQueue(\u0026amp;Q, i);\t//将此顶点入队列 //若当前队列不为空 while(!QueueEmpty(Q)){ DeQueue(\u0026amp;Q, \u0026amp;i);\t//顶点i出队列 //FirstNeighbor(G,v):求图G中顶点v的第一个邻接点，若有则返回顶点号，否则返回-1。 //NextNeighbor(G,v,w):假设图G中顶点w是顶点v的一个邻接点，返回除w外顶点v for(j=FirstNeighbor(G, i); j\u0026gt;=0; j=NextNeighbor(G, i, j)){ //检验i的所有邻接点 if(!visited[j]){ visit(j);\t//访问顶点j visited[j] = TRUE;\t//访问标记 EnQueue(Q, j);\t//顶点j入队列 } } } } } } 九、十大算法 1. 查找算法 锚点连接\n2. 分治算法 ① 基本概念 在计算机科学中，分治法是一种很重要的算法。字面上的解释是“分而治之”，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题\u0026hellip;\u0026hellip;直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。这个技巧是很多高效算法的基础，如排序算法(快速排序，归并排序)，傅立叶变换(快速傅立叶变换)\u0026hellip;\u0026hellip;\n任何一个可以用计算机求解的问题所需的计算时间都与其规模有关。问题的规模越小，越容易直接求解，解题所需的计算时间也越少。例如，对于n个元素的排序问题，当n=1时，不需任何计算。n=2时，只要作一次比较即可排好序。n=3时只要作3次比较即可。而当n较大时，问题就不那么容易处理了。要想直接解决一个规模较大的问题，有时是相当困难的\n② 基本思想及策略 分治法的设计思想是：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。\n分治策略是：对于一个规模为n的问题，若该问题可以容易地解决（比如说规模n较小）则直接解决，否则将其分解为k个规模较小的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。这种算法设计策略叫做分治法。\n如果原问题可分割成k个子问题，1\u0026lt;k≤n，且这些子问题都可解并可利用这些子问题的解求出原问题的解，那么这种分治法就是可行的。由分治法产生的子问题往往是原问题的较小模式，这就为使用递归技术提供了方便。在这种情况下，反复应用分治手段，可以使子问题与原问题类型一致而其规模却不断缩小，最终使子问题缩小到很容易直接求出其解。这自然导致递归过程的产生。分治与递归像一对孪生兄弟，经常同时应用在算法设计之中，并由此产生许多高效算法。\n③ 适用的情况 分治法所能解决的问题一般具有以下几个特征：\n该问题的规模缩小到一定的程度就可以容易地解决 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质 利用该问题分解出的子问题的解可以合并为该问题的解 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题 第一条特征是绝大多数问题都可以满足的，因为问题的计算复杂性一般是随着问题规模的增加而增加\n第二条特征是应用分治法的前提，它也是大多数问题可以满足的，此特征反映了递归思想的应用\n第三条特征是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法\n第四条特征涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复分解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好\n④ 基本步骤 分治法在每一层递归上都有三个步骤：\n分解：将原问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题； 解决：若子问题规模较小而容易被解决则直接解决，否则递归地解决各个子问题 合并：将各个子问题的解合并为原问题的解。 它的一般的算法设计模式如下：\nDivide-and-Conquer(P)：\nif |P|\u0026lt;=n~0~ then return(ADHOC(P)) 将P分解为较小的子问题 P~1~ ,P~2~ ,\u0026hellip;,P~k~ for i ← 1 to k do y~i~ ← Divide-and-Conquer(P~i~) 递归解决P~i~ T ← MERGE(y~1~,y~2~,\u0026hellip;,y~k~) 合并子问题 return(T) 其中|P|表示问题P的规模；n~0~为一阈值，表示当问题P的规模不超过n~0~时，问题已容易直接解出，不必再继续分解。ADHOC(P)是该分治法中的基本子算法，用于直接解小规模的问题P。因此，当P的规模不超过n0时直接用算法ADHOC(P)求解。算法MERGE(y~1~,y~2~,\u0026hellip;,y~k~)是该分治法中的合并子算法，用于将P的子问题P~1~ ,P~2~ ,\u0026hellip;,P~k~的相应的解y~1~,y~2~,\u0026hellip;,y~k~合并为P的解。\n⑤ 复杂性分析 一个分治法将规模为n的问题分成k个规模为n／m的子问题去解。设分解阀值n0=1，且ADHOC(基本子算法)解规模为1的问题耗费1个单位时间。再设将原问题分解为k个子问题以及用merge将k个子问题的解合并为原问题的解需用f(n)个单位时间。用T(n)表示该分治法解规模为|P| = n的问题所需的计算时间，则有：\nT(n)= k T(n/m)+f(n)\n通过迭代法求得方程的解：\n递归方程及其解只给出n等于m的方幂时T(n)的值，但是如果认为T(n)足够平滑，那么由n等于m的方幂时T(n)的值可以估计T(n)的增长速度。通常假定T(n)是单调上升的，从而当mi≤n\u0026lt;mi+1时，T(mi)≤T(n)\u0026lt;T(mi+1)。\n⑥ 分治法求解的一些经典问题 二分搜索 大整数乘法 Strassen矩阵乘法 棋盘覆盖 合并排序 快速排序 线性时间选择 最接近点对问题 循环赛日程表 汉诺塔 ⑦ 依据分治法设计程序时的思维过程 实际上就是类似于数学归纳法，找到解决本问题的求解方程公式，然后根据方程公式设计递归程序。\n一定是先找到最小问题规模时的求解方法 然后考虑随着问题规模增大时的求解方法 找到求解的递归函数式后（各种规模或因子），设计递归程序即可。 ⑧ 案例 汉诺塔 题目\n汉诺塔：汉诺塔（又称河内塔）问题是源于印度一个古老传说的益智玩具。大梵天创造世界的时候做了三根金刚石柱子，在一根柱子上从下往上按照大小顺序摞着64片黄金圆盘。大梵天命令婆罗门把圆盘从下面开始按大小顺序重新摆放在另一根柱子上。并且规定，在小圆盘上不能放大圆盘，在三根柱子之间一次只能移动一个圆盘。\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package algorithm.dac; public class hanoitower { public static void main(String[] args) { move(5, \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;); } //将第num个盘从a移到c处，中间借助b public static void move(int num, char a, char b, char c) { if (num == 1) { System.out.println(\u0026#34;第\u0026#34; + num + \u0026#34;个盘从 \u0026#34; + a + \u0026#34;-\u0026gt;\u0026#34; + c); return; } //先将最上面的所有盘，从a移动b move(num - 1, a, c, b); //把最下边的盘，从a移动到c System.out.println(\u0026#34;第\u0026#34; + num + \u0026#34;个盘从 \u0026#34; + a + \u0026#34;-\u0026gt;\u0026#34; + c); // move(1,a,b,c); //再把b塔所有盘，从b移动至c，借助a塔 move(num - 1, b, a, c); } } 3. 动态规划 ① 算法的核心 理解一个算法就要理解一个算法的核心，动态规划算法的核心是一个小故事。\n1 2 3 4 5 6 7 8 9 10 11 A \u0026#34;1+1+1+1+1+1+1+1 =？\u0026#34; A : \u0026#34;上面等式的值是多少\u0026#34; B : 计算 \u0026#34;8\u0026#34; A : 在上面等式的左边写上 \u0026#34;1+\u0026#34; A : \u0026#34;此时等式的值为多少\u0026#34; B : quickly* \u0026#34;9\u0026#34; A : \u0026#34;你怎么这么快就知道答案了\u0026#34; A : \u0026#34;只要在8的基础上加1就行了\u0026#34; A : \u0026#34;所以你不用重新计算因为你记住了第一个等式的值为8!动态规划算法也可以说是 \u0026#39;记住求过的解来节省时间\u0026#39;\u0026#34; 由上面的小故事可以知道动态规划算法的核心就是记住已经解决过的子问题的解。\n② 算法的两种形式 上面已经知道动态规划算法的核心是记住已经求过的解，记住求解的方式有两种：①自顶向下的备忘录法 ②自底向上。 为了说明动态规划的这两种方法，举一个最简单的例子：求斐波拉契数列**Fibonacci **。先看一下这个问题：\n1 2 3 4 5 Fibonacci (n) = 1; n = 0 Fibonacci (n) = 1; n = 1 Fibonacci (n) = Fibonacci(n-1) + Fibonacci(n-2) 以前学c语言的时候写过这个算法使用递归十分的简单。先使用递归版本来实现这个算法：\n1 2 3 4 5 6 7 8 9 public int fib(int n){ if(n\u0026lt;=0) return 0; if(n==1) return 1; return fib( n-1)+fib(n-2); } //输入6 //输出：8 先来分析一下递归算法的执行流程，假如输入6，那么执行的递归树如下：\n上面的递归树中的每一个子节点都会执行一次，很多重复的节点被执行，fib(2)被重复执行了5次。由于调用每一个函数的时候都要保留上下文，所以空间上开销也不小。这么多的子节点被重复执行，如果在执行的时候把执行过的子节点保存起来，后面要用到的时候直接查表调用的话可以节约大量的时间。下面就看看动态规划的两种方法怎样来解决斐波拉契数列**Fibonacci **数列问题。\n自顶向下的备忘录法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static int Fibonacci(int n){ if(n\u0026lt;=0) return n; int []Memo=new int[n+1]; for(int i=0;i\u0026lt;=n;i++) Memo[i]=-1; return fib(n, Memo); } public static int fib(int n,int []Memo){ if(Memo[n]!=-1) return Memo[n]; //如果已经求出了fib（n）的值直接返回，否则将求出的值保存在Memo备忘录中。\tif(n\u0026lt;=2) Memo[n]=1; else Memo[n]=fib( n-1,Memo)+fib(n-2,Memo);\treturn Memo[n]; } 备忘录法也是比较好理解的，创建了一个n+1大小的数组来保存求出的斐波拉契数列中的每一个值，在递归的时候如果发现前面fib（n）的值计算出来了就不再计算，如果未计算出来，则计算出来后保存在Memo数组中，下次在调用fib（n）的时候就不会重新递归了。比如上面的递归树中在计算fib（6）的时候先计算fib（5），调用fib（5）算出了fib（4）后，fib（6）再调用fib（4）就不会在递归fib（4）的子树了，因为fib（4）的值已经保存在Memo[4]中。\n暴力递归之中有很多节点需要重复计算，为了减少重复计算，需要使用一个缓存表（一般是一位或二位数组）记录当前递归节点的结果，以后需要再次使用时，无需递归，直接从缓存里拿结果，从而提升速度。即记忆化搜索\n自底向上的动态规划 备忘录法还是利用了递归，上面算法不管怎样，计算fib（6）的时候最后还是要计算出fib（1），fib（2），fib（3）\u0026hellip;\u0026hellip;,那么何不先计算出fib（1），fib（2），fib（3）\u0026hellip;\u0026hellip;呢？这也就是动态规划的核心，先计算子问题，再由子问题计算父问题。\n动态规划算法与分治算法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。\n与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。 ( 即下一个子阶段的求解是建立在上一个子阶段的解的基础上，进行进一步的求解 )\n动态规划可以通过填表的方式来逐步推进，得到最优解\n1 2 3 4 5 6 7 8 9 10 11 12 public static int fib(int n){ if(n\u0026lt;=0) return n; int []Memo=new int[n+1]; Memo[0]=0; Memo[1]=1; for(int i=2;i\u0026lt;=n;i++) { Memo[i]=Memo[i-1]+Memo[i-2]; }\treturn Memo[n]; } 自底向上方法也是利用数组保存了先计算的值，为后面的调用服务。观察参与循环的只有 i，i-1 , i-2三项，因此该方法的空间可以进一步的压缩如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public static int fib(int n){ if(n\u0026lt;=1) return n; int Memo_i_2=0; int Memo_i_1=1; int Memo_i=1; for(int i=2;i\u0026lt;=n;i++) { Memo_i=Memo_i_2+Memo_i_1; Memo_i_2=Memo_i_1; Memo_i_1=Memo_i; }\treturn Memo_i; } 一般来说由于备忘录方式的动态规划方法使用了递归，递归的时候会产生额外的开销，使用自底向上的动态规划方法要比备忘录方法好。\n③ 案例 01背包问题 题目\n背包问题：有一个背包，容量为4磅，现有如下物品。要求达到的目标为装入的背包的总价值最大，并且重量不超出要求装入的物品不能重复\n物品 重量 价格 吉他(G) 1 1500 音响(S) 4 3000 电脑(L) 3 2000 思路\n背包问题主要是指一个给定容量的背包、若干具有一定价值和重量的物品，如何选择物品放入背包使物品的价值最大。其中又分01背包和完全背包(完全背包指的是：每种物品都有无限件可用)这里的问题属于01背包，即每个物品最多放一个。而无限背包可以转化为01背包。\n算法的主要思想，利用动态规划来解决。每次遍历到的第i个物品，根据w[i]和v[i]来确定是否需要将该物品放入背包中。即对于给定的n个物品，设v[i]、w[i]分别为第i个物品的价值和重量，C为背包的容量。再令v[i][j]表示在前i个物品中能够装入容量为j的背包中的最大价值。则我们有下面的结果：\n物品 0 磅 1磅 2磅 3磅 4磅 0 0 0 0 0 吉他(G) 0 1500(G) 1500(G) 1500(G) 1500(G) 音响(S) 0 1500(G) 1500(G) 1500(G) 3000(S) 电脑(L) 0 1500(G) 1500(G) 2000(L) 3500(L+G) 1 2 3 4 5 6 7 8 9 (1) v[i][0]=v[0][j]=0; //表示 填入表 第一行和第一列是0 (2) 当w[i]\u0026gt; j 时：v[i][j]=v[i-1][j] //当准备加入新增的商品的容量大于当前背包的容量时，就直接使用上一个单元格的装入策略 (3) 当j\u0026gt;=w[i]时： v[i][j]=max{v[i-1][j], v[i]+v[i-1][j-w[i]]} //当准备加入的新增的商品的容量小于等于当前背包的容量, // 装入的方式: v[i-1][j] ： 就是上一个单元格的装入的最大值 v[i] : 表示当前商品的价值 v[i-1][j-w[i]] ： 装入i-1商品，到剩余空间j-w[i]的最大值 当j\u0026gt;=w[i]时 ： v[i][j]=max{v[i-1][j], v[i]+v[i-1][j-w[i]]} 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package algorithm.DP; import java.util.Arrays; /* * 01背包问题 * */ public class backpackProblem { public static void main(String[] args) { //创建表 int[][] v = new int[4][5];//前i个物品中能够装入容量为j的背包中的最大价值 int[] w = new int[4];//第i个物品重量 w[1] = 1; w[2] = 4; w[3] = 3; int[] a = new int[4];//物品第i个物品单价 a[1] = 1500; a[2] = 3000; a[3] = 2000; //开始决策填表 //填充第一列 int rowNum = v.length; for (int i = 0; i \u0026lt; rowNum; i++) { v[i][0] = 0; } //填充第一行 int colNum = v[0].length; for (int j = 0; j \u0026lt; colNum; j++) { v[0][j] = 0; } //填充第二行：只能放第一个商品 for (int j = 0; j \u0026lt; colNum; j++) { if (w[1] \u0026lt;= j) v[1][j] = a[1]; } //决策其他单元格 for (int i = 2; i \u0026lt; rowNum; i++) { for (int j = 1; j \u0026lt; colNum; j++) { //当前物品放不下，只有依赖上次决策的结果 if (w[i] \u0026gt; j) v[i][j] = v[i - 1][j]; //当前物品放得下，需要比较将当前物品放入之后再填放以前物品的加之和不放当前物品的价值 else v[i][j] = Math.max(v[i - 1][j], a[i] + v[i - 1][j - w[i]]); } } for (int i = 0; i \u0026lt; rowNum; i++) { System.out.println(Arrays.toString(v[i])); System.out.println(); } } } 4. KMP算法 很详尽KMP算法（厉害）\n① 暴力匹配算法 假设现在我们面临这样一个问题：有一个文本串S，和一个模式串P，现在要查找P在S中的位置，怎么查找呢？\n如果用暴力匹配的思路，并假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置，则有：\n如果当前字符匹配成功（即S[i] == P[j]），则i++，j++，继续匹配下一个字符；\n如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0。相当于每次匹配失败时，i 回溯，j 被置为0。\n理清楚了暴力匹配算法的流程及内在的逻辑，咱们可以写出暴力匹配的代码，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 int ViolentMatch(char* s, char* p) { int sLen = strlen(s); int pLen = strlen(p); int i = 0; int j = 0; while (i \u0026lt; sLen \u0026amp;\u0026amp; j \u0026lt; pLen) { if (s[i] == p[j]) { //①如果当前字符匹配成功（即S[i] == P[j]），则i++，j++ i++; j++; } else { //②如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0 i = i - j + 1; j = 0; } } //匹配成功，返回模式串p在文本串s中的位置，否则返回-1 if (j == pLen) return i - j; else return -1; } 举个例子，如果给定文本串S:“BBC ABCDAB ABCDABCDABDE”，和模式串P:“ABCDABD”，现在要拿模式串P去跟文本串S匹配，整个过程如下所示：\nS[0]为B，P[0]为A，不匹配，执行第②条指令：\u0026ldquo;如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0\u0026quot;。S[1]跟P[0]匹配，相当于模式串要往右移动一位（i=1，j=0）\nS[1]跟P[0]还是不匹配，继续执行第②条指令：“如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0”，S[2]跟P[0]匹配（i=2，j=0），从而模式串不断的向右移动一位（不断的执行“令i = i - (j - 1)，j = 0”，i从2变到4，j一直为0）\n直到S[4]跟P[0]匹配成功（i=4，j=0），此时按照上面的暴力匹配算法的思路，转而执行第①条指令：“如果当前字符匹配成功（即S[i] == P[j]），则i++，j++”，可得S[i]为S[5]，P[j]为P[1]，即接下来S[5]跟P[1]匹配（i=5，j=1）\nS[5]跟P[1]匹配成功，继续执行第①条指令：“如果当前字符匹配成功（即S[i] == P[j]），则i++，j++”，得到S[6]跟P[2]匹配（i=6，j=2），如此进行下去\n直到S[10]为空格字符，P[6]为字符D（i=10，j=6），因为不匹配，重新执行第②条指令：“如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0”，相当于S[5]跟P[0]匹配（i=5，j=0）\n至此，我们可以看到，如果按照暴力匹配算法的思路，尽管之前文本串和模式串已经分别匹配到了S[9]、P[5]，但因为S[10]跟P[6]不匹配，所以文本串回溯到S[5]，模式串回溯到P[0]，从而让S[5]跟P[0]匹配。\n而S[5]肯定跟P[0]失配。为什么呢？因为在之前第4步匹配中，我们已经得知S[5] = P[1] = B，而P[0] = A，即P[1] != P[0]，故S[5]必定不等于P[0]，所以回溯过去必然会导致失配。那有没有一种算法，让i不往回退，只需要移动j即可呢？\n答案是肯定的。这种算法就是本文的主旨KMP算法，它利用之前已经部分匹配这个有效信息，保持i不回溯，通过修改j的位置，让模式串尽量地移动到有效的位置。\n② KMP算法 定义 下面先直接给出KMP的算法流程\n假设现在文本串S匹配到 i 位置，模式串P匹配到j位置 如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++，继续匹配下一个字符； 如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]。此举意味着失配时，模式串P相对于文本串S向右移动了j - next [j] 位。（注意：是相对，文本串没有移动，只是将模式串的j指针往前（向左）移动到next[j]位置，因此相对文本串来说：模式串S向右移动了j-next[j]位） 换言之，当匹配失败时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的next值），即移动的实际位数为：j - next[j]，且此值大于等于1。 很快，你也会意识到next 数组各值的含义：代表当前字符之前的字符串中，有多大长度的相同前缀后缀。例如如果next [j] = k，代表j 之前的字符串中有最大长度为k 的相同前缀后缀。\n此也意味着在某个字符失配时，该字符对应的next 值会告诉你下一步匹配中，模式串应该跳到哪个位置（跳到next [j] 的位置）。如果next [j] 等于0或-1，则跳到模式串的开头字符，若next [j] = k 且 k \u0026gt; 0，代表下次匹配跳到j 之前的某个字符，而不是跳到开头，且具体跳过了k 个字符。\n步骤 寻找前缀后缀最长公共元素长度\n对于P= p~0~,p~1~\u0026hellip;p~j-1~，p~j~，寻找模式串P中长度最大且相等的前缀和后缀。如果存在p~0~,p~1~ \u0026hellip;p~k-1~,p~k~ = p~j-k~,p~j-k+1~\u0026hellip;p~j-1~,p~j~，那么在包含p~j~的模式串中有最大长度为k+1的相同前缀后缀。举个例子，如果给定的模式串为“abab”，那么它的各个子串的前缀后缀的公共元素的最大长度如下表格所示：\n比如对于字符串aba来说，它有长度为1的相同前缀后缀a；而对于字符串abab来说，它有长度为2的相同前缀后缀ab（相同前缀后缀的长度为k + 1，k + 1 = 2)。\n根据前缀后缀最长公共元素长度，求next数组（也可以不求，直接根据最大长度表得出结果）\n将第①步骤中求得的值整体右移一位，然后初值赋为-1，即结果\n根据next数组进行匹配\n匹配失配，j = next [j]，模式串向右移动的位数为：j - next[j]。换言之，当模式串的后缀p~j-k~,p~j-k+1~, \u0026hellip;, p~j-1~ 跟文本串 s~i-k~,s~i-k+1~, \u0026hellip;, s~i-1~匹配成功，但 p~j~ 跟 s~i~ 匹配失败时，因为next[j] = k，相当于在不包含 p~j~ 的模式串中有最大长度为 k 的相同前缀后缀，即p~0~,p~1~ \u0026hellip;p~k-1~ = p~j-k~,p~j-k+1~\u0026hellip;p~j-1~，故令j = next[j]，从而让模式串右移j - next[j] 位，使得模式串的前缀p~0~,p~1~\u0026hellip;p~k-1~对应着文本串 s~i-k~,s~i-k+1~\u0026hellip;s~i-1~，而后让p~k~跟s~i~继续匹配。如下图所示：\n综上，KMP的next 数组相当于告诉我们：当模式串中的某个字符跟文本串中的某个字符匹配失配时，模式串下一步应该跳到哪个位置。如模式串中在j 处的字符跟文本串在i 处的字符匹配失配时，下一步用next [j] 处的字符继续跟文本串i 处的字符匹配，相当于模式串向右移动 j - next[j] 位。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package algorithm.kmp; import java.util.Arrays; public class kmp { public static void main(String[] args) { String str1 = \u0026#34;BBC ABCDAB ABCDABCDABDE\u0026#34;; String str2 = \u0026#34;ABCDABD\u0026#34;; // String str2 = \u0026#34;ABCDABDABCE\u0026#34;; // String str2 = \u0026#34;agctagcagctagct\u0026#34;; // System.out.println(Arrays.toString(kmpNext(str2))); System.out.println(\u0026#34;第一次出现的位置：\u0026#34; + kmpSearch(str1, str2, kmpNext(str2))); } //kmp搜索 public static int kmpSearch(String str1, String str2, int[] next) { for (int i = 0, j = 0; i \u0026lt; str1.length(); i++) { //在部分匹配表往前递归，知道找到相等字符值才停止， while (j \u0026gt; 0 \u0026amp;\u0026amp; str1.charAt(i) != str2.charAt(j)) { j = next[j - 1]; } if (str1.charAt(i) == str2.charAt(j)) { j++; } if (j == str2.length()) { return i - j + 1; } } return -1; } //获取子串的部分匹配值表 public static int[] kmpNext(String dest) { int[] next = new int[dest.length()]; for (int i = 1; i \u0026lt; dest.length(); i++) { int j = next[i - 1];//j指针记录移动的位置，也是移动了的步数 //不断递归判断是否存在子对称，k=0说明不再有子对称， // Pattern[i] != Pattern[k]说明虽然对称，但是 // 对称后面的值和当前的字符值不相等，所以继续往前递推，递推的关系式：j=next[j-1]; // 意即：不断的向前子串中寻找一个字符，使得这个字符等于当前字符 while (j != 0 \u0026amp;\u0026amp; dest.charAt(i) != dest.charAt(j)) { j = next[j - 1]; } //找到了这个子对称，或者是直接继承了前面的对称性，这两种都在前面的基础上++：就是将j指针移动后一位，一比较下一位是否相同 if (dest.charAt(i) == dest.charAt(j)) { next[i] = j + 1; } else {//如果遍历了所有子对称都无效，说明这个新字符不具有对称性，清0 next[i] = 0; } } return next; } } 5. 贪心算法 ① 基本概念 所谓贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解。 贪心算法没有固定的算法框架，算法设计的关键是贪心策略的选择。必须注意的是，贪心算法不是对所有问题都能得到整体最优解，选择的贪心策略必须具备无后效性（即某个状态以后的过程不会影响以前的状态，只与当前状态有关。） 所以，对所采用的贪心策略一定要仔细分析其是否满足无后效性。\n② 贪心算法的基本思路 建立数学模型来描述问题 把求解的问题分成若干个子问题 对每个子问题求解，得到子问题的局部最优解 把子问题的解局部最优解合成原来问题的一个解 ③ 该算法存在的问题 不能保证求得的最后解是最佳的 不能用来求最大值或最小值的问题 只能求满足某些约束条件的可行解的范围 ④ 贪心算法适用的问题 贪心策略适用的前提是：局部最优策略能导致产生全局最优解。 实际上，贪心算法适用的情况很少。一般对一个问题分析是否适用于贪心算法，可以先选择该问题下的几个实际数据进行分析，就可以做出判断。\n⑤ 贪心选择性质 所谓贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择，换句话说，当考虑做何种选择的时候，我们只考虑对当前问题最佳的选择而不考虑子问题的结果。这是贪心算法可行的第一个基本要素。贪心算法以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为规模更小的子问题。对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。 当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。问题的最优子结构性质是该问题可用贪心算法求解的关键特征。\n⑥ 贪心算法的实现框架 从问题的某一初始解出发： while (朝给定总目标前进一步) { 利用可行的决策，求出可行解的一个解元素。 } 由所有解元素组合成问题的一个可行解；\n⑦ 例题分析 【背包问题】有一个背包，容量是M=150，有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。\n物品： 重量：35 30 60 50 40 10 25 价值：10 40 30 50 35 40 30\n分析： 目标函数： ∑p~i~最大 约束条件是装入的物品总质量不超过背包容量：∑w~i~\u0026lt;=M( M=150) （1）根据贪心的策略，每次挑选价值最大的物品装入背包，得到的结果是否最优？ （2）每次挑选所占重量最小的物品装入是否能得到最优解？ （3）每次选取单位重量价值最大的物品，成为解本题的策略\n值得注意的是，贪心算法并不是完全不可以使用，贪心策略一旦经过证明成立后，它就是一种高效的算法。比如，求最小生成树的Prim算法和Kruskal算法都是漂亮的贪心算法。 贪心算法还是很常见的算法之一，这是由于它简单易行，构造贪心策略不是很困难。 可惜的是，它需要证明后才能真正运用到题目的算法中。 一般来说，贪心算法的证明围绕着：整个问题的最优解一定由在贪心策略中存在的子问题的最优解得来的。 对于例题中的3种贪心策略，都是无法成立（无法被证明）的，解释如下： 贪心策略：选取价值最大者。\n反例：\nW=30\n物品：A B C\n重量：28 12 12\n价值：30 20 20\n根据策略，首先选取物品A，接下来就无法再选取了，可是，选取B、C则更好。\n（2）贪心策略：选取重量最小。它的反例与第一种策略的反例差不多。\n（3）贪心策略：选取单位重量价值最大的物品。反例：\nW=30\n物品：A B C\n重量：28 20 10\n价值：28 20 10\n根据策略，三种物品单位重量价值一样，程序无法依据现有策略作出判断，如果选择A，则答案错误。但是果在条件中加一句当遇见单位价值相同的时候,优先装重量小的,这样的问题就可以解决.\n所以需要说明的是，贪心算法可以与随机化算法一起使用，具体的例子就不再多举了。（因为这一类算法普及性不高，而且技术含量是非常高的，需要通过一些反例确定随机的对象是什么，随机程度如何，但也是不能保证完全正确，只能是极大的几率正确）\n6. 最小生成树（MST） 定义：\n在给定一张无向图，如果在它的子图中，任意两个顶点都是互相连通，并且是一个树结构，那么这棵树叫做生成树。当连接顶点之间的图有权重时，权重之和最小的树结构为最小生成树！\n性质：\n给定一个带权的无向连通图,如何选取一棵生成树,使树上所有边上权的总和为最小,这叫最小生成树\nN个顶点，一定有N-1条边\n包含全部顶点N-1条边都在图中\n举例说明(如图:)\n求最小生成树的算法主要是普里姆算法和克鲁斯卡尔算法\n① Prim算法（普里姆算法） Prim算法是另一种贪心算法，和Kuskral算法的贪心策略不同，Kuskral算法主要对边进行操作，而Prim算法则是对节点进行操作，每次遍历添加一个点，这时候我们就不需要使用并查集了。具体步骤为：\n设G=(V,E)是连通网，T=(U,D)是最小生成树，V,U是顶点集合，E,D是边的集合 若从顶点u开始构造最小生成树，则从集合V中取出顶点u放入集合U中，标记顶点v的visited[u]=1 若集合U中顶点u~i~与集合V-U中的顶点v~j~之间存在边，则寻找这些边中权值最小的边，但不能构成回路，将顶点v~j~加入集合U中，将边（u~i~,v~j~）加入集合D中，标记visited[v~j~]=1 重复步骤②，直到U与V相等，即所有顶点都被标记为访问过，此时D中有n-1条边 注意：对于单连通，从一个节点出发就可以直接找到完整的最小生成树，因此最外的for循环也可以为一个顺序结构，但多连通，就需要从不同的节点出发了！！！\n代码实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 package algorithm.prim; import java.util.Arrays; /* * prim算法解决最小生成树问题 * */ public class prim { public static void main(String[] args) { char[] vertexes = new char[]{\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;}; int vertexNum = vertexes.length; int max = Integer.MAX_VALUE; int[][] weight = new int[][]{ {max, 5, 7, max, max, max, 2}, {5, max, max, 9, max, max, 3}, {7, max, max, max, 8, max, max}, {max, 9, max, max, max, 4, max}, {max, max, 8, max, max, 5, 4}, {max, max, max, 4, 5, max, 6}, {2, 3, max, max, 4, 6, max}, }; MGraph graph = new MGraph(vertexNum); MST mst = new MST(); mst.createGraph(graph, vertexNum, vertexes, weight); // mst.showGraph(graph); mst.prim(graph, 1); } } //创建最小生成树 class MST { //构建图 public void createGraph(MGraph graph, int vertexNum, char[] vertexes, int[][] weight) { graph.vertexNum = vertexNum; //浅拷贝 for (int i = 0; i \u0026lt; vertexNum; i++) { graph.vertexes[i] = vertexes[i]; for (int j = 0; j \u0026lt; vertexNum; j++) { graph.weight[i][j] = weight[i][j]; } } } public void showGraph(MGraph graph) { for (int[] row : graph.weight) { System.out.println(Arrays.toString(row)); } } //使用prim算法，从第v个节点开始进行访问 public void prim(MGraph graph, int v) { boolean[] isVisited = new boolean[graph.vertexNum]; //标记第一个节点已被访问 isVisited[v] = true; //记录最小边的to节点下标 int x = -1; int y = -1; int min = Integer.MAX_VALUE; //需要循环 graph.vertexNum-1 次，因为有 graph.vertexNum-1 条边 for (int k = 1; k \u0026lt; graph.vertexNum; k++) { //这个是确定每一次生成的子图 ，和哪个结点的距离最近，就找到一个边：所有被访问节点与未访问节点中，寻找最短边 for (int i = 0; i \u0026lt; graph.vertexNum; i++) { for (int j = 0; j \u0026lt; graph.vertexNum; j++) { //这里i表示已访问节点，j表示未访问节点 if (isVisited[i] \u0026amp;\u0026amp; !isVisited[j] \u0026amp;\u0026amp; graph.weight[i][j] \u0026lt; min) { min = graph.weight[i][j]; x = i; y = j; } } } System.out.println(\u0026#34;边\u0026lt;\u0026#34; + graph.vertexes[x] + \u0026#34;--\u0026#34; + graph.vertexes[y] + \u0026#34;\u0026gt;\u0026#34; + \u0026#34; 权值：\u0026#34; + graph.weight[x][y]); //还原 isVisited[y] = true; min = Integer.MAX_VALUE; } } } class MGraph { int vertexNum;//图节点的个数 char[] vertexes;//存放节点 int[][] weight;//存放边，邻接矩阵 public MGraph(int nodeNum) { this.vertexNum = nodeNum; this.vertexes = new char[nodeNum]; this.weight = new int[nodeNum][nodeNum]; } } ② Kruskal算法（克鲁斯卡尔算法） 在含有n个顶点的连通图中选择n-1条边，构成一棵极小连通子图，并使该连通子图中n-1条边上权值之和达到最小，则称其为连通网的最小生成树。 例如，对于如上图G4所示的连通网可以有多棵权值总和不相同的生成树。\n克鲁斯卡尔算法图解\n以上图G4为例，来对克鲁斯卡尔进行演示(假设，用数组R保存最小生成树结果)。\n第1步：将边\u0026lt;E,F\u0026gt;加入R中。 边\u0026lt;E,F\u0026gt;的权值最小，因此将它加入到最小生成树结果R中。 第2步：将边\u0026lt;C,D\u0026gt;加入R中。 上一步操作之后，边\u0026lt;C,D\u0026gt;的权值最小，因此将它加入到最小生成树结果R中。 第3步：将边\u0026lt;D,E\u0026gt;加入R中。 上一步操作之后，边\u0026lt;D,E\u0026gt;的权值最小，因此将它加入到最小生成树结果R中。 第4步：将边\u0026lt;B,F\u0026gt;加入R中。 上一步操作之后，边\u0026lt;C,E\u0026gt;的权值最小，但\u0026lt;C,E\u0026gt;会和已有的边构成回路；因此，跳过边\u0026lt;C,E\u0026gt;。同理，跳过边\u0026lt;C,F\u0026gt;。将边\u0026lt;B,F\u0026gt;加入到最小生成树结果R中。 第5步：将边\u0026lt;E,G\u0026gt;加入R中。 上一步操作之后，边\u0026lt;E,G\u0026gt;的权值最小，因此将它加入到最小生成树结果R中。 第6步：将边\u0026lt;A,B\u0026gt;加入R中。 上一步操作之后，边\u0026lt;F,G\u0026gt;的权值最小，但\u0026lt;F,G\u0026gt;会和已有的边构成回路；因此，跳过边\u0026lt;F,G\u0026gt;。同理，跳过边\u0026lt;B,C\u0026gt;。将边\u0026lt;A,B\u0026gt;加入到最小生成树结果R中。\n此时，最小生成树构造完成！它包括的边依次是：\u0026lt;E,F\u0026gt; \u0026lt;C,D\u0026gt; \u0026lt;D,E\u0026gt; \u0026lt;B,F\u0026gt; \u0026lt;E,G\u0026gt; \u0026lt;A,B\u0026gt;。\n克鲁斯卡尔算法分析\n根据前面介绍的克鲁斯卡尔算法的基本思想和做法，我们能够了解到，克鲁斯卡尔算法重点需要解决的以下两个问题： 问题一 对图的所有边按照权值大小进行排序。 问题二 将边添加到最小生成树中时，怎么样判断是否形成了回路。\n问题一很好解决，采用排序算法进行排序即可。\n问题二，实现克鲁斯卡尔算法的难点在于“如何判断一个新边是否会和已选择的边构成环路”，有一种判断的方法：\n初始状态下，为连通网中的各个顶点配置不同的标记。对于一个新边，如果它两端顶点的标记不同，就不会构成环路，可以组成最小生成树。一旦新边被选择，需要将它的两个顶点以及和它直接相连的所有已选边两端的顶点改为相同的标记；反之，如果新边两端顶点的标记相同，就表示会构成环路。\n代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 package algorithm.kruskal; import java.util.ArrayList; import java.util.Arrays; import java.util.Collections; /* * 克鲁斯卡尔算法求最小生成树 * */ public class kruskal { private int edgeNum = 0; private char[] vertexes; private int[][] matrix; private static final int INF = Integer.MAX_VALUE;//两个顶点不能联通 public static void main(String[] args) { char[] vertexes = {\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;}; //克鲁斯卡尔算法的邻接矩阵 int matrix[][] = { //@**/*A*//*B*//*C*//*D*//*E*//*F*//*G*/ /*A*/ {0, 12, INF, INF, INF, 16, 14}, /*B*/ {12, 0, 10, INF, INF, 7, INF}, /*C*/ {INF, 10, 0, 3, 5, 6, INF}, /*D*/ {INF, INF, 3, 0, 4, INF, INF}, /*E*/ {INF, INF, 5, 4, 0, 2, 8}, /*F*/ {16, 7, 6, INF, 2, 0, 9}, /*G*/ {14, INF, INF, INF, 8, 9, 0}}; kruskal k = new kruskal(vertexes, matrix); k.show(); edge[] minTree = k.getMinTree(); System.out.println(\u0026#34;最短修边如下：\\n\u0026#34; + Arrays.toString(minTree)); } //获取最小生成树，kruskal算法 public edge[] getMinTree() { //获取边 ArrayList\u0026lt;edge\u0026gt; edges = getEdges(); //将边进行排序 Collections.sort(edges); edge[] result = new edge[this.edgeNum];//结果边 int index = 0; int[] mark = new int[this.vertexes.length];//标记不同顶点 for (int i = 0; i \u0026lt; mark.length; i++) { mark[i] = i; } //取出边，把两端点终点不相同的边放入结果里 for (edge e : edges) { int start = getPosition(e.start); int end = getPosition(e.end); if (mark[start] != mark[end]) {//没有构成回路 //添加 result[index++] = e; int e1 = mark[end]; //将标记数组的这两个端点更新成同一个数据即可，既可以是都变成end，也可以是都变成start //这样标记数组里，相同的值代表是直接或间接联通的。这样可以避免联通端点再联通形成环状结构 for (int i = 0; i \u0026lt; mark.length; i++) { if (mark[i] == e1) { mark[i] = mark[start]; } } ////todo 或者 // int e1 = mark[start]; // //更新标记 // for (int i = 0; i \u0026lt; mark.length; i++) { // if (mark[i] == e1){ // mark[i] = mark[end]; // } // } //提前退出 if (index == this.vertexes.length - 1) break; } } return result; } //获取边的数组 public ArrayList\u0026lt;edge\u0026gt; getEdges() { ArrayList\u0026lt;edge\u0026gt; edges = new ArrayList\u0026lt;\u0026gt;(this.edgeNum); for (int i = 0; i \u0026lt; this.vertexes.length; i++) { for (int j = i + 1; j \u0026lt; this.vertexes.length; j++) {//省略自己和下半角 if (this.matrix[i][j] != INF) { edges.add(new edge(getChar(i), getChar(j), this.matrix[i][j])); } } } return edges; } //获取对应字符的下标 public int getPosition(char ch) { for (int i = 0; i \u0026lt; this.vertexes.length; i++) { if (ch == this.vertexes[i]) return i; } return -1; } //根据下标获取对应字符 public char getChar(int index) { return this.vertexes[index]; } public kruskal(char[] vertexes, int[][] matrix) { int vLen = vertexes.length; this.vertexes = new char[vLen]; this.matrix = new int[vLen][vLen]; //浅拷贝 for (int i = 0; i \u0026lt; vLen; i++) { this.vertexes[i] = vertexes[i]; for (int j = 0; j \u0026lt; vLen; j++) { this.matrix[i][j] = matrix[i][j]; } } for (int i = 0; i \u0026lt; this.vertexes.length; i++) { for (int j = i + 1; j \u0026lt; this.vertexes.length; j++) { if (this.matrix[i][j] != INF) { this.edgeNum++; } } } } public void show() { System.out.println(\u0026#34;邻接矩阵：\u0026#34;); for (int i = 0; i \u0026lt; this.vertexes.length; i++) { for (int j = 0; j \u0026lt; this.vertexes.length; j++) { System.out.printf(\u0026#34;%12d\u0026#34;, this.matrix[i][j]); } System.out.println(); } } } class edge implements Comparable\u0026lt;edge\u0026gt; { char start; char end; int weight; public edge(char start, char end, int weight) { this.start = start; this.end = end; this.weight = weight; } @Override public String toString() { return \u0026#34;edge{\u0026#34; + \u0026#34;start=\u0026#34; + start + \u0026#34;, end=\u0026#34; + end + \u0026#34;, weight=\u0026#34; + weight + \u0026#39;}\u0026#39;; } @Override public int compareTo(edge o) { return this.weight - o.weight; } } 7. 最短路径 ① Dijkstra算法（迪杰斯特拉算法） 看一个应用场景和问题：\n战争时期，胜利乡有7个村庄(A, B, C, D, E, F, G) ，现在有六个邮差，从G点出发，需要分别把邮件分别送到 A, B, C , D, E, F 六个村庄各个村庄的距离用边线表示(权) ，比如 A – B 距离 5公里问：如何计算出G村庄到 其它各个村庄的最短距离? 如果从其它点出发到各个点的最短距离又是多少?\n**介绍: **\n迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个结点到其他结点的最短路径。 它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。\n算法过程:\n设置出发顶点为v，顶点集合V{v1,v2,vi\u0026hellip;}，v到V中各顶点的距离构成距离集合Dis，Dis{d1,d2,di\u0026hellip;}，Dis集合记录着v到图中各顶点的距离(到自身可以看作0，v到vi距离对应为di)\n从Dis中选择值最小的di并移出Dis集合，同时移出V集合中对应的顶点vi，此时的v到vi即为最短路径 更新Dis集合，更新规则为：比较v到V集合中顶点的距离值，与v通过vi到V集合中顶点的距离值，保留值较小的一个(同时也应该更新顶点的前驱节点为vi，表明是通过vi到达的) 重复执行两步骤，直到最短路径顶点为目标顶点即可结束 代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 package algorithm.dijkstra; import java.util.Arrays; /* * 迪杰斯特拉算法求解最短路径问题 * */ public class dijkstra { public static void main(String[] args) { char[] vertex = {\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;}; //邻接矩阵 int[][] matrix = new int[vertex.length][vertex.length]; final int N = 65535;// 表示不可以连接 matrix[0] = new int[]{N, 5, 7, N, N, N, 2}; matrix[1] = new int[]{5, N, N, 9, N, N, 3}; matrix[2] = new int[]{7, N, N, N, 8, N, N}; matrix[3] = new int[]{N, 9, N, N, N, 4, N}; matrix[4] = new int[]{N, N, 8, N, N, 5, 4}; matrix[5] = new int[]{N, N, N, 4, 5, N, 6}; matrix[6] = new int[]{2, 3, N, N, 4, 6, N}; //创建 Graph对象 Graph graph = new Graph(vertex, matrix); //测试, 看看图的邻接矩阵是否ok graph.showGraph(); //测试迪杰斯特拉算法 graph.dsj(0);//A graph.showDijkstra(); } } class Graph { private char[] vertex; // 顶点数组 private int[][] matrix; // 邻接矩阵 private VisitedVertex vv; //已经访问的顶点的集合 // 构造器 public Graph(char[] vertex, int[][] matrix) { this.vertex = vertex; this.matrix = matrix; } //显示结果 public void showDijkstra() { vv.show(); } // 显示图 public void showGraph() { for (int[] link : matrix) { System.out.println(Arrays.toString(link)); } } //迪杰斯特拉算法实现 /** * @param index 表示出发顶点对应的下标 */ public void dsj(int index) { vv = new VisitedVertex(vertex.length, index); update(index);//更新index顶点到周围顶点的距离和前驱顶点 for (int j = 1; j \u0026lt; vertex.length; j++) { index = vv.updateArr();// 选择并返回新的访问顶点 update(index); // 更新index顶点到周围顶点的距离和前驱顶点 } } //更新index下标顶点到周围顶点的距离和周围顶点的前驱顶点, private void update(int index) { int len = 0; //根据遍历我们的邻接矩阵的 matrix[index]行 for (int j = 0; j \u0026lt; matrix[index].length; j++) { // len 含义是 : 出发顶点到index顶点的距离 + 从index顶点到j顶点的距离的和 len = vv.getDis(index) + matrix[index][j]; // 如果j顶点没有被访问过，并且 len 小于出发顶点到j顶点的距离，就需要更新 if (!vv.in(j) \u0026amp;\u0026amp; len \u0026lt; vv.getDis(j)) { vv.updatePre(j, index); //更新j顶点的前驱为index顶点 vv.updateDis(j, len); //更新出发顶点到j顶点的距离 } } } } // 已访问顶点集合 class VisitedVertex { // 记录各个顶点是否访问过 1表示访问过,0未访问,会动态更新 public int[] already_arr; // 每个下标对应的值为前一个顶点下标, 会动态更新 public int[] pre_visited; // 记录出发顶点到其他所有顶点的距离,比如G为出发顶点，就会记录G到其它顶点的距离，会动态更新，求的最短距离就会存放到dis public int[] dis; //构造器 /** * @param length :表示顶点的个数 * @param index: 出发顶点对应的下标, 比如G顶点，下标就是6 */ public VisitedVertex(int length, int index) { this.already_arr = new int[length]; this.pre_visited = new int[length]; this.dis = new int[length]; //初始化 dis数组 Arrays.fill(dis, 65535); this.already_arr[index] = 1; //设置出发顶点被访问过 this.dis[index] = 0;//设置出发顶点的访问距离为0 } /** * 功能: 判断index顶点是否被访问过 * * @param index * @return 如果访问过，就返回true, 否则访问false */ public boolean in(int index) { return already_arr[index] == 1; } /** * 功能: 更新出发顶点到index顶点的距离 * * @param index * @param len */ public void updateDis(int index, int len) { dis[index] = len; } /** * 功能: 更新pre这个顶点的前驱顶点为index顶点 * * @param pre * @param index */ public void updatePre(int pre, int index) { pre_visited[pre] = index; } /** * 功能:返回出发顶点到index顶点的距离 * * @param index */ public int getDis(int index) { return dis[index]; } /** * 继续选择并返回新的访问顶点， 比如这里的G 完后，就是 A点作为新的访问顶点(注意不是出发顶点) * * @return */ public int updateArr() { int min = 65535, index = 0; for (int i = 0; i \u0026lt; already_arr.length; i++) { if (already_arr[i] == 0 \u0026amp;\u0026amp; dis[i] \u0026lt; min) { min = dis[i]; index = i; } } //更新 index 顶点被访问过 already_arr[index] = 1; return index; } //显示最后的结果 //即将三个数组的情况输出 public void show() { System.out.println(\u0026#34;==========================\u0026#34;); //输出already_arr for (int i : already_arr) { System.out.print(i + \u0026#34; \u0026#34;); } System.out.println(); //输出pre_visited for (int i : pre_visited) { System.out.print(i + \u0026#34; \u0026#34;); } System.out.println(); //输出dis for (int i : dis) { System.out.print(i + \u0026#34; \u0026#34;); } System.out.println(); //为了好看最后的最短距离，我们处理 char[] vertex = {\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;}; int count = 0; for (int i : dis) { if (i != 65535) { System.out.print(vertex[count] + \u0026#34;(\u0026#34; + i + \u0026#34;) \u0026#34;); } else { System.out.println(\u0026#34;N \u0026#34;); } count++; } System.out.println(); } } ② Floyd算法(弗洛伊德算法) 用来求图中所有点对之间的最短路径 Dijkstra算法是求单源最短路径的，那如果求图中所有点对的最短路径的话则有以下两种解法： 解法一： 以图中的每个顶点作为源点，调用Dijkstra算法，时间复杂度为O(n^3^)； 解法二： Floyd（弗洛伊德算法）更简洁，算法复杂度仍为O(n^3^)。 正如大多数教材中所讲到的，求单源点无负边最短路径用Dijkstra，而求所有点最短路径用Floyd。确实，我们将用到Floyd算法，但是，并不是说所有情况下Floyd都是最佳选择。 对于没有学过Floyd的人来说，在掌握了Dijkstra之后遇到All-Pairs最短路径问题的第一反应可能会是：计算所有点的单源点最短路径，不就可以得到所有点的最短路径了吗。简单得描述一下算法就是执行n次Dijkstra算法。 Floyd可以说是Warshall算法的扩展了，三个for循环便可以解决一个复杂的问题，应该说是十分经典的。从它的三层循环可以看出，它的复杂度是n^3^，除了在第二层for中加点判断可以略微提高效率，几乎没有其他办法再减少它的复杂度。 比较两种算法，不难得出以下的结论：对于稀疏的图，采用n次Dijkstra比较出色，对于茂密的图，可以使用Floyd算法。另外，Floyd可以处理带负边的图。 下面对Floyd算法进行介绍：\nFloyd算法的基本思想： 可以将问题分解: 第一、先找出最短的距离 第二、然后在考虑如何找出对应的行进路线。 如何找出最短路径呢，这里还是用到动态规划的知识，对于任何一个城市而言，i到j的最短距离不外乎存在经过i与j之间经过k和不经过k两种可能，所以可以令k=1，2，3，\u0026hellip;，n(n是城市的数目)，在检查d(ij)与d(ik)+d(kj)的值；在此d(ik)与d(kj)分别是目前为止所知道的i到k与k到j的最短距离，因此d(ik)+d(kj)就是i到j经过k的最短距离。所以，若有d(ij)\u0026gt;d(ik)+d(kj)，就表示从i出发经过k再到j的距离要比原来的i到j距离短，自然把i到j的d(ij)重写为d(ik)+d(kj)，每当一个k查完了，d(ij)就是目前的i到j的最短距离。重复这一过程，最后当查完所有的k时，d(ij)里面存放的就是i到j之间的最短距离了。 Floyd算法的基本步骤：\n定义n×n的方阵序列D~-1~, D~0~ , … D~n－1~,\n初始化： D~-1~＝C\nD~-1~[i][j]＝边\u0026lt;i,j\u0026gt;的长度，表示初始的从i到j的最短路径长度，即它是从i到j的中间不经过其他中间点的最短路径。 迭代：设D~k-1~已求出，如何得到D~k~（0≤k≤n-1）？\nD~k-1~[i][j]表示从i到j的中间点不大于k-1的最短路径p：i…j， 考虑将顶点k加入路径p得到顶点序列q：i…k…j， 若q不是路径，则当前的最短路径仍是上一步结果：D~k~[i][j]= D~k－1~[i][j]； 否则若q的长度小于p的长度，则用q取代p作为从i到j的最短路径 因为q的两条子路径i…k和k…j皆是中间点不大于k－1的最短路径，所以从i到j中间点不大于k的最短路径长度为：\nD~k~[i][j]＝min{ D~k-1~[i][j], D~k-1~[i][k] + D~k-1~[k][j] }\nFloyd算法实现：\n可以用三个for循环把问题搞定了，但是有一个问题需要注意，那就是for循环的嵌套的顺序：我们可能随手就会写出这样的程序，但是仔细考虑的话，会发现是有问题的。\nfor(int i=0; i\u0026lt;n; i++) for(int j=0; j\u0026lt;n; j++) for(int k=0; k\u0026lt;n; k++)\n问题出在我们太早的把i-k-j的距离确定下来了，假设一旦找到了i-p-j最短的距离后，i到j就相当处理完了，以后不会在改变了，一旦以后有使i到j的更短的距离时也不能再去更新了，所以结果一定是不对的。所以应当象下面一样来写程序：\nfor(int k=0; k\u0026lt;n; k++) for(int i=0; i\u0026lt;n; i++) for(int j=0; j\u0026lt;n; j++)\n这样做的意义在于固定了k，把所有i到j而经过k的距离找出来，然后象开头所提到的那样进行比较和重写，因为k是在最外层的，所以会把所有的i到j都处理完后，才会移动到下一个k，这样就不会有问题了，看来多层循环的时候，我们一定要当心，否则很容易就弄错了。\n路径查找\n接下来就要看一看如何找出最短路径所行经的城市了，这里要用到另一个矩阵P，它的定义是这样的：p(ij)的值如果为p，就表示i到j的最短行经为i-\u0026gt;\u0026hellip;-\u0026gt;p-\u0026gt;j，也就是说p是i到j的最短行径中的j之前的最后一个城市。P矩阵的初值为p(ij)=i。有了这个矩阵之后，要找最短路径就轻而易举了。对于i到j而言找出p(ij)，令为p，就知道了路径i-\u0026gt;\u0026hellip;-\u0026gt;p-\u0026gt;j；再去找p(ip)，如果值为q，i到p的最短路径为i-\u0026gt;\u0026hellip;-\u0026gt;q-\u0026gt;p；再去找p(iq)，如果值为r，i到q的最短路径为i-\u0026gt;\u0026hellip;-\u0026gt;r-\u0026gt;q；所以一再反复，到了某个p(it)的值为i时，就表示i到t的最短路径为i-\u0026gt;t，就会的到答案了，i到j的最短行径为i-\u0026gt;t-\u0026gt;\u0026hellip;-\u0026gt;q-\u0026gt;p-\u0026gt;j。因为上述的算法是从终点到起点的顺序找出来的，所以输出的时候要把它倒过来。 但是，如何动态的回填P矩阵的值呢？回想一下，当d(ij)\u0026gt;d(ik)+d(kj)时，就要让i到j的最短路径改为走i-\u0026gt;\u0026hellip;-\u0026gt;k-\u0026gt;\u0026hellip;-\u0026gt;j这一条路，但是d(kj)的值是已知的，换句话说，就是k-\u0026gt;\u0026hellip;-\u0026gt;j这条路是已知的，所以k-\u0026gt;\u0026hellip;-\u0026gt;j这条路上j的上一个城市(即p(kj))也是已知的，当然，因为要改走i-\u0026gt;\u0026hellip;-\u0026gt;k-\u0026gt;\u0026hellip;-\u0026gt;j这一条路，j的上一个城市正好是p(kj)。所以一旦发现d(ij)\u0026gt;d(ik)+d(kj)，就把p(kj)存入p(ij)。 小例子\n胜利乡有7个村庄(A, B, C, D, E, F, G)各个村庄的距离用边线表示(权) ，比如 A – B 距离 5公里问：如何计算出各村庄到 其它各村庄的最短距离?\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 package algorithm.floyd; /* * 弗洛伊德算法解决最短路径问题 * */ public class floyd { public static void main(String[] args) { // 测试看看图是否创建成功 char[] vertexes = {\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;}; //创建邻接矩阵 int[][] matrix = new int[vertexes.length][vertexes.length]; final int N = 65535; matrix[0] = new int[]{0, 5, 7, N, N, N, 2}; matrix[1] = new int[]{5, 0, N, 9, N, N, 3}; matrix[2] = new int[]{7, N, 0, N, 8, N, N}; matrix[3] = new int[]{N, 9, N, 0, N, 4, N}; matrix[4] = new int[]{N, N, 8, N, 0, 5, 4}; matrix[5] = new int[]{N, N, N, 4, 5, 0, 6}; matrix[6] = new int[]{2, 3, N, N, 4, 6, 0}; graph map = new graph(matrix, vertexes); // map.show(); map.floyd(); map.show(); // map.getPath(map.getPosition(\u0026#39;A\u0026#39;), map.getPosition(\u0026#39;D\u0026#39;), map.pre); map.getAllPath(); } } class graph { char[] vertexes;//顶点集合 int[][] dis;//保存从各个顶点出发到各个顶点的距离 int[][] pre;//保存到达目标顶点的前驱顶点 public int getPosition(char ch) { for (int i = 0; i \u0026lt; this.vertexes.length; i++) { if (this.vertexes[i] == ch) return i; } return -1; } public graph(int[][] matrix, char[] vertexes) { this.vertexes = new char[vertexes.length]; this.dis = new int[vertexes.length][vertexes.length]; this.pre = new int[vertexes.length][vertexes.length]; for (int i = 0; i \u0026lt; vertexes.length; i++) { this.vertexes[i] = vertexes[i]; for (int j = 0; j \u0026lt; vertexes.length; j++) { this.dis[i][j] = matrix[i][j]; this.pre[i][j] = i; } } } public void show() { for (int i = 0; i \u0026lt; this.vertexes.length; i++) { for (int j = 0; j \u0026lt; this.vertexes.length; j++) { System.out.print(this.vertexes[pre[i][j]] + \u0026#34; \u0026#34;); } System.out.println(); for (int j = 0; j \u0026lt; this.vertexes.length; j++) { System.out.print(\u0026#34;(\u0026#34; + this.vertexes[i] + \u0026#34;到\u0026#34; + this.vertexes[j] + \u0026#34;的最短距离\u0026#34; + dis[i][j] + \u0026#34;)\u0026#34;); } System.out.println(); System.out.println(); } } public void floyd() { //求出最短路径长度 //k为中间节点 for (int k = 0; k \u0026lt; this.vertexes.length; k++) { //i为起点 for (int i = 0; i \u0026lt; this.vertexes.length; i++) { //j为终点 for (int j = 0; j \u0026lt; this.vertexes.length; j++) { //遍历所有路径可能，将小的值（也就是更短的路径）进行覆盖 if (this.dis[i][k] + this.dis[k][j] \u0026lt; this.dis[i][j]) { this.dis[i][j] = this.dis[i][k] + this.dis[k][j];//更新dis this.pre[i][j] = this.pre[k][j];//更新前驱顶点 } } } } } public void getAllPath() { System.out.print(\u0026#34;最短路径：\u0026#34;); for (int i = 0; i \u0026lt; this.vertexes.length; i++) { for (int j = 0; j \u0026lt; this.vertexes.length; j++) { getPath(i, j, this.pre); System.out.println(); } } } //求出最短路径的走法 public void getPath(int i, int j, int[][] path) { if (path[i][j] == i) { //不输出相同的点 if (i != j) System.out.print(this.vertexes[i] + \u0026#34; \u0026gt; \u0026#34; + this.vertexes[j]); } else { getPath(i, this.pre[i][j], path); System.out.print(\u0026#34; \u0026gt; \u0026#34; + this.vertexes[j]); } } } 8. 马踏棋盘算法 需求\n将马随机放在国际象棋的Board[0～7][0～7]的某个方格中，马按走棋规则进行移动。，走遍棋盘上全部64个方格。编制非递归程序，求出马的行走路线，并按求出的行走路线，将数字1，2，…，64依次填入一个8×8的方阵，输出之。\n分析\n最基本的应该是深度优先搜索，但是对于一个8×8的棋盘，如果采取暴力搜索，将会耗费很长时间而得不到一个结果，如果采用贪心算法，对路径有目的地筛选，尽量选择出口少的路先走，也就是对当前点的下一个落脚点（可能是8个）进行排序，优先走可走的路最少的那个点，使得走法较好。通俗来讲，就是先预判下一个可能落脚点的出口数，出口数最少的先走掉。\n贪心算法\n贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。 贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择，选择的贪心策略必须具备无后效性，即某个状态以前的过程不会影响以后的状态，只与当前状态有关。 ——百度百科\n代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 package algorithm.horse; import java.awt.Point; import java.util.ArrayList; import java.util.Comparator; public class horse { private static int X; // 棋盘的列数 private static int Y; // 棋盘的行数 //创建一个数组，标记棋盘的各个位置是否被访问过 private static boolean[] visited; //使用一个属性，标记是否棋盘的所有位置都被访问 private static boolean finished; // 如果为true,表示成功 public static void main(String[] args) { System.out.println(\u0026#34;骑士周游算法，开始运行~~\u0026#34;); //测试骑士周游算法是否正确 X = 8; Y = 8; int row = 1; //马儿初始位置的行，从1开始编号 int column = 1; //马儿初始位置的列，从1开始编号 //创建棋盘 int[][] chessboard = new int[X][Y]; visited = new boolean[X * Y];//初始值都是false //测试一下耗时 long start = System.currentTimeMillis(); traversalChessboard(chessboard, row - 1, column - 1, 1); long end = System.currentTimeMillis(); System.out.println(\u0026#34;共耗时: \u0026#34; + (end - start) + \u0026#34; 毫秒\u0026#34;); //输出棋盘的最后情况 for (int[] rows : chessboard) { for (int step : rows) { System.out.print(step + \u0026#34;\\t\u0026#34;); } System.out.println(); } } /** * 完成骑士周游问题的算法 * * @param chessboard 棋盘 * @param row 马儿当前的位置的行 从0开始 * @param column 马儿当前的位置的列 从0开始 * @param step 是第几步 ,初始位置就是第1步 */ public static void traversalChessboard(int[][] chessboard, int row, int column, int step) { chessboard[row][column] = step; //row = 4 X = 8 column = 4 = 4 * 8 + 4 = 36 visited[row * X + column] = true; //标记该位置已经访问 //获取当前位置可以走的下一个位置的集合 ArrayList\u0026lt;Point\u0026gt; ps = next(new Point(row, column)); //贪心策略优化：对ps进行排序,排序的规则就是对ps的所有的Point对象的下一步的位置的数目，进行非递减排序 sort(ps); //遍历 ps while (!ps.isEmpty()) { Point p = ps.remove(0);//取出下一个可以走的位置 //判断该点是否已经访问过 if (!visited[p.x * X + p.y]) {//说明还没有访问过 traversalChessboard(chessboard, p.x, p.y, step + 1); } } //判断马儿是否完成了任务，使用 step 和应该走的步数比较 ， //如果没有达到数量，则表示没有完成任务，将整个棋盘置0 //说明: step \u0026lt; X * Y 成立的情况有两种 //1. 棋盘到目前位置,仍然没有走完 //2. 棋盘处于一个回溯过程 if (step \u0026lt; X * Y \u0026amp;\u0026amp; !finished) { chessboard[row][column] = 0; visited[row * X + column] = false; } else { finished = true; } } /** * 功能： 根据当前位置(Point对象)，计算马儿还能走哪些位置(Point)，并放入到一个集合中(ArrayList), 最多有8个位置 * * @param curPoint * @return */ public static ArrayList\u0026lt;Point\u0026gt; next(Point curPoint) { //创建一个ArrayList ArrayList\u0026lt;Point\u0026gt; ps = new ArrayList\u0026lt;\u0026gt;(); //创建一个Point Point p1 = new Point(); //表示马儿可以走5这个位置 if ((p1.x = curPoint.x - 2) \u0026gt;= 0 \u0026amp;\u0026amp; (p1.y = curPoint.y - 1) \u0026gt;= 0) { ps.add(new Point(p1)); } //判断马儿可以走6这个位置 if ((p1.x = curPoint.x - 1) \u0026gt;= 0 \u0026amp;\u0026amp; (p1.y = curPoint.y - 2) \u0026gt;= 0) { ps.add(new Point(p1)); } //判断马儿可以走7这个位置 if ((p1.x = curPoint.x + 1) \u0026lt; X \u0026amp;\u0026amp; (p1.y = curPoint.y - 2) \u0026gt;= 0) { ps.add(new Point(p1)); } //判断马儿可以走0这个位置 if ((p1.x = curPoint.x + 2) \u0026lt; X \u0026amp;\u0026amp; (p1.y = curPoint.y - 1) \u0026gt;= 0) { ps.add(new Point(p1)); } //判断马儿可以走1这个位置 if ((p1.x = curPoint.x + 2) \u0026lt; X \u0026amp;\u0026amp; (p1.y = curPoint.y + 1) \u0026lt; Y) { ps.add(new Point(p1)); } //判断马儿可以走2这个位置 if ((p1.x = curPoint.x + 1) \u0026lt; X \u0026amp;\u0026amp; (p1.y = curPoint.y + 2) \u0026lt; Y) { ps.add(new Point(p1)); } //判断马儿可以走3这个位置 if ((p1.x = curPoint.x - 1) \u0026gt;= 0 \u0026amp;\u0026amp; (p1.y = curPoint.y + 2) \u0026lt; Y) { ps.add(new Point(p1)); } //判断马儿可以走4这个位置 if ((p1.x = curPoint.x - 2) \u0026gt;= 0 \u0026amp;\u0026amp; (p1.y = curPoint.y + 1) \u0026lt; Y) { ps.add(new Point(p1)); } return ps; } //根据当前这个一步的所有的下一步的选择位置，进行非递减排序, 减少回溯的次数 public static void sort(ArrayList\u0026lt;Point\u0026gt; ps) { ps.sort(new Comparator\u0026lt;Point\u0026gt;() { @Override public int compare(Point o1, Point o2) { // TODO Auto-generated method stub //获取到o1的下一步的所有位置个数 int count1 = next(o1).size(); //获取到o2的下一步的所有位置个数 int count2 = next(o2).size(); if (count1 \u0026lt; count2) { return -1; } else if (count1 == count2) { return 0; } else { return 1; } } }); } } ","permalink":"https://cold-bin.github.io/post/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/","tags":["算法","排序算法","查找算法","树","链表","队列","稀疏数组","栈","dfs","bfs","图","动态规划"],"title":"数据结构与算法（java版）"},{"categories":null,"contents":"notes 学习笔记托管\n","permalink":"https://cold-bin.github.io/post/readme/","tags":null,"title":""}]